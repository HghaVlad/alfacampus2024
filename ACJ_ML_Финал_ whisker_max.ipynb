{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Welcome to *whisker max* solution\n",
        "##### Импорт библиотек и чтение исходного датасета"
      ],
      "metadata": {
        "collapsed": false,
        "id": "sYXIzzjglLVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install catboost optuna xgboost lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjSNzmtFmGdo",
        "outputId": "62032461-7979-4869-cab8-c456000f2755"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:18.739810Z",
          "end_time": "2024-05-14T18:10:19.445621Z"
        },
        "id": "j9e2qWcWlLVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"dataset.csv\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:19.446352Z",
          "end_time": "2024-05-14T18:10:19.494631Z"
        },
        "id": "PaHqwMGllLVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 130818 entries, 0 to 130817\n",
            "Data columns (total 5 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   clientbankpartner_pin   130818 non-null  int64 \n",
            " 1   client_pin              130818 non-null  int64 \n",
            " 2   partner_src_type_ccode  130818 non-null  int64 \n",
            " 3   client_start_date       130818 non-null  object\n",
            " 4   partnerrolestart_date   130818 non-null  object\n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 5.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:19.496671Z",
          "end_time": "2024-05-14T18:10:19.509525Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZazOKps-lLVW",
        "outputId": "876e298f-af74-43f5-959f-32271efd9e96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       clientbankpartner_pin     client_pin  partner_src_type_ccode  \\\n",
              "count          130818.000000  130818.000000           130818.000000   \n",
              "mean           126038.565006  185136.559197                3.637290   \n",
              "min                 1.000000    5579.000000                0.000000   \n",
              "25%             10093.000000   71995.750000                4.000000   \n",
              "50%            125573.000000  205454.500000                4.000000   \n",
              "75%            191599.000000  272905.500000                4.000000   \n",
              "max            333515.000000  333513.000000                5.000000   \n",
              "std             93263.794349  101297.060675                1.328811   \n",
              "\n",
              "                   client_start_date          partnerrolestart_date  \n",
              "count                         130818                         130818  \n",
              "mean   2020-04-02 21:34:45.896435712  2019-09-17 20:55:53.153235712  \n",
              "min              2019-03-01 00:00:00            2018-12-01 00:00:00  \n",
              "25%              2019-12-14 00:00:00            2019-05-01 00:00:00  \n",
              "50%              2020-04-26 00:00:00            2019-09-01 00:00:00  \n",
              "75%              2020-08-11 00:00:00            2020-02-01 00:00:00  \n",
              "max              2020-11-30 00:00:00            2020-11-01 00:00:00  \n",
              "std                              NaN                            NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7aaa63c2-ab0c-4974-9d98-7af50606b6ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientbankpartner_pin</th>\n",
              "      <th>client_pin</th>\n",
              "      <th>partner_src_type_ccode</th>\n",
              "      <th>client_start_date</th>\n",
              "      <th>partnerrolestart_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>130818.000000</td>\n",
              "      <td>130818.000000</td>\n",
              "      <td>130818.000000</td>\n",
              "      <td>130818</td>\n",
              "      <td>130818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>126038.565006</td>\n",
              "      <td>185136.559197</td>\n",
              "      <td>3.637290</td>\n",
              "      <td>2020-04-02 21:34:45.896435712</td>\n",
              "      <td>2019-09-17 20:55:53.153235712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>5579.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2019-03-01 00:00:00</td>\n",
              "      <td>2018-12-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10093.000000</td>\n",
              "      <td>71995.750000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2019-12-14 00:00:00</td>\n",
              "      <td>2019-05-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>125573.000000</td>\n",
              "      <td>205454.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2020-04-26 00:00:00</td>\n",
              "      <td>2019-09-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>191599.000000</td>\n",
              "      <td>272905.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2020-08-11 00:00:00</td>\n",
              "      <td>2020-02-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>333515.000000</td>\n",
              "      <td>333513.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2020-11-30 00:00:00</td>\n",
              "      <td>2020-11-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>93263.794349</td>\n",
              "      <td>101297.060675</td>\n",
              "      <td>1.328811</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7aaa63c2-ab0c-4974-9d98-7af50606b6ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7aaa63c2-ab0c-4974-9d98-7af50606b6ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7aaa63c2-ab0c-4974-9d98-7af50606b6ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2546d167-762f-4b6b-9b23-9139b8236e96\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2546d167-762f-4b6b-9b23-9139b8236e96')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2546d167-762f-4b6b-9b23-9139b8236e96 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"clientbankpartner_pin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 105403.90921250699,\n        \"min\": 1.0,\n        \"max\": 333515.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          126038.5650063447,\n          191599.0,\n          130818.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"client_pin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 107803.7116391909,\n        \"min\": 5579.0,\n        \"max\": 333513.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          185136.55919674662,\n          272905.5,\n          130818.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partner_src_type_ccode\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 46250.038024416455,\n        \"min\": 0.0,\n        \"max\": 130818.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          130818.0,\n          3.6372899753856505,\n          1.3288113910747796\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"client_start_date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000130818\",\n        \"max\": \"2020-11-30 00:00:00\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"130818\",\n          \"2020-04-02 21:34:45.896435712\",\n          \"2020-08-11 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"partnerrolestart_date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000130818\",\n        \"max\": \"2020-11-01 00:00:00\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"130818\",\n          \"2019-09-17 20:55:53.153235712\",\n          \"2020-02-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df[\"client_start_date\"] = pd.to_datetime(df[\"client_start_date\"])\n",
        "df[\"partnerrolestart_date\"] = pd.to_datetime(df[\"partnerrolestart_date\"])\n",
        "df.describe()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:19.513824Z",
          "end_time": "2024-05-14T18:10:19.555887Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vIuWbK29lLVW",
        "outputId": "f8b0dfef-7301-4677-89a1-8df67053df0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "# Тестовая выборка\n",
        "test_date = pd.to_datetime(\"2020-09-01\")\n",
        "test_partner_pins = df[df[\"client_start_date\"] >= test_date][\"clientbankpartner_pin\"].unique()"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:19.557724Z",
          "end_time": "2024-05-14T18:10:19.566400Z"
        },
        "id": "p_oLkWlnlLVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучающий датасет\n",
        "\n",
        "**Итоговые признаки:**\n",
        "* Общее кол-во привлечений за все время\n",
        "* Кол-во привлечений в среднем в месяц/неделю\n",
        "* Сколько дней проходит между первым-вторым, предпоследним-последним привлечением, медиана от всех разниц\n",
        "* Сколько дней прошло с последнего/медианного привлечения\n",
        "* Сколько дней партнер всего привлекает людей\n",
        "* Сколько людей привлекли за последние 30/60/90/180/270/365 дней"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1h4Oa2HelLVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "collapsed": false,
        "id": "GI-vduuMlLVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def days_between_invites(days: list):\n",
        "    if len(days) == 1:\n",
        "        return -100,-100,-100,-100,-100\n",
        "    diff_days = []\n",
        "    for i in range(1, len(days)):\n",
        "        diff_days.append((days[i] - days[i-1]).days)\n",
        "    if len(diff_days) == 1:\n",
        "        return diff_days[0], -100, sum(diff_days) / len(days), diff_days[-1], -100\n",
        "    return diff_days[0], diff_days[1], sum(diff_days) / len(days), diff_days[-1], diff_days[-2]\n",
        "\n",
        "def clients_invited_last_n_days(days, end_date):\n",
        "      return lambda x: sum((end_date - d).days <= days for d in x)\n",
        "\n",
        "def make_dataset(input_df:pd.DataFrame, end_date, is_train=False):\n",
        "    input_df = input_df.copy()\n",
        "    output_df = input_df.groupby(\"clientbankpartner_pin\").agg({\"client_pin\": \"count\", \"partnerrolestart_date\": \"min\", \"client_start_date\": [\"min\", \"max\", \"median\", lambda x: sorted(list(x))]})\n",
        "\n",
        "    # Кол-во привлечений в неделю/месяц\n",
        "    output_df[\"average_month_invites\"] = output_df[\"client_pin\"][\"count\"]/ output_df[\"client_start_date\"][\"<lambda_0>\"].agg(lambda x: len(set(map(lambda y: y.month, x))) )\n",
        "    output_df[\"average_week_invites\"] = output_df[\"client_pin\"][\"count\"]/ output_df[\"client_start_date\"][\"<lambda_0>\"].agg(lambda x: len(set(map(lambda y: y.week, x))) )\n",
        "    # Сколько дней проходит между первым-вторым, предпоследним-последним привлечением\n",
        "    output_df[[\"diff_day_first\", \"diff_day_second\", \"diff_days_mean\", \"diff_days_last\", \"diff_days_prelast\"]] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(days_between_invites).tolist()\n",
        "    # Сколько дней прошло с последнего/медианного привлечения\n",
        "    output_df[\"diff_test_date_and_last_invite\"] = end_date - output_df[\"client_start_date\"][\"max\"]\n",
        "    output_df[\"end_date_clientstart_median\"] = end_date - output_df[\"client_start_date\"][\"median\"]\n",
        "    # Сколько дней партнер всего привлекает людей\n",
        "    output_df[\"end_date_clientstart_max-min\"] = output_df[\"client_start_date\"][\"max\"] - output_df[\"client_start_date\"][\"min\"]\n",
        "    output_df[\"difference_median_and_max-min\"] = output_df[\"end_date_clientstart_max-min\"] - output_df[\"end_date_clientstart_median\"]\n",
        "\n",
        "    # Перевод дат в целые числа\n",
        "    output_df[\"diff_first_lastdt\"] = output_df[\"diff_day_first\"] - output_df[\"diff_days_last\"]\n",
        "    output_df[\"diff_between_end_date_and_first_client\"] = end_date - output_df[\"client_start_date\"][\"min\"]\n",
        "    output_df[\"diff_test_date_and_last_invite\"] =output_df[\"diff_test_date_and_last_invite\"].dt.days\n",
        "    output_df[\"end_date_clientstart_median\"] =output_df[\"end_date_clientstart_median\"].dt.days\n",
        "    output_df[\"end_date_clientstart_max-min\"] =output_df[\"end_date_clientstart_max-min\"].dt.days\n",
        "    output_df[\"difference_median_and_max-min\"] =output_df[\"difference_median_and_max-min\"].dt.days\n",
        "    output_df[\"diff_between_end_date_and_first_client\"] = output_df[\"diff_between_end_date_and_first_client\"].dt.days\n",
        "\n",
        "    # Сколько людей привлекли за последние 30/60/90/180/270/365 дней\n",
        "    output_df[\"clients_per_day\"] = output_df[\"diff_between_end_date_and_first_client\"] / output_df[\"client_pin\"][\"count\"]\n",
        "    output_df[\"clients_invited_last_30_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(30, end_date))\n",
        "    output_df[\"clients_invited_last_60_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(60, end_date))\n",
        "    output_df[\"clients_invited_last_90_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(90, end_date))\n",
        "    output_df[\"clients_invited_last_180_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(180, end_date))\n",
        "    output_df[\"clients_invited_last_270_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(270, end_date))\n",
        "    output_df[\"clients_invited_last_365_days\"] = output_df[\"client_start_date\"][\"<lambda_0>\"].apply(clients_invited_last_n_days(365, end_date))\n",
        "\n",
        "    output_df = output_df.drop(columns=[(\"client_start_date\", \"<lambda_0>\"), (\"partnerrolestart_date\", \"min\"), (\"client_start_date\", \"min\"), (\"client_start_date\", \"max\"), (\"client_start_date\", \"median\"), ('client_pin','count')])\n",
        "\n",
        "    if is_train:\n",
        "        output_df[\"churn\"] = list(map(lambda x: int(x in test_partner_pins), output_df.index.values))\n",
        "    return output_df\n",
        "\n",
        "train_dataset = make_dataset(df[df[\"client_start_date\"] < test_date], pd.to_datetime(\"2020-09-01\"), True)\n",
        "test_dataset = make_dataset(df[df[\"clientbankpartner_pin\"].isin(test_partner_pins)], pd.to_datetime(\"2020-12-01\"))"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:19.570886Z",
          "end_time": "2024-05-14T18:10:21.905559Z"
        },
        "id": "lxkrZWTplLVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      average_month_invites average_week_invites  \\\n",
              "                                                                   \n",
              "clientbankpartner_pin                                              \n",
              "1                                      1.00                  1.0   \n",
              "2                                      1.75                  1.4   \n",
              "5                                      1.00                  1.0   \n",
              "6                                      1.00                  1.0   \n",
              "9                                      1.00                  1.0   \n",
              "...                                     ...                  ...   \n",
              "333400                                 1.00                  1.0   \n",
              "333459                                 1.00                  1.0   \n",
              "333492                                 1.00                  1.0   \n",
              "333511                                 4.00                  2.0   \n",
              "333515                                 1.00                  1.0   \n",
              "\n",
              "                      diff_day_first diff_day_second diff_days_mean  \\\n",
              "                                                                      \n",
              "clientbankpartner_pin                                                 \n",
              "1                              187.0             7.0      64.666667   \n",
              "2                                2.0            26.0      58.714286   \n",
              "5                               17.0            32.0      31.400000   \n",
              "6                             -100.0          -100.0    -100.000000   \n",
              "9                               23.0          -100.0      11.500000   \n",
              "...                              ...             ...            ...   \n",
              "333400                        -100.0          -100.0    -100.000000   \n",
              "333459                        -100.0          -100.0    -100.000000   \n",
              "333492                        -100.0          -100.0    -100.000000   \n",
              "333511                           0.0             2.0       2.125000   \n",
              "333515                        -100.0          -100.0    -100.000000   \n",
              "\n",
              "                      diff_days_last diff_days_prelast  \\\n",
              "                                                         \n",
              "clientbankpartner_pin                                    \n",
              "1                                7.0             187.0   \n",
              "2                               22.0             114.0   \n",
              "5                               30.0              78.0   \n",
              "6                             -100.0            -100.0   \n",
              "9                               23.0            -100.0   \n",
              "...                              ...               ...   \n",
              "333400                        -100.0            -100.0   \n",
              "333459                        -100.0            -100.0   \n",
              "333492                        -100.0            -100.0   \n",
              "333511                           1.0               6.0   \n",
              "333515                        -100.0            -100.0   \n",
              "\n",
              "                      diff_test_date_and_last_invite  \\\n",
              "                                                       \n",
              "clientbankpartner_pin                                  \n",
              "1                                                 88   \n",
              "2                                                 63   \n",
              "5                                                225   \n",
              "6                                                337   \n",
              "9                                                224   \n",
              "...                                              ...   \n",
              "333400                                            33   \n",
              "333459                                            29   \n",
              "333492                                            12   \n",
              "333511                                            22   \n",
              "333515                                           324   \n",
              "\n",
              "                      end_date_clientstart_median  \\\n",
              "                                                    \n",
              "clientbankpartner_pin                               \n",
              "1                                              95   \n",
              "2                                             279   \n",
              "5                                             333   \n",
              "6                                             337   \n",
              "9                                             235   \n",
              "...                                           ...   \n",
              "333400                                         33   \n",
              "333459                                         29   \n",
              "333492                                         12   \n",
              "333511                                         33   \n",
              "333515                                        324   \n",
              "\n",
              "                      end_date_clientstart_max-min  ... diff_first_lastdt  \\\n",
              "                                                    ...                     \n",
              "clientbankpartner_pin                               ...                     \n",
              "1                                              194  ...             180.0   \n",
              "2                                              411  ...             -20.0   \n",
              "5                                              157  ...             -13.0   \n",
              "6                                                0  ...               0.0   \n",
              "9                                               23  ...               0.0   \n",
              "...                                            ...  ...               ...   \n",
              "333400                                           0  ...               0.0   \n",
              "333459                                           0  ...               0.0   \n",
              "333492                                           0  ...               0.0   \n",
              "333511                                          17  ...              -1.0   \n",
              "333515                                           0  ...               0.0   \n",
              "\n",
              "                      diff_between_end_date_and_first_client clients_per_day  \\\n",
              "                                                                               \n",
              "clientbankpartner_pin                                                          \n",
              "1                                                        282       94.000000   \n",
              "2                                                        474       67.714286   \n",
              "5                                                        382       76.400000   \n",
              "6                                                        337      337.000000   \n",
              "9                                                        247      123.500000   \n",
              "...                                                      ...             ...   \n",
              "333400                                                    33       33.000000   \n",
              "333459                                                    29       29.000000   \n",
              "333492                                                    12       12.000000   \n",
              "333511                                                    39        4.875000   \n",
              "333515                                                   324      324.000000   \n",
              "\n",
              "                      clients_invited_last_30_days  \\\n",
              "                                                     \n",
              "clientbankpartner_pin                                \n",
              "1                                                0   \n",
              "2                                                0   \n",
              "5                                                0   \n",
              "6                                                0   \n",
              "9                                                0   \n",
              "...                                            ...   \n",
              "333400                                           0   \n",
              "333459                                           1   \n",
              "333492                                           1   \n",
              "333511                                           3   \n",
              "333515                                           0   \n",
              "\n",
              "                      clients_invited_last_60_days  \\\n",
              "                                                     \n",
              "clientbankpartner_pin                                \n",
              "1                                                0   \n",
              "2                                                0   \n",
              "5                                                0   \n",
              "6                                                0   \n",
              "9                                                0   \n",
              "...                                            ...   \n",
              "333400                                           1   \n",
              "333459                                           1   \n",
              "333492                                           1   \n",
              "333511                                           8   \n",
              "333515                                           0   \n",
              "\n",
              "                      clients_invited_last_90_days  \\\n",
              "                                                     \n",
              "clientbankpartner_pin                                \n",
              "1                                                1   \n",
              "2                                                2   \n",
              "5                                                0   \n",
              "6                                                0   \n",
              "9                                                0   \n",
              "...                                            ...   \n",
              "333400                                           1   \n",
              "333459                                           1   \n",
              "333492                                           1   \n",
              "333511                                           8   \n",
              "333515                                           0   \n",
              "\n",
              "                      clients_invited_last_180_days  \\\n",
              "                                                      \n",
              "clientbankpartner_pin                                 \n",
              "1                                                 2   \n",
              "2                                                 2   \n",
              "5                                                 0   \n",
              "6                                                 0   \n",
              "9                                                 0   \n",
              "...                                             ...   \n",
              "333400                                            1   \n",
              "333459                                            1   \n",
              "333492                                            1   \n",
              "333511                                            8   \n",
              "333515                                            0   \n",
              "\n",
              "                      clients_invited_last_270_days  \\\n",
              "                                                      \n",
              "clientbankpartner_pin                                 \n",
              "1                                                 2   \n",
              "2                                                 3   \n",
              "5                                                 2   \n",
              "6                                                 0   \n",
              "9                                                 2   \n",
              "...                                             ...   \n",
              "333400                                            1   \n",
              "333459                                            1   \n",
              "333492                                            1   \n",
              "333511                                            8   \n",
              "333515                                            0   \n",
              "\n",
              "                      clients_invited_last_365_days churn  \n",
              "                                                           \n",
              "clientbankpartner_pin                                      \n",
              "1                                                 3     1  \n",
              "2                                                 4     1  \n",
              "5                                                 4     0  \n",
              "6                                                 1     0  \n",
              "9                                                 2     0  \n",
              "...                                             ...   ...  \n",
              "333400                                            1     1  \n",
              "333459                                            1     1  \n",
              "333492                                            1     1  \n",
              "333511                                            8     0  \n",
              "333515                                            1     0  \n",
              "\n",
              "[7887 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-438a775a-5778-44af-a754-c94c0b7edb5b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>average_month_invites</th>\n",
              "      <th>average_week_invites</th>\n",
              "      <th>diff_day_first</th>\n",
              "      <th>diff_day_second</th>\n",
              "      <th>diff_days_mean</th>\n",
              "      <th>diff_days_last</th>\n",
              "      <th>diff_days_prelast</th>\n",
              "      <th>diff_test_date_and_last_invite</th>\n",
              "      <th>end_date_clientstart_median</th>\n",
              "      <th>end_date_clientstart_max-min</th>\n",
              "      <th>...</th>\n",
              "      <th>diff_first_lastdt</th>\n",
              "      <th>diff_between_end_date_and_first_client</th>\n",
              "      <th>clients_per_day</th>\n",
              "      <th>clients_invited_last_30_days</th>\n",
              "      <th>clients_invited_last_60_days</th>\n",
              "      <th>clients_invited_last_90_days</th>\n",
              "      <th>clients_invited_last_180_days</th>\n",
              "      <th>clients_invited_last_270_days</th>\n",
              "      <th>clients_invited_last_365_days</th>\n",
              "      <th>churn</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>...</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clientbankpartner_pin</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>64.666667</td>\n",
              "      <td>7.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>88</td>\n",
              "      <td>95</td>\n",
              "      <td>194</td>\n",
              "      <td>...</td>\n",
              "      <td>180.0</td>\n",
              "      <td>282</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.75</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>58.714286</td>\n",
              "      <td>22.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>63</td>\n",
              "      <td>279</td>\n",
              "      <td>411</td>\n",
              "      <td>...</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>474</td>\n",
              "      <td>67.714286</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>31.400000</td>\n",
              "      <td>30.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>225</td>\n",
              "      <td>333</td>\n",
              "      <td>157</td>\n",
              "      <td>...</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>382</td>\n",
              "      <td>76.400000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>337</td>\n",
              "      <td>337</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>337</td>\n",
              "      <td>337.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>11.500000</td>\n",
              "      <td>23.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>224</td>\n",
              "      <td>235</td>\n",
              "      <td>23</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247</td>\n",
              "      <td>123.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333400</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333459</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333492</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333511</th>\n",
              "      <td>4.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.125000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>22</td>\n",
              "      <td>33</td>\n",
              "      <td>17</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>39</td>\n",
              "      <td>4.875000</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333515</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.000000</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>-100.0</td>\n",
              "      <td>324</td>\n",
              "      <td>324</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>324</td>\n",
              "      <td>324.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7887 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-438a775a-5778-44af-a754-c94c0b7edb5b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-438a775a-5778-44af-a754-c94c0b7edb5b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-438a775a-5778-44af-a754-c94c0b7edb5b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-05c55fbc-ebe7-4333-b96e-b4cea995f19b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05c55fbc-ebe7-4333-b96e-b4cea995f19b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-05c55fbc-ebe7-4333-b96e-b4cea995f19b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_dataset"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:32:11.522467Z",
          "end_time": "2024-05-13T21:32:11.541389Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "phjOgRiSlLVY",
        "outputId": "4e6a759f-3851-476b-a2d3-2b9c442e5ff6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Простые модели"
      ],
      "metadata": {
        "collapsed": false,
        "id": "SbptLMx2lLVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:29.762891Z",
          "end_time": "2024-05-14T18:10:31.052515Z"
        },
        "id": "k7YHT9S1lLVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train_dataset.drop([\"churn\"], axis=1), train_dataset['churn'], random_state=20)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:31.024752Z",
          "end_time": "2024-05-14T18:10:31.054902Z"
        },
        "id": "vuIUuLVZlLVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": [
        "def estimate_model(my_model):\n",
        "    pred = my_model.predict(x_test)\n",
        "    rmse = (np.sqrt(mean_squared_error(y_test, pred)))\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    roc_auc = roc_auc_score(y_test, pred)\n",
        "    score = my_model.score(x_test, y_test)\n",
        "    local_score = my_model.score(x_train, y_train)\n",
        "    print(\"Testing performance\")\n",
        "    print(\"RMSE: {:.3f}\".format(rmse))\n",
        "    print(\"R2: {:.3f}\".format(r2))\n",
        "    print(\"ROC AUC: {:.5f}\".format(roc_auc))\n",
        "    print(\"Score: {:.4f}\".format(score))\n",
        "    print(\"Local Score: {:.4f}\".format(local_score))\n",
        "\n",
        "    print(\"Best params: \", my_model.get_params())"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:31.696321Z",
          "end_time": "2024-05-14T18:10:31.761009Z"
        },
        "id": "aJKe4VbllLVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Catboost"
      ],
      "metadata": {
        "collapsed": false,
        "id": "vZJoCp4klLVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:33.671766Z",
          "end_time": "2024-05-14T18:10:33.834159Z"
        },
        "id": "GHWHfroslLVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.022007\n",
            "0:\tlearn: 0.6815034\ttotal: 66.1ms\tremaining: 1m 6s\n",
            "1:\tlearn: 0.6715729\ttotal: 79.1ms\tremaining: 39.5s\n",
            "2:\tlearn: 0.6607198\ttotal: 90.8ms\tremaining: 30.2s\n",
            "3:\tlearn: 0.6505033\ttotal: 104ms\tremaining: 25.8s\n",
            "4:\tlearn: 0.6404601\ttotal: 129ms\tremaining: 25.7s\n",
            "5:\tlearn: 0.6313640\ttotal: 144ms\tremaining: 23.9s\n",
            "6:\tlearn: 0.6228389\ttotal: 158ms\tremaining: 22.5s\n",
            "7:\tlearn: 0.6143779\ttotal: 175ms\tremaining: 21.7s\n",
            "8:\tlearn: 0.6071310\ttotal: 190ms\tremaining: 20.9s\n",
            "9:\tlearn: 0.6002430\ttotal: 213ms\tremaining: 21.1s\n",
            "10:\tlearn: 0.5932430\ttotal: 229ms\tremaining: 20.6s\n",
            "11:\tlearn: 0.5862178\ttotal: 255ms\tremaining: 21s\n",
            "12:\tlearn: 0.5801465\ttotal: 280ms\tremaining: 21.3s\n",
            "13:\tlearn: 0.5738401\ttotal: 299ms\tremaining: 21.1s\n",
            "14:\tlearn: 0.5687066\ttotal: 329ms\tremaining: 21.6s\n",
            "15:\tlearn: 0.5622294\ttotal: 352ms\tremaining: 21.7s\n",
            "16:\tlearn: 0.5567883\ttotal: 368ms\tremaining: 21.3s\n",
            "17:\tlearn: 0.5516727\ttotal: 393ms\tremaining: 21.5s\n",
            "18:\tlearn: 0.5467433\ttotal: 438ms\tremaining: 22.6s\n",
            "19:\tlearn: 0.5418891\ttotal: 451ms\tremaining: 22.1s\n",
            "20:\tlearn: 0.5375321\ttotal: 478ms\tremaining: 22.3s\n",
            "21:\tlearn: 0.5334536\ttotal: 494ms\tremaining: 22s\n",
            "22:\tlearn: 0.5294180\ttotal: 527ms\tremaining: 22.4s\n",
            "23:\tlearn: 0.5253874\ttotal: 543ms\tremaining: 22.1s\n",
            "24:\tlearn: 0.5218484\ttotal: 560ms\tremaining: 21.8s\n",
            "25:\tlearn: 0.5184720\ttotal: 573ms\tremaining: 21.5s\n",
            "26:\tlearn: 0.5149021\ttotal: 588ms\tremaining: 21.2s\n",
            "27:\tlearn: 0.5117210\ttotal: 603ms\tremaining: 20.9s\n",
            "28:\tlearn: 0.5087854\ttotal: 614ms\tremaining: 20.6s\n",
            "29:\tlearn: 0.5057332\ttotal: 621ms\tremaining: 20.1s\n",
            "30:\tlearn: 0.5029002\ttotal: 642ms\tremaining: 20.1s\n",
            "31:\tlearn: 0.5001570\ttotal: 657ms\tremaining: 19.9s\n",
            "32:\tlearn: 0.4978225\ttotal: 701ms\tremaining: 20.5s\n",
            "33:\tlearn: 0.4952280\ttotal: 721ms\tremaining: 20.5s\n",
            "34:\tlearn: 0.4927151\ttotal: 733ms\tremaining: 20.2s\n",
            "35:\tlearn: 0.4905448\ttotal: 746ms\tremaining: 20s\n",
            "36:\tlearn: 0.4880342\ttotal: 758ms\tremaining: 19.7s\n",
            "37:\tlearn: 0.4863048\ttotal: 775ms\tremaining: 19.6s\n",
            "38:\tlearn: 0.4840869\ttotal: 783ms\tremaining: 19.3s\n",
            "39:\tlearn: 0.4823248\ttotal: 790ms\tremaining: 19s\n",
            "40:\tlearn: 0.4806877\ttotal: 803ms\tremaining: 18.8s\n",
            "41:\tlearn: 0.4785445\ttotal: 816ms\tremaining: 18.6s\n",
            "42:\tlearn: 0.4767627\ttotal: 832ms\tremaining: 18.5s\n",
            "43:\tlearn: 0.4754186\ttotal: 858ms\tremaining: 18.6s\n",
            "44:\tlearn: 0.4739345\ttotal: 881ms\tremaining: 18.7s\n",
            "45:\tlearn: 0.4727271\ttotal: 897ms\tremaining: 18.6s\n",
            "46:\tlearn: 0.4714250\ttotal: 910ms\tremaining: 18.5s\n",
            "47:\tlearn: 0.4699654\ttotal: 928ms\tremaining: 18.4s\n",
            "48:\tlearn: 0.4687649\ttotal: 945ms\tremaining: 18.3s\n",
            "49:\tlearn: 0.4675323\ttotal: 959ms\tremaining: 18.2s\n",
            "50:\tlearn: 0.4663132\ttotal: 978ms\tremaining: 18.2s\n",
            "51:\tlearn: 0.4651543\ttotal: 996ms\tremaining: 18.2s\n",
            "52:\tlearn: 0.4640217\ttotal: 1.02s\tremaining: 18.3s\n",
            "53:\tlearn: 0.4628560\ttotal: 1.07s\tremaining: 18.7s\n",
            "54:\tlearn: 0.4620251\ttotal: 1.09s\tremaining: 18.7s\n",
            "55:\tlearn: 0.4610521\ttotal: 1.11s\tremaining: 18.7s\n",
            "56:\tlearn: 0.4601361\ttotal: 1.12s\tremaining: 18.5s\n",
            "57:\tlearn: 0.4590089\ttotal: 1.13s\tremaining: 18.4s\n",
            "58:\tlearn: 0.4580924\ttotal: 1.15s\tremaining: 18.4s\n",
            "59:\tlearn: 0.4573450\ttotal: 1.17s\tremaining: 18.3s\n",
            "60:\tlearn: 0.4565714\ttotal: 1.18s\tremaining: 18.1s\n",
            "61:\tlearn: 0.4558715\ttotal: 1.2s\tremaining: 18.1s\n",
            "62:\tlearn: 0.4549656\ttotal: 1.21s\tremaining: 18s\n",
            "63:\tlearn: 0.4543439\ttotal: 1.23s\tremaining: 18s\n",
            "64:\tlearn: 0.4535071\ttotal: 1.24s\tremaining: 17.9s\n",
            "65:\tlearn: 0.4528608\ttotal: 1.26s\tremaining: 17.8s\n",
            "66:\tlearn: 0.4521502\ttotal: 1.28s\tremaining: 17.9s\n",
            "67:\tlearn: 0.4515025\ttotal: 1.3s\tremaining: 17.8s\n",
            "68:\tlearn: 0.4509174\ttotal: 1.31s\tremaining: 17.7s\n",
            "69:\tlearn: 0.4502442\ttotal: 1.33s\tremaining: 17.7s\n",
            "70:\tlearn: 0.4496826\ttotal: 1.35s\tremaining: 17.6s\n",
            "71:\tlearn: 0.4492533\ttotal: 1.36s\tremaining: 17.5s\n",
            "72:\tlearn: 0.4488387\ttotal: 1.38s\tremaining: 17.5s\n",
            "73:\tlearn: 0.4481414\ttotal: 1.4s\tremaining: 17.5s\n",
            "74:\tlearn: 0.4474243\ttotal: 1.4s\tremaining: 17.3s\n",
            "75:\tlearn: 0.4467206\ttotal: 1.41s\tremaining: 17.2s\n",
            "76:\tlearn: 0.4462525\ttotal: 1.42s\tremaining: 17s\n",
            "77:\tlearn: 0.4456163\ttotal: 1.43s\tremaining: 16.9s\n",
            "78:\tlearn: 0.4452743\ttotal: 1.44s\tremaining: 16.7s\n",
            "79:\tlearn: 0.4448519\ttotal: 1.45s\tremaining: 16.6s\n",
            "80:\tlearn: 0.4444693\ttotal: 1.46s\tremaining: 16.5s\n",
            "81:\tlearn: 0.4438851\ttotal: 1.46s\tremaining: 16.4s\n",
            "82:\tlearn: 0.4434851\ttotal: 1.47s\tremaining: 16.3s\n",
            "83:\tlearn: 0.4428876\ttotal: 1.48s\tremaining: 16.2s\n",
            "84:\tlearn: 0.4424478\ttotal: 1.49s\tremaining: 16s\n",
            "85:\tlearn: 0.4420363\ttotal: 1.5s\tremaining: 15.9s\n",
            "86:\tlearn: 0.4415207\ttotal: 1.5s\tremaining: 15.8s\n",
            "87:\tlearn: 0.4410883\ttotal: 1.51s\tremaining: 15.7s\n",
            "88:\tlearn: 0.4406099\ttotal: 1.52s\tremaining: 15.6s\n",
            "89:\tlearn: 0.4402406\ttotal: 1.53s\tremaining: 15.4s\n",
            "90:\tlearn: 0.4397562\ttotal: 1.55s\tremaining: 15.5s\n",
            "91:\tlearn: 0.4395031\ttotal: 1.57s\tremaining: 15.5s\n",
            "92:\tlearn: 0.4392642\ttotal: 1.59s\tremaining: 15.6s\n",
            "93:\tlearn: 0.4388266\ttotal: 1.61s\tremaining: 15.5s\n",
            "94:\tlearn: 0.4385301\ttotal: 1.64s\tremaining: 15.7s\n",
            "95:\tlearn: 0.4382792\ttotal: 1.68s\tremaining: 15.8s\n",
            "96:\tlearn: 0.4380010\ttotal: 1.69s\tremaining: 15.8s\n",
            "97:\tlearn: 0.4375930\ttotal: 1.72s\tremaining: 15.8s\n",
            "98:\tlearn: 0.4372734\ttotal: 1.74s\tremaining: 15.8s\n",
            "99:\tlearn: 0.4369365\ttotal: 1.75s\tremaining: 15.7s\n",
            "100:\tlearn: 0.4366560\ttotal: 1.77s\tremaining: 15.8s\n",
            "101:\tlearn: 0.4363606\ttotal: 1.79s\tremaining: 15.7s\n",
            "102:\tlearn: 0.4361124\ttotal: 1.8s\tremaining: 15.7s\n",
            "103:\tlearn: 0.4358880\ttotal: 1.82s\tremaining: 15.7s\n",
            "104:\tlearn: 0.4356014\ttotal: 1.85s\tremaining: 15.8s\n",
            "105:\tlearn: 0.4353816\ttotal: 1.86s\tremaining: 15.7s\n",
            "106:\tlearn: 0.4351350\ttotal: 1.89s\tremaining: 15.7s\n",
            "107:\tlearn: 0.4348524\ttotal: 1.9s\tremaining: 15.7s\n",
            "108:\tlearn: 0.4345203\ttotal: 1.92s\tremaining: 15.7s\n",
            "109:\tlearn: 0.4343565\ttotal: 1.93s\tremaining: 15.6s\n",
            "110:\tlearn: 0.4341722\ttotal: 1.95s\tremaining: 15.6s\n",
            "111:\tlearn: 0.4339576\ttotal: 1.96s\tremaining: 15.5s\n",
            "112:\tlearn: 0.4337825\ttotal: 1.97s\tremaining: 15.5s\n",
            "113:\tlearn: 0.4336305\ttotal: 1.98s\tremaining: 15.4s\n",
            "114:\tlearn: 0.4333909\ttotal: 1.98s\tremaining: 15.3s\n",
            "115:\tlearn: 0.4331670\ttotal: 1.99s\tremaining: 15.2s\n",
            "116:\tlearn: 0.4329306\ttotal: 2.04s\tremaining: 15.4s\n",
            "117:\tlearn: 0.4326922\ttotal: 2.05s\tremaining: 15.3s\n",
            "118:\tlearn: 0.4324539\ttotal: 2.06s\tremaining: 15.3s\n",
            "119:\tlearn: 0.4321990\ttotal: 2.08s\tremaining: 15.3s\n",
            "120:\tlearn: 0.4320176\ttotal: 2.11s\tremaining: 15.3s\n",
            "121:\tlearn: 0.4318808\ttotal: 2.13s\tremaining: 15.3s\n",
            "122:\tlearn: 0.4316551\ttotal: 2.14s\tremaining: 15.3s\n",
            "123:\tlearn: 0.4314226\ttotal: 2.15s\tremaining: 15.2s\n",
            "124:\tlearn: 0.4312876\ttotal: 2.16s\tremaining: 15.1s\n",
            "125:\tlearn: 0.4310890\ttotal: 2.17s\tremaining: 15s\n",
            "126:\tlearn: 0.4308144\ttotal: 2.18s\tremaining: 15s\n",
            "127:\tlearn: 0.4306515\ttotal: 2.19s\tremaining: 14.9s\n",
            "128:\tlearn: 0.4304011\ttotal: 2.21s\tremaining: 14.9s\n",
            "129:\tlearn: 0.4301944\ttotal: 2.22s\tremaining: 14.8s\n",
            "130:\tlearn: 0.4300581\ttotal: 2.23s\tremaining: 14.8s\n",
            "131:\tlearn: 0.4298228\ttotal: 2.23s\tremaining: 14.7s\n",
            "132:\tlearn: 0.4296373\ttotal: 2.25s\tremaining: 14.6s\n",
            "133:\tlearn: 0.4293990\ttotal: 2.25s\tremaining: 14.6s\n",
            "134:\tlearn: 0.4291944\ttotal: 2.26s\tremaining: 14.5s\n",
            "135:\tlearn: 0.4289566\ttotal: 2.27s\tremaining: 14.4s\n",
            "136:\tlearn: 0.4287606\ttotal: 2.29s\tremaining: 14.4s\n",
            "137:\tlearn: 0.4285394\ttotal: 2.3s\tremaining: 14.4s\n",
            "138:\tlearn: 0.4283330\ttotal: 2.31s\tremaining: 14.3s\n",
            "139:\tlearn: 0.4281525\ttotal: 2.31s\tremaining: 14.2s\n",
            "140:\tlearn: 0.4279813\ttotal: 2.32s\tremaining: 14.1s\n",
            "141:\tlearn: 0.4277870\ttotal: 2.33s\tremaining: 14.1s\n",
            "142:\tlearn: 0.4276256\ttotal: 2.34s\tremaining: 14s\n",
            "143:\tlearn: 0.4275262\ttotal: 2.34s\tremaining: 13.9s\n",
            "144:\tlearn: 0.4274308\ttotal: 2.35s\tremaining: 13.9s\n",
            "145:\tlearn: 0.4273314\ttotal: 2.37s\tremaining: 13.8s\n",
            "146:\tlearn: 0.4272033\ttotal: 2.38s\tremaining: 13.8s\n",
            "147:\tlearn: 0.4269613\ttotal: 2.39s\tremaining: 13.8s\n",
            "148:\tlearn: 0.4267819\ttotal: 2.4s\tremaining: 13.7s\n",
            "149:\tlearn: 0.4265963\ttotal: 2.41s\tremaining: 13.6s\n",
            "150:\tlearn: 0.4264358\ttotal: 2.42s\tremaining: 13.6s\n",
            "151:\tlearn: 0.4262359\ttotal: 2.42s\tremaining: 13.5s\n",
            "152:\tlearn: 0.4260594\ttotal: 2.44s\tremaining: 13.5s\n",
            "153:\tlearn: 0.4259267\ttotal: 2.44s\tremaining: 13.4s\n",
            "154:\tlearn: 0.4257855\ttotal: 2.45s\tremaining: 13.4s\n",
            "155:\tlearn: 0.4256457\ttotal: 2.46s\tremaining: 13.3s\n",
            "156:\tlearn: 0.4255355\ttotal: 2.47s\tremaining: 13.3s\n",
            "157:\tlearn: 0.4253813\ttotal: 2.48s\tremaining: 13.2s\n",
            "158:\tlearn: 0.4252497\ttotal: 2.5s\tremaining: 13.2s\n",
            "159:\tlearn: 0.4250324\ttotal: 2.53s\tremaining: 13.3s\n",
            "160:\tlearn: 0.4248720\ttotal: 2.55s\tremaining: 13.3s\n",
            "161:\tlearn: 0.4246736\ttotal: 2.58s\tremaining: 13.3s\n",
            "162:\tlearn: 0.4245366\ttotal: 2.59s\tremaining: 13.3s\n",
            "163:\tlearn: 0.4244440\ttotal: 2.63s\tremaining: 13.4s\n",
            "164:\tlearn: 0.4242740\ttotal: 2.64s\tremaining: 13.4s\n",
            "165:\tlearn: 0.4241104\ttotal: 2.66s\tremaining: 13.3s\n",
            "166:\tlearn: 0.4239375\ttotal: 2.67s\tremaining: 13.3s\n",
            "167:\tlearn: 0.4237773\ttotal: 2.68s\tremaining: 13.3s\n",
            "168:\tlearn: 0.4236363\ttotal: 2.69s\tremaining: 13.3s\n",
            "169:\tlearn: 0.4234380\ttotal: 2.7s\tremaining: 13.2s\n",
            "170:\tlearn: 0.4233340\ttotal: 2.71s\tremaining: 13.1s\n",
            "171:\tlearn: 0.4232406\ttotal: 2.72s\tremaining: 13.1s\n",
            "172:\tlearn: 0.4230275\ttotal: 2.73s\tremaining: 13s\n",
            "173:\tlearn: 0.4228358\ttotal: 2.74s\tremaining: 13s\n",
            "174:\tlearn: 0.4227633\ttotal: 2.75s\tremaining: 13s\n",
            "175:\tlearn: 0.4226240\ttotal: 2.76s\tremaining: 12.9s\n",
            "176:\tlearn: 0.4225228\ttotal: 2.77s\tremaining: 12.9s\n",
            "177:\tlearn: 0.4224312\ttotal: 2.77s\tremaining: 12.8s\n",
            "178:\tlearn: 0.4222472\ttotal: 2.78s\tremaining: 12.8s\n",
            "179:\tlearn: 0.4221733\ttotal: 2.79s\tremaining: 12.7s\n",
            "180:\tlearn: 0.4220851\ttotal: 2.8s\tremaining: 12.7s\n",
            "181:\tlearn: 0.4219667\ttotal: 2.8s\tremaining: 12.6s\n",
            "182:\tlearn: 0.4219132\ttotal: 2.81s\tremaining: 12.6s\n",
            "183:\tlearn: 0.4217174\ttotal: 2.83s\tremaining: 12.6s\n",
            "184:\tlearn: 0.4215918\ttotal: 2.85s\tremaining: 12.6s\n",
            "185:\tlearn: 0.4214429\ttotal: 2.87s\tremaining: 12.5s\n",
            "186:\tlearn: 0.4213958\ttotal: 2.88s\tremaining: 12.5s\n",
            "187:\tlearn: 0.4212999\ttotal: 2.9s\tremaining: 12.5s\n",
            "188:\tlearn: 0.4211558\ttotal: 2.91s\tremaining: 12.5s\n",
            "189:\tlearn: 0.4209909\ttotal: 2.93s\tremaining: 12.5s\n",
            "190:\tlearn: 0.4208483\ttotal: 2.95s\tremaining: 12.5s\n",
            "191:\tlearn: 0.4206502\ttotal: 2.97s\tremaining: 12.5s\n",
            "192:\tlearn: 0.4205040\ttotal: 2.98s\tremaining: 12.5s\n",
            "193:\tlearn: 0.4202679\ttotal: 3s\tremaining: 12.5s\n",
            "194:\tlearn: 0.4200040\ttotal: 3.01s\tremaining: 12.4s\n",
            "195:\tlearn: 0.4198770\ttotal: 3.04s\tremaining: 12.5s\n",
            "196:\tlearn: 0.4197395\ttotal: 3.05s\tremaining: 12.4s\n",
            "197:\tlearn: 0.4196494\ttotal: 3.07s\tremaining: 12.4s\n",
            "198:\tlearn: 0.4195788\ttotal: 3.09s\tremaining: 12.4s\n",
            "199:\tlearn: 0.4194462\ttotal: 3.1s\tremaining: 12.4s\n",
            "200:\tlearn: 0.4192440\ttotal: 3.12s\tremaining: 12.4s\n",
            "201:\tlearn: 0.4190960\ttotal: 3.14s\tremaining: 12.4s\n",
            "202:\tlearn: 0.4189635\ttotal: 3.15s\tremaining: 12.4s\n",
            "203:\tlearn: 0.4188722\ttotal: 3.16s\tremaining: 12.3s\n",
            "204:\tlearn: 0.4187305\ttotal: 3.17s\tremaining: 12.3s\n",
            "205:\tlearn: 0.4185032\ttotal: 3.18s\tremaining: 12.2s\n",
            "206:\tlearn: 0.4184402\ttotal: 3.18s\tremaining: 12.2s\n",
            "207:\tlearn: 0.4182702\ttotal: 3.19s\tremaining: 12.2s\n",
            "208:\tlearn: 0.4180445\ttotal: 3.2s\tremaining: 12.1s\n",
            "209:\tlearn: 0.4179503\ttotal: 3.21s\tremaining: 12.1s\n",
            "210:\tlearn: 0.4178331\ttotal: 3.22s\tremaining: 12s\n",
            "211:\tlearn: 0.4177762\ttotal: 3.23s\tremaining: 12s\n",
            "212:\tlearn: 0.4176954\ttotal: 3.25s\tremaining: 12s\n",
            "213:\tlearn: 0.4175947\ttotal: 3.27s\tremaining: 12s\n",
            "214:\tlearn: 0.4174066\ttotal: 3.29s\tremaining: 12s\n",
            "215:\tlearn: 0.4172400\ttotal: 3.31s\tremaining: 12s\n",
            "216:\tlearn: 0.4171514\ttotal: 3.33s\tremaining: 12s\n",
            "217:\tlearn: 0.4170323\ttotal: 3.36s\tremaining: 12.1s\n",
            "218:\tlearn: 0.4169195\ttotal: 3.41s\tremaining: 12.1s\n",
            "219:\tlearn: 0.4167808\ttotal: 3.43s\tremaining: 12.2s\n",
            "220:\tlearn: 0.4166683\ttotal: 3.46s\tremaining: 12.2s\n",
            "221:\tlearn: 0.4165867\ttotal: 3.48s\tremaining: 12.2s\n",
            "222:\tlearn: 0.4164833\ttotal: 3.5s\tremaining: 12.2s\n",
            "223:\tlearn: 0.4163711\ttotal: 3.51s\tremaining: 12.2s\n",
            "224:\tlearn: 0.4163014\ttotal: 3.56s\tremaining: 12.2s\n",
            "225:\tlearn: 0.4161844\ttotal: 3.58s\tremaining: 12.2s\n",
            "226:\tlearn: 0.4160762\ttotal: 3.59s\tremaining: 12.2s\n",
            "227:\tlearn: 0.4160068\ttotal: 3.61s\tremaining: 12.2s\n",
            "228:\tlearn: 0.4159651\ttotal: 3.62s\tremaining: 12.2s\n",
            "229:\tlearn: 0.4158952\ttotal: 3.64s\tremaining: 12.2s\n",
            "230:\tlearn: 0.4157331\ttotal: 3.66s\tremaining: 12.2s\n",
            "231:\tlearn: 0.4155675\ttotal: 3.67s\tremaining: 12.2s\n",
            "232:\tlearn: 0.4155145\ttotal: 3.69s\tremaining: 12.1s\n",
            "233:\tlearn: 0.4154296\ttotal: 3.7s\tremaining: 12.1s\n",
            "234:\tlearn: 0.4153305\ttotal: 3.72s\tremaining: 12.1s\n",
            "235:\tlearn: 0.4151710\ttotal: 3.74s\tremaining: 12.1s\n",
            "236:\tlearn: 0.4150596\ttotal: 3.75s\tremaining: 12.1s\n",
            "237:\tlearn: 0.4149686\ttotal: 3.76s\tremaining: 12.1s\n",
            "238:\tlearn: 0.4148079\ttotal: 3.79s\tremaining: 12.1s\n",
            "239:\tlearn: 0.4146541\ttotal: 3.8s\tremaining: 12s\n",
            "240:\tlearn: 0.4145112\ttotal: 3.81s\tremaining: 12s\n",
            "241:\tlearn: 0.4144101\ttotal: 3.82s\tremaining: 12s\n",
            "242:\tlearn: 0.4142930\ttotal: 3.83s\tremaining: 11.9s\n",
            "243:\tlearn: 0.4142174\ttotal: 3.84s\tremaining: 11.9s\n",
            "244:\tlearn: 0.4141663\ttotal: 3.86s\tremaining: 11.9s\n",
            "245:\tlearn: 0.4139225\ttotal: 3.87s\tremaining: 11.9s\n",
            "246:\tlearn: 0.4138531\ttotal: 3.88s\tremaining: 11.8s\n",
            "247:\tlearn: 0.4137579\ttotal: 3.88s\tremaining: 11.8s\n",
            "248:\tlearn: 0.4136262\ttotal: 3.9s\tremaining: 11.8s\n",
            "249:\tlearn: 0.4135141\ttotal: 3.92s\tremaining: 11.7s\n",
            "250:\tlearn: 0.4133845\ttotal: 3.93s\tremaining: 11.7s\n",
            "251:\tlearn: 0.4132894\ttotal: 3.94s\tremaining: 11.7s\n",
            "252:\tlearn: 0.4132386\ttotal: 3.98s\tremaining: 11.8s\n",
            "253:\tlearn: 0.4132153\ttotal: 4s\tremaining: 11.8s\n",
            "254:\tlearn: 0.4131425\ttotal: 4.03s\tremaining: 11.8s\n",
            "255:\tlearn: 0.4130726\ttotal: 4.06s\tremaining: 11.8s\n",
            "256:\tlearn: 0.4128998\ttotal: 4.09s\tremaining: 11.8s\n",
            "257:\tlearn: 0.4128573\ttotal: 4.11s\tremaining: 11.8s\n",
            "258:\tlearn: 0.4127387\ttotal: 4.13s\tremaining: 11.8s\n",
            "259:\tlearn: 0.4126644\ttotal: 4.13s\tremaining: 11.8s\n",
            "260:\tlearn: 0.4125664\ttotal: 4.14s\tremaining: 11.7s\n",
            "261:\tlearn: 0.4124921\ttotal: 4.15s\tremaining: 11.7s\n",
            "262:\tlearn: 0.4124371\ttotal: 4.16s\tremaining: 11.6s\n",
            "263:\tlearn: 0.4123616\ttotal: 4.16s\tremaining: 11.6s\n",
            "264:\tlearn: 0.4123006\ttotal: 4.17s\tremaining: 11.6s\n",
            "265:\tlearn: 0.4121955\ttotal: 4.18s\tremaining: 11.5s\n",
            "266:\tlearn: 0.4121690\ttotal: 4.18s\tremaining: 11.5s\n",
            "267:\tlearn: 0.4120610\ttotal: 4.19s\tremaining: 11.5s\n",
            "268:\tlearn: 0.4119803\ttotal: 4.2s\tremaining: 11.4s\n",
            "269:\tlearn: 0.4118790\ttotal: 4.21s\tremaining: 11.4s\n",
            "270:\tlearn: 0.4118515\ttotal: 4.22s\tremaining: 11.4s\n",
            "271:\tlearn: 0.4117090\ttotal: 4.23s\tremaining: 11.3s\n",
            "272:\tlearn: 0.4115424\ttotal: 4.24s\tremaining: 11.3s\n",
            "273:\tlearn: 0.4113703\ttotal: 4.27s\tremaining: 11.3s\n",
            "274:\tlearn: 0.4112719\ttotal: 4.27s\tremaining: 11.3s\n",
            "275:\tlearn: 0.4112489\ttotal: 4.29s\tremaining: 11.2s\n",
            "276:\tlearn: 0.4111677\ttotal: 4.3s\tremaining: 11.2s\n",
            "277:\tlearn: 0.4110229\ttotal: 4.31s\tremaining: 11.2s\n",
            "278:\tlearn: 0.4109535\ttotal: 4.32s\tremaining: 11.2s\n",
            "279:\tlearn: 0.4108682\ttotal: 4.34s\tremaining: 11.2s\n",
            "280:\tlearn: 0.4107701\ttotal: 4.35s\tremaining: 11.1s\n",
            "281:\tlearn: 0.4107477\ttotal: 4.37s\tremaining: 11.1s\n",
            "282:\tlearn: 0.4106318\ttotal: 4.4s\tremaining: 11.1s\n",
            "283:\tlearn: 0.4105578\ttotal: 4.41s\tremaining: 11.1s\n",
            "284:\tlearn: 0.4105090\ttotal: 4.44s\tremaining: 11.1s\n",
            "285:\tlearn: 0.4104277\ttotal: 4.46s\tremaining: 11.1s\n",
            "286:\tlearn: 0.4102773\ttotal: 4.47s\tremaining: 11.1s\n",
            "287:\tlearn: 0.4101754\ttotal: 4.49s\tremaining: 11.1s\n",
            "288:\tlearn: 0.4101065\ttotal: 4.5s\tremaining: 11.1s\n",
            "289:\tlearn: 0.4100552\ttotal: 4.51s\tremaining: 11s\n",
            "290:\tlearn: 0.4098885\ttotal: 4.53s\tremaining: 11s\n",
            "291:\tlearn: 0.4097122\ttotal: 4.54s\tremaining: 11s\n",
            "292:\tlearn: 0.4095922\ttotal: 4.55s\tremaining: 11s\n",
            "293:\tlearn: 0.4094840\ttotal: 4.57s\tremaining: 11s\n",
            "294:\tlearn: 0.4092628\ttotal: 4.58s\tremaining: 10.9s\n",
            "295:\tlearn: 0.4091546\ttotal: 4.61s\tremaining: 11s\n",
            "296:\tlearn: 0.4090575\ttotal: 4.63s\tremaining: 11s\n",
            "297:\tlearn: 0.4089530\ttotal: 4.65s\tremaining: 11s\n",
            "298:\tlearn: 0.4088793\ttotal: 4.66s\tremaining: 10.9s\n",
            "299:\tlearn: 0.4087978\ttotal: 4.68s\tremaining: 10.9s\n",
            "300:\tlearn: 0.4087544\ttotal: 4.7s\tremaining: 10.9s\n",
            "301:\tlearn: 0.4087011\ttotal: 4.7s\tremaining: 10.9s\n",
            "302:\tlearn: 0.4086262\ttotal: 4.74s\tremaining: 10.9s\n",
            "303:\tlearn: 0.4085135\ttotal: 4.76s\tremaining: 10.9s\n",
            "304:\tlearn: 0.4083978\ttotal: 4.78s\tremaining: 10.9s\n",
            "305:\tlearn: 0.4082136\ttotal: 4.79s\tremaining: 10.9s\n",
            "306:\tlearn: 0.4081416\ttotal: 4.81s\tremaining: 10.9s\n",
            "307:\tlearn: 0.4080764\ttotal: 4.83s\tremaining: 10.8s\n",
            "308:\tlearn: 0.4080156\ttotal: 4.84s\tremaining: 10.8s\n",
            "309:\tlearn: 0.4079444\ttotal: 4.87s\tremaining: 10.8s\n",
            "310:\tlearn: 0.4078653\ttotal: 4.89s\tremaining: 10.8s\n",
            "311:\tlearn: 0.4078057\ttotal: 4.91s\tremaining: 10.8s\n",
            "312:\tlearn: 0.4077659\ttotal: 4.92s\tremaining: 10.8s\n",
            "313:\tlearn: 0.4076811\ttotal: 4.96s\tremaining: 10.8s\n",
            "314:\tlearn: 0.4075457\ttotal: 4.99s\tremaining: 10.8s\n",
            "315:\tlearn: 0.4074213\ttotal: 5.01s\tremaining: 10.8s\n",
            "316:\tlearn: 0.4073567\ttotal: 5.02s\tremaining: 10.8s\n",
            "317:\tlearn: 0.4072328\ttotal: 5.05s\tremaining: 10.8s\n",
            "318:\tlearn: 0.4071794\ttotal: 5.07s\tremaining: 10.8s\n",
            "319:\tlearn: 0.4070739\ttotal: 5.08s\tremaining: 10.8s\n",
            "320:\tlearn: 0.4069576\ttotal: 5.09s\tremaining: 10.8s\n",
            "321:\tlearn: 0.4068746\ttotal: 5.11s\tremaining: 10.7s\n",
            "322:\tlearn: 0.4066946\ttotal: 5.12s\tremaining: 10.7s\n",
            "323:\tlearn: 0.4066502\ttotal: 5.14s\tremaining: 10.7s\n",
            "324:\tlearn: 0.4064899\ttotal: 5.17s\tremaining: 10.7s\n",
            "325:\tlearn: 0.4064348\ttotal: 5.19s\tremaining: 10.7s\n",
            "326:\tlearn: 0.4063570\ttotal: 5.23s\tremaining: 10.8s\n",
            "327:\tlearn: 0.4063105\ttotal: 5.24s\tremaining: 10.7s\n",
            "328:\tlearn: 0.4062034\ttotal: 5.25s\tremaining: 10.7s\n",
            "329:\tlearn: 0.4060695\ttotal: 5.26s\tremaining: 10.7s\n",
            "330:\tlearn: 0.4060269\ttotal: 5.29s\tremaining: 10.7s\n",
            "331:\tlearn: 0.4059374\ttotal: 5.32s\tremaining: 10.7s\n",
            "332:\tlearn: 0.4058717\ttotal: 5.37s\tremaining: 10.8s\n",
            "333:\tlearn: 0.4057530\ttotal: 5.38s\tremaining: 10.7s\n",
            "334:\tlearn: 0.4056158\ttotal: 5.39s\tremaining: 10.7s\n",
            "335:\tlearn: 0.4054361\ttotal: 5.4s\tremaining: 10.7s\n",
            "336:\tlearn: 0.4053398\ttotal: 5.41s\tremaining: 10.7s\n",
            "337:\tlearn: 0.4051437\ttotal: 5.43s\tremaining: 10.6s\n",
            "338:\tlearn: 0.4050600\ttotal: 5.45s\tremaining: 10.6s\n",
            "339:\tlearn: 0.4048550\ttotal: 5.47s\tremaining: 10.6s\n",
            "340:\tlearn: 0.4047529\ttotal: 5.48s\tremaining: 10.6s\n",
            "341:\tlearn: 0.4046267\ttotal: 5.5s\tremaining: 10.6s\n",
            "342:\tlearn: 0.4044773\ttotal: 5.53s\tremaining: 10.6s\n",
            "343:\tlearn: 0.4044032\ttotal: 5.56s\tremaining: 10.6s\n",
            "344:\tlearn: 0.4042837\ttotal: 5.58s\tremaining: 10.6s\n",
            "345:\tlearn: 0.4041204\ttotal: 5.61s\tremaining: 10.6s\n",
            "346:\tlearn: 0.4040247\ttotal: 5.64s\tremaining: 10.6s\n",
            "347:\tlearn: 0.4038756\ttotal: 5.65s\tremaining: 10.6s\n",
            "348:\tlearn: 0.4037376\ttotal: 5.68s\tremaining: 10.6s\n",
            "349:\tlearn: 0.4037006\ttotal: 5.71s\tremaining: 10.6s\n",
            "350:\tlearn: 0.4035857\ttotal: 5.74s\tremaining: 10.6s\n",
            "351:\tlearn: 0.4035017\ttotal: 5.78s\tremaining: 10.6s\n",
            "352:\tlearn: 0.4034024\ttotal: 5.81s\tremaining: 10.7s\n",
            "353:\tlearn: 0.4032988\ttotal: 5.85s\tremaining: 10.7s\n",
            "354:\tlearn: 0.4032428\ttotal: 5.89s\tremaining: 10.7s\n",
            "355:\tlearn: 0.4031789\ttotal: 5.92s\tremaining: 10.7s\n",
            "356:\tlearn: 0.4030055\ttotal: 5.94s\tremaining: 10.7s\n",
            "357:\tlearn: 0.4029764\ttotal: 5.97s\tremaining: 10.7s\n",
            "358:\tlearn: 0.4028963\ttotal: 6s\tremaining: 10.7s\n",
            "359:\tlearn: 0.4027110\ttotal: 6.03s\tremaining: 10.7s\n",
            "360:\tlearn: 0.4025501\ttotal: 6.06s\tremaining: 10.7s\n",
            "361:\tlearn: 0.4023493\ttotal: 6.08s\tremaining: 10.7s\n",
            "362:\tlearn: 0.4022705\ttotal: 6.1s\tremaining: 10.7s\n",
            "363:\tlearn: 0.4021418\ttotal: 6.12s\tremaining: 10.7s\n",
            "364:\tlearn: 0.4019488\ttotal: 6.15s\tremaining: 10.7s\n",
            "365:\tlearn: 0.4018822\ttotal: 6.18s\tremaining: 10.7s\n",
            "366:\tlearn: 0.4017197\ttotal: 6.2s\tremaining: 10.7s\n",
            "367:\tlearn: 0.4016475\ttotal: 6.22s\tremaining: 10.7s\n",
            "368:\tlearn: 0.4015126\ttotal: 6.25s\tremaining: 10.7s\n",
            "369:\tlearn: 0.4014063\ttotal: 6.27s\tremaining: 10.7s\n",
            "370:\tlearn: 0.4012511\ttotal: 6.31s\tremaining: 10.7s\n",
            "371:\tlearn: 0.4011434\ttotal: 6.34s\tremaining: 10.7s\n",
            "372:\tlearn: 0.4010323\ttotal: 6.38s\tremaining: 10.7s\n",
            "373:\tlearn: 0.4009128\ttotal: 6.41s\tremaining: 10.7s\n",
            "374:\tlearn: 0.4007697\ttotal: 6.44s\tremaining: 10.7s\n",
            "375:\tlearn: 0.4006068\ttotal: 6.5s\tremaining: 10.8s\n",
            "376:\tlearn: 0.4004862\ttotal: 6.54s\tremaining: 10.8s\n",
            "377:\tlearn: 0.4004530\ttotal: 6.58s\tremaining: 10.8s\n",
            "378:\tlearn: 0.4004085\ttotal: 6.62s\tremaining: 10.8s\n",
            "379:\tlearn: 0.4002227\ttotal: 6.66s\tremaining: 10.9s\n",
            "380:\tlearn: 0.4000963\ttotal: 6.68s\tremaining: 10.9s\n",
            "381:\tlearn: 0.4000169\ttotal: 6.72s\tremaining: 10.9s\n",
            "382:\tlearn: 0.3998423\ttotal: 6.77s\tremaining: 10.9s\n",
            "383:\tlearn: 0.3996862\ttotal: 6.8s\tremaining: 10.9s\n",
            "384:\tlearn: 0.3996065\ttotal: 6.83s\tremaining: 10.9s\n",
            "385:\tlearn: 0.3994852\ttotal: 6.84s\tremaining: 10.9s\n",
            "386:\tlearn: 0.3993321\ttotal: 6.86s\tremaining: 10.9s\n",
            "387:\tlearn: 0.3990713\ttotal: 6.88s\tremaining: 10.9s\n",
            "388:\tlearn: 0.3988677\ttotal: 6.9s\tremaining: 10.8s\n",
            "389:\tlearn: 0.3987811\ttotal: 6.92s\tremaining: 10.8s\n",
            "390:\tlearn: 0.3987116\ttotal: 6.93s\tremaining: 10.8s\n",
            "391:\tlearn: 0.3986342\ttotal: 6.95s\tremaining: 10.8s\n",
            "392:\tlearn: 0.3984444\ttotal: 6.97s\tremaining: 10.8s\n",
            "393:\tlearn: 0.3983445\ttotal: 6.98s\tremaining: 10.7s\n",
            "394:\tlearn: 0.3982976\ttotal: 7s\tremaining: 10.7s\n",
            "395:\tlearn: 0.3981523\ttotal: 7.03s\tremaining: 10.7s\n",
            "396:\tlearn: 0.3980730\ttotal: 7.04s\tremaining: 10.7s\n",
            "397:\tlearn: 0.3980072\ttotal: 7.06s\tremaining: 10.7s\n",
            "398:\tlearn: 0.3978913\ttotal: 7.09s\tremaining: 10.7s\n",
            "399:\tlearn: 0.3976576\ttotal: 7.18s\tremaining: 10.8s\n",
            "400:\tlearn: 0.3975061\ttotal: 7.21s\tremaining: 10.8s\n",
            "401:\tlearn: 0.3973770\ttotal: 7.26s\tremaining: 10.8s\n",
            "402:\tlearn: 0.3972710\ttotal: 7.29s\tremaining: 10.8s\n",
            "403:\tlearn: 0.3971922\ttotal: 7.31s\tremaining: 10.8s\n",
            "404:\tlearn: 0.3969945\ttotal: 7.32s\tremaining: 10.8s\n",
            "405:\tlearn: 0.3969299\ttotal: 7.36s\tremaining: 10.8s\n",
            "406:\tlearn: 0.3967815\ttotal: 7.38s\tremaining: 10.8s\n",
            "407:\tlearn: 0.3966506\ttotal: 7.41s\tremaining: 10.7s\n",
            "408:\tlearn: 0.3966349\ttotal: 7.43s\tremaining: 10.7s\n",
            "409:\tlearn: 0.3965539\ttotal: 7.45s\tremaining: 10.7s\n",
            "410:\tlearn: 0.3964078\ttotal: 7.47s\tremaining: 10.7s\n",
            "411:\tlearn: 0.3962215\ttotal: 7.5s\tremaining: 10.7s\n",
            "412:\tlearn: 0.3961314\ttotal: 7.52s\tremaining: 10.7s\n",
            "413:\tlearn: 0.3959971\ttotal: 7.55s\tremaining: 10.7s\n",
            "414:\tlearn: 0.3959138\ttotal: 7.57s\tremaining: 10.7s\n",
            "415:\tlearn: 0.3958943\ttotal: 7.59s\tremaining: 10.7s\n",
            "416:\tlearn: 0.3957795\ttotal: 7.61s\tremaining: 10.6s\n",
            "417:\tlearn: 0.3957314\ttotal: 7.64s\tremaining: 10.6s\n",
            "418:\tlearn: 0.3957174\ttotal: 7.65s\tremaining: 10.6s\n",
            "419:\tlearn: 0.3955413\ttotal: 7.7s\tremaining: 10.6s\n",
            "420:\tlearn: 0.3954948\ttotal: 7.73s\tremaining: 10.6s\n",
            "421:\tlearn: 0.3953784\ttotal: 7.76s\tremaining: 10.6s\n",
            "422:\tlearn: 0.3952870\ttotal: 7.78s\tremaining: 10.6s\n",
            "423:\tlearn: 0.3952362\ttotal: 7.81s\tremaining: 10.6s\n",
            "424:\tlearn: 0.3950543\ttotal: 7.87s\tremaining: 10.7s\n",
            "425:\tlearn: 0.3950221\ttotal: 7.91s\tremaining: 10.7s\n",
            "426:\tlearn: 0.3948638\ttotal: 7.93s\tremaining: 10.6s\n",
            "427:\tlearn: 0.3947621\ttotal: 7.97s\tremaining: 10.6s\n",
            "428:\tlearn: 0.3946856\ttotal: 7.99s\tremaining: 10.6s\n",
            "429:\tlearn: 0.3945852\ttotal: 8.01s\tremaining: 10.6s\n",
            "430:\tlearn: 0.3944965\ttotal: 8.04s\tremaining: 10.6s\n",
            "431:\tlearn: 0.3942876\ttotal: 8.06s\tremaining: 10.6s\n",
            "432:\tlearn: 0.3941499\ttotal: 8.08s\tremaining: 10.6s\n",
            "433:\tlearn: 0.3940556\ttotal: 8.1s\tremaining: 10.6s\n",
            "434:\tlearn: 0.3939859\ttotal: 8.15s\tremaining: 10.6s\n",
            "435:\tlearn: 0.3938797\ttotal: 8.17s\tremaining: 10.6s\n",
            "436:\tlearn: 0.3937319\ttotal: 8.19s\tremaining: 10.6s\n",
            "437:\tlearn: 0.3936265\ttotal: 8.23s\tremaining: 10.6s\n",
            "438:\tlearn: 0.3935382\ttotal: 8.27s\tremaining: 10.6s\n",
            "439:\tlearn: 0.3933957\ttotal: 8.32s\tremaining: 10.6s\n",
            "440:\tlearn: 0.3932767\ttotal: 8.34s\tremaining: 10.6s\n",
            "441:\tlearn: 0.3931557\ttotal: 8.36s\tremaining: 10.6s\n",
            "442:\tlearn: 0.3931289\ttotal: 8.38s\tremaining: 10.5s\n",
            "443:\tlearn: 0.3930930\ttotal: 8.43s\tremaining: 10.6s\n",
            "444:\tlearn: 0.3930094\ttotal: 8.49s\tremaining: 10.6s\n",
            "445:\tlearn: 0.3929656\ttotal: 8.51s\tremaining: 10.6s\n",
            "446:\tlearn: 0.3928571\ttotal: 8.53s\tremaining: 10.5s\n",
            "447:\tlearn: 0.3927796\ttotal: 8.56s\tremaining: 10.5s\n",
            "448:\tlearn: 0.3927618\ttotal: 8.59s\tremaining: 10.5s\n",
            "449:\tlearn: 0.3927522\ttotal: 8.61s\tremaining: 10.5s\n",
            "450:\tlearn: 0.3927315\ttotal: 8.64s\tremaining: 10.5s\n",
            "451:\tlearn: 0.3926155\ttotal: 8.68s\tremaining: 10.5s\n",
            "452:\tlearn: 0.3925719\ttotal: 8.72s\tremaining: 10.5s\n",
            "453:\tlearn: 0.3924751\ttotal: 8.74s\tremaining: 10.5s\n",
            "454:\tlearn: 0.3923662\ttotal: 8.76s\tremaining: 10.5s\n",
            "455:\tlearn: 0.3922291\ttotal: 8.77s\tremaining: 10.5s\n",
            "456:\tlearn: 0.3921718\ttotal: 8.79s\tremaining: 10.4s\n",
            "457:\tlearn: 0.3920528\ttotal: 8.8s\tremaining: 10.4s\n",
            "458:\tlearn: 0.3919776\ttotal: 8.82s\tremaining: 10.4s\n",
            "459:\tlearn: 0.3919588\ttotal: 8.83s\tremaining: 10.4s\n",
            "460:\tlearn: 0.3918406\ttotal: 8.85s\tremaining: 10.3s\n",
            "461:\tlearn: 0.3918018\ttotal: 8.86s\tremaining: 10.3s\n",
            "462:\tlearn: 0.3916357\ttotal: 8.88s\tremaining: 10.3s\n",
            "463:\tlearn: 0.3914729\ttotal: 8.9s\tremaining: 10.3s\n",
            "464:\tlearn: 0.3913231\ttotal: 8.92s\tremaining: 10.3s\n",
            "465:\tlearn: 0.3912261\ttotal: 8.95s\tremaining: 10.3s\n",
            "466:\tlearn: 0.3910734\ttotal: 8.96s\tremaining: 10.2s\n",
            "467:\tlearn: 0.3908984\ttotal: 8.99s\tremaining: 10.2s\n",
            "468:\tlearn: 0.3908855\ttotal: 9.01s\tremaining: 10.2s\n",
            "469:\tlearn: 0.3908652\ttotal: 9.03s\tremaining: 10.2s\n",
            "470:\tlearn: 0.3907936\ttotal: 9.05s\tremaining: 10.2s\n",
            "471:\tlearn: 0.3907878\ttotal: 9.06s\tremaining: 10.1s\n",
            "472:\tlearn: 0.3906430\ttotal: 9.07s\tremaining: 10.1s\n",
            "473:\tlearn: 0.3905142\ttotal: 9.09s\tremaining: 10.1s\n",
            "474:\tlearn: 0.3904940\ttotal: 9.1s\tremaining: 10.1s\n",
            "475:\tlearn: 0.3903840\ttotal: 9.12s\tremaining: 10s\n",
            "476:\tlearn: 0.3902952\ttotal: 9.13s\tremaining: 10s\n",
            "477:\tlearn: 0.3902356\ttotal: 9.15s\tremaining: 9.99s\n",
            "478:\tlearn: 0.3901911\ttotal: 9.16s\tremaining: 9.96s\n",
            "479:\tlearn: 0.3900488\ttotal: 9.18s\tremaining: 9.94s\n",
            "480:\tlearn: 0.3899275\ttotal: 9.19s\tremaining: 9.92s\n",
            "481:\tlearn: 0.3897656\ttotal: 9.21s\tremaining: 9.89s\n",
            "482:\tlearn: 0.3897480\ttotal: 9.22s\tremaining: 9.87s\n",
            "483:\tlearn: 0.3896917\ttotal: 9.25s\tremaining: 9.86s\n",
            "484:\tlearn: 0.3895209\ttotal: 9.27s\tremaining: 9.84s\n",
            "485:\tlearn: 0.3894879\ttotal: 9.29s\tremaining: 9.82s\n",
            "486:\tlearn: 0.3893612\ttotal: 9.3s\tremaining: 9.8s\n",
            "487:\tlearn: 0.3893172\ttotal: 9.32s\tremaining: 9.78s\n",
            "488:\tlearn: 0.3892561\ttotal: 9.34s\tremaining: 9.76s\n",
            "489:\tlearn: 0.3891416\ttotal: 9.37s\tremaining: 9.76s\n",
            "490:\tlearn: 0.3890631\ttotal: 9.38s\tremaining: 9.73s\n",
            "491:\tlearn: 0.3890490\ttotal: 9.4s\tremaining: 9.71s\n",
            "492:\tlearn: 0.3888942\ttotal: 9.41s\tremaining: 9.68s\n",
            "493:\tlearn: 0.3888394\ttotal: 9.43s\tremaining: 9.66s\n",
            "494:\tlearn: 0.3886891\ttotal: 9.45s\tremaining: 9.64s\n",
            "495:\tlearn: 0.3885265\ttotal: 9.46s\tremaining: 9.62s\n",
            "496:\tlearn: 0.3884631\ttotal: 9.48s\tremaining: 9.59s\n",
            "497:\tlearn: 0.3883933\ttotal: 9.52s\tremaining: 9.59s\n",
            "498:\tlearn: 0.3883657\ttotal: 9.53s\tremaining: 9.57s\n",
            "499:\tlearn: 0.3882879\ttotal: 9.55s\tremaining: 9.55s\n",
            "500:\tlearn: 0.3882663\ttotal: 9.57s\tremaining: 9.53s\n",
            "501:\tlearn: 0.3882553\ttotal: 9.58s\tremaining: 9.51s\n",
            "502:\tlearn: 0.3881600\ttotal: 9.6s\tremaining: 9.48s\n",
            "503:\tlearn: 0.3881485\ttotal: 9.61s\tremaining: 9.46s\n",
            "504:\tlearn: 0.3881287\ttotal: 9.64s\tremaining: 9.45s\n",
            "505:\tlearn: 0.3880820\ttotal: 9.68s\tremaining: 9.45s\n",
            "506:\tlearn: 0.3880624\ttotal: 9.71s\tremaining: 9.44s\n",
            "507:\tlearn: 0.3880300\ttotal: 9.74s\tremaining: 9.43s\n",
            "508:\tlearn: 0.3878787\ttotal: 9.77s\tremaining: 9.43s\n",
            "509:\tlearn: 0.3878029\ttotal: 9.78s\tremaining: 9.4s\n",
            "510:\tlearn: 0.3877499\ttotal: 9.8s\tremaining: 9.38s\n",
            "511:\tlearn: 0.3875472\ttotal: 9.82s\tremaining: 9.36s\n",
            "512:\tlearn: 0.3874932\ttotal: 9.83s\tremaining: 9.33s\n",
            "513:\tlearn: 0.3873278\ttotal: 9.86s\tremaining: 9.32s\n",
            "514:\tlearn: 0.3871756\ttotal: 9.88s\tremaining: 9.3s\n",
            "515:\tlearn: 0.3871596\ttotal: 9.9s\tremaining: 9.28s\n",
            "516:\tlearn: 0.3869955\ttotal: 9.93s\tremaining: 9.28s\n",
            "517:\tlearn: 0.3869499\ttotal: 9.95s\tremaining: 9.26s\n",
            "518:\tlearn: 0.3869213\ttotal: 9.97s\tremaining: 9.24s\n",
            "519:\tlearn: 0.3868768\ttotal: 9.99s\tremaining: 9.22s\n",
            "520:\tlearn: 0.3868317\ttotal: 10s\tremaining: 9.21s\n",
            "521:\tlearn: 0.3867157\ttotal: 10.1s\tremaining: 9.2s\n",
            "522:\tlearn: 0.3865790\ttotal: 10.1s\tremaining: 9.19s\n",
            "523:\tlearn: 0.3865253\ttotal: 10.1s\tremaining: 9.17s\n",
            "524:\tlearn: 0.3864192\ttotal: 10.1s\tremaining: 9.17s\n",
            "525:\tlearn: 0.3864012\ttotal: 10.2s\tremaining: 9.15s\n",
            "526:\tlearn: 0.3862285\ttotal: 10.2s\tremaining: 9.13s\n",
            "527:\tlearn: 0.3860790\ttotal: 10.2s\tremaining: 9.1s\n",
            "528:\tlearn: 0.3859480\ttotal: 10.2s\tremaining: 9.07s\n",
            "529:\tlearn: 0.3858009\ttotal: 10.2s\tremaining: 9.05s\n",
            "530:\tlearn: 0.3857036\ttotal: 10.2s\tremaining: 9.02s\n",
            "531:\tlearn: 0.3855477\ttotal: 10.2s\tremaining: 8.99s\n",
            "532:\tlearn: 0.3853754\ttotal: 10.2s\tremaining: 8.96s\n",
            "533:\tlearn: 0.3851893\ttotal: 10.2s\tremaining: 8.93s\n",
            "534:\tlearn: 0.3850746\ttotal: 10.2s\tremaining: 8.91s\n",
            "535:\tlearn: 0.3849759\ttotal: 10.3s\tremaining: 8.89s\n",
            "536:\tlearn: 0.3848946\ttotal: 10.3s\tremaining: 8.88s\n",
            "537:\tlearn: 0.3848774\ttotal: 10.3s\tremaining: 8.85s\n",
            "538:\tlearn: 0.3847201\ttotal: 10.3s\tremaining: 8.83s\n",
            "539:\tlearn: 0.3846596\ttotal: 10.3s\tremaining: 8.8s\n",
            "540:\tlearn: 0.3845554\ttotal: 10.3s\tremaining: 8.77s\n",
            "541:\tlearn: 0.3844065\ttotal: 10.3s\tremaining: 8.74s\n",
            "542:\tlearn: 0.3842558\ttotal: 10.4s\tremaining: 8.71s\n",
            "543:\tlearn: 0.3841119\ttotal: 10.4s\tremaining: 8.68s\n",
            "544:\tlearn: 0.3839613\ttotal: 10.4s\tremaining: 8.66s\n",
            "545:\tlearn: 0.3837538\ttotal: 10.4s\tremaining: 8.63s\n",
            "546:\tlearn: 0.3837406\ttotal: 10.4s\tremaining: 8.61s\n",
            "547:\tlearn: 0.3836194\ttotal: 10.4s\tremaining: 8.59s\n",
            "548:\tlearn: 0.3836024\ttotal: 10.4s\tremaining: 8.57s\n",
            "549:\tlearn: 0.3835410\ttotal: 10.4s\tremaining: 8.54s\n",
            "550:\tlearn: 0.3833812\ttotal: 10.5s\tremaining: 8.53s\n",
            "551:\tlearn: 0.3833637\ttotal: 10.5s\tremaining: 8.51s\n",
            "552:\tlearn: 0.3833505\ttotal: 10.5s\tremaining: 8.49s\n",
            "553:\tlearn: 0.3832467\ttotal: 10.5s\tremaining: 8.46s\n",
            "554:\tlearn: 0.3831794\ttotal: 10.5s\tremaining: 8.45s\n",
            "555:\tlearn: 0.3831157\ttotal: 10.5s\tremaining: 8.42s\n",
            "556:\tlearn: 0.3830247\ttotal: 10.6s\tremaining: 8.4s\n",
            "557:\tlearn: 0.3828890\ttotal: 10.6s\tremaining: 8.38s\n",
            "558:\tlearn: 0.3826754\ttotal: 10.6s\tremaining: 8.37s\n",
            "559:\tlearn: 0.3825322\ttotal: 10.6s\tremaining: 8.36s\n",
            "560:\tlearn: 0.3825244\ttotal: 10.7s\tremaining: 8.34s\n",
            "561:\tlearn: 0.3823807\ttotal: 10.7s\tremaining: 8.33s\n",
            "562:\tlearn: 0.3823162\ttotal: 10.7s\tremaining: 8.31s\n",
            "563:\tlearn: 0.3821392\ttotal: 10.7s\tremaining: 8.29s\n",
            "564:\tlearn: 0.3819976\ttotal: 10.7s\tremaining: 8.27s\n",
            "565:\tlearn: 0.3818880\ttotal: 10.8s\tremaining: 8.25s\n",
            "566:\tlearn: 0.3817293\ttotal: 10.8s\tremaining: 8.24s\n",
            "567:\tlearn: 0.3816380\ttotal: 10.8s\tremaining: 8.22s\n",
            "568:\tlearn: 0.3814467\ttotal: 10.8s\tremaining: 8.2s\n",
            "569:\tlearn: 0.3813663\ttotal: 10.8s\tremaining: 8.18s\n",
            "570:\tlearn: 0.3813501\ttotal: 10.9s\tremaining: 8.17s\n",
            "571:\tlearn: 0.3811656\ttotal: 10.9s\tremaining: 8.15s\n",
            "572:\tlearn: 0.3811521\ttotal: 10.9s\tremaining: 8.13s\n",
            "573:\tlearn: 0.3810472\ttotal: 10.9s\tremaining: 8.11s\n",
            "574:\tlearn: 0.3810193\ttotal: 10.9s\tremaining: 8.09s\n",
            "575:\tlearn: 0.3808873\ttotal: 11s\tremaining: 8.07s\n",
            "576:\tlearn: 0.3807485\ttotal: 11s\tremaining: 8.04s\n",
            "577:\tlearn: 0.3806445\ttotal: 11s\tremaining: 8.02s\n",
            "578:\tlearn: 0.3804942\ttotal: 11s\tremaining: 8s\n",
            "579:\tlearn: 0.3803478\ttotal: 11s\tremaining: 7.98s\n",
            "580:\tlearn: 0.3803351\ttotal: 11s\tremaining: 7.96s\n",
            "581:\tlearn: 0.3803301\ttotal: 11.1s\tremaining: 7.94s\n",
            "582:\tlearn: 0.3802281\ttotal: 11.1s\tremaining: 7.92s\n",
            "583:\tlearn: 0.3800489\ttotal: 11.1s\tremaining: 7.89s\n",
            "584:\tlearn: 0.3799708\ttotal: 11.1s\tremaining: 7.88s\n",
            "585:\tlearn: 0.3798325\ttotal: 11.1s\tremaining: 7.86s\n",
            "586:\tlearn: 0.3797583\ttotal: 11.1s\tremaining: 7.83s\n",
            "587:\tlearn: 0.3797161\ttotal: 11.1s\tremaining: 7.81s\n",
            "588:\tlearn: 0.3796151\ttotal: 11.2s\tremaining: 7.79s\n",
            "589:\tlearn: 0.3794654\ttotal: 11.2s\tremaining: 7.77s\n",
            "590:\tlearn: 0.3791963\ttotal: 11.2s\tremaining: 7.75s\n",
            "591:\tlearn: 0.3791860\ttotal: 11.2s\tremaining: 7.72s\n",
            "592:\tlearn: 0.3790397\ttotal: 11.2s\tremaining: 7.7s\n",
            "593:\tlearn: 0.3789678\ttotal: 11.2s\tremaining: 7.68s\n",
            "594:\tlearn: 0.3788824\ttotal: 11.2s\tremaining: 7.65s\n",
            "595:\tlearn: 0.3788234\ttotal: 11.3s\tremaining: 7.63s\n",
            "596:\tlearn: 0.3788156\ttotal: 11.3s\tremaining: 7.6s\n",
            "597:\tlearn: 0.3787293\ttotal: 11.3s\tremaining: 7.58s\n",
            "598:\tlearn: 0.3785159\ttotal: 11.3s\tremaining: 7.55s\n",
            "599:\tlearn: 0.3784207\ttotal: 11.3s\tremaining: 7.53s\n",
            "600:\tlearn: 0.3782948\ttotal: 11.3s\tremaining: 7.5s\n",
            "601:\tlearn: 0.3782832\ttotal: 11.3s\tremaining: 7.48s\n",
            "602:\tlearn: 0.3781043\ttotal: 11.3s\tremaining: 7.46s\n",
            "603:\tlearn: 0.3779922\ttotal: 11.3s\tremaining: 7.43s\n",
            "604:\tlearn: 0.3777917\ttotal: 11.3s\tremaining: 7.4s\n",
            "605:\tlearn: 0.3776305\ttotal: 11.3s\tremaining: 7.38s\n",
            "606:\tlearn: 0.3775620\ttotal: 11.4s\tremaining: 7.35s\n",
            "607:\tlearn: 0.3773653\ttotal: 11.4s\tremaining: 7.33s\n",
            "608:\tlearn: 0.3772461\ttotal: 11.4s\tremaining: 7.31s\n",
            "609:\tlearn: 0.3770555\ttotal: 11.4s\tremaining: 7.29s\n",
            "610:\tlearn: 0.3769774\ttotal: 11.4s\tremaining: 7.27s\n",
            "611:\tlearn: 0.3767577\ttotal: 11.4s\tremaining: 7.25s\n",
            "612:\tlearn: 0.3766924\ttotal: 11.4s\tremaining: 7.22s\n",
            "613:\tlearn: 0.3765692\ttotal: 11.5s\tremaining: 7.21s\n",
            "614:\tlearn: 0.3764963\ttotal: 11.5s\tremaining: 7.19s\n",
            "615:\tlearn: 0.3763402\ttotal: 11.5s\tremaining: 7.17s\n",
            "616:\tlearn: 0.3762655\ttotal: 11.5s\tremaining: 7.15s\n",
            "617:\tlearn: 0.3760845\ttotal: 11.5s\tremaining: 7.13s\n",
            "618:\tlearn: 0.3759465\ttotal: 11.6s\tremaining: 7.11s\n",
            "619:\tlearn: 0.3757894\ttotal: 11.6s\tremaining: 7.09s\n",
            "620:\tlearn: 0.3757281\ttotal: 11.6s\tremaining: 7.08s\n",
            "621:\tlearn: 0.3756188\ttotal: 11.6s\tremaining: 7.06s\n",
            "622:\tlearn: 0.3754026\ttotal: 11.6s\tremaining: 7.04s\n",
            "623:\tlearn: 0.3752779\ttotal: 11.6s\tremaining: 7.02s\n",
            "624:\tlearn: 0.3751722\ttotal: 11.7s\tremaining: 7s\n",
            "625:\tlearn: 0.3750420\ttotal: 11.7s\tremaining: 6.98s\n",
            "626:\tlearn: 0.3748789\ttotal: 11.7s\tremaining: 6.96s\n",
            "627:\tlearn: 0.3746735\ttotal: 11.7s\tremaining: 6.95s\n",
            "628:\tlearn: 0.3745324\ttotal: 11.8s\tremaining: 6.94s\n",
            "629:\tlearn: 0.3744454\ttotal: 11.8s\tremaining: 6.92s\n",
            "630:\tlearn: 0.3744059\ttotal: 11.8s\tremaining: 6.91s\n",
            "631:\tlearn: 0.3743275\ttotal: 11.8s\tremaining: 6.89s\n",
            "632:\tlearn: 0.3743223\ttotal: 11.9s\tremaining: 6.88s\n",
            "633:\tlearn: 0.3742179\ttotal: 11.9s\tremaining: 6.88s\n",
            "634:\tlearn: 0.3741289\ttotal: 11.9s\tremaining: 6.86s\n",
            "635:\tlearn: 0.3741076\ttotal: 12s\tremaining: 6.84s\n",
            "636:\tlearn: 0.3740321\ttotal: 12s\tremaining: 6.83s\n",
            "637:\tlearn: 0.3739245\ttotal: 12s\tremaining: 6.81s\n",
            "638:\tlearn: 0.3737568\ttotal: 12s\tremaining: 6.79s\n",
            "639:\tlearn: 0.3736536\ttotal: 12s\tremaining: 6.78s\n",
            "640:\tlearn: 0.3736478\ttotal: 12.1s\tremaining: 6.75s\n",
            "641:\tlearn: 0.3735128\ttotal: 12.1s\tremaining: 6.73s\n",
            "642:\tlearn: 0.3733893\ttotal: 12.1s\tremaining: 6.71s\n",
            "643:\tlearn: 0.3733493\ttotal: 12.1s\tremaining: 6.7s\n",
            "644:\tlearn: 0.3733056\ttotal: 12.1s\tremaining: 6.67s\n",
            "645:\tlearn: 0.3732097\ttotal: 12.1s\tremaining: 6.65s\n",
            "646:\tlearn: 0.3730287\ttotal: 12.1s\tremaining: 6.62s\n",
            "647:\tlearn: 0.3730236\ttotal: 12.2s\tremaining: 6.6s\n",
            "648:\tlearn: 0.3729389\ttotal: 12.2s\tremaining: 6.58s\n",
            "649:\tlearn: 0.3728230\ttotal: 12.2s\tremaining: 6.56s\n",
            "650:\tlearn: 0.3726965\ttotal: 12.2s\tremaining: 6.54s\n",
            "651:\tlearn: 0.3725766\ttotal: 12.2s\tremaining: 6.52s\n",
            "652:\tlearn: 0.3725563\ttotal: 12.2s\tremaining: 6.49s\n",
            "653:\tlearn: 0.3725156\ttotal: 12.2s\tremaining: 6.47s\n",
            "654:\tlearn: 0.3724776\ttotal: 12.2s\tremaining: 6.44s\n",
            "655:\tlearn: 0.3723255\ttotal: 12.2s\tremaining: 6.42s\n",
            "656:\tlearn: 0.3723021\ttotal: 12.3s\tremaining: 6.4s\n",
            "657:\tlearn: 0.3721457\ttotal: 12.3s\tremaining: 6.37s\n",
            "658:\tlearn: 0.3720979\ttotal: 12.3s\tremaining: 6.35s\n",
            "659:\tlearn: 0.3720175\ttotal: 12.3s\tremaining: 6.33s\n",
            "660:\tlearn: 0.3718035\ttotal: 12.3s\tremaining: 6.3s\n",
            "661:\tlearn: 0.3716635\ttotal: 12.3s\tremaining: 6.28s\n",
            "662:\tlearn: 0.3716141\ttotal: 12.3s\tremaining: 6.26s\n",
            "663:\tlearn: 0.3715679\ttotal: 12.3s\tremaining: 6.24s\n",
            "664:\tlearn: 0.3714317\ttotal: 12.3s\tremaining: 6.21s\n",
            "665:\tlearn: 0.3712259\ttotal: 12.3s\tremaining: 6.19s\n",
            "666:\tlearn: 0.3711921\ttotal: 12.3s\tremaining: 6.16s\n",
            "667:\tlearn: 0.3711248\ttotal: 12.4s\tremaining: 6.14s\n",
            "668:\tlearn: 0.3711006\ttotal: 12.4s\tremaining: 6.12s\n",
            "669:\tlearn: 0.3709070\ttotal: 12.4s\tremaining: 6.09s\n",
            "670:\tlearn: 0.3707665\ttotal: 12.4s\tremaining: 6.07s\n",
            "671:\tlearn: 0.3706479\ttotal: 12.4s\tremaining: 6.04s\n",
            "672:\tlearn: 0.3705239\ttotal: 12.4s\tremaining: 6.02s\n",
            "673:\tlearn: 0.3703409\ttotal: 12.4s\tremaining: 6s\n",
            "674:\tlearn: 0.3702262\ttotal: 12.4s\tremaining: 5.99s\n",
            "675:\tlearn: 0.3701601\ttotal: 12.5s\tremaining: 5.97s\n",
            "676:\tlearn: 0.3701353\ttotal: 12.5s\tremaining: 5.95s\n",
            "677:\tlearn: 0.3699521\ttotal: 12.5s\tremaining: 5.93s\n",
            "678:\tlearn: 0.3698972\ttotal: 12.5s\tremaining: 5.92s\n",
            "679:\tlearn: 0.3697049\ttotal: 12.5s\tremaining: 5.89s\n",
            "680:\tlearn: 0.3695547\ttotal: 12.5s\tremaining: 5.88s\n",
            "681:\tlearn: 0.3694104\ttotal: 12.6s\tremaining: 5.86s\n",
            "682:\tlearn: 0.3693109\ttotal: 12.6s\tremaining: 5.83s\n",
            "683:\tlearn: 0.3691847\ttotal: 12.6s\tremaining: 5.82s\n",
            "684:\tlearn: 0.3690641\ttotal: 12.6s\tremaining: 5.79s\n",
            "685:\tlearn: 0.3689608\ttotal: 12.6s\tremaining: 5.78s\n",
            "686:\tlearn: 0.3689012\ttotal: 12.6s\tremaining: 5.76s\n",
            "687:\tlearn: 0.3686843\ttotal: 12.7s\tremaining: 5.74s\n",
            "688:\tlearn: 0.3685606\ttotal: 12.7s\tremaining: 5.72s\n",
            "689:\tlearn: 0.3684357\ttotal: 12.7s\tremaining: 5.71s\n",
            "690:\tlearn: 0.3684308\ttotal: 12.7s\tremaining: 5.69s\n",
            "691:\tlearn: 0.3682920\ttotal: 12.7s\tremaining: 5.67s\n",
            "692:\tlearn: 0.3680763\ttotal: 12.7s\tremaining: 5.65s\n",
            "693:\tlearn: 0.3679324\ttotal: 12.8s\tremaining: 5.63s\n",
            "694:\tlearn: 0.3678538\ttotal: 12.8s\tremaining: 5.61s\n",
            "695:\tlearn: 0.3677395\ttotal: 12.8s\tremaining: 5.59s\n",
            "696:\tlearn: 0.3677061\ttotal: 12.8s\tremaining: 5.57s\n",
            "697:\tlearn: 0.3676853\ttotal: 12.8s\tremaining: 5.55s\n",
            "698:\tlearn: 0.3675366\ttotal: 12.9s\tremaining: 5.53s\n",
            "699:\tlearn: 0.3673675\ttotal: 12.9s\tremaining: 5.52s\n",
            "700:\tlearn: 0.3672463\ttotal: 12.9s\tremaining: 5.5s\n",
            "701:\tlearn: 0.3671416\ttotal: 12.9s\tremaining: 5.48s\n",
            "702:\tlearn: 0.3670326\ttotal: 12.9s\tremaining: 5.46s\n",
            "703:\tlearn: 0.3670298\ttotal: 12.9s\tremaining: 5.44s\n",
            "704:\tlearn: 0.3668599\ttotal: 13s\tremaining: 5.42s\n",
            "705:\tlearn: 0.3668292\ttotal: 13s\tremaining: 5.4s\n",
            "706:\tlearn: 0.3666598\ttotal: 13s\tremaining: 5.38s\n",
            "707:\tlearn: 0.3665446\ttotal: 13s\tremaining: 5.36s\n",
            "708:\tlearn: 0.3664551\ttotal: 13s\tremaining: 5.34s\n",
            "709:\tlearn: 0.3662634\ttotal: 13s\tremaining: 5.32s\n",
            "710:\tlearn: 0.3661447\ttotal: 13s\tremaining: 5.3s\n",
            "711:\tlearn: 0.3659595\ttotal: 13.1s\tremaining: 5.29s\n",
            "712:\tlearn: 0.3658009\ttotal: 13.1s\tremaining: 5.27s\n",
            "713:\tlearn: 0.3656044\ttotal: 13.1s\tremaining: 5.25s\n",
            "714:\tlearn: 0.3655271\ttotal: 13.1s\tremaining: 5.23s\n",
            "715:\tlearn: 0.3653423\ttotal: 13.1s\tremaining: 5.21s\n",
            "716:\tlearn: 0.3652625\ttotal: 13.2s\tremaining: 5.19s\n",
            "717:\tlearn: 0.3650492\ttotal: 13.2s\tremaining: 5.17s\n",
            "718:\tlearn: 0.3649931\ttotal: 13.2s\tremaining: 5.16s\n",
            "719:\tlearn: 0.3649273\ttotal: 13.2s\tremaining: 5.14s\n",
            "720:\tlearn: 0.3648519\ttotal: 13.3s\tremaining: 5.13s\n",
            "721:\tlearn: 0.3647358\ttotal: 13.3s\tremaining: 5.12s\n",
            "722:\tlearn: 0.3646255\ttotal: 13.3s\tremaining: 5.1s\n",
            "723:\tlearn: 0.3645172\ttotal: 13.3s\tremaining: 5.08s\n",
            "724:\tlearn: 0.3643813\ttotal: 13.4s\tremaining: 5.07s\n",
            "725:\tlearn: 0.3643034\ttotal: 13.4s\tremaining: 5.05s\n",
            "726:\tlearn: 0.3642793\ttotal: 13.4s\tremaining: 5.03s\n",
            "727:\tlearn: 0.3641035\ttotal: 13.4s\tremaining: 5.01s\n",
            "728:\tlearn: 0.3639871\ttotal: 13.4s\tremaining: 5s\n",
            "729:\tlearn: 0.3638694\ttotal: 13.5s\tremaining: 4.98s\n",
            "730:\tlearn: 0.3637441\ttotal: 13.5s\tremaining: 4.96s\n",
            "731:\tlearn: 0.3636145\ttotal: 13.5s\tremaining: 4.94s\n",
            "732:\tlearn: 0.3634984\ttotal: 13.5s\tremaining: 4.92s\n",
            "733:\tlearn: 0.3633575\ttotal: 13.5s\tremaining: 4.9s\n",
            "734:\tlearn: 0.3633113\ttotal: 13.5s\tremaining: 4.88s\n",
            "735:\tlearn: 0.3631810\ttotal: 13.6s\tremaining: 4.86s\n",
            "736:\tlearn: 0.3631019\ttotal: 13.6s\tremaining: 4.84s\n",
            "737:\tlearn: 0.3630378\ttotal: 13.6s\tremaining: 4.82s\n",
            "738:\tlearn: 0.3629875\ttotal: 13.6s\tremaining: 4.79s\n",
            "739:\tlearn: 0.3628700\ttotal: 13.6s\tremaining: 4.77s\n",
            "740:\tlearn: 0.3627433\ttotal: 13.6s\tremaining: 4.75s\n",
            "741:\tlearn: 0.3626297\ttotal: 13.6s\tremaining: 4.73s\n",
            "742:\tlearn: 0.3624888\ttotal: 13.6s\tremaining: 4.7s\n",
            "743:\tlearn: 0.3624550\ttotal: 13.6s\tremaining: 4.68s\n",
            "744:\tlearn: 0.3623490\ttotal: 13.6s\tremaining: 4.66s\n",
            "745:\tlearn: 0.3621777\ttotal: 13.6s\tremaining: 4.64s\n",
            "746:\tlearn: 0.3620604\ttotal: 13.6s\tremaining: 4.62s\n",
            "747:\tlearn: 0.3618968\ttotal: 13.6s\tremaining: 4.59s\n",
            "748:\tlearn: 0.3617791\ttotal: 13.7s\tremaining: 4.58s\n",
            "749:\tlearn: 0.3617208\ttotal: 13.7s\tremaining: 4.56s\n",
            "750:\tlearn: 0.3616338\ttotal: 13.7s\tremaining: 4.54s\n",
            "751:\tlearn: 0.3616302\ttotal: 13.7s\tremaining: 4.52s\n",
            "752:\tlearn: 0.3615187\ttotal: 13.7s\tremaining: 4.5s\n",
            "753:\tlearn: 0.3613489\ttotal: 13.7s\tremaining: 4.47s\n",
            "754:\tlearn: 0.3613369\ttotal: 13.7s\tremaining: 4.45s\n",
            "755:\tlearn: 0.3612996\ttotal: 13.7s\tremaining: 4.43s\n",
            "756:\tlearn: 0.3612254\ttotal: 13.7s\tremaining: 4.41s\n",
            "757:\tlearn: 0.3611899\ttotal: 13.7s\tremaining: 4.39s\n",
            "758:\tlearn: 0.3610319\ttotal: 13.8s\tremaining: 4.37s\n",
            "759:\tlearn: 0.3608700\ttotal: 13.8s\tremaining: 4.35s\n",
            "760:\tlearn: 0.3608553\ttotal: 13.8s\tremaining: 4.33s\n",
            "761:\tlearn: 0.3607393\ttotal: 13.8s\tremaining: 4.31s\n",
            "762:\tlearn: 0.3606380\ttotal: 13.8s\tremaining: 4.29s\n",
            "763:\tlearn: 0.3605723\ttotal: 13.8s\tremaining: 4.27s\n",
            "764:\tlearn: 0.3604659\ttotal: 13.9s\tremaining: 4.26s\n",
            "765:\tlearn: 0.3603669\ttotal: 13.9s\tremaining: 4.24s\n",
            "766:\tlearn: 0.3602421\ttotal: 13.9s\tremaining: 4.23s\n",
            "767:\tlearn: 0.3602120\ttotal: 13.9s\tremaining: 4.21s\n",
            "768:\tlearn: 0.3600670\ttotal: 14s\tremaining: 4.19s\n",
            "769:\tlearn: 0.3599255\ttotal: 14s\tremaining: 4.17s\n",
            "770:\tlearn: 0.3598734\ttotal: 14s\tremaining: 4.16s\n",
            "771:\tlearn: 0.3598198\ttotal: 14s\tremaining: 4.14s\n",
            "772:\tlearn: 0.3597749\ttotal: 14s\tremaining: 4.12s\n",
            "773:\tlearn: 0.3596127\ttotal: 14s\tremaining: 4.1s\n",
            "774:\tlearn: 0.3595493\ttotal: 14.1s\tremaining: 4.09s\n",
            "775:\tlearn: 0.3594216\ttotal: 14.1s\tremaining: 4.07s\n",
            "776:\tlearn: 0.3592836\ttotal: 14.1s\tremaining: 4.05s\n",
            "777:\tlearn: 0.3592637\ttotal: 14.1s\tremaining: 4.03s\n",
            "778:\tlearn: 0.3592597\ttotal: 14.1s\tremaining: 4.01s\n",
            "779:\tlearn: 0.3592278\ttotal: 14.1s\tremaining: 3.99s\n",
            "780:\tlearn: 0.3590500\ttotal: 14.2s\tremaining: 3.97s\n",
            "781:\tlearn: 0.3589741\ttotal: 14.2s\tremaining: 3.95s\n",
            "782:\tlearn: 0.3588403\ttotal: 14.2s\tremaining: 3.93s\n",
            "783:\tlearn: 0.3587299\ttotal: 14.2s\tremaining: 3.91s\n",
            "784:\tlearn: 0.3587058\ttotal: 14.2s\tremaining: 3.89s\n",
            "785:\tlearn: 0.3586758\ttotal: 14.2s\tremaining: 3.88s\n",
            "786:\tlearn: 0.3585394\ttotal: 14.2s\tremaining: 3.86s\n",
            "787:\tlearn: 0.3584535\ttotal: 14.3s\tremaining: 3.84s\n",
            "788:\tlearn: 0.3582608\ttotal: 14.3s\tremaining: 3.82s\n",
            "789:\tlearn: 0.3582529\ttotal: 14.3s\tremaining: 3.8s\n",
            "790:\tlearn: 0.3581574\ttotal: 14.3s\tremaining: 3.78s\n",
            "791:\tlearn: 0.3580875\ttotal: 14.3s\tremaining: 3.76s\n",
            "792:\tlearn: 0.3579331\ttotal: 14.3s\tremaining: 3.75s\n",
            "793:\tlearn: 0.3577962\ttotal: 14.4s\tremaining: 3.73s\n",
            "794:\tlearn: 0.3577333\ttotal: 14.4s\tremaining: 3.71s\n",
            "795:\tlearn: 0.3576445\ttotal: 14.4s\tremaining: 3.69s\n",
            "796:\tlearn: 0.3575976\ttotal: 14.4s\tremaining: 3.67s\n",
            "797:\tlearn: 0.3575508\ttotal: 14.4s\tremaining: 3.65s\n",
            "798:\tlearn: 0.3575203\ttotal: 14.5s\tremaining: 3.63s\n",
            "799:\tlearn: 0.3573712\ttotal: 14.5s\tremaining: 3.62s\n",
            "800:\tlearn: 0.3572439\ttotal: 14.5s\tremaining: 3.6s\n",
            "801:\tlearn: 0.3572356\ttotal: 14.5s\tremaining: 3.58s\n",
            "802:\tlearn: 0.3570799\ttotal: 14.5s\tremaining: 3.57s\n",
            "803:\tlearn: 0.3570133\ttotal: 14.6s\tremaining: 3.55s\n",
            "804:\tlearn: 0.3569681\ttotal: 14.6s\tremaining: 3.53s\n",
            "805:\tlearn: 0.3568661\ttotal: 14.6s\tremaining: 3.52s\n",
            "806:\tlearn: 0.3567161\ttotal: 14.6s\tremaining: 3.49s\n",
            "807:\tlearn: 0.3566409\ttotal: 14.6s\tremaining: 3.47s\n",
            "808:\tlearn: 0.3564202\ttotal: 14.6s\tremaining: 3.45s\n",
            "809:\tlearn: 0.3562900\ttotal: 14.6s\tremaining: 3.43s\n",
            "810:\tlearn: 0.3561775\ttotal: 14.6s\tremaining: 3.41s\n",
            "811:\tlearn: 0.3560841\ttotal: 14.7s\tremaining: 3.39s\n",
            "812:\tlearn: 0.3559727\ttotal: 14.7s\tremaining: 3.37s\n",
            "813:\tlearn: 0.3558392\ttotal: 14.7s\tremaining: 3.35s\n",
            "814:\tlearn: 0.3557226\ttotal: 14.7s\tremaining: 3.33s\n",
            "815:\tlearn: 0.3555888\ttotal: 14.7s\tremaining: 3.32s\n",
            "816:\tlearn: 0.3554633\ttotal: 14.7s\tremaining: 3.3s\n",
            "817:\tlearn: 0.3554423\ttotal: 14.7s\tremaining: 3.28s\n",
            "818:\tlearn: 0.3554284\ttotal: 14.8s\tremaining: 3.26s\n",
            "819:\tlearn: 0.3553349\ttotal: 14.8s\tremaining: 3.24s\n",
            "820:\tlearn: 0.3552165\ttotal: 14.8s\tremaining: 3.22s\n",
            "821:\tlearn: 0.3551985\ttotal: 14.8s\tremaining: 3.2s\n",
            "822:\tlearn: 0.3551034\ttotal: 14.8s\tremaining: 3.18s\n",
            "823:\tlearn: 0.3550213\ttotal: 14.8s\tremaining: 3.16s\n",
            "824:\tlearn: 0.3549679\ttotal: 14.8s\tremaining: 3.14s\n",
            "825:\tlearn: 0.3548460\ttotal: 14.8s\tremaining: 3.12s\n",
            "826:\tlearn: 0.3548046\ttotal: 14.8s\tremaining: 3.1s\n",
            "827:\tlearn: 0.3547895\ttotal: 14.8s\tremaining: 3.08s\n",
            "828:\tlearn: 0.3547276\ttotal: 14.8s\tremaining: 3.06s\n",
            "829:\tlearn: 0.3545986\ttotal: 14.8s\tremaining: 3.04s\n",
            "830:\tlearn: 0.3544711\ttotal: 14.9s\tremaining: 3.03s\n",
            "831:\tlearn: 0.3543802\ttotal: 14.9s\tremaining: 3.01s\n",
            "832:\tlearn: 0.3543580\ttotal: 14.9s\tremaining: 2.99s\n",
            "833:\tlearn: 0.3542934\ttotal: 14.9s\tremaining: 2.97s\n",
            "834:\tlearn: 0.3541711\ttotal: 14.9s\tremaining: 2.95s\n",
            "835:\tlearn: 0.3540967\ttotal: 14.9s\tremaining: 2.93s\n",
            "836:\tlearn: 0.3539463\ttotal: 14.9s\tremaining: 2.91s\n",
            "837:\tlearn: 0.3538818\ttotal: 15s\tremaining: 2.89s\n",
            "838:\tlearn: 0.3538721\ttotal: 15s\tremaining: 2.87s\n",
            "839:\tlearn: 0.3538662\ttotal: 15s\tremaining: 2.85s\n",
            "840:\tlearn: 0.3538113\ttotal: 15s\tremaining: 2.83s\n",
            "841:\tlearn: 0.3537268\ttotal: 15s\tremaining: 2.81s\n",
            "842:\tlearn: 0.3536687\ttotal: 15s\tremaining: 2.79s\n",
            "843:\tlearn: 0.3535447\ttotal: 15s\tremaining: 2.77s\n",
            "844:\tlearn: 0.3533544\ttotal: 15s\tremaining: 2.75s\n",
            "845:\tlearn: 0.3532550\ttotal: 15s\tremaining: 2.73s\n",
            "846:\tlearn: 0.3532338\ttotal: 15.1s\tremaining: 2.72s\n",
            "847:\tlearn: 0.3530941\ttotal: 15.1s\tremaining: 2.71s\n",
            "848:\tlearn: 0.3530895\ttotal: 15.1s\tremaining: 2.69s\n",
            "849:\tlearn: 0.3530747\ttotal: 15.1s\tremaining: 2.67s\n",
            "850:\tlearn: 0.3529778\ttotal: 15.2s\tremaining: 2.66s\n",
            "851:\tlearn: 0.3528745\ttotal: 15.2s\tremaining: 2.64s\n",
            "852:\tlearn: 0.3528474\ttotal: 15.2s\tremaining: 2.62s\n",
            "853:\tlearn: 0.3528305\ttotal: 15.2s\tremaining: 2.6s\n",
            "854:\tlearn: 0.3528103\ttotal: 15.3s\tremaining: 2.59s\n",
            "855:\tlearn: 0.3527598\ttotal: 15.3s\tremaining: 2.57s\n",
            "856:\tlearn: 0.3526568\ttotal: 15.3s\tremaining: 2.55s\n",
            "857:\tlearn: 0.3525830\ttotal: 15.3s\tremaining: 2.54s\n",
            "858:\tlearn: 0.3524780\ttotal: 15.3s\tremaining: 2.52s\n",
            "859:\tlearn: 0.3524448\ttotal: 15.4s\tremaining: 2.5s\n",
            "860:\tlearn: 0.3523180\ttotal: 15.4s\tremaining: 2.48s\n",
            "861:\tlearn: 0.3522249\ttotal: 15.4s\tremaining: 2.46s\n",
            "862:\tlearn: 0.3521368\ttotal: 15.4s\tremaining: 2.45s\n",
            "863:\tlearn: 0.3520225\ttotal: 15.4s\tremaining: 2.43s\n",
            "864:\tlearn: 0.3518876\ttotal: 15.4s\tremaining: 2.41s\n",
            "865:\tlearn: 0.3518493\ttotal: 15.4s\tremaining: 2.39s\n",
            "866:\tlearn: 0.3517822\ttotal: 15.5s\tremaining: 2.37s\n",
            "867:\tlearn: 0.3516851\ttotal: 15.5s\tremaining: 2.35s\n",
            "868:\tlearn: 0.3515331\ttotal: 15.5s\tremaining: 2.34s\n",
            "869:\tlearn: 0.3514124\ttotal: 15.5s\tremaining: 2.32s\n",
            "870:\tlearn: 0.3513680\ttotal: 15.5s\tremaining: 2.3s\n",
            "871:\tlearn: 0.3511997\ttotal: 15.6s\tremaining: 2.28s\n",
            "872:\tlearn: 0.3510851\ttotal: 15.6s\tremaining: 2.26s\n",
            "873:\tlearn: 0.3509544\ttotal: 15.6s\tremaining: 2.25s\n",
            "874:\tlearn: 0.3508462\ttotal: 15.6s\tremaining: 2.23s\n",
            "875:\tlearn: 0.3507360\ttotal: 15.6s\tremaining: 2.21s\n",
            "876:\tlearn: 0.3505959\ttotal: 15.6s\tremaining: 2.19s\n",
            "877:\tlearn: 0.3505540\ttotal: 15.6s\tremaining: 2.17s\n",
            "878:\tlearn: 0.3504407\ttotal: 15.7s\tremaining: 2.15s\n",
            "879:\tlearn: 0.3503773\ttotal: 15.7s\tremaining: 2.14s\n",
            "880:\tlearn: 0.3502153\ttotal: 15.7s\tremaining: 2.12s\n",
            "881:\tlearn: 0.3501074\ttotal: 15.7s\tremaining: 2.1s\n",
            "882:\tlearn: 0.3500888\ttotal: 15.7s\tremaining: 2.08s\n",
            "883:\tlearn: 0.3500025\ttotal: 15.7s\tremaining: 2.07s\n",
            "884:\tlearn: 0.3499796\ttotal: 15.8s\tremaining: 2.05s\n",
            "885:\tlearn: 0.3499062\ttotal: 15.8s\tremaining: 2.03s\n",
            "886:\tlearn: 0.3498201\ttotal: 15.8s\tremaining: 2.01s\n",
            "887:\tlearn: 0.3497412\ttotal: 15.8s\tremaining: 1.99s\n",
            "888:\tlearn: 0.3496186\ttotal: 15.8s\tremaining: 1.97s\n",
            "889:\tlearn: 0.3495697\ttotal: 15.8s\tremaining: 1.95s\n",
            "890:\tlearn: 0.3494326\ttotal: 15.8s\tremaining: 1.93s\n",
            "891:\tlearn: 0.3493285\ttotal: 15.8s\tremaining: 1.92s\n",
            "892:\tlearn: 0.3492760\ttotal: 15.8s\tremaining: 1.9s\n",
            "893:\tlearn: 0.3491933\ttotal: 15.8s\tremaining: 1.88s\n",
            "894:\tlearn: 0.3491372\ttotal: 15.8s\tremaining: 1.86s\n",
            "895:\tlearn: 0.3490058\ttotal: 15.9s\tremaining: 1.84s\n",
            "896:\tlearn: 0.3489732\ttotal: 15.9s\tremaining: 1.82s\n",
            "897:\tlearn: 0.3488632\ttotal: 15.9s\tremaining: 1.8s\n",
            "898:\tlearn: 0.3487542\ttotal: 15.9s\tremaining: 1.79s\n",
            "899:\tlearn: 0.3486416\ttotal: 15.9s\tremaining: 1.77s\n",
            "900:\tlearn: 0.3485416\ttotal: 16s\tremaining: 1.75s\n",
            "901:\tlearn: 0.3483796\ttotal: 16s\tremaining: 1.73s\n",
            "902:\tlearn: 0.3482561\ttotal: 16s\tremaining: 1.72s\n",
            "903:\tlearn: 0.3481611\ttotal: 16s\tremaining: 1.7s\n",
            "904:\tlearn: 0.3480683\ttotal: 16s\tremaining: 1.68s\n",
            "905:\tlearn: 0.3479984\ttotal: 16s\tremaining: 1.66s\n",
            "906:\tlearn: 0.3479263\ttotal: 16s\tremaining: 1.64s\n",
            "907:\tlearn: 0.3478592\ttotal: 16s\tremaining: 1.62s\n",
            "908:\tlearn: 0.3477252\ttotal: 16s\tremaining: 1.6s\n",
            "909:\tlearn: 0.3476465\ttotal: 16s\tremaining: 1.59s\n",
            "910:\tlearn: 0.3475260\ttotal: 16.1s\tremaining: 1.57s\n",
            "911:\tlearn: 0.3473753\ttotal: 16.1s\tremaining: 1.55s\n",
            "912:\tlearn: 0.3473566\ttotal: 16.1s\tremaining: 1.53s\n",
            "913:\tlearn: 0.3471562\ttotal: 16.1s\tremaining: 1.51s\n",
            "914:\tlearn: 0.3470535\ttotal: 16.1s\tremaining: 1.49s\n",
            "915:\tlearn: 0.3469411\ttotal: 16.1s\tremaining: 1.48s\n",
            "916:\tlearn: 0.3468687\ttotal: 16.1s\tremaining: 1.46s\n",
            "917:\tlearn: 0.3467691\ttotal: 16.1s\tremaining: 1.44s\n",
            "918:\tlearn: 0.3467369\ttotal: 16.1s\tremaining: 1.42s\n",
            "919:\tlearn: 0.3466801\ttotal: 16.1s\tremaining: 1.4s\n",
            "920:\tlearn: 0.3466012\ttotal: 16.1s\tremaining: 1.39s\n",
            "921:\tlearn: 0.3465993\ttotal: 16.2s\tremaining: 1.37s\n",
            "922:\tlearn: 0.3464895\ttotal: 16.2s\tremaining: 1.35s\n",
            "923:\tlearn: 0.3463882\ttotal: 16.2s\tremaining: 1.33s\n",
            "924:\tlearn: 0.3463060\ttotal: 16.2s\tremaining: 1.31s\n",
            "925:\tlearn: 0.3462904\ttotal: 16.2s\tremaining: 1.29s\n",
            "926:\tlearn: 0.3462016\ttotal: 16.2s\tremaining: 1.27s\n",
            "927:\tlearn: 0.3460998\ttotal: 16.2s\tremaining: 1.26s\n",
            "928:\tlearn: 0.3460943\ttotal: 16.2s\tremaining: 1.24s\n",
            "929:\tlearn: 0.3460291\ttotal: 16.2s\tremaining: 1.22s\n",
            "930:\tlearn: 0.3459209\ttotal: 16.2s\tremaining: 1.2s\n",
            "931:\tlearn: 0.3458102\ttotal: 16.3s\tremaining: 1.19s\n",
            "932:\tlearn: 0.3457294\ttotal: 16.3s\tremaining: 1.17s\n",
            "933:\tlearn: 0.3456823\ttotal: 16.3s\tremaining: 1.15s\n",
            "934:\tlearn: 0.3455099\ttotal: 16.3s\tremaining: 1.13s\n",
            "935:\tlearn: 0.3454096\ttotal: 16.3s\tremaining: 1.12s\n",
            "936:\tlearn: 0.3453132\ttotal: 16.3s\tremaining: 1.1s\n",
            "937:\tlearn: 0.3452372\ttotal: 16.4s\tremaining: 1.08s\n",
            "938:\tlearn: 0.3452338\ttotal: 16.4s\tremaining: 1.06s\n",
            "939:\tlearn: 0.3451599\ttotal: 16.4s\tremaining: 1.04s\n",
            "940:\tlearn: 0.3451356\ttotal: 16.4s\tremaining: 1.03s\n",
            "941:\tlearn: 0.3450337\ttotal: 16.4s\tremaining: 1.01s\n",
            "942:\tlearn: 0.3448937\ttotal: 16.4s\tremaining: 994ms\n",
            "943:\tlearn: 0.3447617\ttotal: 16.5s\tremaining: 977ms\n",
            "944:\tlearn: 0.3446550\ttotal: 16.5s\tremaining: 959ms\n",
            "945:\tlearn: 0.3446222\ttotal: 16.5s\tremaining: 942ms\n",
            "946:\tlearn: 0.3446123\ttotal: 16.5s\tremaining: 925ms\n",
            "947:\tlearn: 0.3444741\ttotal: 16.5s\tremaining: 907ms\n",
            "948:\tlearn: 0.3444263\ttotal: 16.6s\tremaining: 889ms\n",
            "949:\tlearn: 0.3443195\ttotal: 16.6s\tremaining: 872ms\n",
            "950:\tlearn: 0.3441424\ttotal: 16.6s\tremaining: 855ms\n",
            "951:\tlearn: 0.3440755\ttotal: 16.6s\tremaining: 837ms\n",
            "952:\tlearn: 0.3439823\ttotal: 16.6s\tremaining: 820ms\n",
            "953:\tlearn: 0.3438494\ttotal: 16.6s\tremaining: 802ms\n",
            "954:\tlearn: 0.3437090\ttotal: 16.7s\tremaining: 785ms\n",
            "955:\tlearn: 0.3436352\ttotal: 16.7s\tremaining: 767ms\n",
            "956:\tlearn: 0.3435726\ttotal: 16.7s\tremaining: 750ms\n",
            "957:\tlearn: 0.3434437\ttotal: 16.7s\tremaining: 732ms\n",
            "958:\tlearn: 0.3433990\ttotal: 16.7s\tremaining: 715ms\n",
            "959:\tlearn: 0.3433560\ttotal: 16.7s\tremaining: 698ms\n",
            "960:\tlearn: 0.3432259\ttotal: 16.8s\tremaining: 680ms\n",
            "961:\tlearn: 0.3431929\ttotal: 16.8s\tremaining: 663ms\n",
            "962:\tlearn: 0.3431276\ttotal: 16.8s\tremaining: 646ms\n",
            "963:\tlearn: 0.3430407\ttotal: 16.8s\tremaining: 628ms\n",
            "964:\tlearn: 0.3429810\ttotal: 16.8s\tremaining: 611ms\n",
            "965:\tlearn: 0.3428659\ttotal: 16.9s\tremaining: 593ms\n",
            "966:\tlearn: 0.3427669\ttotal: 16.9s\tremaining: 576ms\n",
            "967:\tlearn: 0.3427084\ttotal: 16.9s\tremaining: 559ms\n",
            "968:\tlearn: 0.3426299\ttotal: 16.9s\tremaining: 541ms\n",
            "969:\tlearn: 0.3425898\ttotal: 16.9s\tremaining: 524ms\n",
            "970:\tlearn: 0.3425067\ttotal: 17s\tremaining: 506ms\n",
            "971:\tlearn: 0.3423890\ttotal: 17s\tremaining: 489ms\n",
            "972:\tlearn: 0.3423821\ttotal: 17s\tremaining: 471ms\n",
            "973:\tlearn: 0.3422607\ttotal: 17s\tremaining: 454ms\n",
            "974:\tlearn: 0.3421625\ttotal: 17s\tremaining: 436ms\n",
            "975:\tlearn: 0.3420757\ttotal: 17s\tremaining: 419ms\n",
            "976:\tlearn: 0.3419441\ttotal: 17.1s\tremaining: 401ms\n",
            "977:\tlearn: 0.3418326\ttotal: 17.1s\tremaining: 384ms\n",
            "978:\tlearn: 0.3417440\ttotal: 17.1s\tremaining: 366ms\n",
            "979:\tlearn: 0.3416393\ttotal: 17.1s\tremaining: 349ms\n",
            "980:\tlearn: 0.3415708\ttotal: 17.1s\tremaining: 331ms\n",
            "981:\tlearn: 0.3414706\ttotal: 17.1s\tremaining: 314ms\n",
            "982:\tlearn: 0.3413558\ttotal: 17.2s\tremaining: 297ms\n",
            "983:\tlearn: 0.3413519\ttotal: 17.2s\tremaining: 279ms\n",
            "984:\tlearn: 0.3412227\ttotal: 17.2s\tremaining: 262ms\n",
            "985:\tlearn: 0.3411772\ttotal: 17.2s\tremaining: 244ms\n",
            "986:\tlearn: 0.3410755\ttotal: 17.2s\tremaining: 227ms\n",
            "987:\tlearn: 0.3409934\ttotal: 17.2s\tremaining: 209ms\n",
            "988:\tlearn: 0.3409073\ttotal: 17.3s\tremaining: 192ms\n",
            "989:\tlearn: 0.3408053\ttotal: 17.3s\tremaining: 174ms\n",
            "990:\tlearn: 0.3406992\ttotal: 17.3s\tremaining: 157ms\n",
            "991:\tlearn: 0.3406413\ttotal: 17.3s\tremaining: 140ms\n",
            "992:\tlearn: 0.3405932\ttotal: 17.3s\tremaining: 122ms\n",
            "993:\tlearn: 0.3405013\ttotal: 17.4s\tremaining: 105ms\n",
            "994:\tlearn: 0.3404399\ttotal: 17.4s\tremaining: 87.2ms\n",
            "995:\tlearn: 0.3403582\ttotal: 17.4s\tremaining: 69.8ms\n",
            "996:\tlearn: 0.3403342\ttotal: 17.4s\tremaining: 52.3ms\n",
            "997:\tlearn: 0.3402613\ttotal: 17.4s\tremaining: 34.8ms\n",
            "998:\tlearn: 0.3401798\ttotal: 17.4s\tremaining: 17.4ms\n",
            "999:\tlearn: 0.3400494\ttotal: 17.4s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7c3155852170>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model1 = CatBoostClassifier()\n",
        "model1.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:34.448406Z",
          "end_time": "2024-05-14T18:10:36.819352Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AssWEg8lLVZ",
        "outputId": "b16fe155-6e9b-4c49-c91d-7a021511c555"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.437\n",
            "R2: 0.171\n",
            "ROC AUC: 0.79055\n",
            "Score: 0.8093\n",
            "Local Score: 0.8619\n",
            "Best params:  {}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:10:38.645935Z",
          "end_time": "2024-05-14T18:10:38.711635Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24a_U7glLVZ",
        "outputId": "2635a365-a51d-4fd9-ffea-b007dbf4cce5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6763611\ttotal: 120ms\tremaining: 5m 59s\n",
            "1:\tlearn: 0.6600647\ttotal: 224ms\tremaining: 5m 35s\n",
            "2:\tlearn: 0.6467230\ttotal: 330ms\tremaining: 5m 29s\n",
            "3:\tlearn: 0.6336712\ttotal: 462ms\tremaining: 5m 45s\n",
            "4:\tlearn: 0.6213586\ttotal: 527ms\tremaining: 5m 15s\n",
            "5:\tlearn: 0.6101490\ttotal: 607ms\tremaining: 5m 2s\n",
            "6:\tlearn: 0.5996576\ttotal: 726ms\tremaining: 5m 10s\n",
            "7:\tlearn: 0.5897288\ttotal: 853ms\tremaining: 5m 18s\n",
            "8:\tlearn: 0.5812944\ttotal: 1.01s\tremaining: 5m 36s\n",
            "9:\tlearn: 0.5724540\ttotal: 1.12s\tremaining: 5m 35s\n",
            "10:\tlearn: 0.5644483\ttotal: 1.24s\tremaining: 5m 35s\n",
            "11:\tlearn: 0.5575787\ttotal: 1.36s\tremaining: 5m 39s\n",
            "12:\tlearn: 0.5508336\ttotal: 1.47s\tremaining: 5m 37s\n",
            "13:\tlearn: 0.5444774\ttotal: 1.55s\tremaining: 5m 31s\n",
            "14:\tlearn: 0.5385806\ttotal: 1.7s\tremaining: 5m 37s\n",
            "15:\tlearn: 0.5329335\ttotal: 1.87s\tremaining: 5m 48s\n",
            "16:\tlearn: 0.5274868\ttotal: 1.98s\tremaining: 5m 48s\n",
            "17:\tlearn: 0.5224734\ttotal: 2.11s\tremaining: 5m 50s\n",
            "18:\tlearn: 0.5183725\ttotal: 2.28s\tremaining: 5m 58s\n",
            "19:\tlearn: 0.5142797\ttotal: 2.45s\tremaining: 6m 5s\n",
            "20:\tlearn: 0.5098897\ttotal: 2.63s\tremaining: 6m 13s\n",
            "21:\tlearn: 0.5061516\ttotal: 2.79s\tremaining: 6m 17s\n",
            "22:\tlearn: 0.5027040\ttotal: 2.88s\tremaining: 6m 12s\n",
            "23:\tlearn: 0.4991082\ttotal: 2.98s\tremaining: 6m 9s\n",
            "24:\tlearn: 0.4959836\ttotal: 3.08s\tremaining: 6m 6s\n",
            "25:\tlearn: 0.4928964\ttotal: 3.17s\tremaining: 6m 3s\n",
            "26:\tlearn: 0.4901151\ttotal: 3.24s\tremaining: 5m 57s\n",
            "27:\tlearn: 0.4874680\ttotal: 3.32s\tremaining: 5m 52s\n",
            "28:\tlearn: 0.4850041\ttotal: 3.44s\tremaining: 5m 52s\n",
            "29:\tlearn: 0.4826614\ttotal: 3.57s\tremaining: 5m 53s\n",
            "30:\tlearn: 0.4805188\ttotal: 3.69s\tremaining: 5m 53s\n",
            "31:\tlearn: 0.4784867\ttotal: 3.81s\tremaining: 5m 53s\n",
            "32:\tlearn: 0.4764148\ttotal: 3.94s\tremaining: 5m 53s\n",
            "33:\tlearn: 0.4744901\ttotal: 4.07s\tremaining: 5m 54s\n",
            "34:\tlearn: 0.4724879\ttotal: 4.2s\tremaining: 5m 56s\n",
            "35:\tlearn: 0.4707341\ttotal: 4.26s\tremaining: 5m 50s\n",
            "36:\tlearn: 0.4689635\ttotal: 4.32s\tremaining: 5m 46s\n",
            "37:\tlearn: 0.4672882\ttotal: 4.38s\tremaining: 5m 41s\n",
            "38:\tlearn: 0.4658724\ttotal: 4.44s\tremaining: 5m 37s\n",
            "39:\tlearn: 0.4646642\ttotal: 4.5s\tremaining: 5m 32s\n",
            "40:\tlearn: 0.4633218\ttotal: 4.55s\tremaining: 5m 28s\n",
            "41:\tlearn: 0.4619083\ttotal: 4.6s\tremaining: 5m 24s\n",
            "42:\tlearn: 0.4607082\ttotal: 4.63s\tremaining: 5m 18s\n",
            "43:\tlearn: 0.4598178\ttotal: 4.66s\tremaining: 5m 12s\n",
            "44:\tlearn: 0.4586781\ttotal: 4.68s\tremaining: 5m 7s\n",
            "45:\tlearn: 0.4576996\ttotal: 4.71s\tremaining: 5m 2s\n",
            "46:\tlearn: 0.4566114\ttotal: 4.74s\tremaining: 4m 57s\n",
            "47:\tlearn: 0.4556801\ttotal: 4.77s\tremaining: 4m 53s\n",
            "48:\tlearn: 0.4546558\ttotal: 4.8s\tremaining: 4m 49s\n",
            "49:\tlearn: 0.4537776\ttotal: 4.83s\tremaining: 4m 45s\n",
            "50:\tlearn: 0.4529147\ttotal: 4.87s\tremaining: 4m 41s\n",
            "51:\tlearn: 0.4522256\ttotal: 4.9s\tremaining: 4m 37s\n",
            "52:\tlearn: 0.4515605\ttotal: 4.94s\tremaining: 4m 34s\n",
            "53:\tlearn: 0.4508980\ttotal: 4.97s\tremaining: 4m 31s\n",
            "54:\tlearn: 0.4501687\ttotal: 5s\tremaining: 4m 27s\n",
            "55:\tlearn: 0.4494700\ttotal: 5.03s\tremaining: 4m 24s\n",
            "56:\tlearn: 0.4488574\ttotal: 5.06s\tremaining: 4m 21s\n",
            "57:\tlearn: 0.4482352\ttotal: 5.09s\tremaining: 4m 18s\n",
            "58:\tlearn: 0.4475158\ttotal: 5.12s\tremaining: 4m 15s\n",
            "59:\tlearn: 0.4469746\ttotal: 5.15s\tremaining: 4m 12s\n",
            "60:\tlearn: 0.4463655\ttotal: 5.17s\tremaining: 4m 9s\n",
            "61:\tlearn: 0.4457515\ttotal: 5.2s\tremaining: 4m 6s\n",
            "62:\tlearn: 0.4451452\ttotal: 5.24s\tremaining: 4m 4s\n",
            "63:\tlearn: 0.4448311\ttotal: 5.27s\tremaining: 4m 1s\n",
            "64:\tlearn: 0.4441956\ttotal: 5.29s\tremaining: 3m 59s\n",
            "65:\tlearn: 0.4436782\ttotal: 5.33s\tremaining: 3m 56s\n",
            "66:\tlearn: 0.4431133\ttotal: 5.35s\tremaining: 3m 54s\n",
            "67:\tlearn: 0.4427448\ttotal: 5.38s\tremaining: 3m 52s\n",
            "68:\tlearn: 0.4422487\ttotal: 5.41s\tremaining: 3m 49s\n",
            "69:\tlearn: 0.4418351\ttotal: 5.44s\tremaining: 3m 47s\n",
            "70:\tlearn: 0.4414165\ttotal: 5.48s\tremaining: 3m 46s\n",
            "71:\tlearn: 0.4410222\ttotal: 5.53s\tremaining: 3m 45s\n",
            "72:\tlearn: 0.4405719\ttotal: 5.59s\tremaining: 3m 44s\n",
            "73:\tlearn: 0.4401827\ttotal: 5.64s\tremaining: 3m 42s\n",
            "74:\tlearn: 0.4398382\ttotal: 5.69s\tremaining: 3m 41s\n",
            "75:\tlearn: 0.4394761\ttotal: 5.74s\tremaining: 3m 40s\n",
            "76:\tlearn: 0.4392563\ttotal: 5.78s\tremaining: 3m 39s\n",
            "77:\tlearn: 0.4389876\ttotal: 5.84s\tremaining: 3m 38s\n",
            "78:\tlearn: 0.4386788\ttotal: 5.89s\tremaining: 3m 37s\n",
            "79:\tlearn: 0.4384373\ttotal: 5.95s\tremaining: 3m 37s\n",
            "80:\tlearn: 0.4381540\ttotal: 6.01s\tremaining: 3m 36s\n",
            "81:\tlearn: 0.4377660\ttotal: 6.06s\tremaining: 3m 35s\n",
            "82:\tlearn: 0.4373967\ttotal: 6.09s\tremaining: 3m 34s\n",
            "83:\tlearn: 0.4370823\ttotal: 6.14s\tremaining: 3m 33s\n",
            "84:\tlearn: 0.4368902\ttotal: 6.18s\tremaining: 3m 32s\n",
            "85:\tlearn: 0.4366253\ttotal: 6.21s\tremaining: 3m 30s\n",
            "86:\tlearn: 0.4363257\ttotal: 6.24s\tremaining: 3m 29s\n",
            "87:\tlearn: 0.4360387\ttotal: 6.28s\tremaining: 3m 27s\n",
            "88:\tlearn: 0.4357634\ttotal: 6.31s\tremaining: 3m 26s\n",
            "89:\tlearn: 0.4354908\ttotal: 6.33s\tremaining: 3m 24s\n",
            "90:\tlearn: 0.4352133\ttotal: 6.36s\tremaining: 3m 23s\n",
            "91:\tlearn: 0.4348745\ttotal: 6.39s\tremaining: 3m 22s\n",
            "92:\tlearn: 0.4346786\ttotal: 6.42s\tremaining: 3m 20s\n",
            "93:\tlearn: 0.4344856\ttotal: 6.44s\tremaining: 3m 19s\n",
            "94:\tlearn: 0.4343376\ttotal: 6.47s\tremaining: 3m 17s\n",
            "95:\tlearn: 0.4340825\ttotal: 6.5s\tremaining: 3m 16s\n",
            "96:\tlearn: 0.4338244\ttotal: 6.53s\tremaining: 3m 15s\n",
            "97:\tlearn: 0.4335355\ttotal: 6.57s\tremaining: 3m 14s\n",
            "98:\tlearn: 0.4332850\ttotal: 6.59s\tremaining: 3m 13s\n",
            "99:\tlearn: 0.4330829\ttotal: 6.62s\tremaining: 3m 12s\n",
            "100:\tlearn: 0.4329635\ttotal: 6.65s\tremaining: 3m 10s\n",
            "101:\tlearn: 0.4327133\ttotal: 6.68s\tremaining: 3m 9s\n",
            "102:\tlearn: 0.4324653\ttotal: 6.72s\tremaining: 3m 8s\n",
            "103:\tlearn: 0.4323107\ttotal: 6.75s\tremaining: 3m 7s\n",
            "104:\tlearn: 0.4320621\ttotal: 6.77s\tremaining: 3m 6s\n",
            "105:\tlearn: 0.4318876\ttotal: 6.8s\tremaining: 3m 5s\n",
            "106:\tlearn: 0.4317346\ttotal: 6.83s\tremaining: 3m 4s\n",
            "107:\tlearn: 0.4314855\ttotal: 6.86s\tremaining: 3m 3s\n",
            "108:\tlearn: 0.4313448\ttotal: 6.88s\tremaining: 3m 2s\n",
            "109:\tlearn: 0.4311170\ttotal: 6.91s\tremaining: 3m 1s\n",
            "110:\tlearn: 0.4309701\ttotal: 6.95s\tremaining: 3m\n",
            "111:\tlearn: 0.4307914\ttotal: 6.98s\tremaining: 2m 59s\n",
            "112:\tlearn: 0.4306403\ttotal: 7.02s\tremaining: 2m 59s\n",
            "113:\tlearn: 0.4304696\ttotal: 7.05s\tremaining: 2m 58s\n",
            "114:\tlearn: 0.4303406\ttotal: 7.08s\tremaining: 2m 57s\n",
            "115:\tlearn: 0.4301410\ttotal: 7.11s\tremaining: 2m 56s\n",
            "116:\tlearn: 0.4299611\ttotal: 7.14s\tremaining: 2m 55s\n",
            "117:\tlearn: 0.4298028\ttotal: 7.17s\tremaining: 2m 55s\n",
            "118:\tlearn: 0.4296182\ttotal: 7.2s\tremaining: 2m 54s\n",
            "119:\tlearn: 0.4295476\ttotal: 7.23s\tremaining: 2m 53s\n",
            "120:\tlearn: 0.4294113\ttotal: 7.25s\tremaining: 2m 52s\n",
            "121:\tlearn: 0.4292621\ttotal: 7.29s\tremaining: 2m 51s\n",
            "122:\tlearn: 0.4290982\ttotal: 7.32s\tremaining: 2m 51s\n",
            "123:\tlearn: 0.4289748\ttotal: 7.34s\tremaining: 2m 50s\n",
            "124:\tlearn: 0.4288130\ttotal: 7.38s\tremaining: 2m 49s\n",
            "125:\tlearn: 0.4286876\ttotal: 7.41s\tremaining: 2m 49s\n",
            "126:\tlearn: 0.4285173\ttotal: 7.44s\tremaining: 2m 48s\n",
            "127:\tlearn: 0.4283399\ttotal: 7.46s\tremaining: 2m 47s\n",
            "128:\tlearn: 0.4281794\ttotal: 7.49s\tremaining: 2m 46s\n",
            "129:\tlearn: 0.4280465\ttotal: 7.52s\tremaining: 2m 46s\n",
            "130:\tlearn: 0.4277160\ttotal: 7.55s\tremaining: 2m 45s\n",
            "131:\tlearn: 0.4275294\ttotal: 7.58s\tremaining: 2m 44s\n",
            "132:\tlearn: 0.4274411\ttotal: 7.61s\tremaining: 2m 44s\n",
            "133:\tlearn: 0.4272705\ttotal: 7.64s\tremaining: 2m 43s\n",
            "134:\tlearn: 0.4271062\ttotal: 7.67s\tremaining: 2m 42s\n",
            "135:\tlearn: 0.4270104\ttotal: 7.7s\tremaining: 2m 42s\n",
            "136:\tlearn: 0.4269225\ttotal: 7.73s\tremaining: 2m 41s\n",
            "137:\tlearn: 0.4267055\ttotal: 7.76s\tremaining: 2m 40s\n",
            "138:\tlearn: 0.4265857\ttotal: 7.8s\tremaining: 2m 40s\n",
            "139:\tlearn: 0.4264665\ttotal: 7.82s\tremaining: 2m 39s\n",
            "140:\tlearn: 0.4263461\ttotal: 7.85s\tremaining: 2m 39s\n",
            "141:\tlearn: 0.4262123\ttotal: 7.88s\tremaining: 2m 38s\n",
            "142:\tlearn: 0.4260808\ttotal: 7.91s\tremaining: 2m 37s\n",
            "143:\tlearn: 0.4260361\ttotal: 7.93s\tremaining: 2m 37s\n",
            "144:\tlearn: 0.4259487\ttotal: 7.96s\tremaining: 2m 36s\n",
            "145:\tlearn: 0.4257668\ttotal: 7.99s\tremaining: 2m 36s\n",
            "146:\tlearn: 0.4255699\ttotal: 8.03s\tremaining: 2m 35s\n",
            "147:\tlearn: 0.4253651\ttotal: 8.07s\tremaining: 2m 35s\n",
            "148:\tlearn: 0.4251336\ttotal: 8.09s\tremaining: 2m 34s\n",
            "149:\tlearn: 0.4249956\ttotal: 8.12s\tremaining: 2m 34s\n",
            "150:\tlearn: 0.4248800\ttotal: 8.15s\tremaining: 2m 33s\n",
            "151:\tlearn: 0.4247738\ttotal: 8.18s\tremaining: 2m 33s\n",
            "152:\tlearn: 0.4246565\ttotal: 8.21s\tremaining: 2m 32s\n",
            "153:\tlearn: 0.4245296\ttotal: 8.25s\tremaining: 2m 32s\n",
            "154:\tlearn: 0.4244473\ttotal: 8.27s\tremaining: 2m 31s\n",
            "155:\tlearn: 0.4243608\ttotal: 8.3s\tremaining: 2m 31s\n",
            "156:\tlearn: 0.4242206\ttotal: 8.33s\tremaining: 2m 30s\n",
            "157:\tlearn: 0.4240765\ttotal: 8.36s\tremaining: 2m 30s\n",
            "158:\tlearn: 0.4239186\ttotal: 8.39s\tremaining: 2m 29s\n",
            "159:\tlearn: 0.4238824\ttotal: 8.42s\tremaining: 2m 29s\n",
            "160:\tlearn: 0.4237101\ttotal: 8.44s\tremaining: 2m 28s\n",
            "161:\tlearn: 0.4234892\ttotal: 8.48s\tremaining: 2m 28s\n",
            "162:\tlearn: 0.4233245\ttotal: 8.51s\tremaining: 2m 28s\n",
            "163:\tlearn: 0.4231597\ttotal: 8.53s\tremaining: 2m 27s\n",
            "164:\tlearn: 0.4230066\ttotal: 8.56s\tremaining: 2m 27s\n",
            "165:\tlearn: 0.4229165\ttotal: 8.59s\tremaining: 2m 26s\n",
            "166:\tlearn: 0.4228210\ttotal: 8.62s\tremaining: 2m 26s\n",
            "167:\tlearn: 0.4227084\ttotal: 8.65s\tremaining: 2m 25s\n",
            "168:\tlearn: 0.4226352\ttotal: 8.68s\tremaining: 2m 25s\n",
            "169:\tlearn: 0.4224425\ttotal: 8.71s\tremaining: 2m 25s\n",
            "170:\tlearn: 0.4223811\ttotal: 8.75s\tremaining: 2m 24s\n",
            "171:\tlearn: 0.4222459\ttotal: 8.77s\tremaining: 2m 24s\n",
            "172:\tlearn: 0.4220884\ttotal: 8.8s\tremaining: 2m 23s\n",
            "173:\tlearn: 0.4219395\ttotal: 8.83s\tremaining: 2m 23s\n",
            "174:\tlearn: 0.4218430\ttotal: 8.86s\tremaining: 2m 22s\n",
            "175:\tlearn: 0.4217529\ttotal: 8.89s\tremaining: 2m 22s\n",
            "176:\tlearn: 0.4216561\ttotal: 8.91s\tremaining: 2m 22s\n",
            "177:\tlearn: 0.4215586\ttotal: 8.95s\tremaining: 2m 21s\n",
            "178:\tlearn: 0.4214540\ttotal: 8.97s\tremaining: 2m 21s\n",
            "179:\tlearn: 0.4213467\ttotal: 9s\tremaining: 2m 21s\n",
            "180:\tlearn: 0.4211143\ttotal: 9.03s\tremaining: 2m 20s\n",
            "181:\tlearn: 0.4210249\ttotal: 9.07s\tremaining: 2m 20s\n",
            "182:\tlearn: 0.4208980\ttotal: 9.11s\tremaining: 2m 20s\n",
            "183:\tlearn: 0.4208135\ttotal: 9.14s\tremaining: 2m 19s\n",
            "184:\tlearn: 0.4206868\ttotal: 9.16s\tremaining: 2m 19s\n",
            "185:\tlearn: 0.4205397\ttotal: 9.19s\tremaining: 2m 19s\n",
            "186:\tlearn: 0.4203803\ttotal: 9.22s\tremaining: 2m 18s\n",
            "187:\tlearn: 0.4203677\ttotal: 9.25s\tremaining: 2m 18s\n",
            "188:\tlearn: 0.4203409\ttotal: 9.28s\tremaining: 2m 18s\n",
            "189:\tlearn: 0.4202312\ttotal: 9.31s\tremaining: 2m 17s\n",
            "190:\tlearn: 0.4201117\ttotal: 9.34s\tremaining: 2m 17s\n",
            "191:\tlearn: 0.4200386\ttotal: 9.37s\tremaining: 2m 17s\n",
            "192:\tlearn: 0.4198736\ttotal: 9.4s\tremaining: 2m 16s\n",
            "193:\tlearn: 0.4197714\ttotal: 9.43s\tremaining: 2m 16s\n",
            "194:\tlearn: 0.4196160\ttotal: 9.46s\tremaining: 2m 16s\n",
            "195:\tlearn: 0.4194583\ttotal: 9.48s\tremaining: 2m 15s\n",
            "196:\tlearn: 0.4193408\ttotal: 9.51s\tremaining: 2m 15s\n",
            "197:\tlearn: 0.4192370\ttotal: 9.54s\tremaining: 2m 15s\n",
            "198:\tlearn: 0.4191099\ttotal: 9.57s\tremaining: 2m 14s\n",
            "199:\tlearn: 0.4190060\ttotal: 9.6s\tremaining: 2m 14s\n",
            "200:\tlearn: 0.4188407\ttotal: 9.64s\tremaining: 2m 14s\n",
            "201:\tlearn: 0.4187899\ttotal: 9.67s\tremaining: 2m 13s\n",
            "202:\tlearn: 0.4186971\ttotal: 9.7s\tremaining: 2m 13s\n",
            "203:\tlearn: 0.4185367\ttotal: 9.73s\tremaining: 2m 13s\n",
            "204:\tlearn: 0.4185214\ttotal: 9.76s\tremaining: 2m 13s\n",
            "205:\tlearn: 0.4184760\ttotal: 9.79s\tremaining: 2m 12s\n",
            "206:\tlearn: 0.4183912\ttotal: 9.81s\tremaining: 2m 12s\n",
            "207:\tlearn: 0.4182711\ttotal: 9.84s\tremaining: 2m 12s\n",
            "208:\tlearn: 0.4181238\ttotal: 9.87s\tremaining: 2m 11s\n",
            "209:\tlearn: 0.4180086\ttotal: 9.9s\tremaining: 2m 11s\n",
            "210:\tlearn: 0.4178546\ttotal: 9.93s\tremaining: 2m 11s\n",
            "211:\tlearn: 0.4177709\ttotal: 9.96s\tremaining: 2m 11s\n",
            "212:\tlearn: 0.4177136\ttotal: 9.99s\tremaining: 2m 10s\n",
            "213:\tlearn: 0.4175782\ttotal: 10s\tremaining: 2m 10s\n",
            "214:\tlearn: 0.4173975\ttotal: 10.1s\tremaining: 2m 10s\n",
            "215:\tlearn: 0.4173016\ttotal: 10.1s\tremaining: 2m 10s\n",
            "216:\tlearn: 0.4171558\ttotal: 10.1s\tremaining: 2m 9s\n",
            "217:\tlearn: 0.4170355\ttotal: 10.2s\tremaining: 2m 9s\n",
            "218:\tlearn: 0.4170041\ttotal: 10.2s\tremaining: 2m 9s\n",
            "219:\tlearn: 0.4169028\ttotal: 10.2s\tremaining: 2m 9s\n",
            "220:\tlearn: 0.4167644\ttotal: 10.2s\tremaining: 2m 8s\n",
            "221:\tlearn: 0.4166603\ttotal: 10.3s\tremaining: 2m 8s\n",
            "222:\tlearn: 0.4164463\ttotal: 10.3s\tremaining: 2m 8s\n",
            "223:\tlearn: 0.4163674\ttotal: 10.3s\tremaining: 2m 7s\n",
            "224:\tlearn: 0.4162285\ttotal: 10.4s\tremaining: 2m 7s\n",
            "225:\tlearn: 0.4161083\ttotal: 10.4s\tremaining: 2m 7s\n",
            "226:\tlearn: 0.4159760\ttotal: 10.4s\tremaining: 2m 7s\n",
            "227:\tlearn: 0.4158299\ttotal: 10.4s\tremaining: 2m 6s\n",
            "228:\tlearn: 0.4157832\ttotal: 10.5s\tremaining: 2m 6s\n",
            "229:\tlearn: 0.4156751\ttotal: 10.5s\tremaining: 2m 6s\n",
            "230:\tlearn: 0.4155319\ttotal: 10.5s\tremaining: 2m 6s\n",
            "231:\tlearn: 0.4154059\ttotal: 10.6s\tremaining: 2m 6s\n",
            "232:\tlearn: 0.4153263\ttotal: 10.6s\tremaining: 2m 5s\n",
            "233:\tlearn: 0.4151716\ttotal: 10.6s\tremaining: 2m 5s\n",
            "234:\tlearn: 0.4150513\ttotal: 10.7s\tremaining: 2m 5s\n",
            "235:\tlearn: 0.4149405\ttotal: 10.7s\tremaining: 2m 5s\n",
            "236:\tlearn: 0.4148128\ttotal: 10.7s\tremaining: 2m 4s\n",
            "237:\tlearn: 0.4146848\ttotal: 10.7s\tremaining: 2m 4s\n",
            "238:\tlearn: 0.4145358\ttotal: 10.8s\tremaining: 2m 4s\n",
            "239:\tlearn: 0.4143686\ttotal: 10.8s\tremaining: 2m 4s\n",
            "240:\tlearn: 0.4142584\ttotal: 10.8s\tremaining: 2m 3s\n",
            "241:\tlearn: 0.4140929\ttotal: 10.9s\tremaining: 2m 3s\n",
            "242:\tlearn: 0.4140380\ttotal: 10.9s\tremaining: 2m 3s\n",
            "243:\tlearn: 0.4139137\ttotal: 10.9s\tremaining: 2m 3s\n",
            "244:\tlearn: 0.4137634\ttotal: 11s\tremaining: 2m 3s\n",
            "245:\tlearn: 0.4136223\ttotal: 11s\tremaining: 2m 2s\n",
            "246:\tlearn: 0.4135606\ttotal: 11s\tremaining: 2m 2s\n",
            "247:\tlearn: 0.4134347\ttotal: 11s\tremaining: 2m 2s\n",
            "248:\tlearn: 0.4134114\ttotal: 11.1s\tremaining: 2m 2s\n",
            "249:\tlearn: 0.4132877\ttotal: 11.1s\tremaining: 2m 2s\n",
            "250:\tlearn: 0.4132218\ttotal: 11.1s\tremaining: 2m 2s\n",
            "251:\tlearn: 0.4131695\ttotal: 11.2s\tremaining: 2m 1s\n",
            "252:\tlearn: 0.4130045\ttotal: 11.2s\tremaining: 2m 1s\n",
            "253:\tlearn: 0.4128532\ttotal: 11.2s\tremaining: 2m 1s\n",
            "254:\tlearn: 0.4127345\ttotal: 11.3s\tremaining: 2m 1s\n",
            "255:\tlearn: 0.4126480\ttotal: 11.3s\tremaining: 2m 1s\n",
            "256:\tlearn: 0.4126086\ttotal: 11.3s\tremaining: 2m\n",
            "257:\tlearn: 0.4125340\ttotal: 11.4s\tremaining: 2m\n",
            "258:\tlearn: 0.4123455\ttotal: 11.4s\tremaining: 2m\n",
            "259:\tlearn: 0.4122323\ttotal: 11.4s\tremaining: 2m\n",
            "260:\tlearn: 0.4121292\ttotal: 11.4s\tremaining: 2m\n",
            "261:\tlearn: 0.4121131\ttotal: 11.5s\tremaining: 1m 59s\n",
            "262:\tlearn: 0.4119914\ttotal: 11.5s\tremaining: 1m 59s\n",
            "263:\tlearn: 0.4119520\ttotal: 11.5s\tremaining: 1m 59s\n",
            "264:\tlearn: 0.4118796\ttotal: 11.6s\tremaining: 1m 59s\n",
            "265:\tlearn: 0.4117868\ttotal: 11.6s\tremaining: 1m 59s\n",
            "266:\tlearn: 0.4117136\ttotal: 11.6s\tremaining: 1m 59s\n",
            "267:\tlearn: 0.4114770\ttotal: 11.7s\tremaining: 1m 58s\n",
            "268:\tlearn: 0.4113818\ttotal: 11.7s\tremaining: 1m 58s\n",
            "269:\tlearn: 0.4112528\ttotal: 11.7s\tremaining: 1m 58s\n",
            "270:\tlearn: 0.4111378\ttotal: 11.8s\tremaining: 1m 58s\n",
            "271:\tlearn: 0.4109844\ttotal: 11.8s\tremaining: 1m 58s\n",
            "272:\tlearn: 0.4108426\ttotal: 11.8s\tremaining: 1m 57s\n",
            "273:\tlearn: 0.4108041\ttotal: 11.8s\tremaining: 1m 57s\n",
            "274:\tlearn: 0.4107815\ttotal: 11.9s\tremaining: 1m 57s\n",
            "275:\tlearn: 0.4106756\ttotal: 11.9s\tremaining: 1m 57s\n",
            "276:\tlearn: 0.4105875\ttotal: 11.9s\tremaining: 1m 57s\n",
            "277:\tlearn: 0.4104895\ttotal: 12s\tremaining: 1m 57s\n",
            "278:\tlearn: 0.4103863\ttotal: 12s\tremaining: 1m 56s\n",
            "279:\tlearn: 0.4102918\ttotal: 12s\tremaining: 1m 56s\n",
            "280:\tlearn: 0.4101654\ttotal: 12.1s\tremaining: 1m 56s\n",
            "281:\tlearn: 0.4100396\ttotal: 12.1s\tremaining: 1m 56s\n",
            "282:\tlearn: 0.4099057\ttotal: 12.1s\tremaining: 1m 56s\n",
            "283:\tlearn: 0.4098309\ttotal: 12.2s\tremaining: 1m 56s\n",
            "284:\tlearn: 0.4097807\ttotal: 12.2s\tremaining: 1m 56s\n",
            "285:\tlearn: 0.4096594\ttotal: 12.2s\tremaining: 1m 55s\n",
            "286:\tlearn: 0.4095363\ttotal: 12.2s\tremaining: 1m 55s\n",
            "287:\tlearn: 0.4094863\ttotal: 12.3s\tremaining: 1m 55s\n",
            "288:\tlearn: 0.4093792\ttotal: 12.3s\tremaining: 1m 55s\n",
            "289:\tlearn: 0.4093245\ttotal: 12.3s\tremaining: 1m 55s\n",
            "290:\tlearn: 0.4092655\ttotal: 12.4s\tremaining: 1m 55s\n",
            "291:\tlearn: 0.4091915\ttotal: 12.4s\tremaining: 1m 54s\n",
            "292:\tlearn: 0.4091378\ttotal: 12.4s\tremaining: 1m 54s\n",
            "293:\tlearn: 0.4090388\ttotal: 12.5s\tremaining: 1m 54s\n",
            "294:\tlearn: 0.4089997\ttotal: 12.5s\tremaining: 1m 54s\n",
            "295:\tlearn: 0.4088805\ttotal: 12.5s\tremaining: 1m 54s\n",
            "296:\tlearn: 0.4086975\ttotal: 12.5s\tremaining: 1m 54s\n",
            "297:\tlearn: 0.4085868\ttotal: 12.6s\tremaining: 1m 54s\n",
            "298:\tlearn: 0.4084732\ttotal: 12.6s\tremaining: 1m 53s\n",
            "299:\tlearn: 0.4083548\ttotal: 12.6s\tremaining: 1m 53s\n",
            "300:\tlearn: 0.4082330\ttotal: 12.7s\tremaining: 1m 53s\n",
            "301:\tlearn: 0.4080933\ttotal: 12.7s\tremaining: 1m 53s\n",
            "302:\tlearn: 0.4079469\ttotal: 12.7s\tremaining: 1m 53s\n",
            "303:\tlearn: 0.4078633\ttotal: 12.8s\tremaining: 1m 53s\n",
            "304:\tlearn: 0.4077701\ttotal: 12.8s\tremaining: 1m 52s\n",
            "305:\tlearn: 0.4076249\ttotal: 12.8s\tremaining: 1m 52s\n",
            "306:\tlearn: 0.4075498\ttotal: 12.8s\tremaining: 1m 52s\n",
            "307:\tlearn: 0.4074593\ttotal: 12.9s\tremaining: 1m 52s\n",
            "308:\tlearn: 0.4073772\ttotal: 12.9s\tremaining: 1m 52s\n",
            "309:\tlearn: 0.4071805\ttotal: 12.9s\tremaining: 1m 52s\n",
            "310:\tlearn: 0.4070356\ttotal: 13s\tremaining: 1m 52s\n",
            "311:\tlearn: 0.4069467\ttotal: 13s\tremaining: 1m 51s\n",
            "312:\tlearn: 0.4068561\ttotal: 13s\tremaining: 1m 51s\n",
            "313:\tlearn: 0.4067537\ttotal: 13.1s\tremaining: 1m 51s\n",
            "314:\tlearn: 0.4066374\ttotal: 13.1s\tremaining: 1m 51s\n",
            "315:\tlearn: 0.4065190\ttotal: 13.1s\tremaining: 1m 51s\n",
            "316:\tlearn: 0.4063447\ttotal: 13.2s\tremaining: 1m 51s\n",
            "317:\tlearn: 0.4061980\ttotal: 13.2s\tremaining: 1m 51s\n",
            "318:\tlearn: 0.4060264\ttotal: 13.2s\tremaining: 1m 51s\n",
            "319:\tlearn: 0.4059327\ttotal: 13.2s\tremaining: 1m 50s\n",
            "320:\tlearn: 0.4058407\ttotal: 13.3s\tremaining: 1m 50s\n",
            "321:\tlearn: 0.4057686\ttotal: 13.3s\tremaining: 1m 50s\n",
            "322:\tlearn: 0.4057086\ttotal: 13.3s\tremaining: 1m 50s\n",
            "323:\tlearn: 0.4055050\ttotal: 13.4s\tremaining: 1m 50s\n",
            "324:\tlearn: 0.4054217\ttotal: 13.4s\tremaining: 1m 50s\n",
            "325:\tlearn: 0.4053635\ttotal: 13.4s\tremaining: 1m 50s\n",
            "326:\tlearn: 0.4052746\ttotal: 13.5s\tremaining: 1m 50s\n",
            "327:\tlearn: 0.4051684\ttotal: 13.5s\tremaining: 1m 49s\n",
            "328:\tlearn: 0.4050976\ttotal: 13.5s\tremaining: 1m 49s\n",
            "329:\tlearn: 0.4049592\ttotal: 13.5s\tremaining: 1m 49s\n",
            "330:\tlearn: 0.4048522\ttotal: 13.6s\tremaining: 1m 49s\n",
            "331:\tlearn: 0.4047911\ttotal: 13.6s\tremaining: 1m 49s\n",
            "332:\tlearn: 0.4046559\ttotal: 13.6s\tremaining: 1m 49s\n",
            "333:\tlearn: 0.4046292\ttotal: 13.7s\tremaining: 1m 49s\n",
            "334:\tlearn: 0.4045302\ttotal: 13.7s\tremaining: 1m 48s\n",
            "335:\tlearn: 0.4044901\ttotal: 13.7s\tremaining: 1m 48s\n",
            "336:\tlearn: 0.4043695\ttotal: 13.7s\tremaining: 1m 48s\n",
            "337:\tlearn: 0.4042873\ttotal: 13.8s\tremaining: 1m 48s\n",
            "338:\tlearn: 0.4041286\ttotal: 13.8s\tremaining: 1m 48s\n",
            "339:\tlearn: 0.4040725\ttotal: 13.8s\tremaining: 1m 48s\n",
            "340:\tlearn: 0.4039876\ttotal: 13.9s\tremaining: 1m 48s\n",
            "341:\tlearn: 0.4038093\ttotal: 13.9s\tremaining: 1m 48s\n",
            "342:\tlearn: 0.4036683\ttotal: 13.9s\tremaining: 1m 47s\n",
            "343:\tlearn: 0.4035515\ttotal: 14s\tremaining: 1m 47s\n",
            "344:\tlearn: 0.4034586\ttotal: 14s\tremaining: 1m 47s\n",
            "345:\tlearn: 0.4034467\ttotal: 14s\tremaining: 1m 47s\n",
            "346:\tlearn: 0.4033244\ttotal: 14.1s\tremaining: 1m 47s\n",
            "347:\tlearn: 0.4032173\ttotal: 14.1s\tremaining: 1m 47s\n",
            "348:\tlearn: 0.4030579\ttotal: 14.1s\tremaining: 1m 47s\n",
            "349:\tlearn: 0.4030461\ttotal: 14.2s\tremaining: 1m 47s\n",
            "350:\tlearn: 0.4029702\ttotal: 14.2s\tremaining: 1m 47s\n",
            "351:\tlearn: 0.4028043\ttotal: 14.2s\tremaining: 1m 46s\n",
            "352:\tlearn: 0.4026940\ttotal: 14.2s\tremaining: 1m 46s\n",
            "353:\tlearn: 0.4026005\ttotal: 14.3s\tremaining: 1m 46s\n",
            "354:\tlearn: 0.4024881\ttotal: 14.3s\tremaining: 1m 46s\n",
            "355:\tlearn: 0.4023882\ttotal: 14.3s\tremaining: 1m 46s\n",
            "356:\tlearn: 0.4022905\ttotal: 14.4s\tremaining: 1m 46s\n",
            "357:\tlearn: 0.4021753\ttotal: 14.4s\tremaining: 1m 46s\n",
            "358:\tlearn: 0.4020808\ttotal: 14.4s\tremaining: 1m 46s\n",
            "359:\tlearn: 0.4019589\ttotal: 14.5s\tremaining: 1m 46s\n",
            "360:\tlearn: 0.4018625\ttotal: 14.5s\tremaining: 1m 45s\n",
            "361:\tlearn: 0.4017515\ttotal: 14.5s\tremaining: 1m 45s\n",
            "362:\tlearn: 0.4016530\ttotal: 14.6s\tremaining: 1m 46s\n",
            "363:\tlearn: 0.4015177\ttotal: 14.6s\tremaining: 1m 46s\n",
            "364:\tlearn: 0.4013869\ttotal: 14.7s\tremaining: 1m 46s\n",
            "365:\tlearn: 0.4013243\ttotal: 14.8s\tremaining: 1m 46s\n",
            "366:\tlearn: 0.4011869\ttotal: 14.8s\tremaining: 1m 46s\n",
            "367:\tlearn: 0.4010669\ttotal: 14.9s\tremaining: 1m 46s\n",
            "368:\tlearn: 0.4009927\ttotal: 14.9s\tremaining: 1m 46s\n",
            "369:\tlearn: 0.4008427\ttotal: 15s\tremaining: 1m 46s\n",
            "370:\tlearn: 0.4007343\ttotal: 15s\tremaining: 1m 46s\n",
            "371:\tlearn: 0.4006306\ttotal: 15.1s\tremaining: 1m 46s\n",
            "372:\tlearn: 0.4005165\ttotal: 15.1s\tremaining: 1m 46s\n",
            "373:\tlearn: 0.4004614\ttotal: 15.2s\tremaining: 1m 46s\n",
            "374:\tlearn: 0.4003192\ttotal: 15.3s\tremaining: 1m 46s\n",
            "375:\tlearn: 0.4002145\ttotal: 15.3s\tremaining: 1m 46s\n",
            "376:\tlearn: 0.4000939\ttotal: 15.4s\tremaining: 1m 46s\n",
            "377:\tlearn: 0.3999828\ttotal: 15.4s\tremaining: 1m 47s\n",
            "378:\tlearn: 0.3998541\ttotal: 15.5s\tremaining: 1m 47s\n",
            "379:\tlearn: 0.3998213\ttotal: 15.5s\tremaining: 1m 47s\n",
            "380:\tlearn: 0.3997448\ttotal: 15.6s\tremaining: 1m 47s\n",
            "381:\tlearn: 0.3996918\ttotal: 15.6s\tremaining: 1m 47s\n",
            "382:\tlearn: 0.3995966\ttotal: 15.7s\tremaining: 1m 47s\n",
            "383:\tlearn: 0.3995458\ttotal: 15.7s\tremaining: 1m 47s\n",
            "384:\tlearn: 0.3994210\ttotal: 15.8s\tremaining: 1m 47s\n",
            "385:\tlearn: 0.3993567\ttotal: 15.8s\tremaining: 1m 47s\n",
            "386:\tlearn: 0.3992788\ttotal: 15.9s\tremaining: 1m 47s\n",
            "387:\tlearn: 0.3991602\ttotal: 16s\tremaining: 1m 47s\n",
            "388:\tlearn: 0.3990086\ttotal: 16s\tremaining: 1m 47s\n",
            "389:\tlearn: 0.3989051\ttotal: 16.1s\tremaining: 1m 47s\n",
            "390:\tlearn: 0.3988207\ttotal: 16.1s\tremaining: 1m 47s\n",
            "391:\tlearn: 0.3986351\ttotal: 16.2s\tremaining: 1m 47s\n",
            "392:\tlearn: 0.3985832\ttotal: 16.2s\tremaining: 1m 47s\n",
            "393:\tlearn: 0.3984623\ttotal: 16.3s\tremaining: 1m 47s\n",
            "394:\tlearn: 0.3983371\ttotal: 16.3s\tremaining: 1m 47s\n",
            "395:\tlearn: 0.3982131\ttotal: 16.4s\tremaining: 1m 47s\n",
            "396:\tlearn: 0.3981083\ttotal: 16.5s\tremaining: 1m 47s\n",
            "397:\tlearn: 0.3980007\ttotal: 16.5s\tremaining: 1m 47s\n",
            "398:\tlearn: 0.3978947\ttotal: 16.6s\tremaining: 1m 47s\n",
            "399:\tlearn: 0.3977899\ttotal: 16.6s\tremaining: 1m 48s\n",
            "400:\tlearn: 0.3977150\ttotal: 16.7s\tremaining: 1m 48s\n",
            "401:\tlearn: 0.3975824\ttotal: 16.7s\tremaining: 1m 48s\n",
            "402:\tlearn: 0.3975258\ttotal: 16.8s\tremaining: 1m 48s\n",
            "403:\tlearn: 0.3974059\ttotal: 16.8s\tremaining: 1m 48s\n",
            "404:\tlearn: 0.3973224\ttotal: 16.9s\tremaining: 1m 48s\n",
            "405:\tlearn: 0.3972037\ttotal: 16.9s\tremaining: 1m 48s\n",
            "406:\tlearn: 0.3970788\ttotal: 17s\tremaining: 1m 48s\n",
            "407:\tlearn: 0.3969979\ttotal: 17s\tremaining: 1m 48s\n",
            "408:\tlearn: 0.3969305\ttotal: 17.1s\tremaining: 1m 48s\n",
            "409:\tlearn: 0.3968143\ttotal: 17.1s\tremaining: 1m 48s\n",
            "410:\tlearn: 0.3967004\ttotal: 17.2s\tremaining: 1m 48s\n",
            "411:\tlearn: 0.3965717\ttotal: 17.3s\tremaining: 1m 48s\n",
            "412:\tlearn: 0.3964501\ttotal: 17.3s\tremaining: 1m 48s\n",
            "413:\tlearn: 0.3963171\ttotal: 17.4s\tremaining: 1m 48s\n",
            "414:\tlearn: 0.3962984\ttotal: 17.4s\tremaining: 1m 48s\n",
            "415:\tlearn: 0.3961819\ttotal: 17.5s\tremaining: 1m 48s\n",
            "416:\tlearn: 0.3961317\ttotal: 17.5s\tremaining: 1m 48s\n",
            "417:\tlearn: 0.3960465\ttotal: 17.6s\tremaining: 1m 48s\n",
            "418:\tlearn: 0.3960152\ttotal: 17.6s\tremaining: 1m 48s\n",
            "419:\tlearn: 0.3958138\ttotal: 17.7s\tremaining: 1m 48s\n",
            "420:\tlearn: 0.3957130\ttotal: 17.7s\tremaining: 1m 48s\n",
            "421:\tlearn: 0.3956461\ttotal: 17.8s\tremaining: 1m 48s\n",
            "422:\tlearn: 0.3955180\ttotal: 17.9s\tremaining: 1m 48s\n",
            "423:\tlearn: 0.3954321\ttotal: 17.9s\tremaining: 1m 48s\n",
            "424:\tlearn: 0.3953030\ttotal: 18s\tremaining: 1m 48s\n",
            "425:\tlearn: 0.3952506\ttotal: 18s\tremaining: 1m 48s\n",
            "426:\tlearn: 0.3952409\ttotal: 18.1s\tremaining: 1m 48s\n",
            "427:\tlearn: 0.3951185\ttotal: 18.1s\tremaining: 1m 48s\n",
            "428:\tlearn: 0.3949894\ttotal: 18.1s\tremaining: 1m 48s\n",
            "429:\tlearn: 0.3948513\ttotal: 18.2s\tremaining: 1m 48s\n",
            "430:\tlearn: 0.3947018\ttotal: 18.2s\tremaining: 1m 48s\n",
            "431:\tlearn: 0.3946292\ttotal: 18.2s\tremaining: 1m 48s\n",
            "432:\tlearn: 0.3945001\ttotal: 18.3s\tremaining: 1m 48s\n",
            "433:\tlearn: 0.3943526\ttotal: 18.3s\tremaining: 1m 48s\n",
            "434:\tlearn: 0.3942903\ttotal: 18.3s\tremaining: 1m 48s\n",
            "435:\tlearn: 0.3941892\ttotal: 18.4s\tremaining: 1m 47s\n",
            "436:\tlearn: 0.3940721\ttotal: 18.4s\tremaining: 1m 47s\n",
            "437:\tlearn: 0.3939678\ttotal: 18.4s\tremaining: 1m 47s\n",
            "438:\tlearn: 0.3939279\ttotal: 18.4s\tremaining: 1m 47s\n",
            "439:\tlearn: 0.3938477\ttotal: 18.5s\tremaining: 1m 47s\n",
            "440:\tlearn: 0.3937441\ttotal: 18.5s\tremaining: 1m 47s\n",
            "441:\tlearn: 0.3936660\ttotal: 18.5s\tremaining: 1m 47s\n",
            "442:\tlearn: 0.3935940\ttotal: 18.6s\tremaining: 1m 47s\n",
            "443:\tlearn: 0.3935155\ttotal: 18.6s\tremaining: 1m 47s\n",
            "444:\tlearn: 0.3934648\ttotal: 18.6s\tremaining: 1m 46s\n",
            "445:\tlearn: 0.3934164\ttotal: 18.6s\tremaining: 1m 46s\n",
            "446:\tlearn: 0.3934128\ttotal: 18.7s\tremaining: 1m 46s\n",
            "447:\tlearn: 0.3933085\ttotal: 18.7s\tremaining: 1m 46s\n",
            "448:\tlearn: 0.3931670\ttotal: 18.7s\tremaining: 1m 46s\n",
            "449:\tlearn: 0.3930726\ttotal: 18.8s\tremaining: 1m 46s\n",
            "450:\tlearn: 0.3929910\ttotal: 18.8s\tremaining: 1m 46s\n",
            "451:\tlearn: 0.3929343\ttotal: 18.8s\tremaining: 1m 46s\n",
            "452:\tlearn: 0.3928422\ttotal: 18.9s\tremaining: 1m 46s\n",
            "453:\tlearn: 0.3927630\ttotal: 18.9s\tremaining: 1m 45s\n",
            "454:\tlearn: 0.3926299\ttotal: 18.9s\tremaining: 1m 45s\n",
            "455:\tlearn: 0.3924907\ttotal: 18.9s\tremaining: 1m 45s\n",
            "456:\tlearn: 0.3923307\ttotal: 19s\tremaining: 1m 45s\n",
            "457:\tlearn: 0.3922528\ttotal: 19s\tremaining: 1m 45s\n",
            "458:\tlearn: 0.3921937\ttotal: 19s\tremaining: 1m 45s\n",
            "459:\tlearn: 0.3921106\ttotal: 19.1s\tremaining: 1m 45s\n",
            "460:\tlearn: 0.3920566\ttotal: 19.1s\tremaining: 1m 45s\n",
            "461:\tlearn: 0.3919698\ttotal: 19.1s\tremaining: 1m 45s\n",
            "462:\tlearn: 0.3918281\ttotal: 19.1s\tremaining: 1m 44s\n",
            "463:\tlearn: 0.3917539\ttotal: 19.2s\tremaining: 1m 44s\n",
            "464:\tlearn: 0.3916794\ttotal: 19.2s\tremaining: 1m 44s\n",
            "465:\tlearn: 0.3915828\ttotal: 19.2s\tremaining: 1m 44s\n",
            "466:\tlearn: 0.3914650\ttotal: 19.3s\tremaining: 1m 44s\n",
            "467:\tlearn: 0.3913772\ttotal: 19.3s\tremaining: 1m 44s\n",
            "468:\tlearn: 0.3913207\ttotal: 19.3s\tremaining: 1m 44s\n",
            "469:\tlearn: 0.3912276\ttotal: 19.4s\tremaining: 1m 44s\n",
            "470:\tlearn: 0.3911006\ttotal: 19.4s\tremaining: 1m 44s\n",
            "471:\tlearn: 0.3909817\ttotal: 19.4s\tremaining: 1m 44s\n",
            "472:\tlearn: 0.3909014\ttotal: 19.5s\tremaining: 1m 43s\n",
            "473:\tlearn: 0.3908118\ttotal: 19.5s\tremaining: 1m 43s\n",
            "474:\tlearn: 0.3906916\ttotal: 19.5s\tremaining: 1m 43s\n",
            "475:\tlearn: 0.3905521\ttotal: 19.5s\tremaining: 1m 43s\n",
            "476:\tlearn: 0.3904503\ttotal: 19.6s\tremaining: 1m 43s\n",
            "477:\tlearn: 0.3903272\ttotal: 19.6s\tremaining: 1m 43s\n",
            "478:\tlearn: 0.3902406\ttotal: 19.6s\tremaining: 1m 43s\n",
            "479:\tlearn: 0.3901688\ttotal: 19.7s\tremaining: 1m 43s\n",
            "480:\tlearn: 0.3900545\ttotal: 19.7s\tremaining: 1m 43s\n",
            "481:\tlearn: 0.3899586\ttotal: 19.7s\tremaining: 1m 43s\n",
            "482:\tlearn: 0.3899044\ttotal: 19.7s\tremaining: 1m 42s\n",
            "483:\tlearn: 0.3898246\ttotal: 19.8s\tremaining: 1m 42s\n",
            "484:\tlearn: 0.3897800\ttotal: 19.8s\tremaining: 1m 42s\n",
            "485:\tlearn: 0.3897261\ttotal: 19.8s\tremaining: 1m 42s\n",
            "486:\tlearn: 0.3896283\ttotal: 19.9s\tremaining: 1m 42s\n",
            "487:\tlearn: 0.3895944\ttotal: 19.9s\tremaining: 1m 42s\n",
            "488:\tlearn: 0.3895595\ttotal: 19.9s\tremaining: 1m 42s\n",
            "489:\tlearn: 0.3894663\ttotal: 20s\tremaining: 1m 42s\n",
            "490:\tlearn: 0.3893354\ttotal: 20s\tremaining: 1m 42s\n",
            "491:\tlearn: 0.3892550\ttotal: 20s\tremaining: 1m 42s\n",
            "492:\tlearn: 0.3891822\ttotal: 20.1s\tremaining: 1m 42s\n",
            "493:\tlearn: 0.3890466\ttotal: 20.1s\tremaining: 1m 41s\n",
            "494:\tlearn: 0.3889561\ttotal: 20.1s\tremaining: 1m 41s\n",
            "495:\tlearn: 0.3888434\ttotal: 20.1s\tremaining: 1m 41s\n",
            "496:\tlearn: 0.3887508\ttotal: 20.2s\tremaining: 1m 41s\n",
            "497:\tlearn: 0.3886192\ttotal: 20.2s\tremaining: 1m 41s\n",
            "498:\tlearn: 0.3885467\ttotal: 20.2s\tremaining: 1m 41s\n",
            "499:\tlearn: 0.3884999\ttotal: 20.3s\tremaining: 1m 41s\n",
            "500:\tlearn: 0.3884204\ttotal: 20.3s\tremaining: 1m 41s\n",
            "501:\tlearn: 0.3883422\ttotal: 20.3s\tremaining: 1m 41s\n",
            "502:\tlearn: 0.3882894\ttotal: 20.4s\tremaining: 1m 41s\n",
            "503:\tlearn: 0.3882832\ttotal: 20.4s\tremaining: 1m 40s\n",
            "504:\tlearn: 0.3881523\ttotal: 20.4s\tremaining: 1m 40s\n",
            "505:\tlearn: 0.3880015\ttotal: 20.5s\tremaining: 1m 40s\n",
            "506:\tlearn: 0.3879410\ttotal: 20.5s\tremaining: 1m 40s\n",
            "507:\tlearn: 0.3877812\ttotal: 20.5s\tremaining: 1m 40s\n",
            "508:\tlearn: 0.3876788\ttotal: 20.5s\tremaining: 1m 40s\n",
            "509:\tlearn: 0.3875945\ttotal: 20.6s\tremaining: 1m 40s\n",
            "510:\tlearn: 0.3874668\ttotal: 20.6s\tremaining: 1m 40s\n",
            "511:\tlearn: 0.3873768\ttotal: 20.6s\tremaining: 1m 40s\n",
            "512:\tlearn: 0.3872791\ttotal: 20.6s\tremaining: 1m 40s\n",
            "513:\tlearn: 0.3871330\ttotal: 20.7s\tremaining: 1m 40s\n",
            "514:\tlearn: 0.3870296\ttotal: 20.7s\tremaining: 1m 39s\n",
            "515:\tlearn: 0.3868930\ttotal: 20.7s\tremaining: 1m 39s\n",
            "516:\tlearn: 0.3867585\ttotal: 20.8s\tremaining: 1m 39s\n",
            "517:\tlearn: 0.3866430\ttotal: 20.8s\tremaining: 1m 39s\n",
            "518:\tlearn: 0.3865541\ttotal: 20.8s\tremaining: 1m 39s\n",
            "519:\tlearn: 0.3865203\ttotal: 20.9s\tremaining: 1m 39s\n",
            "520:\tlearn: 0.3863963\ttotal: 20.9s\tremaining: 1m 39s\n",
            "521:\tlearn: 0.3863386\ttotal: 20.9s\tremaining: 1m 39s\n",
            "522:\tlearn: 0.3862623\ttotal: 20.9s\tremaining: 1m 39s\n",
            "523:\tlearn: 0.3862104\ttotal: 21s\tremaining: 1m 39s\n",
            "524:\tlearn: 0.3861243\ttotal: 21s\tremaining: 1m 39s\n",
            "525:\tlearn: 0.3859815\ttotal: 21s\tremaining: 1m 38s\n",
            "526:\tlearn: 0.3859288\ttotal: 21.1s\tremaining: 1m 38s\n",
            "527:\tlearn: 0.3858245\ttotal: 21.1s\tremaining: 1m 38s\n",
            "528:\tlearn: 0.3856554\ttotal: 21.1s\tremaining: 1m 38s\n",
            "529:\tlearn: 0.3856133\ttotal: 21.2s\tremaining: 1m 38s\n",
            "530:\tlearn: 0.3855439\ttotal: 21.2s\tremaining: 1m 38s\n",
            "531:\tlearn: 0.3854335\ttotal: 21.2s\tremaining: 1m 38s\n",
            "532:\tlearn: 0.3853326\ttotal: 21.2s\tremaining: 1m 38s\n",
            "533:\tlearn: 0.3852035\ttotal: 21.3s\tremaining: 1m 38s\n",
            "534:\tlearn: 0.3850623\ttotal: 21.3s\tremaining: 1m 38s\n",
            "535:\tlearn: 0.3849526\ttotal: 21.3s\tremaining: 1m 38s\n",
            "536:\tlearn: 0.3848652\ttotal: 21.4s\tremaining: 1m 38s\n",
            "537:\tlearn: 0.3847748\ttotal: 21.4s\tremaining: 1m 37s\n",
            "538:\tlearn: 0.3847140\ttotal: 21.4s\tremaining: 1m 37s\n",
            "539:\tlearn: 0.3845756\ttotal: 21.5s\tremaining: 1m 37s\n",
            "540:\tlearn: 0.3844683\ttotal: 21.5s\tremaining: 1m 37s\n",
            "541:\tlearn: 0.3843368\ttotal: 21.5s\tremaining: 1m 37s\n",
            "542:\tlearn: 0.3842870\ttotal: 21.6s\tremaining: 1m 37s\n",
            "543:\tlearn: 0.3841895\ttotal: 21.6s\tremaining: 1m 37s\n",
            "544:\tlearn: 0.3840781\ttotal: 21.6s\tremaining: 1m 37s\n",
            "545:\tlearn: 0.3839715\ttotal: 21.6s\tremaining: 1m 37s\n",
            "546:\tlearn: 0.3838779\ttotal: 21.7s\tremaining: 1m 37s\n",
            "547:\tlearn: 0.3838171\ttotal: 21.7s\tremaining: 1m 37s\n",
            "548:\tlearn: 0.3836835\ttotal: 21.7s\tremaining: 1m 37s\n",
            "549:\tlearn: 0.3836628\ttotal: 21.8s\tremaining: 1m 36s\n",
            "550:\tlearn: 0.3835823\ttotal: 21.8s\tremaining: 1m 36s\n",
            "551:\tlearn: 0.3835541\ttotal: 21.8s\tremaining: 1m 36s\n",
            "552:\tlearn: 0.3835155\ttotal: 21.9s\tremaining: 1m 36s\n",
            "553:\tlearn: 0.3834257\ttotal: 21.9s\tremaining: 1m 36s\n",
            "554:\tlearn: 0.3834028\ttotal: 21.9s\tremaining: 1m 36s\n",
            "555:\tlearn: 0.3833170\ttotal: 21.9s\tremaining: 1m 36s\n",
            "556:\tlearn: 0.3832526\ttotal: 22s\tremaining: 1m 36s\n",
            "557:\tlearn: 0.3831699\ttotal: 22s\tremaining: 1m 36s\n",
            "558:\tlearn: 0.3830102\ttotal: 22s\tremaining: 1m 36s\n",
            "559:\tlearn: 0.3828985\ttotal: 22.1s\tremaining: 1m 36s\n",
            "560:\tlearn: 0.3828038\ttotal: 22.1s\tremaining: 1m 36s\n",
            "561:\tlearn: 0.3826730\ttotal: 22.1s\tremaining: 1m 35s\n",
            "562:\tlearn: 0.3825614\ttotal: 22.1s\tremaining: 1m 35s\n",
            "563:\tlearn: 0.3824753\ttotal: 22.2s\tremaining: 1m 35s\n",
            "564:\tlearn: 0.3823313\ttotal: 22.2s\tremaining: 1m 35s\n",
            "565:\tlearn: 0.3822671\ttotal: 22.2s\tremaining: 1m 35s\n",
            "566:\tlearn: 0.3822068\ttotal: 22.3s\tremaining: 1m 35s\n",
            "567:\tlearn: 0.3821386\ttotal: 22.3s\tremaining: 1m 35s\n",
            "568:\tlearn: 0.3819811\ttotal: 22.3s\tremaining: 1m 35s\n",
            "569:\tlearn: 0.3818623\ttotal: 22.4s\tremaining: 1m 35s\n",
            "570:\tlearn: 0.3817354\ttotal: 22.4s\tremaining: 1m 35s\n",
            "571:\tlearn: 0.3816036\ttotal: 22.4s\tremaining: 1m 35s\n",
            "572:\tlearn: 0.3815045\ttotal: 22.5s\tremaining: 1m 35s\n",
            "573:\tlearn: 0.3814226\ttotal: 22.5s\tremaining: 1m 35s\n",
            "574:\tlearn: 0.3813898\ttotal: 22.5s\tremaining: 1m 34s\n",
            "575:\tlearn: 0.3812159\ttotal: 22.6s\tremaining: 1m 34s\n",
            "576:\tlearn: 0.3811298\ttotal: 22.6s\tremaining: 1m 34s\n",
            "577:\tlearn: 0.3810665\ttotal: 22.6s\tremaining: 1m 34s\n",
            "578:\tlearn: 0.3809629\ttotal: 22.6s\tremaining: 1m 34s\n",
            "579:\tlearn: 0.3809123\ttotal: 22.7s\tremaining: 1m 34s\n",
            "580:\tlearn: 0.3807857\ttotal: 22.7s\tremaining: 1m 34s\n",
            "581:\tlearn: 0.3806917\ttotal: 22.7s\tremaining: 1m 34s\n",
            "582:\tlearn: 0.3805779\ttotal: 22.8s\tremaining: 1m 34s\n",
            "583:\tlearn: 0.3805082\ttotal: 22.8s\tremaining: 1m 34s\n",
            "584:\tlearn: 0.3804344\ttotal: 22.8s\tremaining: 1m 34s\n",
            "585:\tlearn: 0.3803770\ttotal: 22.9s\tremaining: 1m 34s\n",
            "586:\tlearn: 0.3803532\ttotal: 22.9s\tremaining: 1m 34s\n",
            "587:\tlearn: 0.3802466\ttotal: 22.9s\tremaining: 1m 33s\n",
            "588:\tlearn: 0.3801932\ttotal: 22.9s\tremaining: 1m 33s\n",
            "589:\tlearn: 0.3801132\ttotal: 23s\tremaining: 1m 33s\n",
            "590:\tlearn: 0.3800043\ttotal: 23s\tremaining: 1m 33s\n",
            "591:\tlearn: 0.3799647\ttotal: 23s\tremaining: 1m 33s\n",
            "592:\tlearn: 0.3798927\ttotal: 23.1s\tremaining: 1m 33s\n",
            "593:\tlearn: 0.3798354\ttotal: 23.1s\tremaining: 1m 33s\n",
            "594:\tlearn: 0.3797530\ttotal: 23.1s\tremaining: 1m 33s\n",
            "595:\tlearn: 0.3797170\ttotal: 23.1s\tremaining: 1m 33s\n",
            "596:\tlearn: 0.3796776\ttotal: 23.2s\tremaining: 1m 33s\n",
            "597:\tlearn: 0.3796201\ttotal: 23.2s\tremaining: 1m 33s\n",
            "598:\tlearn: 0.3795500\ttotal: 23.2s\tremaining: 1m 33s\n",
            "599:\tlearn: 0.3794128\ttotal: 23.3s\tremaining: 1m 33s\n",
            "600:\tlearn: 0.3793113\ttotal: 23.3s\tremaining: 1m 33s\n",
            "601:\tlearn: 0.3792500\ttotal: 23.3s\tremaining: 1m 32s\n",
            "602:\tlearn: 0.3791477\ttotal: 23.4s\tremaining: 1m 32s\n",
            "603:\tlearn: 0.3790692\ttotal: 23.4s\tremaining: 1m 32s\n",
            "604:\tlearn: 0.3789761\ttotal: 23.4s\tremaining: 1m 32s\n",
            "605:\tlearn: 0.3789496\ttotal: 23.5s\tremaining: 1m 32s\n",
            "606:\tlearn: 0.3788337\ttotal: 23.5s\tremaining: 1m 32s\n",
            "607:\tlearn: 0.3787649\ttotal: 23.5s\tremaining: 1m 32s\n",
            "608:\tlearn: 0.3786784\ttotal: 23.5s\tremaining: 1m 32s\n",
            "609:\tlearn: 0.3785869\ttotal: 23.6s\tremaining: 1m 32s\n",
            "610:\tlearn: 0.3785078\ttotal: 23.6s\tremaining: 1m 32s\n",
            "611:\tlearn: 0.3783998\ttotal: 23.6s\tremaining: 1m 32s\n",
            "612:\tlearn: 0.3782893\ttotal: 23.7s\tremaining: 1m 32s\n",
            "613:\tlearn: 0.3781906\ttotal: 23.7s\tremaining: 1m 32s\n",
            "614:\tlearn: 0.3780995\ttotal: 23.7s\tremaining: 1m 32s\n",
            "615:\tlearn: 0.3779669\ttotal: 23.8s\tremaining: 1m 31s\n",
            "616:\tlearn: 0.3778878\ttotal: 23.8s\tremaining: 1m 31s\n",
            "617:\tlearn: 0.3778464\ttotal: 23.8s\tremaining: 1m 31s\n",
            "618:\tlearn: 0.3777480\ttotal: 23.8s\tremaining: 1m 31s\n",
            "619:\tlearn: 0.3776841\ttotal: 23.9s\tremaining: 1m 31s\n",
            "620:\tlearn: 0.3776062\ttotal: 23.9s\tremaining: 1m 31s\n",
            "621:\tlearn: 0.3775618\ttotal: 23.9s\tremaining: 1m 31s\n",
            "622:\tlearn: 0.3775193\ttotal: 24s\tremaining: 1m 31s\n",
            "623:\tlearn: 0.3774575\ttotal: 24s\tremaining: 1m 31s\n",
            "624:\tlearn: 0.3773949\ttotal: 24s\tremaining: 1m 31s\n",
            "625:\tlearn: 0.3773031\ttotal: 24.1s\tremaining: 1m 31s\n",
            "626:\tlearn: 0.3772553\ttotal: 24.1s\tremaining: 1m 31s\n",
            "627:\tlearn: 0.3771872\ttotal: 24.1s\tremaining: 1m 31s\n",
            "628:\tlearn: 0.3771529\ttotal: 24.1s\tremaining: 1m 30s\n",
            "629:\tlearn: 0.3770417\ttotal: 24.2s\tremaining: 1m 30s\n",
            "630:\tlearn: 0.3770279\ttotal: 24.2s\tremaining: 1m 30s\n",
            "631:\tlearn: 0.3769673\ttotal: 24.2s\tremaining: 1m 30s\n",
            "632:\tlearn: 0.3769100\ttotal: 24.3s\tremaining: 1m 30s\n",
            "633:\tlearn: 0.3768337\ttotal: 24.3s\tremaining: 1m 30s\n",
            "634:\tlearn: 0.3767485\ttotal: 24.3s\tremaining: 1m 30s\n",
            "635:\tlearn: 0.3766301\ttotal: 24.4s\tremaining: 1m 30s\n",
            "636:\tlearn: 0.3765236\ttotal: 24.4s\tremaining: 1m 30s\n",
            "637:\tlearn: 0.3764325\ttotal: 24.4s\tremaining: 1m 30s\n",
            "638:\tlearn: 0.3763176\ttotal: 24.5s\tremaining: 1m 30s\n",
            "639:\tlearn: 0.3762573\ttotal: 24.5s\tremaining: 1m 30s\n",
            "640:\tlearn: 0.3762220\ttotal: 24.5s\tremaining: 1m 30s\n",
            "641:\tlearn: 0.3761273\ttotal: 24.5s\tremaining: 1m 30s\n",
            "642:\tlearn: 0.3760906\ttotal: 24.6s\tremaining: 1m 30s\n",
            "643:\tlearn: 0.3760224\ttotal: 24.6s\tremaining: 1m 30s\n",
            "644:\tlearn: 0.3759363\ttotal: 24.6s\tremaining: 1m 29s\n",
            "645:\tlearn: 0.3758257\ttotal: 24.7s\tremaining: 1m 29s\n",
            "646:\tlearn: 0.3757480\ttotal: 24.7s\tremaining: 1m 29s\n",
            "647:\tlearn: 0.3756966\ttotal: 24.7s\tremaining: 1m 29s\n",
            "648:\tlearn: 0.3756174\ttotal: 24.8s\tremaining: 1m 29s\n",
            "649:\tlearn: 0.3754889\ttotal: 24.8s\tremaining: 1m 29s\n",
            "650:\tlearn: 0.3753831\ttotal: 24.8s\tremaining: 1m 29s\n",
            "651:\tlearn: 0.3752996\ttotal: 24.8s\tremaining: 1m 29s\n",
            "652:\tlearn: 0.3751816\ttotal: 24.9s\tremaining: 1m 29s\n",
            "653:\tlearn: 0.3750652\ttotal: 24.9s\tremaining: 1m 29s\n",
            "654:\tlearn: 0.3749931\ttotal: 24.9s\tremaining: 1m 29s\n",
            "655:\tlearn: 0.3749326\ttotal: 25s\tremaining: 1m 29s\n",
            "656:\tlearn: 0.3748652\ttotal: 25s\tremaining: 1m 29s\n",
            "657:\tlearn: 0.3748104\ttotal: 25s\tremaining: 1m 29s\n",
            "658:\tlearn: 0.3746858\ttotal: 25.1s\tremaining: 1m 29s\n",
            "659:\tlearn: 0.3745979\ttotal: 25.1s\tremaining: 1m 29s\n",
            "660:\tlearn: 0.3744890\ttotal: 25.1s\tremaining: 1m 28s\n",
            "661:\tlearn: 0.3744557\ttotal: 25.2s\tremaining: 1m 28s\n",
            "662:\tlearn: 0.3743905\ttotal: 25.2s\tremaining: 1m 28s\n",
            "663:\tlearn: 0.3743156\ttotal: 25.2s\tremaining: 1m 28s\n",
            "664:\tlearn: 0.3743092\ttotal: 25.2s\tremaining: 1m 28s\n",
            "665:\tlearn: 0.3742546\ttotal: 25.3s\tremaining: 1m 28s\n",
            "666:\tlearn: 0.3741678\ttotal: 25.3s\tremaining: 1m 28s\n",
            "667:\tlearn: 0.3741181\ttotal: 25.3s\tremaining: 1m 28s\n",
            "668:\tlearn: 0.3740648\ttotal: 25.4s\tremaining: 1m 28s\n",
            "669:\tlearn: 0.3739712\ttotal: 25.4s\tremaining: 1m 28s\n",
            "670:\tlearn: 0.3739118\ttotal: 25.4s\tremaining: 1m 28s\n",
            "671:\tlearn: 0.3738530\ttotal: 25.5s\tremaining: 1m 28s\n",
            "672:\tlearn: 0.3738079\ttotal: 25.5s\tremaining: 1m 28s\n",
            "673:\tlearn: 0.3737991\ttotal: 25.5s\tremaining: 1m 28s\n",
            "674:\tlearn: 0.3737326\ttotal: 25.6s\tremaining: 1m 28s\n",
            "675:\tlearn: 0.3736933\ttotal: 25.6s\tremaining: 1m 27s\n",
            "676:\tlearn: 0.3736005\ttotal: 25.6s\tremaining: 1m 27s\n",
            "677:\tlearn: 0.3734950\ttotal: 25.7s\tremaining: 1m 27s\n",
            "678:\tlearn: 0.3734281\ttotal: 25.7s\tremaining: 1m 27s\n",
            "679:\tlearn: 0.3733189\ttotal: 25.7s\tremaining: 1m 27s\n",
            "680:\tlearn: 0.3732279\ttotal: 25.7s\tremaining: 1m 27s\n",
            "681:\tlearn: 0.3731433\ttotal: 25.8s\tremaining: 1m 27s\n",
            "682:\tlearn: 0.3730686\ttotal: 25.8s\tremaining: 1m 27s\n",
            "683:\tlearn: 0.3729799\ttotal: 25.8s\tremaining: 1m 27s\n",
            "684:\tlearn: 0.3729456\ttotal: 25.9s\tremaining: 1m 27s\n",
            "685:\tlearn: 0.3729054\ttotal: 25.9s\tremaining: 1m 27s\n",
            "686:\tlearn: 0.3728375\ttotal: 25.9s\tremaining: 1m 27s\n",
            "687:\tlearn: 0.3727705\ttotal: 25.9s\tremaining: 1m 27s\n",
            "688:\tlearn: 0.3726910\ttotal: 26s\tremaining: 1m 27s\n",
            "689:\tlearn: 0.3726237\ttotal: 26s\tremaining: 1m 27s\n",
            "690:\tlearn: 0.3725256\ttotal: 26s\tremaining: 1m 27s\n",
            "691:\tlearn: 0.3724195\ttotal: 26.1s\tremaining: 1m 26s\n",
            "692:\tlearn: 0.3722926\ttotal: 26.1s\tremaining: 1m 26s\n",
            "693:\tlearn: 0.3722305\ttotal: 26.1s\tremaining: 1m 26s\n",
            "694:\tlearn: 0.3721524\ttotal: 26.2s\tremaining: 1m 26s\n",
            "695:\tlearn: 0.3720578\ttotal: 26.2s\tremaining: 1m 26s\n",
            "696:\tlearn: 0.3720269\ttotal: 26.2s\tremaining: 1m 26s\n",
            "697:\tlearn: 0.3720008\ttotal: 26.2s\tremaining: 1m 26s\n",
            "698:\tlearn: 0.3718935\ttotal: 26.3s\tremaining: 1m 26s\n",
            "699:\tlearn: 0.3718032\ttotal: 26.3s\tremaining: 1m 26s\n",
            "700:\tlearn: 0.3717273\ttotal: 26.3s\tremaining: 1m 26s\n",
            "701:\tlearn: 0.3716598\ttotal: 26.4s\tremaining: 1m 26s\n",
            "702:\tlearn: 0.3715731\ttotal: 26.4s\tremaining: 1m 26s\n",
            "703:\tlearn: 0.3714594\ttotal: 26.4s\tremaining: 1m 26s\n",
            "704:\tlearn: 0.3713684\ttotal: 26.5s\tremaining: 1m 26s\n",
            "705:\tlearn: 0.3712997\ttotal: 26.5s\tremaining: 1m 26s\n",
            "706:\tlearn: 0.3712166\ttotal: 26.5s\tremaining: 1m 26s\n",
            "707:\tlearn: 0.3711524\ttotal: 26.6s\tremaining: 1m 25s\n",
            "708:\tlearn: 0.3710423\ttotal: 26.6s\tremaining: 1m 25s\n",
            "709:\tlearn: 0.3709846\ttotal: 26.6s\tremaining: 1m 25s\n",
            "710:\tlearn: 0.3708850\ttotal: 26.6s\tremaining: 1m 25s\n",
            "711:\tlearn: 0.3707582\ttotal: 26.7s\tremaining: 1m 25s\n",
            "712:\tlearn: 0.3707321\ttotal: 26.7s\tremaining: 1m 25s\n",
            "713:\tlearn: 0.3706436\ttotal: 26.7s\tremaining: 1m 25s\n",
            "714:\tlearn: 0.3705749\ttotal: 26.8s\tremaining: 1m 25s\n",
            "715:\tlearn: 0.3705146\ttotal: 26.8s\tremaining: 1m 25s\n",
            "716:\tlearn: 0.3703864\ttotal: 26.8s\tremaining: 1m 25s\n",
            "717:\tlearn: 0.3703258\ttotal: 26.9s\tremaining: 1m 25s\n",
            "718:\tlearn: 0.3702698\ttotal: 26.9s\tremaining: 1m 25s\n",
            "719:\tlearn: 0.3702244\ttotal: 26.9s\tremaining: 1m 25s\n",
            "720:\tlearn: 0.3701500\ttotal: 26.9s\tremaining: 1m 25s\n",
            "721:\tlearn: 0.3700267\ttotal: 27s\tremaining: 1m 25s\n",
            "722:\tlearn: 0.3699171\ttotal: 27s\tremaining: 1m 25s\n",
            "723:\tlearn: 0.3698866\ttotal: 27s\tremaining: 1m 24s\n",
            "724:\tlearn: 0.3698161\ttotal: 27.1s\tremaining: 1m 24s\n",
            "725:\tlearn: 0.3697719\ttotal: 27.1s\tremaining: 1m 24s\n",
            "726:\tlearn: 0.3697352\ttotal: 27.1s\tremaining: 1m 24s\n",
            "727:\tlearn: 0.3696239\ttotal: 27.1s\tremaining: 1m 24s\n",
            "728:\tlearn: 0.3695584\ttotal: 27.2s\tremaining: 1m 24s\n",
            "729:\tlearn: 0.3694757\ttotal: 27.2s\tremaining: 1m 24s\n",
            "730:\tlearn: 0.3694386\ttotal: 27.2s\tremaining: 1m 24s\n",
            "731:\tlearn: 0.3693358\ttotal: 27.3s\tremaining: 1m 24s\n",
            "732:\tlearn: 0.3692650\ttotal: 27.3s\tremaining: 1m 24s\n",
            "733:\tlearn: 0.3691778\ttotal: 27.3s\tremaining: 1m 24s\n",
            "734:\tlearn: 0.3691123\ttotal: 27.4s\tremaining: 1m 24s\n",
            "735:\tlearn: 0.3690272\ttotal: 27.4s\tremaining: 1m 24s\n",
            "736:\tlearn: 0.3689289\ttotal: 27.4s\tremaining: 1m 24s\n",
            "737:\tlearn: 0.3688579\ttotal: 27.4s\tremaining: 1m 24s\n",
            "738:\tlearn: 0.3688152\ttotal: 27.5s\tremaining: 1m 24s\n",
            "739:\tlearn: 0.3687333\ttotal: 27.5s\tremaining: 1m 24s\n",
            "740:\tlearn: 0.3686415\ttotal: 27.5s\tremaining: 1m 23s\n",
            "741:\tlearn: 0.3685854\ttotal: 27.6s\tremaining: 1m 23s\n",
            "742:\tlearn: 0.3685423\ttotal: 27.6s\tremaining: 1m 23s\n",
            "743:\tlearn: 0.3684978\ttotal: 27.6s\tremaining: 1m 23s\n",
            "744:\tlearn: 0.3684261\ttotal: 27.7s\tremaining: 1m 23s\n",
            "745:\tlearn: 0.3683483\ttotal: 27.7s\tremaining: 1m 23s\n",
            "746:\tlearn: 0.3682621\ttotal: 27.7s\tremaining: 1m 23s\n",
            "747:\tlearn: 0.3681758\ttotal: 27.8s\tremaining: 1m 23s\n",
            "748:\tlearn: 0.3680947\ttotal: 27.8s\tremaining: 1m 23s\n",
            "749:\tlearn: 0.3680070\ttotal: 27.8s\tremaining: 1m 23s\n",
            "750:\tlearn: 0.3679073\ttotal: 27.8s\tremaining: 1m 23s\n",
            "751:\tlearn: 0.3678440\ttotal: 27.9s\tremaining: 1m 23s\n",
            "752:\tlearn: 0.3677734\ttotal: 27.9s\tremaining: 1m 23s\n",
            "753:\tlearn: 0.3676080\ttotal: 27.9s\tremaining: 1m 23s\n",
            "754:\tlearn: 0.3675289\ttotal: 27.9s\tremaining: 1m 23s\n",
            "755:\tlearn: 0.3674888\ttotal: 28s\tremaining: 1m 23s\n",
            "756:\tlearn: 0.3673898\ttotal: 28s\tremaining: 1m 23s\n",
            "757:\tlearn: 0.3673048\ttotal: 28.1s\tremaining: 1m 23s\n",
            "758:\tlearn: 0.3672089\ttotal: 28.1s\tremaining: 1m 23s\n",
            "759:\tlearn: 0.3671588\ttotal: 28.2s\tremaining: 1m 23s\n",
            "760:\tlearn: 0.3670807\ttotal: 28.2s\tremaining: 1m 23s\n",
            "761:\tlearn: 0.3669964\ttotal: 28.3s\tremaining: 1m 23s\n",
            "762:\tlearn: 0.3669381\ttotal: 28.4s\tremaining: 1m 23s\n",
            "763:\tlearn: 0.3669075\ttotal: 28.4s\tremaining: 1m 23s\n",
            "764:\tlearn: 0.3668101\ttotal: 28.5s\tremaining: 1m 23s\n",
            "765:\tlearn: 0.3667483\ttotal: 28.5s\tremaining: 1m 23s\n",
            "766:\tlearn: 0.3666885\ttotal: 28.6s\tremaining: 1m 23s\n",
            "767:\tlearn: 0.3666161\ttotal: 28.6s\tremaining: 1m 23s\n",
            "768:\tlearn: 0.3665506\ttotal: 28.7s\tremaining: 1m 23s\n",
            "769:\tlearn: 0.3664869\ttotal: 28.8s\tremaining: 1m 23s\n",
            "770:\tlearn: 0.3664417\ttotal: 28.8s\tremaining: 1m 23s\n",
            "771:\tlearn: 0.3664268\ttotal: 28.9s\tremaining: 1m 23s\n",
            "772:\tlearn: 0.3663532\ttotal: 28.9s\tremaining: 1m 23s\n",
            "773:\tlearn: 0.3663068\ttotal: 29s\tremaining: 1m 23s\n",
            "774:\tlearn: 0.3662497\ttotal: 29s\tremaining: 1m 23s\n",
            "775:\tlearn: 0.3662414\ttotal: 29.1s\tremaining: 1m 23s\n",
            "776:\tlearn: 0.3661946\ttotal: 29.2s\tremaining: 1m 23s\n",
            "777:\tlearn: 0.3661213\ttotal: 29.2s\tremaining: 1m 23s\n",
            "778:\tlearn: 0.3660382\ttotal: 29.3s\tremaining: 1m 23s\n",
            "779:\tlearn: 0.3659840\ttotal: 29.3s\tremaining: 1m 23s\n",
            "780:\tlearn: 0.3659459\ttotal: 29.4s\tremaining: 1m 23s\n",
            "781:\tlearn: 0.3658549\ttotal: 29.4s\tremaining: 1m 23s\n",
            "782:\tlearn: 0.3658179\ttotal: 29.5s\tremaining: 1m 23s\n",
            "783:\tlearn: 0.3657739\ttotal: 29.5s\tremaining: 1m 23s\n",
            "784:\tlearn: 0.3657053\ttotal: 29.6s\tremaining: 1m 23s\n",
            "785:\tlearn: 0.3656138\ttotal: 29.7s\tremaining: 1m 23s\n",
            "786:\tlearn: 0.3655117\ttotal: 29.7s\tremaining: 1m 23s\n",
            "787:\tlearn: 0.3653838\ttotal: 29.8s\tremaining: 1m 23s\n",
            "788:\tlearn: 0.3653201\ttotal: 29.8s\tremaining: 1m 23s\n",
            "789:\tlearn: 0.3652645\ttotal: 29.9s\tremaining: 1m 23s\n",
            "790:\tlearn: 0.3651796\ttotal: 29.9s\tremaining: 1m 23s\n",
            "791:\tlearn: 0.3650849\ttotal: 30s\tremaining: 1m 23s\n",
            "792:\tlearn: 0.3649625\ttotal: 30s\tremaining: 1m 23s\n",
            "793:\tlearn: 0.3648993\ttotal: 30.1s\tremaining: 1m 23s\n",
            "794:\tlearn: 0.3648099\ttotal: 30.2s\tremaining: 1m 23s\n",
            "795:\tlearn: 0.3647144\ttotal: 30.2s\tremaining: 1m 23s\n",
            "796:\tlearn: 0.3646314\ttotal: 30.3s\tremaining: 1m 23s\n",
            "797:\tlearn: 0.3645397\ttotal: 30.3s\tremaining: 1m 23s\n",
            "798:\tlearn: 0.3644314\ttotal: 30.4s\tremaining: 1m 23s\n",
            "799:\tlearn: 0.3643735\ttotal: 30.4s\tremaining: 1m 23s\n",
            "800:\tlearn: 0.3643271\ttotal: 30.5s\tremaining: 1m 23s\n",
            "801:\tlearn: 0.3642202\ttotal: 30.6s\tremaining: 1m 23s\n",
            "802:\tlearn: 0.3641789\ttotal: 30.6s\tremaining: 1m 23s\n",
            "803:\tlearn: 0.3640952\ttotal: 30.7s\tremaining: 1m 23s\n",
            "804:\tlearn: 0.3640870\ttotal: 30.7s\tremaining: 1m 23s\n",
            "805:\tlearn: 0.3640116\ttotal: 30.8s\tremaining: 1m 23s\n",
            "806:\tlearn: 0.3639256\ttotal: 30.8s\tremaining: 1m 23s\n",
            "807:\tlearn: 0.3638679\ttotal: 30.9s\tremaining: 1m 23s\n",
            "808:\tlearn: 0.3638000\ttotal: 31s\tremaining: 1m 23s\n",
            "809:\tlearn: 0.3637592\ttotal: 31s\tremaining: 1m 23s\n",
            "810:\tlearn: 0.3636822\ttotal: 31.1s\tremaining: 1m 23s\n",
            "811:\tlearn: 0.3636182\ttotal: 31.1s\tremaining: 1m 23s\n",
            "812:\tlearn: 0.3634853\ttotal: 31.2s\tremaining: 1m 23s\n",
            "813:\tlearn: 0.3634371\ttotal: 31.2s\tremaining: 1m 23s\n",
            "814:\tlearn: 0.3633455\ttotal: 31.3s\tremaining: 1m 23s\n",
            "815:\tlearn: 0.3632197\ttotal: 31.3s\tremaining: 1m 23s\n",
            "816:\tlearn: 0.3631659\ttotal: 31.4s\tremaining: 1m 23s\n",
            "817:\tlearn: 0.3631279\ttotal: 31.5s\tremaining: 1m 23s\n",
            "818:\tlearn: 0.3630276\ttotal: 31.5s\tremaining: 1m 23s\n",
            "819:\tlearn: 0.3629542\ttotal: 31.5s\tremaining: 1m 23s\n",
            "820:\tlearn: 0.3628724\ttotal: 31.5s\tremaining: 1m 23s\n",
            "821:\tlearn: 0.3628254\ttotal: 31.6s\tremaining: 1m 23s\n",
            "822:\tlearn: 0.3627730\ttotal: 31.6s\tremaining: 1m 23s\n",
            "823:\tlearn: 0.3626876\ttotal: 31.6s\tremaining: 1m 23s\n",
            "824:\tlearn: 0.3626504\ttotal: 31.7s\tremaining: 1m 23s\n",
            "825:\tlearn: 0.3625960\ttotal: 31.7s\tremaining: 1m 23s\n",
            "826:\tlearn: 0.3625209\ttotal: 31.7s\tremaining: 1m 23s\n",
            "827:\tlearn: 0.3624533\ttotal: 31.8s\tremaining: 1m 23s\n",
            "828:\tlearn: 0.3623394\ttotal: 31.8s\tremaining: 1m 23s\n",
            "829:\tlearn: 0.3623147\ttotal: 31.8s\tremaining: 1m 23s\n",
            "830:\tlearn: 0.3622225\ttotal: 31.8s\tremaining: 1m 23s\n",
            "831:\tlearn: 0.3621832\ttotal: 31.9s\tremaining: 1m 23s\n",
            "832:\tlearn: 0.3621355\ttotal: 31.9s\tremaining: 1m 22s\n",
            "833:\tlearn: 0.3620622\ttotal: 31.9s\tremaining: 1m 22s\n",
            "834:\tlearn: 0.3620215\ttotal: 32s\tremaining: 1m 22s\n",
            "835:\tlearn: 0.3619474\ttotal: 32s\tremaining: 1m 22s\n",
            "836:\tlearn: 0.3618450\ttotal: 32s\tremaining: 1m 22s\n",
            "837:\tlearn: 0.3617829\ttotal: 32.1s\tremaining: 1m 22s\n",
            "838:\tlearn: 0.3617219\ttotal: 32.1s\tremaining: 1m 22s\n",
            "839:\tlearn: 0.3616035\ttotal: 32.1s\tremaining: 1m 22s\n",
            "840:\tlearn: 0.3615937\ttotal: 32.1s\tremaining: 1m 22s\n",
            "841:\tlearn: 0.3615084\ttotal: 32.2s\tremaining: 1m 22s\n",
            "842:\tlearn: 0.3614372\ttotal: 32.2s\tremaining: 1m 22s\n",
            "843:\tlearn: 0.3613727\ttotal: 32.2s\tremaining: 1m 22s\n",
            "844:\tlearn: 0.3613388\ttotal: 32.3s\tremaining: 1m 22s\n",
            "845:\tlearn: 0.3612287\ttotal: 32.3s\tremaining: 1m 22s\n",
            "846:\tlearn: 0.3611313\ttotal: 32.3s\tremaining: 1m 22s\n",
            "847:\tlearn: 0.3610122\ttotal: 32.4s\tremaining: 1m 22s\n",
            "848:\tlearn: 0.3609384\ttotal: 32.4s\tremaining: 1m 22s\n",
            "849:\tlearn: 0.3608735\ttotal: 32.4s\tremaining: 1m 21s\n",
            "850:\tlearn: 0.3607752\ttotal: 32.4s\tremaining: 1m 21s\n",
            "851:\tlearn: 0.3607023\ttotal: 32.5s\tremaining: 1m 21s\n",
            "852:\tlearn: 0.3606657\ttotal: 32.5s\tremaining: 1m 21s\n",
            "853:\tlearn: 0.3605905\ttotal: 32.5s\tremaining: 1m 21s\n",
            "854:\tlearn: 0.3605712\ttotal: 32.6s\tremaining: 1m 21s\n",
            "855:\tlearn: 0.3605285\ttotal: 32.6s\tremaining: 1m 21s\n",
            "856:\tlearn: 0.3603831\ttotal: 32.6s\tremaining: 1m 21s\n",
            "857:\tlearn: 0.3603077\ttotal: 32.7s\tremaining: 1m 21s\n",
            "858:\tlearn: 0.3602545\ttotal: 32.7s\tremaining: 1m 21s\n",
            "859:\tlearn: 0.3602263\ttotal: 32.7s\tremaining: 1m 21s\n",
            "860:\tlearn: 0.3601642\ttotal: 32.8s\tremaining: 1m 21s\n",
            "861:\tlearn: 0.3601024\ttotal: 32.8s\tremaining: 1m 21s\n",
            "862:\tlearn: 0.3600253\ttotal: 32.8s\tremaining: 1m 21s\n",
            "863:\tlearn: 0.3599331\ttotal: 32.8s\tremaining: 1m 21s\n",
            "864:\tlearn: 0.3598641\ttotal: 32.9s\tremaining: 1m 21s\n",
            "865:\tlearn: 0.3597953\ttotal: 32.9s\tremaining: 1m 21s\n",
            "866:\tlearn: 0.3597619\ttotal: 32.9s\tremaining: 1m 21s\n",
            "867:\tlearn: 0.3597084\ttotal: 33s\tremaining: 1m 20s\n",
            "868:\tlearn: 0.3596544\ttotal: 33s\tremaining: 1m 20s\n",
            "869:\tlearn: 0.3596296\ttotal: 33s\tremaining: 1m 20s\n",
            "870:\tlearn: 0.3595319\ttotal: 33s\tremaining: 1m 20s\n",
            "871:\tlearn: 0.3594587\ttotal: 33.1s\tremaining: 1m 20s\n",
            "872:\tlearn: 0.3593964\ttotal: 33.1s\tremaining: 1m 20s\n",
            "873:\tlearn: 0.3593443\ttotal: 33.1s\tremaining: 1m 20s\n",
            "874:\tlearn: 0.3592893\ttotal: 33.2s\tremaining: 1m 20s\n",
            "875:\tlearn: 0.3591937\ttotal: 33.2s\tremaining: 1m 20s\n",
            "876:\tlearn: 0.3591250\ttotal: 33.2s\tremaining: 1m 20s\n",
            "877:\tlearn: 0.3590480\ttotal: 33.2s\tremaining: 1m 20s\n",
            "878:\tlearn: 0.3590171\ttotal: 33.3s\tremaining: 1m 20s\n",
            "879:\tlearn: 0.3589032\ttotal: 33.3s\tremaining: 1m 20s\n",
            "880:\tlearn: 0.3588540\ttotal: 33.3s\tremaining: 1m 20s\n",
            "881:\tlearn: 0.3587618\ttotal: 33.4s\tremaining: 1m 20s\n",
            "882:\tlearn: 0.3587328\ttotal: 33.4s\tremaining: 1m 20s\n",
            "883:\tlearn: 0.3586494\ttotal: 33.4s\tremaining: 1m 20s\n",
            "884:\tlearn: 0.3585869\ttotal: 33.5s\tremaining: 1m 19s\n",
            "885:\tlearn: 0.3585257\ttotal: 33.5s\tremaining: 1m 19s\n",
            "886:\tlearn: 0.3584601\ttotal: 33.5s\tremaining: 1m 19s\n",
            "887:\tlearn: 0.3583798\ttotal: 33.6s\tremaining: 1m 19s\n",
            "888:\tlearn: 0.3583314\ttotal: 33.6s\tremaining: 1m 19s\n",
            "889:\tlearn: 0.3582577\ttotal: 33.6s\tremaining: 1m 19s\n",
            "890:\tlearn: 0.3581648\ttotal: 33.6s\tremaining: 1m 19s\n",
            "891:\tlearn: 0.3581132\ttotal: 33.7s\tremaining: 1m 19s\n",
            "892:\tlearn: 0.3580166\ttotal: 33.7s\tremaining: 1m 19s\n",
            "893:\tlearn: 0.3579648\ttotal: 33.7s\tremaining: 1m 19s\n",
            "894:\tlearn: 0.3578259\ttotal: 33.8s\tremaining: 1m 19s\n",
            "895:\tlearn: 0.3577626\ttotal: 33.8s\tremaining: 1m 19s\n",
            "896:\tlearn: 0.3576823\ttotal: 33.8s\tremaining: 1m 19s\n",
            "897:\tlearn: 0.3576435\ttotal: 33.9s\tremaining: 1m 19s\n",
            "898:\tlearn: 0.3575550\ttotal: 33.9s\tremaining: 1m 19s\n",
            "899:\tlearn: 0.3574943\ttotal: 33.9s\tremaining: 1m 19s\n",
            "900:\tlearn: 0.3573914\ttotal: 33.9s\tremaining: 1m 19s\n",
            "901:\tlearn: 0.3573004\ttotal: 34s\tremaining: 1m 19s\n",
            "902:\tlearn: 0.3572172\ttotal: 34s\tremaining: 1m 18s\n",
            "903:\tlearn: 0.3571418\ttotal: 34s\tremaining: 1m 18s\n",
            "904:\tlearn: 0.3570863\ttotal: 34.1s\tremaining: 1m 18s\n",
            "905:\tlearn: 0.3569700\ttotal: 34.1s\tremaining: 1m 18s\n",
            "906:\tlearn: 0.3569036\ttotal: 34.1s\tremaining: 1m 18s\n",
            "907:\tlearn: 0.3568732\ttotal: 34.2s\tremaining: 1m 18s\n",
            "908:\tlearn: 0.3568242\ttotal: 34.2s\tremaining: 1m 18s\n",
            "909:\tlearn: 0.3567482\ttotal: 34.2s\tremaining: 1m 18s\n",
            "910:\tlearn: 0.3566315\ttotal: 34.2s\tremaining: 1m 18s\n",
            "911:\tlearn: 0.3565847\ttotal: 34.3s\tremaining: 1m 18s\n",
            "912:\tlearn: 0.3565125\ttotal: 34.3s\tremaining: 1m 18s\n",
            "913:\tlearn: 0.3564025\ttotal: 34.3s\tremaining: 1m 18s\n",
            "914:\tlearn: 0.3563653\ttotal: 34.4s\tremaining: 1m 18s\n",
            "915:\tlearn: 0.3563031\ttotal: 34.4s\tremaining: 1m 18s\n",
            "916:\tlearn: 0.3562732\ttotal: 34.4s\tremaining: 1m 18s\n",
            "917:\tlearn: 0.3562207\ttotal: 34.5s\tremaining: 1m 18s\n",
            "918:\tlearn: 0.3561425\ttotal: 34.5s\tremaining: 1m 18s\n",
            "919:\tlearn: 0.3560422\ttotal: 34.5s\tremaining: 1m 18s\n",
            "920:\tlearn: 0.3559413\ttotal: 34.5s\tremaining: 1m 17s\n",
            "921:\tlearn: 0.3558875\ttotal: 34.6s\tremaining: 1m 17s\n",
            "922:\tlearn: 0.3558511\ttotal: 34.6s\tremaining: 1m 17s\n",
            "923:\tlearn: 0.3558004\ttotal: 34.6s\tremaining: 1m 17s\n",
            "924:\tlearn: 0.3557311\ttotal: 34.7s\tremaining: 1m 17s\n",
            "925:\tlearn: 0.3556840\ttotal: 34.7s\tremaining: 1m 17s\n",
            "926:\tlearn: 0.3556118\ttotal: 34.7s\tremaining: 1m 17s\n",
            "927:\tlearn: 0.3555473\ttotal: 34.8s\tremaining: 1m 17s\n",
            "928:\tlearn: 0.3555294\ttotal: 34.8s\tremaining: 1m 17s\n",
            "929:\tlearn: 0.3554127\ttotal: 34.8s\tremaining: 1m 17s\n",
            "930:\tlearn: 0.3553672\ttotal: 34.8s\tremaining: 1m 17s\n",
            "931:\tlearn: 0.3552741\ttotal: 34.9s\tremaining: 1m 17s\n",
            "932:\tlearn: 0.3552158\ttotal: 34.9s\tremaining: 1m 17s\n",
            "933:\tlearn: 0.3551865\ttotal: 34.9s\tremaining: 1m 17s\n",
            "934:\tlearn: 0.3551277\ttotal: 35s\tremaining: 1m 17s\n",
            "935:\tlearn: 0.3550703\ttotal: 35s\tremaining: 1m 17s\n",
            "936:\tlearn: 0.3549974\ttotal: 35s\tremaining: 1m 17s\n",
            "937:\tlearn: 0.3549492\ttotal: 35.1s\tremaining: 1m 17s\n",
            "938:\tlearn: 0.3548822\ttotal: 35.1s\tremaining: 1m 17s\n",
            "939:\tlearn: 0.3547936\ttotal: 35.1s\tremaining: 1m 16s\n",
            "940:\tlearn: 0.3547718\ttotal: 35.1s\tremaining: 1m 16s\n",
            "941:\tlearn: 0.3547000\ttotal: 35.2s\tremaining: 1m 16s\n",
            "942:\tlearn: 0.3546509\ttotal: 35.2s\tremaining: 1m 16s\n",
            "943:\tlearn: 0.3545985\ttotal: 35.2s\tremaining: 1m 16s\n",
            "944:\tlearn: 0.3545715\ttotal: 35.3s\tremaining: 1m 16s\n",
            "945:\tlearn: 0.3545106\ttotal: 35.3s\tremaining: 1m 16s\n",
            "946:\tlearn: 0.3544479\ttotal: 35.3s\tremaining: 1m 16s\n",
            "947:\tlearn: 0.3543661\ttotal: 35.4s\tremaining: 1m 16s\n",
            "948:\tlearn: 0.3542861\ttotal: 35.4s\tremaining: 1m 16s\n",
            "949:\tlearn: 0.3541811\ttotal: 35.4s\tremaining: 1m 16s\n",
            "950:\tlearn: 0.3540748\ttotal: 35.4s\tremaining: 1m 16s\n",
            "951:\tlearn: 0.3539659\ttotal: 35.5s\tremaining: 1m 16s\n",
            "952:\tlearn: 0.3539056\ttotal: 35.5s\tremaining: 1m 16s\n",
            "953:\tlearn: 0.3538477\ttotal: 35.5s\tremaining: 1m 16s\n",
            "954:\tlearn: 0.3537797\ttotal: 35.6s\tremaining: 1m 16s\n",
            "955:\tlearn: 0.3537303\ttotal: 35.6s\tremaining: 1m 16s\n",
            "956:\tlearn: 0.3536979\ttotal: 35.6s\tremaining: 1m 16s\n",
            "957:\tlearn: 0.3536487\ttotal: 35.6s\tremaining: 1m 15s\n",
            "958:\tlearn: 0.3536110\ttotal: 35.7s\tremaining: 1m 15s\n",
            "959:\tlearn: 0.3535200\ttotal: 35.7s\tremaining: 1m 15s\n",
            "960:\tlearn: 0.3534736\ttotal: 35.8s\tremaining: 1m 15s\n",
            "961:\tlearn: 0.3534358\ttotal: 35.8s\tremaining: 1m 15s\n",
            "962:\tlearn: 0.3533847\ttotal: 35.8s\tremaining: 1m 15s\n",
            "963:\tlearn: 0.3533419\ttotal: 35.8s\tremaining: 1m 15s\n",
            "964:\tlearn: 0.3532667\ttotal: 35.9s\tremaining: 1m 15s\n",
            "965:\tlearn: 0.3532066\ttotal: 35.9s\tremaining: 1m 15s\n",
            "966:\tlearn: 0.3531641\ttotal: 35.9s\tremaining: 1m 15s\n",
            "967:\tlearn: 0.3531227\ttotal: 36s\tremaining: 1m 15s\n",
            "968:\tlearn: 0.3530513\ttotal: 36s\tremaining: 1m 15s\n",
            "969:\tlearn: 0.3529990\ttotal: 36s\tremaining: 1m 15s\n",
            "970:\tlearn: 0.3529234\ttotal: 36.1s\tremaining: 1m 15s\n",
            "971:\tlearn: 0.3528497\ttotal: 36.1s\tremaining: 1m 15s\n",
            "972:\tlearn: 0.3527974\ttotal: 36.1s\tremaining: 1m 15s\n",
            "973:\tlearn: 0.3527414\ttotal: 36.2s\tremaining: 1m 15s\n",
            "974:\tlearn: 0.3526546\ttotal: 36.2s\tremaining: 1m 15s\n",
            "975:\tlearn: 0.3525923\ttotal: 36.2s\tremaining: 1m 15s\n",
            "976:\tlearn: 0.3525401\ttotal: 36.2s\tremaining: 1m 15s\n",
            "977:\tlearn: 0.3524898\ttotal: 36.3s\tremaining: 1m 14s\n",
            "978:\tlearn: 0.3524855\ttotal: 36.3s\tremaining: 1m 14s\n",
            "979:\tlearn: 0.3523975\ttotal: 36.3s\tremaining: 1m 14s\n",
            "980:\tlearn: 0.3523433\ttotal: 36.4s\tremaining: 1m 14s\n",
            "981:\tlearn: 0.3522533\ttotal: 36.4s\tremaining: 1m 14s\n",
            "982:\tlearn: 0.3522189\ttotal: 36.4s\tremaining: 1m 14s\n",
            "983:\tlearn: 0.3521683\ttotal: 36.4s\tremaining: 1m 14s\n",
            "984:\tlearn: 0.3520906\ttotal: 36.5s\tremaining: 1m 14s\n",
            "985:\tlearn: 0.3520142\ttotal: 36.5s\tremaining: 1m 14s\n",
            "986:\tlearn: 0.3519562\ttotal: 36.5s\tremaining: 1m 14s\n",
            "987:\tlearn: 0.3518587\ttotal: 36.6s\tremaining: 1m 14s\n",
            "988:\tlearn: 0.3518256\ttotal: 36.6s\tremaining: 1m 14s\n",
            "989:\tlearn: 0.3517101\ttotal: 36.6s\tremaining: 1m 14s\n",
            "990:\tlearn: 0.3516339\ttotal: 36.7s\tremaining: 1m 14s\n",
            "991:\tlearn: 0.3515031\ttotal: 36.7s\tremaining: 1m 14s\n",
            "992:\tlearn: 0.3514332\ttotal: 36.7s\tremaining: 1m 14s\n",
            "993:\tlearn: 0.3513404\ttotal: 36.8s\tremaining: 1m 14s\n",
            "994:\tlearn: 0.3513231\ttotal: 36.8s\tremaining: 1m 14s\n",
            "995:\tlearn: 0.3512799\ttotal: 36.8s\tremaining: 1m 14s\n",
            "996:\tlearn: 0.3512436\ttotal: 36.8s\tremaining: 1m 14s\n",
            "997:\tlearn: 0.3511855\ttotal: 36.9s\tremaining: 1m 13s\n",
            "998:\tlearn: 0.3511064\ttotal: 36.9s\tremaining: 1m 13s\n",
            "999:\tlearn: 0.3510531\ttotal: 36.9s\tremaining: 1m 13s\n",
            "1000:\tlearn: 0.3510092\ttotal: 37s\tremaining: 1m 13s\n",
            "1001:\tlearn: 0.3509683\ttotal: 37s\tremaining: 1m 13s\n",
            "1002:\tlearn: 0.3508930\ttotal: 37s\tremaining: 1m 13s\n",
            "1003:\tlearn: 0.3508224\ttotal: 37.1s\tremaining: 1m 13s\n",
            "1004:\tlearn: 0.3507938\ttotal: 37.1s\tremaining: 1m 13s\n",
            "1005:\tlearn: 0.3507064\ttotal: 37.1s\tremaining: 1m 13s\n",
            "1006:\tlearn: 0.3506503\ttotal: 37.1s\tremaining: 1m 13s\n",
            "1007:\tlearn: 0.3506298\ttotal: 37.2s\tremaining: 1m 13s\n",
            "1008:\tlearn: 0.3505710\ttotal: 37.2s\tremaining: 1m 13s\n",
            "1009:\tlearn: 0.3504501\ttotal: 37.2s\tremaining: 1m 13s\n",
            "1010:\tlearn: 0.3503599\ttotal: 37.3s\tremaining: 1m 13s\n",
            "1011:\tlearn: 0.3502852\ttotal: 37.3s\tremaining: 1m 13s\n",
            "1012:\tlearn: 0.3502177\ttotal: 37.3s\tremaining: 1m 13s\n",
            "1013:\tlearn: 0.3501597\ttotal: 37.4s\tremaining: 1m 13s\n",
            "1014:\tlearn: 0.3500810\ttotal: 37.4s\tremaining: 1m 13s\n",
            "1015:\tlearn: 0.3500360\ttotal: 37.4s\tremaining: 1m 13s\n",
            "1016:\tlearn: 0.3499870\ttotal: 37.5s\tremaining: 1m 13s\n",
            "1017:\tlearn: 0.3499524\ttotal: 37.5s\tremaining: 1m 12s\n",
            "1018:\tlearn: 0.3499099\ttotal: 37.5s\tremaining: 1m 12s\n",
            "1019:\tlearn: 0.3498379\ttotal: 37.6s\tremaining: 1m 12s\n",
            "1020:\tlearn: 0.3497510\ttotal: 37.6s\tremaining: 1m 12s\n",
            "1021:\tlearn: 0.3496762\ttotal: 37.6s\tremaining: 1m 12s\n",
            "1022:\tlearn: 0.3496406\ttotal: 37.6s\tremaining: 1m 12s\n",
            "1023:\tlearn: 0.3495553\ttotal: 37.7s\tremaining: 1m 12s\n",
            "1024:\tlearn: 0.3494749\ttotal: 37.7s\tremaining: 1m 12s\n",
            "1025:\tlearn: 0.3494412\ttotal: 37.7s\tremaining: 1m 12s\n",
            "1026:\tlearn: 0.3493733\ttotal: 37.8s\tremaining: 1m 12s\n",
            "1027:\tlearn: 0.3493118\ttotal: 37.8s\tremaining: 1m 12s\n",
            "1028:\tlearn: 0.3492722\ttotal: 37.8s\tremaining: 1m 12s\n",
            "1029:\tlearn: 0.3492262\ttotal: 37.9s\tremaining: 1m 12s\n",
            "1030:\tlearn: 0.3491958\ttotal: 37.9s\tremaining: 1m 12s\n",
            "1031:\tlearn: 0.3491083\ttotal: 37.9s\tremaining: 1m 12s\n",
            "1032:\tlearn: 0.3490781\ttotal: 38s\tremaining: 1m 12s\n",
            "1033:\tlearn: 0.3490185\ttotal: 38s\tremaining: 1m 12s\n",
            "1034:\tlearn: 0.3489576\ttotal: 38s\tremaining: 1m 12s\n",
            "1035:\tlearn: 0.3489055\ttotal: 38s\tremaining: 1m 12s\n",
            "1036:\tlearn: 0.3488606\ttotal: 38.1s\tremaining: 1m 12s\n",
            "1037:\tlearn: 0.3488105\ttotal: 38.1s\tremaining: 1m 12s\n",
            "1038:\tlearn: 0.3487518\ttotal: 38.1s\tremaining: 1m 11s\n",
            "1039:\tlearn: 0.3486470\ttotal: 38.2s\tremaining: 1m 11s\n",
            "1040:\tlearn: 0.3486068\ttotal: 38.2s\tremaining: 1m 11s\n",
            "1041:\tlearn: 0.3484644\ttotal: 38.2s\tremaining: 1m 11s\n",
            "1042:\tlearn: 0.3483463\ttotal: 38.3s\tremaining: 1m 11s\n",
            "1043:\tlearn: 0.3483200\ttotal: 38.3s\tremaining: 1m 11s\n",
            "1044:\tlearn: 0.3482254\ttotal: 38.3s\tremaining: 1m 11s\n",
            "1045:\tlearn: 0.3481410\ttotal: 38.4s\tremaining: 1m 11s\n",
            "1046:\tlearn: 0.3480309\ttotal: 38.4s\tremaining: 1m 11s\n",
            "1047:\tlearn: 0.3479816\ttotal: 38.4s\tremaining: 1m 11s\n",
            "1048:\tlearn: 0.3479289\ttotal: 38.4s\tremaining: 1m 11s\n",
            "1049:\tlearn: 0.3478615\ttotal: 38.5s\tremaining: 1m 11s\n",
            "1050:\tlearn: 0.3477779\ttotal: 38.5s\tremaining: 1m 11s\n",
            "1051:\tlearn: 0.3476918\ttotal: 38.5s\tremaining: 1m 11s\n",
            "1052:\tlearn: 0.3476070\ttotal: 38.6s\tremaining: 1m 11s\n",
            "1053:\tlearn: 0.3475170\ttotal: 38.6s\tremaining: 1m 11s\n",
            "1054:\tlearn: 0.3474646\ttotal: 38.6s\tremaining: 1m 11s\n",
            "1055:\tlearn: 0.3473953\ttotal: 38.6s\tremaining: 1m 11s\n",
            "1056:\tlearn: 0.3473160\ttotal: 38.7s\tremaining: 1m 11s\n",
            "1057:\tlearn: 0.3472525\ttotal: 38.7s\tremaining: 1m 11s\n",
            "1058:\tlearn: 0.3472037\ttotal: 38.7s\tremaining: 1m 10s\n",
            "1059:\tlearn: 0.3471349\ttotal: 38.8s\tremaining: 1m 10s\n",
            "1060:\tlearn: 0.3470396\ttotal: 38.8s\tremaining: 1m 10s\n",
            "1061:\tlearn: 0.3469364\ttotal: 38.8s\tremaining: 1m 10s\n",
            "1062:\tlearn: 0.3468459\ttotal: 38.9s\tremaining: 1m 10s\n",
            "1063:\tlearn: 0.3468426\ttotal: 38.9s\tremaining: 1m 10s\n",
            "1064:\tlearn: 0.3468172\ttotal: 38.9s\tremaining: 1m 10s\n",
            "1065:\tlearn: 0.3467943\ttotal: 38.9s\tremaining: 1m 10s\n",
            "1066:\tlearn: 0.3466678\ttotal: 39s\tremaining: 1m 10s\n",
            "1067:\tlearn: 0.3465636\ttotal: 39s\tremaining: 1m 10s\n",
            "1068:\tlearn: 0.3465073\ttotal: 39s\tremaining: 1m 10s\n",
            "1069:\tlearn: 0.3464861\ttotal: 39.1s\tremaining: 1m 10s\n",
            "1070:\tlearn: 0.3464260\ttotal: 39.1s\tremaining: 1m 10s\n",
            "1071:\tlearn: 0.3463496\ttotal: 39.1s\tremaining: 1m 10s\n",
            "1072:\tlearn: 0.3463133\ttotal: 39.2s\tremaining: 1m 10s\n",
            "1073:\tlearn: 0.3462546\ttotal: 39.2s\tremaining: 1m 10s\n",
            "1074:\tlearn: 0.3461733\ttotal: 39.2s\tremaining: 1m 10s\n",
            "1075:\tlearn: 0.3461681\ttotal: 39.3s\tremaining: 1m 10s\n",
            "1076:\tlearn: 0.3461217\ttotal: 39.3s\tremaining: 1m 10s\n",
            "1077:\tlearn: 0.3460638\ttotal: 39.3s\tremaining: 1m 10s\n",
            "1078:\tlearn: 0.3460147\ttotal: 39.3s\tremaining: 1m 10s\n",
            "1079:\tlearn: 0.3460013\ttotal: 39.4s\tremaining: 1m 9s\n",
            "1080:\tlearn: 0.3458969\ttotal: 39.4s\tremaining: 1m 9s\n",
            "1081:\tlearn: 0.3458505\ttotal: 39.4s\tremaining: 1m 9s\n",
            "1082:\tlearn: 0.3458273\ttotal: 39.5s\tremaining: 1m 9s\n",
            "1083:\tlearn: 0.3457484\ttotal: 39.5s\tremaining: 1m 9s\n",
            "1084:\tlearn: 0.3456879\ttotal: 39.5s\tremaining: 1m 9s\n",
            "1085:\tlearn: 0.3456420\ttotal: 39.5s\tremaining: 1m 9s\n",
            "1086:\tlearn: 0.3455548\ttotal: 39.6s\tremaining: 1m 9s\n",
            "1087:\tlearn: 0.3454623\ttotal: 39.6s\tremaining: 1m 9s\n",
            "1088:\tlearn: 0.3453727\ttotal: 39.6s\tremaining: 1m 9s\n",
            "1089:\tlearn: 0.3452916\ttotal: 39.7s\tremaining: 1m 9s\n",
            "1090:\tlearn: 0.3452188\ttotal: 39.7s\tremaining: 1m 9s\n",
            "1091:\tlearn: 0.3451771\ttotal: 39.7s\tremaining: 1m 9s\n",
            "1092:\tlearn: 0.3451181\ttotal: 39.7s\tremaining: 1m 9s\n",
            "1093:\tlearn: 0.3450507\ttotal: 39.8s\tremaining: 1m 9s\n",
            "1094:\tlearn: 0.3450110\ttotal: 39.8s\tremaining: 1m 9s\n",
            "1095:\tlearn: 0.3449994\ttotal: 39.9s\tremaining: 1m 9s\n",
            "1096:\tlearn: 0.3449130\ttotal: 39.9s\tremaining: 1m 9s\n",
            "1097:\tlearn: 0.3448279\ttotal: 39.9s\tremaining: 1m 9s\n",
            "1098:\tlearn: 0.3447774\ttotal: 39.9s\tremaining: 1m 9s\n",
            "1099:\tlearn: 0.3447410\ttotal: 40s\tremaining: 1m 9s\n",
            "1100:\tlearn: 0.3446894\ttotal: 40s\tremaining: 1m 8s\n",
            "1101:\tlearn: 0.3446336\ttotal: 40s\tremaining: 1m 8s\n",
            "1102:\tlearn: 0.3445411\ttotal: 40.1s\tremaining: 1m 8s\n",
            "1103:\tlearn: 0.3444922\ttotal: 40.1s\tremaining: 1m 8s\n",
            "1104:\tlearn: 0.3444342\ttotal: 40.1s\tremaining: 1m 8s\n",
            "1105:\tlearn: 0.3443483\ttotal: 40.1s\tremaining: 1m 8s\n",
            "1106:\tlearn: 0.3442641\ttotal: 40.2s\tremaining: 1m 8s\n",
            "1107:\tlearn: 0.3442091\ttotal: 40.2s\tremaining: 1m 8s\n",
            "1108:\tlearn: 0.3441426\ttotal: 40.2s\tremaining: 1m 8s\n",
            "1109:\tlearn: 0.3440546\ttotal: 40.3s\tremaining: 1m 8s\n",
            "1110:\tlearn: 0.3439424\ttotal: 40.3s\tremaining: 1m 8s\n",
            "1111:\tlearn: 0.3438668\ttotal: 40.3s\tremaining: 1m 8s\n",
            "1112:\tlearn: 0.3438236\ttotal: 40.4s\tremaining: 1m 8s\n",
            "1113:\tlearn: 0.3437441\ttotal: 40.4s\tremaining: 1m 8s\n",
            "1114:\tlearn: 0.3437031\ttotal: 40.4s\tremaining: 1m 8s\n",
            "1115:\tlearn: 0.3435892\ttotal: 40.5s\tremaining: 1m 8s\n",
            "1116:\tlearn: 0.3435404\ttotal: 40.5s\tremaining: 1m 8s\n",
            "1117:\tlearn: 0.3434518\ttotal: 40.5s\tremaining: 1m 8s\n",
            "1118:\tlearn: 0.3434319\ttotal: 40.5s\tremaining: 1m 8s\n",
            "1119:\tlearn: 0.3433769\ttotal: 40.6s\tremaining: 1m 8s\n",
            "1120:\tlearn: 0.3433715\ttotal: 40.6s\tremaining: 1m 8s\n",
            "1121:\tlearn: 0.3433271\ttotal: 40.6s\tremaining: 1m 7s\n",
            "1122:\tlearn: 0.3432736\ttotal: 40.7s\tremaining: 1m 7s\n",
            "1123:\tlearn: 0.3431934\ttotal: 40.7s\tremaining: 1m 7s\n",
            "1124:\tlearn: 0.3431016\ttotal: 40.7s\tremaining: 1m 7s\n",
            "1125:\tlearn: 0.3430628\ttotal: 40.7s\tremaining: 1m 7s\n",
            "1126:\tlearn: 0.3430080\ttotal: 40.8s\tremaining: 1m 7s\n",
            "1127:\tlearn: 0.3429291\ttotal: 40.8s\tremaining: 1m 7s\n",
            "1128:\tlearn: 0.3428936\ttotal: 40.8s\tremaining: 1m 7s\n",
            "1129:\tlearn: 0.3428226\ttotal: 40.9s\tremaining: 1m 7s\n",
            "1130:\tlearn: 0.3427509\ttotal: 40.9s\tremaining: 1m 7s\n",
            "1131:\tlearn: 0.3427020\ttotal: 40.9s\tremaining: 1m 7s\n",
            "1132:\tlearn: 0.3426767\ttotal: 41s\tremaining: 1m 7s\n",
            "1133:\tlearn: 0.3426033\ttotal: 41s\tremaining: 1m 7s\n",
            "1134:\tlearn: 0.3425108\ttotal: 41s\tremaining: 1m 7s\n",
            "1135:\tlearn: 0.3424639\ttotal: 41s\tremaining: 1m 7s\n",
            "1136:\tlearn: 0.3424072\ttotal: 41.1s\tremaining: 1m 7s\n",
            "1137:\tlearn: 0.3423817\ttotal: 41.1s\tremaining: 1m 7s\n",
            "1138:\tlearn: 0.3423398\ttotal: 41.1s\tremaining: 1m 7s\n",
            "1139:\tlearn: 0.3422705\ttotal: 41.2s\tremaining: 1m 7s\n",
            "1140:\tlearn: 0.3421865\ttotal: 41.2s\tremaining: 1m 7s\n",
            "1141:\tlearn: 0.3420709\ttotal: 41.2s\tremaining: 1m 7s\n",
            "1142:\tlearn: 0.3420420\ttotal: 41.3s\tremaining: 1m 7s\n",
            "1143:\tlearn: 0.3419541\ttotal: 41.3s\tremaining: 1m 6s\n",
            "1144:\tlearn: 0.3419203\ttotal: 41.3s\tremaining: 1m 6s\n",
            "1145:\tlearn: 0.3418910\ttotal: 41.4s\tremaining: 1m 6s\n",
            "1146:\tlearn: 0.3418528\ttotal: 41.4s\tremaining: 1m 6s\n",
            "1147:\tlearn: 0.3417766\ttotal: 41.5s\tremaining: 1m 6s\n",
            "1148:\tlearn: 0.3417181\ttotal: 41.5s\tremaining: 1m 6s\n",
            "1149:\tlearn: 0.3416841\ttotal: 41.6s\tremaining: 1m 6s\n",
            "1150:\tlearn: 0.3416210\ttotal: 41.6s\tremaining: 1m 6s\n",
            "1151:\tlearn: 0.3415698\ttotal: 41.7s\tremaining: 1m 6s\n",
            "1152:\tlearn: 0.3414930\ttotal: 41.7s\tremaining: 1m 6s\n",
            "1153:\tlearn: 0.3414643\ttotal: 41.8s\tremaining: 1m 6s\n",
            "1154:\tlearn: 0.3413915\ttotal: 41.8s\tremaining: 1m 6s\n",
            "1155:\tlearn: 0.3413400\ttotal: 41.9s\tremaining: 1m 6s\n",
            "1156:\tlearn: 0.3412796\ttotal: 41.9s\tremaining: 1m 6s\n",
            "1157:\tlearn: 0.3412001\ttotal: 42s\tremaining: 1m 6s\n",
            "1158:\tlearn: 0.3411394\ttotal: 42s\tremaining: 1m 6s\n",
            "1159:\tlearn: 0.3411050\ttotal: 42.1s\tremaining: 1m 6s\n",
            "1160:\tlearn: 0.3410275\ttotal: 42.2s\tremaining: 1m 6s\n",
            "1161:\tlearn: 0.3409420\ttotal: 42.2s\tremaining: 1m 6s\n",
            "1162:\tlearn: 0.3408885\ttotal: 42.3s\tremaining: 1m 6s\n",
            "1163:\tlearn: 0.3408579\ttotal: 42.3s\tremaining: 1m 6s\n",
            "1164:\tlearn: 0.3408071\ttotal: 42.4s\tremaining: 1m 6s\n",
            "1165:\tlearn: 0.3407570\ttotal: 42.5s\tremaining: 1m 6s\n",
            "1166:\tlearn: 0.3406864\ttotal: 42.5s\tremaining: 1m 6s\n",
            "1167:\tlearn: 0.3406609\ttotal: 42.6s\tremaining: 1m 6s\n",
            "1168:\tlearn: 0.3405905\ttotal: 42.6s\tremaining: 1m 6s\n",
            "1169:\tlearn: 0.3404928\ttotal: 42.7s\tremaining: 1m 6s\n",
            "1170:\tlearn: 0.3404384\ttotal: 42.7s\tremaining: 1m 6s\n",
            "1171:\tlearn: 0.3404196\ttotal: 42.8s\tremaining: 1m 6s\n",
            "1172:\tlearn: 0.3403641\ttotal: 42.8s\tremaining: 1m 6s\n",
            "1173:\tlearn: 0.3403026\ttotal: 42.9s\tremaining: 1m 6s\n",
            "1174:\tlearn: 0.3402272\ttotal: 43s\tremaining: 1m 6s\n",
            "1175:\tlearn: 0.3401344\ttotal: 43s\tremaining: 1m 6s\n",
            "1176:\tlearn: 0.3400140\ttotal: 43.1s\tremaining: 1m 6s\n",
            "1177:\tlearn: 0.3399726\ttotal: 43.1s\tremaining: 1m 6s\n",
            "1178:\tlearn: 0.3399180\ttotal: 43.2s\tremaining: 1m 6s\n",
            "1179:\tlearn: 0.3398388\ttotal: 43.2s\tremaining: 1m 6s\n",
            "1180:\tlearn: 0.3397616\ttotal: 43.3s\tremaining: 1m 6s\n",
            "1181:\tlearn: 0.3397579\ttotal: 43.3s\tremaining: 1m 6s\n",
            "1182:\tlearn: 0.3397087\ttotal: 43.4s\tremaining: 1m 6s\n",
            "1183:\tlearn: 0.3396320\ttotal: 43.4s\tremaining: 1m 6s\n",
            "1184:\tlearn: 0.3395771\ttotal: 43.5s\tremaining: 1m 6s\n",
            "1185:\tlearn: 0.3395320\ttotal: 43.5s\tremaining: 1m 6s\n",
            "1186:\tlearn: 0.3394455\ttotal: 43.6s\tremaining: 1m 6s\n",
            "1187:\tlearn: 0.3393410\ttotal: 43.7s\tremaining: 1m 6s\n",
            "1188:\tlearn: 0.3392903\ttotal: 43.7s\tremaining: 1m 6s\n",
            "1189:\tlearn: 0.3392163\ttotal: 43.8s\tremaining: 1m 6s\n",
            "1190:\tlearn: 0.3391967\ttotal: 43.8s\tremaining: 1m 6s\n",
            "1191:\tlearn: 0.3391582\ttotal: 43.9s\tremaining: 1m 6s\n",
            "1192:\tlearn: 0.3390500\ttotal: 43.9s\tremaining: 1m 6s\n",
            "1193:\tlearn: 0.3389910\ttotal: 44s\tremaining: 1m 6s\n",
            "1194:\tlearn: 0.3389571\ttotal: 44s\tremaining: 1m 6s\n",
            "1195:\tlearn: 0.3389015\ttotal: 44.1s\tremaining: 1m 6s\n",
            "1196:\tlearn: 0.3388133\ttotal: 44.1s\tremaining: 1m 6s\n",
            "1197:\tlearn: 0.3387852\ttotal: 44.2s\tremaining: 1m 6s\n",
            "1198:\tlearn: 0.3387431\ttotal: 44.2s\tremaining: 1m 6s\n",
            "1199:\tlearn: 0.3386780\ttotal: 44.3s\tremaining: 1m 6s\n",
            "1200:\tlearn: 0.3386424\ttotal: 44.3s\tremaining: 1m 6s\n",
            "1201:\tlearn: 0.3386226\ttotal: 44.4s\tremaining: 1m 6s\n",
            "1202:\tlearn: 0.3385165\ttotal: 44.4s\tremaining: 1m 6s\n",
            "1203:\tlearn: 0.3384521\ttotal: 44.5s\tremaining: 1m 6s\n",
            "1204:\tlearn: 0.3383928\ttotal: 44.5s\tremaining: 1m 6s\n",
            "1205:\tlearn: 0.3383069\ttotal: 44.6s\tremaining: 1m 6s\n",
            "1206:\tlearn: 0.3382495\ttotal: 44.6s\tremaining: 1m 6s\n",
            "1207:\tlearn: 0.3382104\ttotal: 44.7s\tremaining: 1m 6s\n",
            "1208:\tlearn: 0.3381284\ttotal: 44.7s\tremaining: 1m 6s\n",
            "1209:\tlearn: 0.3380891\ttotal: 44.8s\tremaining: 1m 6s\n",
            "1210:\tlearn: 0.3379872\ttotal: 44.9s\tremaining: 1m 6s\n",
            "1211:\tlearn: 0.3379436\ttotal: 44.9s\tremaining: 1m 6s\n",
            "1212:\tlearn: 0.3378834\ttotal: 44.9s\tremaining: 1m 6s\n",
            "1213:\tlearn: 0.3378245\ttotal: 45s\tremaining: 1m 6s\n",
            "1214:\tlearn: 0.3377732\ttotal: 45s\tremaining: 1m 6s\n",
            "1215:\tlearn: 0.3377310\ttotal: 45s\tremaining: 1m 6s\n",
            "1216:\tlearn: 0.3376533\ttotal: 45.1s\tremaining: 1m 6s\n",
            "1217:\tlearn: 0.3375815\ttotal: 45.1s\tremaining: 1m 5s\n",
            "1218:\tlearn: 0.3375398\ttotal: 45.1s\tremaining: 1m 5s\n",
            "1219:\tlearn: 0.3374672\ttotal: 45.2s\tremaining: 1m 5s\n",
            "1220:\tlearn: 0.3374264\ttotal: 45.2s\tremaining: 1m 5s\n",
            "1221:\tlearn: 0.3373944\ttotal: 45.2s\tremaining: 1m 5s\n",
            "1222:\tlearn: 0.3373371\ttotal: 45.3s\tremaining: 1m 5s\n",
            "1223:\tlearn: 0.3373232\ttotal: 45.3s\tremaining: 1m 5s\n",
            "1224:\tlearn: 0.3372721\ttotal: 45.3s\tremaining: 1m 5s\n",
            "1225:\tlearn: 0.3372177\ttotal: 45.3s\tremaining: 1m 5s\n",
            "1226:\tlearn: 0.3371944\ttotal: 45.4s\tremaining: 1m 5s\n",
            "1227:\tlearn: 0.3371251\ttotal: 45.4s\tremaining: 1m 5s\n",
            "1228:\tlearn: 0.3370707\ttotal: 45.4s\tremaining: 1m 5s\n",
            "1229:\tlearn: 0.3370090\ttotal: 45.5s\tremaining: 1m 5s\n",
            "1230:\tlearn: 0.3369345\ttotal: 45.5s\tremaining: 1m 5s\n",
            "1231:\tlearn: 0.3368879\ttotal: 45.5s\tremaining: 1m 5s\n",
            "1232:\tlearn: 0.3368274\ttotal: 45.6s\tremaining: 1m 5s\n",
            "1233:\tlearn: 0.3367934\ttotal: 45.6s\tremaining: 1m 5s\n",
            "1234:\tlearn: 0.3367706\ttotal: 45.6s\tremaining: 1m 5s\n",
            "1235:\tlearn: 0.3366812\ttotal: 45.6s\tremaining: 1m 5s\n",
            "1236:\tlearn: 0.3366268\ttotal: 45.7s\tremaining: 1m 5s\n",
            "1237:\tlearn: 0.3365840\ttotal: 45.7s\tremaining: 1m 5s\n",
            "1238:\tlearn: 0.3365213\ttotal: 45.7s\tremaining: 1m 5s\n",
            "1239:\tlearn: 0.3364627\ttotal: 45.8s\tremaining: 1m 4s\n",
            "1240:\tlearn: 0.3364323\ttotal: 45.8s\tremaining: 1m 4s\n",
            "1241:\tlearn: 0.3363647\ttotal: 45.8s\tremaining: 1m 4s\n",
            "1242:\tlearn: 0.3363376\ttotal: 45.8s\tremaining: 1m 4s\n",
            "1243:\tlearn: 0.3363119\ttotal: 45.9s\tremaining: 1m 4s\n",
            "1244:\tlearn: 0.3362874\ttotal: 45.9s\tremaining: 1m 4s\n",
            "1245:\tlearn: 0.3362650\ttotal: 46s\tremaining: 1m 4s\n",
            "1246:\tlearn: 0.3362178\ttotal: 46s\tremaining: 1m 4s\n",
            "1247:\tlearn: 0.3361641\ttotal: 46s\tremaining: 1m 4s\n",
            "1248:\tlearn: 0.3361269\ttotal: 46s\tremaining: 1m 4s\n",
            "1249:\tlearn: 0.3360717\ttotal: 46.1s\tremaining: 1m 4s\n",
            "1250:\tlearn: 0.3360386\ttotal: 46.1s\tremaining: 1m 4s\n",
            "1251:\tlearn: 0.3359687\ttotal: 46.1s\tremaining: 1m 4s\n",
            "1252:\tlearn: 0.3358872\ttotal: 46.2s\tremaining: 1m 4s\n",
            "1253:\tlearn: 0.3358252\ttotal: 46.2s\tremaining: 1m 4s\n",
            "1254:\tlearn: 0.3357598\ttotal: 46.2s\tremaining: 1m 4s\n",
            "1255:\tlearn: 0.3356936\ttotal: 46.3s\tremaining: 1m 4s\n",
            "1256:\tlearn: 0.3356426\ttotal: 46.3s\tremaining: 1m 4s\n",
            "1257:\tlearn: 0.3355612\ttotal: 46.3s\tremaining: 1m 4s\n",
            "1258:\tlearn: 0.3354841\ttotal: 46.3s\tremaining: 1m 4s\n",
            "1259:\tlearn: 0.3354414\ttotal: 46.4s\tremaining: 1m 4s\n",
            "1260:\tlearn: 0.3353978\ttotal: 46.4s\tremaining: 1m 3s\n",
            "1261:\tlearn: 0.3353221\ttotal: 46.4s\tremaining: 1m 3s\n",
            "1262:\tlearn: 0.3352288\ttotal: 46.5s\tremaining: 1m 3s\n",
            "1263:\tlearn: 0.3351630\ttotal: 46.5s\tremaining: 1m 3s\n",
            "1264:\tlearn: 0.3351063\ttotal: 46.5s\tremaining: 1m 3s\n",
            "1265:\tlearn: 0.3350521\ttotal: 46.6s\tremaining: 1m 3s\n",
            "1266:\tlearn: 0.3350090\ttotal: 46.6s\tremaining: 1m 3s\n",
            "1267:\tlearn: 0.3349706\ttotal: 46.6s\tremaining: 1m 3s\n",
            "1268:\tlearn: 0.3349676\ttotal: 46.6s\tremaining: 1m 3s\n",
            "1269:\tlearn: 0.3348920\ttotal: 46.7s\tremaining: 1m 3s\n",
            "1270:\tlearn: 0.3348285\ttotal: 46.7s\tremaining: 1m 3s\n",
            "1271:\tlearn: 0.3347866\ttotal: 46.7s\tremaining: 1m 3s\n",
            "1272:\tlearn: 0.3347384\ttotal: 46.8s\tremaining: 1m 3s\n",
            "1273:\tlearn: 0.3346973\ttotal: 46.8s\tremaining: 1m 3s\n",
            "1274:\tlearn: 0.3346456\ttotal: 46.8s\tremaining: 1m 3s\n",
            "1275:\tlearn: 0.3345787\ttotal: 46.8s\tremaining: 1m 3s\n",
            "1276:\tlearn: 0.3345319\ttotal: 46.9s\tremaining: 1m 3s\n",
            "1277:\tlearn: 0.3344817\ttotal: 46.9s\tremaining: 1m 3s\n",
            "1278:\tlearn: 0.3344333\ttotal: 46.9s\tremaining: 1m 3s\n",
            "1279:\tlearn: 0.3343996\ttotal: 47s\tremaining: 1m 3s\n",
            "1280:\tlearn: 0.3343673\ttotal: 47s\tremaining: 1m 3s\n",
            "1281:\tlearn: 0.3342852\ttotal: 47s\tremaining: 1m 3s\n",
            "1282:\tlearn: 0.3342714\ttotal: 47.1s\tremaining: 1m 2s\n",
            "1283:\tlearn: 0.3342234\ttotal: 47.1s\tremaining: 1m 2s\n",
            "1284:\tlearn: 0.3341532\ttotal: 47.1s\tremaining: 1m 2s\n",
            "1285:\tlearn: 0.3341133\ttotal: 47.2s\tremaining: 1m 2s\n",
            "1286:\tlearn: 0.3340688\ttotal: 47.2s\tremaining: 1m 2s\n",
            "1287:\tlearn: 0.3340416\ttotal: 47.2s\tremaining: 1m 2s\n",
            "1288:\tlearn: 0.3340018\ttotal: 47.3s\tremaining: 1m 2s\n",
            "1289:\tlearn: 0.3339353\ttotal: 47.3s\tremaining: 1m 2s\n",
            "1290:\tlearn: 0.3339051\ttotal: 47.3s\tremaining: 1m 2s\n",
            "1291:\tlearn: 0.3338332\ttotal: 47.3s\tremaining: 1m 2s\n",
            "1292:\tlearn: 0.3338145\ttotal: 47.4s\tremaining: 1m 2s\n",
            "1293:\tlearn: 0.3337599\ttotal: 47.4s\tremaining: 1m 2s\n",
            "1294:\tlearn: 0.3337030\ttotal: 47.4s\tremaining: 1m 2s\n",
            "1295:\tlearn: 0.3336549\ttotal: 47.5s\tremaining: 1m 2s\n",
            "1296:\tlearn: 0.3335842\ttotal: 47.5s\tremaining: 1m 2s\n",
            "1297:\tlearn: 0.3335039\ttotal: 47.5s\tremaining: 1m 2s\n",
            "1298:\tlearn: 0.3334226\ttotal: 47.5s\tremaining: 1m 2s\n",
            "1299:\tlearn: 0.3333555\ttotal: 47.6s\tremaining: 1m 2s\n",
            "1300:\tlearn: 0.3332869\ttotal: 47.6s\tremaining: 1m 2s\n",
            "1301:\tlearn: 0.3332093\ttotal: 47.6s\tremaining: 1m 2s\n",
            "1302:\tlearn: 0.3331709\ttotal: 47.7s\tremaining: 1m 2s\n",
            "1303:\tlearn: 0.3330902\ttotal: 47.7s\tremaining: 1m 2s\n",
            "1304:\tlearn: 0.3330108\ttotal: 47.7s\tremaining: 1m 1s\n",
            "1305:\tlearn: 0.3329444\ttotal: 47.8s\tremaining: 1m 1s\n",
            "1306:\tlearn: 0.3328655\ttotal: 47.8s\tremaining: 1m 1s\n",
            "1307:\tlearn: 0.3328121\ttotal: 47.8s\tremaining: 1m 1s\n",
            "1308:\tlearn: 0.3327756\ttotal: 47.8s\tremaining: 1m 1s\n",
            "1309:\tlearn: 0.3327130\ttotal: 47.9s\tremaining: 1m 1s\n",
            "1310:\tlearn: 0.3326411\ttotal: 47.9s\tremaining: 1m 1s\n",
            "1311:\tlearn: 0.3326137\ttotal: 47.9s\tremaining: 1m 1s\n",
            "1312:\tlearn: 0.3325491\ttotal: 48s\tremaining: 1m 1s\n",
            "1313:\tlearn: 0.3325065\ttotal: 48s\tremaining: 1m 1s\n",
            "1314:\tlearn: 0.3324801\ttotal: 48s\tremaining: 1m 1s\n",
            "1315:\tlearn: 0.3324437\ttotal: 48.1s\tremaining: 1m 1s\n",
            "1316:\tlearn: 0.3324309\ttotal: 48.1s\tremaining: 1m 1s\n",
            "1317:\tlearn: 0.3323819\ttotal: 48.1s\tremaining: 1m 1s\n",
            "1318:\tlearn: 0.3323588\ttotal: 48.2s\tremaining: 1m 1s\n",
            "1319:\tlearn: 0.3323215\ttotal: 48.2s\tremaining: 1m 1s\n",
            "1320:\tlearn: 0.3322341\ttotal: 48.2s\tremaining: 1m 1s\n",
            "1321:\tlearn: 0.3321939\ttotal: 48.3s\tremaining: 1m 1s\n",
            "1322:\tlearn: 0.3321604\ttotal: 48.3s\tremaining: 1m 1s\n",
            "1323:\tlearn: 0.3320998\ttotal: 48.3s\tremaining: 1m 1s\n",
            "1324:\tlearn: 0.3320600\ttotal: 48.3s\tremaining: 1m 1s\n",
            "1325:\tlearn: 0.3320025\ttotal: 48.4s\tremaining: 1m 1s\n",
            "1326:\tlearn: 0.3319589\ttotal: 48.4s\tremaining: 1m 1s\n",
            "1327:\tlearn: 0.3318965\ttotal: 48.4s\tremaining: 1m\n",
            "1328:\tlearn: 0.3318410\ttotal: 48.5s\tremaining: 1m\n",
            "1329:\tlearn: 0.3317912\ttotal: 48.5s\tremaining: 1m\n",
            "1330:\tlearn: 0.3317246\ttotal: 48.5s\tremaining: 1m\n",
            "1331:\tlearn: 0.3316900\ttotal: 48.6s\tremaining: 1m\n",
            "1332:\tlearn: 0.3316480\ttotal: 48.6s\tremaining: 1m\n",
            "1333:\tlearn: 0.3316116\ttotal: 48.6s\tremaining: 1m\n",
            "1334:\tlearn: 0.3315671\ttotal: 48.6s\tremaining: 1m\n",
            "1335:\tlearn: 0.3315054\ttotal: 48.7s\tremaining: 1m\n",
            "1336:\tlearn: 0.3314647\ttotal: 48.7s\tremaining: 1m\n",
            "1337:\tlearn: 0.3314063\ttotal: 48.7s\tremaining: 1m\n",
            "1338:\tlearn: 0.3313817\ttotal: 48.8s\tremaining: 1m\n",
            "1339:\tlearn: 0.3313204\ttotal: 48.8s\tremaining: 1m\n",
            "1340:\tlearn: 0.3312569\ttotal: 48.8s\tremaining: 1m\n",
            "1341:\tlearn: 0.3312470\ttotal: 48.9s\tremaining: 1m\n",
            "1342:\tlearn: 0.3311851\ttotal: 48.9s\tremaining: 1m\n",
            "1343:\tlearn: 0.3311086\ttotal: 48.9s\tremaining: 1m\n",
            "1344:\tlearn: 0.3310911\ttotal: 48.9s\tremaining: 1m\n",
            "1345:\tlearn: 0.3310616\ttotal: 49s\tremaining: 1m\n",
            "1346:\tlearn: 0.3309773\ttotal: 49s\tremaining: 1m\n",
            "1347:\tlearn: 0.3309310\ttotal: 49s\tremaining: 1m\n",
            "1348:\tlearn: 0.3308772\ttotal: 49.1s\tremaining: 1m\n",
            "1349:\tlearn: 0.3308031\ttotal: 49.1s\tremaining: 1m\n",
            "1350:\tlearn: 0.3307238\ttotal: 49.1s\tremaining: 60s\n",
            "1351:\tlearn: 0.3306540\ttotal: 49.2s\tremaining: 59.9s\n",
            "1352:\tlearn: 0.3306331\ttotal: 49.2s\tremaining: 59.9s\n",
            "1353:\tlearn: 0.3305782\ttotal: 49.2s\tremaining: 59.8s\n",
            "1354:\tlearn: 0.3305209\ttotal: 49.3s\tremaining: 59.8s\n",
            "1355:\tlearn: 0.3304675\ttotal: 49.3s\tremaining: 59.8s\n",
            "1356:\tlearn: 0.3304160\ttotal: 49.3s\tremaining: 59.7s\n",
            "1357:\tlearn: 0.3303968\ttotal: 49.3s\tremaining: 59.7s\n",
            "1358:\tlearn: 0.3303485\ttotal: 49.4s\tremaining: 59.6s\n",
            "1359:\tlearn: 0.3303069\ttotal: 49.4s\tremaining: 59.6s\n",
            "1360:\tlearn: 0.3302460\ttotal: 49.4s\tremaining: 59.5s\n",
            "1361:\tlearn: 0.3301728\ttotal: 49.5s\tremaining: 59.5s\n",
            "1362:\tlearn: 0.3301241\ttotal: 49.5s\tremaining: 59.4s\n",
            "1363:\tlearn: 0.3300918\ttotal: 49.5s\tremaining: 59.4s\n",
            "1364:\tlearn: 0.3300239\ttotal: 49.6s\tremaining: 59.4s\n",
            "1365:\tlearn: 0.3299974\ttotal: 49.6s\tremaining: 59.3s\n",
            "1366:\tlearn: 0.3299399\ttotal: 49.6s\tremaining: 59.3s\n",
            "1367:\tlearn: 0.3298993\ttotal: 49.7s\tremaining: 59.2s\n",
            "1368:\tlearn: 0.3298387\ttotal: 49.7s\tremaining: 59.2s\n",
            "1369:\tlearn: 0.3297823\ttotal: 49.7s\tremaining: 59.1s\n",
            "1370:\tlearn: 0.3297055\ttotal: 49.8s\tremaining: 59.1s\n",
            "1371:\tlearn: 0.3296402\ttotal: 49.8s\tremaining: 59.1s\n",
            "1372:\tlearn: 0.3296059\ttotal: 49.8s\tremaining: 59s\n",
            "1373:\tlearn: 0.3295867\ttotal: 49.8s\tremaining: 59s\n",
            "1374:\tlearn: 0.3294968\ttotal: 49.9s\tremaining: 58.9s\n",
            "1375:\tlearn: 0.3294468\ttotal: 49.9s\tremaining: 58.9s\n",
            "1376:\tlearn: 0.3294101\ttotal: 49.9s\tremaining: 58.8s\n",
            "1377:\tlearn: 0.3293369\ttotal: 50s\tremaining: 58.8s\n",
            "1378:\tlearn: 0.3292985\ttotal: 50s\tremaining: 58.8s\n",
            "1379:\tlearn: 0.3292564\ttotal: 50s\tremaining: 58.7s\n",
            "1380:\tlearn: 0.3291981\ttotal: 50.1s\tremaining: 58.7s\n",
            "1381:\tlearn: 0.3291461\ttotal: 50.1s\tremaining: 58.6s\n",
            "1382:\tlearn: 0.3291007\ttotal: 50.1s\tremaining: 58.6s\n",
            "1383:\tlearn: 0.3290511\ttotal: 50.1s\tremaining: 58.6s\n",
            "1384:\tlearn: 0.3290155\ttotal: 50.2s\tremaining: 58.5s\n",
            "1385:\tlearn: 0.3290102\ttotal: 50.2s\tremaining: 58.5s\n",
            "1386:\tlearn: 0.3289677\ttotal: 50.2s\tremaining: 58.4s\n",
            "1387:\tlearn: 0.3289087\ttotal: 50.3s\tremaining: 58.4s\n",
            "1388:\tlearn: 0.3288686\ttotal: 50.3s\tremaining: 58.3s\n",
            "1389:\tlearn: 0.3288272\ttotal: 50.3s\tremaining: 58.3s\n",
            "1390:\tlearn: 0.3287782\ttotal: 50.4s\tremaining: 58.2s\n",
            "1391:\tlearn: 0.3286992\ttotal: 50.4s\tremaining: 58.2s\n",
            "1392:\tlearn: 0.3286488\ttotal: 50.4s\tremaining: 58.1s\n",
            "1393:\tlearn: 0.3286096\ttotal: 50.4s\tremaining: 58.1s\n",
            "1394:\tlearn: 0.3285844\ttotal: 50.5s\tremaining: 58.1s\n",
            "1395:\tlearn: 0.3285588\ttotal: 50.5s\tremaining: 58s\n",
            "1396:\tlearn: 0.3285092\ttotal: 50.5s\tremaining: 58s\n",
            "1397:\tlearn: 0.3284331\ttotal: 50.6s\tremaining: 57.9s\n",
            "1398:\tlearn: 0.3283874\ttotal: 50.6s\tremaining: 57.9s\n",
            "1399:\tlearn: 0.3283329\ttotal: 50.6s\tremaining: 57.8s\n",
            "1400:\tlearn: 0.3283043\ttotal: 50.7s\tremaining: 57.8s\n",
            "1401:\tlearn: 0.3282676\ttotal: 50.7s\tremaining: 57.8s\n",
            "1402:\tlearn: 0.3282301\ttotal: 50.7s\tremaining: 57.7s\n",
            "1403:\tlearn: 0.3281722\ttotal: 50.7s\tremaining: 57.7s\n",
            "1404:\tlearn: 0.3281334\ttotal: 50.8s\tremaining: 57.6s\n",
            "1405:\tlearn: 0.3280899\ttotal: 50.8s\tremaining: 57.6s\n",
            "1406:\tlearn: 0.3280317\ttotal: 50.8s\tremaining: 57.5s\n",
            "1407:\tlearn: 0.3280046\ttotal: 50.9s\tremaining: 57.5s\n",
            "1408:\tlearn: 0.3279717\ttotal: 50.9s\tremaining: 57.5s\n",
            "1409:\tlearn: 0.3278952\ttotal: 50.9s\tremaining: 57.4s\n",
            "1410:\tlearn: 0.3278870\ttotal: 50.9s\tremaining: 57.4s\n",
            "1411:\tlearn: 0.3278070\ttotal: 51s\tremaining: 57.3s\n",
            "1412:\tlearn: 0.3277339\ttotal: 51s\tremaining: 57.3s\n",
            "1413:\tlearn: 0.3276454\ttotal: 51s\tremaining: 57.2s\n",
            "1414:\tlearn: 0.3276280\ttotal: 51.1s\tremaining: 57.2s\n",
            "1415:\tlearn: 0.3275612\ttotal: 51.1s\tremaining: 57.2s\n",
            "1416:\tlearn: 0.3275164\ttotal: 51.1s\tremaining: 57.1s\n",
            "1417:\tlearn: 0.3274866\ttotal: 51.2s\tremaining: 57.1s\n",
            "1418:\tlearn: 0.3274275\ttotal: 51.2s\tremaining: 57s\n",
            "1419:\tlearn: 0.3273670\ttotal: 51.2s\tremaining: 57s\n",
            "1420:\tlearn: 0.3273220\ttotal: 51.3s\tremaining: 56.9s\n",
            "1421:\tlearn: 0.3272908\ttotal: 51.3s\tremaining: 56.9s\n",
            "1422:\tlearn: 0.3272392\ttotal: 51.3s\tremaining: 56.9s\n",
            "1423:\tlearn: 0.3271791\ttotal: 51.3s\tremaining: 56.8s\n",
            "1424:\tlearn: 0.3271248\ttotal: 51.4s\tremaining: 56.8s\n",
            "1425:\tlearn: 0.3270322\ttotal: 51.4s\tremaining: 56.7s\n",
            "1426:\tlearn: 0.3269737\ttotal: 51.4s\tremaining: 56.7s\n",
            "1427:\tlearn: 0.3269386\ttotal: 51.5s\tremaining: 56.6s\n",
            "1428:\tlearn: 0.3268970\ttotal: 51.5s\tremaining: 56.6s\n",
            "1429:\tlearn: 0.3268250\ttotal: 51.5s\tremaining: 56.6s\n",
            "1430:\tlearn: 0.3267581\ttotal: 51.6s\tremaining: 56.5s\n",
            "1431:\tlearn: 0.3267162\ttotal: 51.6s\tremaining: 56.5s\n",
            "1432:\tlearn: 0.3266784\ttotal: 51.6s\tremaining: 56.4s\n",
            "1433:\tlearn: 0.3266186\ttotal: 51.6s\tremaining: 56.4s\n",
            "1434:\tlearn: 0.3265843\ttotal: 51.7s\tremaining: 56.3s\n",
            "1435:\tlearn: 0.3265489\ttotal: 51.7s\tremaining: 56.3s\n",
            "1436:\tlearn: 0.3264930\ttotal: 51.7s\tremaining: 56.3s\n",
            "1437:\tlearn: 0.3264440\ttotal: 51.7s\tremaining: 56.2s\n",
            "1438:\tlearn: 0.3263860\ttotal: 51.8s\tremaining: 56.2s\n",
            "1439:\tlearn: 0.3263464\ttotal: 51.8s\tremaining: 56.1s\n",
            "1440:\tlearn: 0.3262865\ttotal: 51.8s\tremaining: 56.1s\n",
            "1441:\tlearn: 0.3262675\ttotal: 51.9s\tremaining: 56s\n",
            "1442:\tlearn: 0.3262113\ttotal: 51.9s\tremaining: 56s\n",
            "1443:\tlearn: 0.3261686\ttotal: 51.9s\tremaining: 55.9s\n",
            "1444:\tlearn: 0.3261279\ttotal: 52s\tremaining: 55.9s\n",
            "1445:\tlearn: 0.3260554\ttotal: 52s\tremaining: 55.9s\n",
            "1446:\tlearn: 0.3260249\ttotal: 52s\tremaining: 55.8s\n",
            "1447:\tlearn: 0.3259722\ttotal: 52.1s\tremaining: 55.8s\n",
            "1448:\tlearn: 0.3259467\ttotal: 52.1s\tremaining: 55.8s\n",
            "1449:\tlearn: 0.3258767\ttotal: 52.1s\tremaining: 55.7s\n",
            "1450:\tlearn: 0.3258316\ttotal: 52.1s\tremaining: 55.7s\n",
            "1451:\tlearn: 0.3258020\ttotal: 52.2s\tremaining: 55.6s\n",
            "1452:\tlearn: 0.3257544\ttotal: 52.2s\tremaining: 55.6s\n",
            "1453:\tlearn: 0.3256946\ttotal: 52.2s\tremaining: 55.6s\n",
            "1454:\tlearn: 0.3256381\ttotal: 52.3s\tremaining: 55.5s\n",
            "1455:\tlearn: 0.3256070\ttotal: 52.3s\tremaining: 55.5s\n",
            "1456:\tlearn: 0.3255662\ttotal: 52.3s\tremaining: 55.4s\n",
            "1457:\tlearn: 0.3255154\ttotal: 52.4s\tremaining: 55.4s\n",
            "1458:\tlearn: 0.3254931\ttotal: 52.4s\tremaining: 55.3s\n",
            "1459:\tlearn: 0.3254548\ttotal: 52.4s\tremaining: 55.3s\n",
            "1460:\tlearn: 0.3254031\ttotal: 52.5s\tremaining: 55.3s\n",
            "1461:\tlearn: 0.3253441\ttotal: 52.5s\tremaining: 55.2s\n",
            "1462:\tlearn: 0.3252722\ttotal: 52.5s\tremaining: 55.2s\n",
            "1463:\tlearn: 0.3252284\ttotal: 52.5s\tremaining: 55.1s\n",
            "1464:\tlearn: 0.3252028\ttotal: 52.6s\tremaining: 55.1s\n",
            "1465:\tlearn: 0.3251454\ttotal: 52.6s\tremaining: 55s\n",
            "1466:\tlearn: 0.3250874\ttotal: 52.6s\tremaining: 55s\n",
            "1467:\tlearn: 0.3249970\ttotal: 52.7s\tremaining: 55s\n",
            "1468:\tlearn: 0.3249452\ttotal: 52.7s\tremaining: 54.9s\n",
            "1469:\tlearn: 0.3249013\ttotal: 52.7s\tremaining: 54.9s\n",
            "1470:\tlearn: 0.3248623\ttotal: 52.8s\tremaining: 54.8s\n",
            "1471:\tlearn: 0.3248391\ttotal: 52.8s\tremaining: 54.8s\n",
            "1472:\tlearn: 0.3247865\ttotal: 52.8s\tremaining: 54.7s\n",
            "1473:\tlearn: 0.3247312\ttotal: 52.8s\tremaining: 54.7s\n",
            "1474:\tlearn: 0.3246999\ttotal: 52.9s\tremaining: 54.7s\n",
            "1475:\tlearn: 0.3246363\ttotal: 52.9s\tremaining: 54.6s\n",
            "1476:\tlearn: 0.3246279\ttotal: 52.9s\tremaining: 54.6s\n",
            "1477:\tlearn: 0.3245588\ttotal: 53s\tremaining: 54.5s\n",
            "1478:\tlearn: 0.3244939\ttotal: 53s\tremaining: 54.5s\n",
            "1479:\tlearn: 0.3244437\ttotal: 53s\tremaining: 54.5s\n",
            "1480:\tlearn: 0.3244159\ttotal: 53.1s\tremaining: 54.4s\n",
            "1481:\tlearn: 0.3243396\ttotal: 53.1s\tremaining: 54.4s\n",
            "1482:\tlearn: 0.3242857\ttotal: 53.1s\tremaining: 54.3s\n",
            "1483:\tlearn: 0.3242305\ttotal: 53.2s\tremaining: 54.3s\n",
            "1484:\tlearn: 0.3241875\ttotal: 53.2s\tremaining: 54.3s\n",
            "1485:\tlearn: 0.3241436\ttotal: 53.2s\tremaining: 54.2s\n",
            "1486:\tlearn: 0.3241126\ttotal: 53.2s\tremaining: 54.2s\n",
            "1487:\tlearn: 0.3240526\ttotal: 53.3s\tremaining: 54.1s\n",
            "1488:\tlearn: 0.3240177\ttotal: 53.3s\tremaining: 54.1s\n",
            "1489:\tlearn: 0.3239845\ttotal: 53.3s\tremaining: 54s\n",
            "1490:\tlearn: 0.3238946\ttotal: 53.4s\tremaining: 54s\n",
            "1491:\tlearn: 0.3238720\ttotal: 53.4s\tremaining: 54s\n",
            "1492:\tlearn: 0.3238308\ttotal: 53.4s\tremaining: 53.9s\n",
            "1493:\tlearn: 0.3238002\ttotal: 53.4s\tremaining: 53.9s\n",
            "1494:\tlearn: 0.3237912\ttotal: 53.5s\tremaining: 53.8s\n",
            "1495:\tlearn: 0.3237447\ttotal: 53.5s\tremaining: 53.8s\n",
            "1496:\tlearn: 0.3237050\ttotal: 53.5s\tremaining: 53.8s\n",
            "1497:\tlearn: 0.3236782\ttotal: 53.6s\tremaining: 53.7s\n",
            "1498:\tlearn: 0.3236045\ttotal: 53.6s\tremaining: 53.7s\n",
            "1499:\tlearn: 0.3235685\ttotal: 53.6s\tremaining: 53.6s\n",
            "1500:\tlearn: 0.3234963\ttotal: 53.7s\tremaining: 53.6s\n",
            "1501:\tlearn: 0.3234591\ttotal: 53.7s\tremaining: 53.5s\n",
            "1502:\tlearn: 0.3234201\ttotal: 53.7s\tremaining: 53.5s\n",
            "1503:\tlearn: 0.3233919\ttotal: 53.8s\tremaining: 53.5s\n",
            "1504:\tlearn: 0.3233632\ttotal: 53.8s\tremaining: 53.4s\n",
            "1505:\tlearn: 0.3232966\ttotal: 53.8s\tremaining: 53.4s\n",
            "1506:\tlearn: 0.3232577\ttotal: 53.8s\tremaining: 53.3s\n",
            "1507:\tlearn: 0.3232384\ttotal: 53.9s\tremaining: 53.3s\n",
            "1508:\tlearn: 0.3231848\ttotal: 53.9s\tremaining: 53.2s\n",
            "1509:\tlearn: 0.3231358\ttotal: 53.9s\tremaining: 53.2s\n",
            "1510:\tlearn: 0.3231173\ttotal: 53.9s\tremaining: 53.2s\n",
            "1511:\tlearn: 0.3230714\ttotal: 54s\tremaining: 53.1s\n",
            "1512:\tlearn: 0.3230425\ttotal: 54s\tremaining: 53.1s\n",
            "1513:\tlearn: 0.3229892\ttotal: 54s\tremaining: 53s\n",
            "1514:\tlearn: 0.3229545\ttotal: 54.1s\tremaining: 53s\n",
            "1515:\tlearn: 0.3229285\ttotal: 54.1s\tremaining: 53s\n",
            "1516:\tlearn: 0.3229026\ttotal: 54.1s\tremaining: 52.9s\n",
            "1517:\tlearn: 0.3228660\ttotal: 54.2s\tremaining: 52.9s\n",
            "1518:\tlearn: 0.3228154\ttotal: 54.2s\tremaining: 52.9s\n",
            "1519:\tlearn: 0.3227806\ttotal: 54.2s\tremaining: 52.8s\n",
            "1520:\tlearn: 0.3227364\ttotal: 54.3s\tremaining: 52.8s\n",
            "1521:\tlearn: 0.3226838\ttotal: 54.3s\tremaining: 52.7s\n",
            "1522:\tlearn: 0.3226700\ttotal: 54.3s\tremaining: 52.7s\n",
            "1523:\tlearn: 0.3226103\ttotal: 54.3s\tremaining: 52.6s\n",
            "1524:\tlearn: 0.3225550\ttotal: 54.4s\tremaining: 52.6s\n",
            "1525:\tlearn: 0.3224960\ttotal: 54.4s\tremaining: 52.6s\n",
            "1526:\tlearn: 0.3224436\ttotal: 54.4s\tremaining: 52.5s\n",
            "1527:\tlearn: 0.3223926\ttotal: 54.5s\tremaining: 52.5s\n",
            "1528:\tlearn: 0.3223131\ttotal: 54.5s\tremaining: 52.4s\n",
            "1529:\tlearn: 0.3222648\ttotal: 54.5s\tremaining: 52.4s\n",
            "1530:\tlearn: 0.3221898\ttotal: 54.6s\tremaining: 52.4s\n",
            "1531:\tlearn: 0.3221846\ttotal: 54.6s\tremaining: 52.3s\n",
            "1532:\tlearn: 0.3221157\ttotal: 54.6s\tremaining: 52.3s\n",
            "1533:\tlearn: 0.3220656\ttotal: 54.7s\tremaining: 52.2s\n",
            "1534:\tlearn: 0.3220313\ttotal: 54.7s\tremaining: 52.2s\n",
            "1535:\tlearn: 0.3220064\ttotal: 54.7s\tremaining: 52.1s\n",
            "1536:\tlearn: 0.3219827\ttotal: 54.7s\tremaining: 52.1s\n",
            "1537:\tlearn: 0.3219168\ttotal: 54.8s\tremaining: 52.1s\n",
            "1538:\tlearn: 0.3219004\ttotal: 54.8s\tremaining: 52s\n",
            "1539:\tlearn: 0.3218682\ttotal: 54.9s\tremaining: 52s\n",
            "1540:\tlearn: 0.3218072\ttotal: 54.9s\tremaining: 52s\n",
            "1541:\tlearn: 0.3217454\ttotal: 54.9s\tremaining: 51.9s\n",
            "1542:\tlearn: 0.3216911\ttotal: 55s\tremaining: 51.9s\n",
            "1543:\tlearn: 0.3216515\ttotal: 55.1s\tremaining: 51.9s\n",
            "1544:\tlearn: 0.3216064\ttotal: 55.1s\tremaining: 51.9s\n",
            "1545:\tlearn: 0.3215832\ttotal: 55.2s\tremaining: 51.9s\n",
            "1546:\tlearn: 0.3215143\ttotal: 55.2s\tremaining: 51.9s\n",
            "1547:\tlearn: 0.3214944\ttotal: 55.3s\tremaining: 51.8s\n",
            "1548:\tlearn: 0.3214648\ttotal: 55.3s\tremaining: 51.8s\n",
            "1549:\tlearn: 0.3214237\ttotal: 55.4s\tremaining: 51.8s\n",
            "1550:\tlearn: 0.3213719\ttotal: 55.4s\tremaining: 51.8s\n",
            "1551:\tlearn: 0.3213299\ttotal: 55.5s\tremaining: 51.8s\n",
            "1552:\tlearn: 0.3212737\ttotal: 55.6s\tremaining: 51.8s\n",
            "1553:\tlearn: 0.3212605\ttotal: 55.6s\tremaining: 51.8s\n",
            "1554:\tlearn: 0.3212432\ttotal: 55.7s\tremaining: 51.7s\n",
            "1555:\tlearn: 0.3211907\ttotal: 55.7s\tremaining: 51.7s\n",
            "1556:\tlearn: 0.3211643\ttotal: 55.8s\tremaining: 51.7s\n",
            "1557:\tlearn: 0.3210770\ttotal: 55.8s\tremaining: 51.7s\n",
            "1558:\tlearn: 0.3210322\ttotal: 55.9s\tremaining: 51.7s\n",
            "1559:\tlearn: 0.3209999\ttotal: 55.9s\tremaining: 51.6s\n",
            "1560:\tlearn: 0.3209151\ttotal: 56s\tremaining: 51.6s\n",
            "1561:\tlearn: 0.3208438\ttotal: 56.1s\tremaining: 51.6s\n",
            "1562:\tlearn: 0.3208005\ttotal: 56.1s\tremaining: 51.6s\n",
            "1563:\tlearn: 0.3207473\ttotal: 56.2s\tremaining: 51.6s\n",
            "1564:\tlearn: 0.3207030\ttotal: 56.2s\tremaining: 51.6s\n",
            "1565:\tlearn: 0.3206376\ttotal: 56.3s\tremaining: 51.5s\n",
            "1566:\tlearn: 0.3205616\ttotal: 56.3s\tremaining: 51.5s\n",
            "1567:\tlearn: 0.3205013\ttotal: 56.4s\tremaining: 51.5s\n",
            "1568:\tlearn: 0.3204749\ttotal: 56.5s\tremaining: 51.5s\n",
            "1569:\tlearn: 0.3204214\ttotal: 56.5s\tremaining: 51.5s\n",
            "1570:\tlearn: 0.3203329\ttotal: 56.6s\tremaining: 51.5s\n",
            "1571:\tlearn: 0.3202630\ttotal: 56.6s\tremaining: 51.4s\n",
            "1572:\tlearn: 0.3202190\ttotal: 56.7s\tremaining: 51.4s\n",
            "1573:\tlearn: 0.3201867\ttotal: 56.7s\tremaining: 51.4s\n",
            "1574:\tlearn: 0.3201559\ttotal: 56.8s\tremaining: 51.4s\n",
            "1575:\tlearn: 0.3201207\ttotal: 56.8s\tremaining: 51.4s\n",
            "1576:\tlearn: 0.3200795\ttotal: 56.9s\tremaining: 51.3s\n",
            "1577:\tlearn: 0.3199929\ttotal: 56.9s\tremaining: 51.3s\n",
            "1578:\tlearn: 0.3199549\ttotal: 57s\tremaining: 51.3s\n",
            "1579:\tlearn: 0.3198916\ttotal: 57s\tremaining: 51.3s\n",
            "1580:\tlearn: 0.3198494\ttotal: 57.1s\tremaining: 51.2s\n",
            "1581:\tlearn: 0.3198307\ttotal: 57.1s\tremaining: 51.2s\n",
            "1582:\tlearn: 0.3198111\ttotal: 57.2s\tremaining: 51.2s\n",
            "1583:\tlearn: 0.3197761\ttotal: 57.3s\tremaining: 51.2s\n",
            "1584:\tlearn: 0.3197408\ttotal: 57.3s\tremaining: 51.1s\n",
            "1585:\tlearn: 0.3196947\ttotal: 57.3s\tremaining: 51.1s\n",
            "1586:\tlearn: 0.3196724\ttotal: 57.4s\tremaining: 51.1s\n",
            "1587:\tlearn: 0.3196674\ttotal: 57.4s\tremaining: 51.1s\n",
            "1588:\tlearn: 0.3195994\ttotal: 57.5s\tremaining: 51s\n",
            "1589:\tlearn: 0.3195761\ttotal: 57.5s\tremaining: 51s\n",
            "1590:\tlearn: 0.3195409\ttotal: 57.6s\tremaining: 51s\n",
            "1591:\tlearn: 0.3194792\ttotal: 57.7s\tremaining: 51s\n",
            "1592:\tlearn: 0.3194287\ttotal: 57.7s\tremaining: 51s\n",
            "1593:\tlearn: 0.3193920\ttotal: 57.8s\tremaining: 51s\n",
            "1594:\tlearn: 0.3193599\ttotal: 57.8s\tremaining: 50.9s\n",
            "1595:\tlearn: 0.3193553\ttotal: 57.9s\tremaining: 50.9s\n",
            "1596:\tlearn: 0.3193030\ttotal: 58s\tremaining: 50.9s\n",
            "1597:\tlearn: 0.3192478\ttotal: 58s\tremaining: 50.9s\n",
            "1598:\tlearn: 0.3191879\ttotal: 58.1s\tremaining: 50.9s\n",
            "1599:\tlearn: 0.3191262\ttotal: 58.1s\tremaining: 50.9s\n",
            "1600:\tlearn: 0.3190934\ttotal: 58.2s\tremaining: 50.9s\n",
            "1601:\tlearn: 0.3190441\ttotal: 58.3s\tremaining: 50.8s\n",
            "1602:\tlearn: 0.3190216\ttotal: 58.3s\tremaining: 50.8s\n",
            "1603:\tlearn: 0.3189776\ttotal: 58.4s\tremaining: 50.8s\n",
            "1604:\tlearn: 0.3189365\ttotal: 58.4s\tremaining: 50.8s\n",
            "1605:\tlearn: 0.3189063\ttotal: 58.5s\tremaining: 50.8s\n",
            "1606:\tlearn: 0.3188184\ttotal: 58.5s\tremaining: 50.7s\n",
            "1607:\tlearn: 0.3187575\ttotal: 58.6s\tremaining: 50.7s\n",
            "1608:\tlearn: 0.3187352\ttotal: 58.6s\tremaining: 50.7s\n",
            "1609:\tlearn: 0.3186713\ttotal: 58.6s\tremaining: 50.6s\n",
            "1610:\tlearn: 0.3186333\ttotal: 58.7s\tremaining: 50.6s\n",
            "1611:\tlearn: 0.3185972\ttotal: 58.7s\tremaining: 50.5s\n",
            "1612:\tlearn: 0.3185565\ttotal: 58.7s\tremaining: 50.5s\n",
            "1613:\tlearn: 0.3185088\ttotal: 58.8s\tremaining: 50.5s\n",
            "1614:\tlearn: 0.3184554\ttotal: 58.8s\tremaining: 50.4s\n",
            "1615:\tlearn: 0.3183900\ttotal: 58.8s\tremaining: 50.4s\n",
            "1616:\tlearn: 0.3183432\ttotal: 58.8s\tremaining: 50.3s\n",
            "1617:\tlearn: 0.3183261\ttotal: 58.9s\tremaining: 50.3s\n",
            "1618:\tlearn: 0.3182943\ttotal: 58.9s\tremaining: 50.2s\n",
            "1619:\tlearn: 0.3182374\ttotal: 58.9s\tremaining: 50.2s\n",
            "1620:\tlearn: 0.3182049\ttotal: 59s\tremaining: 50.2s\n",
            "1621:\tlearn: 0.3181851\ttotal: 59s\tremaining: 50.1s\n",
            "1622:\tlearn: 0.3181519\ttotal: 59s\tremaining: 50.1s\n",
            "1623:\tlearn: 0.3180976\ttotal: 59.1s\tremaining: 50s\n",
            "1624:\tlearn: 0.3180540\ttotal: 59.1s\tremaining: 50s\n",
            "1625:\tlearn: 0.3179726\ttotal: 59.1s\tremaining: 50s\n",
            "1626:\tlearn: 0.3179236\ttotal: 59.1s\tremaining: 49.9s\n",
            "1627:\tlearn: 0.3179207\ttotal: 59.2s\tremaining: 49.9s\n",
            "1628:\tlearn: 0.3178821\ttotal: 59.2s\tremaining: 49.8s\n",
            "1629:\tlearn: 0.3178410\ttotal: 59.2s\tremaining: 49.8s\n",
            "1630:\tlearn: 0.3178087\ttotal: 59.3s\tremaining: 49.7s\n",
            "1631:\tlearn: 0.3177904\ttotal: 59.3s\tremaining: 49.7s\n",
            "1632:\tlearn: 0.3177359\ttotal: 59.3s\tremaining: 49.7s\n",
            "1633:\tlearn: 0.3176953\ttotal: 59.4s\tremaining: 49.6s\n",
            "1634:\tlearn: 0.3176471\ttotal: 59.4s\tremaining: 49.6s\n",
            "1635:\tlearn: 0.3176174\ttotal: 59.4s\tremaining: 49.5s\n",
            "1636:\tlearn: 0.3175820\ttotal: 59.5s\tremaining: 49.5s\n",
            "1637:\tlearn: 0.3175188\ttotal: 59.5s\tremaining: 49.5s\n",
            "1638:\tlearn: 0.3174921\ttotal: 59.5s\tremaining: 49.4s\n",
            "1639:\tlearn: 0.3174500\ttotal: 59.5s\tremaining: 49.4s\n",
            "1640:\tlearn: 0.3174120\ttotal: 59.6s\tremaining: 49.3s\n",
            "1641:\tlearn: 0.3174066\ttotal: 59.6s\tremaining: 49.3s\n",
            "1642:\tlearn: 0.3173607\ttotal: 59.6s\tremaining: 49.3s\n",
            "1643:\tlearn: 0.3172725\ttotal: 59.7s\tremaining: 49.2s\n",
            "1644:\tlearn: 0.3172524\ttotal: 59.7s\tremaining: 49.2s\n",
            "1645:\tlearn: 0.3172033\ttotal: 59.7s\tremaining: 49.1s\n",
            "1646:\tlearn: 0.3171704\ttotal: 59.8s\tremaining: 49.1s\n",
            "1647:\tlearn: 0.3171022\ttotal: 59.8s\tremaining: 49s\n",
            "1648:\tlearn: 0.3170427\ttotal: 59.8s\tremaining: 49s\n",
            "1649:\tlearn: 0.3169664\ttotal: 59.8s\tremaining: 49s\n",
            "1650:\tlearn: 0.3169429\ttotal: 59.9s\tremaining: 48.9s\n",
            "1651:\tlearn: 0.3168980\ttotal: 59.9s\tremaining: 48.9s\n",
            "1652:\tlearn: 0.3168661\ttotal: 59.9s\tremaining: 48.8s\n",
            "1653:\tlearn: 0.3168251\ttotal: 60s\tremaining: 48.8s\n",
            "1654:\tlearn: 0.3167800\ttotal: 60s\tremaining: 48.8s\n",
            "1655:\tlearn: 0.3167407\ttotal: 1m\tremaining: 48.7s\n",
            "1656:\tlearn: 0.3167070\ttotal: 1m\tremaining: 48.7s\n",
            "1657:\tlearn: 0.3166972\ttotal: 1m\tremaining: 48.6s\n",
            "1658:\tlearn: 0.3166471\ttotal: 1m\tremaining: 48.6s\n",
            "1659:\tlearn: 0.3165569\ttotal: 1m\tremaining: 48.5s\n",
            "1660:\tlearn: 0.3165458\ttotal: 1m\tremaining: 48.5s\n",
            "1661:\tlearn: 0.3165226\ttotal: 1m\tremaining: 48.5s\n",
            "1662:\tlearn: 0.3164702\ttotal: 1m\tremaining: 48.4s\n",
            "1663:\tlearn: 0.3164384\ttotal: 1m\tremaining: 48.4s\n",
            "1664:\tlearn: 0.3163993\ttotal: 1m\tremaining: 48.3s\n",
            "1665:\tlearn: 0.3163293\ttotal: 1m\tremaining: 48.3s\n",
            "1666:\tlearn: 0.3162739\ttotal: 1m\tremaining: 48.3s\n",
            "1667:\tlearn: 0.3161922\ttotal: 1m\tremaining: 48.2s\n",
            "1668:\tlearn: 0.3161523\ttotal: 1m\tremaining: 48.2s\n",
            "1669:\tlearn: 0.3161195\ttotal: 1m\tremaining: 48.1s\n",
            "1670:\tlearn: 0.3160764\ttotal: 1m\tremaining: 48.1s\n",
            "1671:\tlearn: 0.3160508\ttotal: 1m\tremaining: 48.1s\n",
            "1672:\tlearn: 0.3160154\ttotal: 1m\tremaining: 48s\n",
            "1673:\tlearn: 0.3159896\ttotal: 1m\tremaining: 48s\n",
            "1674:\tlearn: 0.3159309\ttotal: 1m\tremaining: 47.9s\n",
            "1675:\tlearn: 0.3158877\ttotal: 1m\tremaining: 47.9s\n",
            "1676:\tlearn: 0.3158470\ttotal: 1m\tremaining: 47.9s\n",
            "1677:\tlearn: 0.3158404\ttotal: 1m\tremaining: 47.8s\n",
            "1678:\tlearn: 0.3157963\ttotal: 1m\tremaining: 47.8s\n",
            "1679:\tlearn: 0.3157329\ttotal: 1m\tremaining: 47.7s\n",
            "1680:\tlearn: 0.3156917\ttotal: 1m\tremaining: 47.7s\n",
            "1681:\tlearn: 0.3156472\ttotal: 1m\tremaining: 47.7s\n",
            "1682:\tlearn: 0.3156076\ttotal: 1m\tremaining: 47.6s\n",
            "1683:\tlearn: 0.3155628\ttotal: 1m\tremaining: 47.6s\n",
            "1684:\tlearn: 0.3155300\ttotal: 1m\tremaining: 47.5s\n",
            "1685:\tlearn: 0.3154645\ttotal: 1m\tremaining: 47.5s\n",
            "1686:\tlearn: 0.3153995\ttotal: 1m\tremaining: 47.4s\n",
            "1687:\tlearn: 0.3153540\ttotal: 1m\tremaining: 47.4s\n",
            "1688:\tlearn: 0.3153102\ttotal: 1m 1s\tremaining: 47.4s\n",
            "1689:\tlearn: 0.3152454\ttotal: 1m 1s\tremaining: 47.3s\n",
            "1690:\tlearn: 0.3151951\ttotal: 1m 1s\tremaining: 47.3s\n",
            "1691:\tlearn: 0.3151470\ttotal: 1m 1s\tremaining: 47.2s\n",
            "1692:\tlearn: 0.3150898\ttotal: 1m 1s\tremaining: 47.2s\n",
            "1693:\tlearn: 0.3150410\ttotal: 1m 1s\tremaining: 47.2s\n",
            "1694:\tlearn: 0.3149852\ttotal: 1m 1s\tremaining: 47.1s\n",
            "1695:\tlearn: 0.3149778\ttotal: 1m 1s\tremaining: 47.1s\n",
            "1696:\tlearn: 0.3149122\ttotal: 1m 1s\tremaining: 47s\n",
            "1697:\tlearn: 0.3148643\ttotal: 1m 1s\tremaining: 47s\n",
            "1698:\tlearn: 0.3148070\ttotal: 1m 1s\tremaining: 47s\n",
            "1699:\tlearn: 0.3147721\ttotal: 1m 1s\tremaining: 46.9s\n",
            "1700:\tlearn: 0.3147498\ttotal: 1m 1s\tremaining: 46.9s\n",
            "1701:\tlearn: 0.3147143\ttotal: 1m 1s\tremaining: 46.8s\n",
            "1702:\tlearn: 0.3147009\ttotal: 1m 1s\tremaining: 46.8s\n",
            "1703:\tlearn: 0.3146243\ttotal: 1m 1s\tremaining: 46.8s\n",
            "1704:\tlearn: 0.3145925\ttotal: 1m 1s\tremaining: 46.7s\n",
            "1705:\tlearn: 0.3145556\ttotal: 1m 1s\tremaining: 46.7s\n",
            "1706:\tlearn: 0.3145504\ttotal: 1m 1s\tremaining: 46.6s\n",
            "1707:\tlearn: 0.3145231\ttotal: 1m 1s\tremaining: 46.6s\n",
            "1708:\tlearn: 0.3145065\ttotal: 1m 1s\tremaining: 46.6s\n",
            "1709:\tlearn: 0.3144542\ttotal: 1m 1s\tremaining: 46.5s\n",
            "1710:\tlearn: 0.3144004\ttotal: 1m 1s\tremaining: 46.5s\n",
            "1711:\tlearn: 0.3143657\ttotal: 1m 1s\tremaining: 46.4s\n",
            "1712:\tlearn: 0.3143309\ttotal: 1m 1s\tremaining: 46.4s\n",
            "1713:\tlearn: 0.3142531\ttotal: 1m 1s\tremaining: 46.4s\n",
            "1714:\tlearn: 0.3142220\ttotal: 1m 1s\tremaining: 46.3s\n",
            "1715:\tlearn: 0.3141988\ttotal: 1m 1s\tremaining: 46.3s\n",
            "1716:\tlearn: 0.3141689\ttotal: 1m 1s\tremaining: 46.2s\n",
            "1717:\tlearn: 0.3141323\ttotal: 1m 1s\tremaining: 46.2s\n",
            "1718:\tlearn: 0.3140956\ttotal: 1m 1s\tremaining: 46.2s\n",
            "1719:\tlearn: 0.3140567\ttotal: 1m 1s\tremaining: 46.1s\n",
            "1720:\tlearn: 0.3140061\ttotal: 1m 1s\tremaining: 46.1s\n",
            "1721:\tlearn: 0.3139807\ttotal: 1m 2s\tremaining: 46s\n",
            "1722:\tlearn: 0.3139279\ttotal: 1m 2s\tremaining: 46s\n",
            "1723:\tlearn: 0.3138884\ttotal: 1m 2s\tremaining: 46s\n",
            "1724:\tlearn: 0.3138140\ttotal: 1m 2s\tremaining: 45.9s\n",
            "1725:\tlearn: 0.3138102\ttotal: 1m 2s\tremaining: 45.9s\n",
            "1726:\tlearn: 0.3137912\ttotal: 1m 2s\tremaining: 45.8s\n",
            "1727:\tlearn: 0.3137665\ttotal: 1m 2s\tremaining: 45.8s\n",
            "1728:\tlearn: 0.3136941\ttotal: 1m 2s\tremaining: 45.7s\n",
            "1729:\tlearn: 0.3136415\ttotal: 1m 2s\tremaining: 45.7s\n",
            "1730:\tlearn: 0.3136206\ttotal: 1m 2s\tremaining: 45.7s\n",
            "1731:\tlearn: 0.3135920\ttotal: 1m 2s\tremaining: 45.6s\n",
            "1732:\tlearn: 0.3135469\ttotal: 1m 2s\tremaining: 45.6s\n",
            "1733:\tlearn: 0.3135118\ttotal: 1m 2s\tremaining: 45.6s\n",
            "1734:\tlearn: 0.3134630\ttotal: 1m 2s\tremaining: 45.5s\n",
            "1735:\tlearn: 0.3134448\ttotal: 1m 2s\tremaining: 45.5s\n",
            "1736:\tlearn: 0.3133963\ttotal: 1m 2s\tremaining: 45.4s\n",
            "1737:\tlearn: 0.3133383\ttotal: 1m 2s\tremaining: 45.4s\n",
            "1738:\tlearn: 0.3132982\ttotal: 1m 2s\tremaining: 45.4s\n",
            "1739:\tlearn: 0.3132535\ttotal: 1m 2s\tremaining: 45.3s\n",
            "1740:\tlearn: 0.3132109\ttotal: 1m 2s\tremaining: 45.3s\n",
            "1741:\tlearn: 0.3131495\ttotal: 1m 2s\tremaining: 45.2s\n",
            "1742:\tlearn: 0.3131144\ttotal: 1m 2s\tremaining: 45.2s\n",
            "1743:\tlearn: 0.3130951\ttotal: 1m 2s\tremaining: 45.2s\n",
            "1744:\tlearn: 0.3130527\ttotal: 1m 2s\tremaining: 45.1s\n",
            "1745:\tlearn: 0.3129950\ttotal: 1m 2s\tremaining: 45.1s\n",
            "1746:\tlearn: 0.3129674\ttotal: 1m 2s\tremaining: 45s\n",
            "1747:\tlearn: 0.3128805\ttotal: 1m 2s\tremaining: 45s\n",
            "1748:\tlearn: 0.3128346\ttotal: 1m 2s\tremaining: 45s\n",
            "1749:\tlearn: 0.3128124\ttotal: 1m 2s\tremaining: 44.9s\n",
            "1750:\tlearn: 0.3127937\ttotal: 1m 2s\tremaining: 44.9s\n",
            "1751:\tlearn: 0.3127707\ttotal: 1m 2s\tremaining: 44.8s\n",
            "1752:\tlearn: 0.3127006\ttotal: 1m 2s\tremaining: 44.8s\n",
            "1753:\tlearn: 0.3126649\ttotal: 1m 3s\tremaining: 44.8s\n",
            "1754:\tlearn: 0.3126137\ttotal: 1m 3s\tremaining: 44.7s\n",
            "1755:\tlearn: 0.3125873\ttotal: 1m 3s\tremaining: 44.7s\n",
            "1756:\tlearn: 0.3125492\ttotal: 1m 3s\tremaining: 44.6s\n",
            "1757:\tlearn: 0.3124940\ttotal: 1m 3s\tremaining: 44.6s\n",
            "1758:\tlearn: 0.3124911\ttotal: 1m 3s\tremaining: 44.6s\n",
            "1759:\tlearn: 0.3124691\ttotal: 1m 3s\tremaining: 44.5s\n",
            "1760:\tlearn: 0.3124436\ttotal: 1m 3s\tremaining: 44.5s\n",
            "1761:\tlearn: 0.3123879\ttotal: 1m 3s\tremaining: 44.4s\n",
            "1762:\tlearn: 0.3123454\ttotal: 1m 3s\tremaining: 44.4s\n",
            "1763:\tlearn: 0.3123212\ttotal: 1m 3s\tremaining: 44.4s\n",
            "1764:\tlearn: 0.3122601\ttotal: 1m 3s\tremaining: 44.3s\n",
            "1765:\tlearn: 0.3122215\ttotal: 1m 3s\tremaining: 44.3s\n",
            "1766:\tlearn: 0.3122066\ttotal: 1m 3s\tremaining: 44.2s\n",
            "1767:\tlearn: 0.3121847\ttotal: 1m 3s\tremaining: 44.2s\n",
            "1768:\tlearn: 0.3121383\ttotal: 1m 3s\tremaining: 44.2s\n",
            "1769:\tlearn: 0.3120968\ttotal: 1m 3s\tremaining: 44.1s\n",
            "1770:\tlearn: 0.3120684\ttotal: 1m 3s\tremaining: 44.1s\n",
            "1771:\tlearn: 0.3120518\ttotal: 1m 3s\tremaining: 44s\n",
            "1772:\tlearn: 0.3120101\ttotal: 1m 3s\tremaining: 44s\n",
            "1773:\tlearn: 0.3119806\ttotal: 1m 3s\tremaining: 44s\n",
            "1774:\tlearn: 0.3119186\ttotal: 1m 3s\tremaining: 43.9s\n",
            "1775:\tlearn: 0.3118820\ttotal: 1m 3s\tremaining: 43.9s\n",
            "1776:\tlearn: 0.3118227\ttotal: 1m 3s\tremaining: 43.8s\n",
            "1777:\tlearn: 0.3117838\ttotal: 1m 3s\tremaining: 43.8s\n",
            "1778:\tlearn: 0.3117464\ttotal: 1m 3s\tremaining: 43.8s\n",
            "1779:\tlearn: 0.3117203\ttotal: 1m 3s\tremaining: 43.7s\n",
            "1780:\tlearn: 0.3116709\ttotal: 1m 3s\tremaining: 43.7s\n",
            "1781:\tlearn: 0.3116084\ttotal: 1m 3s\tremaining: 43.6s\n",
            "1782:\tlearn: 0.3115578\ttotal: 1m 3s\tremaining: 43.6s\n",
            "1783:\tlearn: 0.3115031\ttotal: 1m 3s\tremaining: 43.6s\n",
            "1784:\tlearn: 0.3114568\ttotal: 1m 3s\tremaining: 43.5s\n",
            "1785:\tlearn: 0.3114537\ttotal: 1m 3s\tremaining: 43.5s\n",
            "1786:\tlearn: 0.3114098\ttotal: 1m 3s\tremaining: 43.4s\n",
            "1787:\tlearn: 0.3113660\ttotal: 1m 4s\tremaining: 43.4s\n",
            "1788:\tlearn: 0.3113097\ttotal: 1m 4s\tremaining: 43.4s\n",
            "1789:\tlearn: 0.3112763\ttotal: 1m 4s\tremaining: 43.3s\n",
            "1790:\tlearn: 0.3112206\ttotal: 1m 4s\tremaining: 43.3s\n",
            "1791:\tlearn: 0.3112104\ttotal: 1m 4s\tremaining: 43.2s\n",
            "1792:\tlearn: 0.3111154\ttotal: 1m 4s\tremaining: 43.2s\n",
            "1793:\tlearn: 0.3110607\ttotal: 1m 4s\tremaining: 43.2s\n",
            "1794:\tlearn: 0.3110173\ttotal: 1m 4s\tremaining: 43.1s\n",
            "1795:\tlearn: 0.3109421\ttotal: 1m 4s\tremaining: 43.1s\n",
            "1796:\tlearn: 0.3108938\ttotal: 1m 4s\tremaining: 43s\n",
            "1797:\tlearn: 0.3108448\ttotal: 1m 4s\tremaining: 43s\n",
            "1798:\tlearn: 0.3108012\ttotal: 1m 4s\tremaining: 43s\n",
            "1799:\tlearn: 0.3107633\ttotal: 1m 4s\tremaining: 42.9s\n",
            "1800:\tlearn: 0.3107355\ttotal: 1m 4s\tremaining: 42.9s\n",
            "1801:\tlearn: 0.3107163\ttotal: 1m 4s\tremaining: 42.9s\n",
            "1802:\tlearn: 0.3107032\ttotal: 1m 4s\tremaining: 42.8s\n",
            "1803:\tlearn: 0.3106632\ttotal: 1m 4s\tremaining: 42.8s\n",
            "1804:\tlearn: 0.3106313\ttotal: 1m 4s\tremaining: 42.7s\n",
            "1805:\tlearn: 0.3105800\ttotal: 1m 4s\tremaining: 42.7s\n",
            "1806:\tlearn: 0.3105350\ttotal: 1m 4s\tremaining: 42.6s\n",
            "1807:\tlearn: 0.3104935\ttotal: 1m 4s\tremaining: 42.6s\n",
            "1808:\tlearn: 0.3104678\ttotal: 1m 4s\tremaining: 42.6s\n",
            "1809:\tlearn: 0.3104239\ttotal: 1m 4s\tremaining: 42.5s\n",
            "1810:\tlearn: 0.3103941\ttotal: 1m 4s\tremaining: 42.5s\n",
            "1811:\tlearn: 0.3103850\ttotal: 1m 4s\tremaining: 42.4s\n",
            "1812:\tlearn: 0.3103732\ttotal: 1m 4s\tremaining: 42.4s\n",
            "1813:\tlearn: 0.3103447\ttotal: 1m 4s\tremaining: 42.4s\n",
            "1814:\tlearn: 0.3103092\ttotal: 1m 4s\tremaining: 42.3s\n",
            "1815:\tlearn: 0.3102548\ttotal: 1m 4s\tremaining: 42.3s\n",
            "1816:\tlearn: 0.3102268\ttotal: 1m 4s\tremaining: 42.2s\n",
            "1817:\tlearn: 0.3101713\ttotal: 1m 4s\tremaining: 42.2s\n",
            "1818:\tlearn: 0.3101276\ttotal: 1m 4s\tremaining: 42.2s\n",
            "1819:\tlearn: 0.3101087\ttotal: 1m 4s\tremaining: 42.1s\n",
            "1820:\tlearn: 0.3100813\ttotal: 1m 5s\tremaining: 42.1s\n",
            "1821:\tlearn: 0.3100366\ttotal: 1m 5s\tremaining: 42.1s\n",
            "1822:\tlearn: 0.3099923\ttotal: 1m 5s\tremaining: 42s\n",
            "1823:\tlearn: 0.3099543\ttotal: 1m 5s\tremaining: 42s\n",
            "1824:\tlearn: 0.3099215\ttotal: 1m 5s\tremaining: 41.9s\n",
            "1825:\tlearn: 0.3098678\ttotal: 1m 5s\tremaining: 41.9s\n",
            "1826:\tlearn: 0.3098285\ttotal: 1m 5s\tremaining: 41.8s\n",
            "1827:\tlearn: 0.3098101\ttotal: 1m 5s\tremaining: 41.8s\n",
            "1828:\tlearn: 0.3097670\ttotal: 1m 5s\tremaining: 41.8s\n",
            "1829:\tlearn: 0.3097149\ttotal: 1m 5s\tremaining: 41.7s\n",
            "1830:\tlearn: 0.3096856\ttotal: 1m 5s\tremaining: 41.7s\n",
            "1831:\tlearn: 0.3096412\ttotal: 1m 5s\tremaining: 41.7s\n",
            "1832:\tlearn: 0.3096198\ttotal: 1m 5s\tremaining: 41.6s\n",
            "1833:\tlearn: 0.3095848\ttotal: 1m 5s\tremaining: 41.6s\n",
            "1834:\tlearn: 0.3095825\ttotal: 1m 5s\tremaining: 41.5s\n",
            "1835:\tlearn: 0.3095561\ttotal: 1m 5s\tremaining: 41.5s\n",
            "1836:\tlearn: 0.3095152\ttotal: 1m 5s\tremaining: 41.5s\n",
            "1837:\tlearn: 0.3094568\ttotal: 1m 5s\tremaining: 41.4s\n",
            "1838:\tlearn: 0.3094235\ttotal: 1m 5s\tremaining: 41.4s\n",
            "1839:\tlearn: 0.3093741\ttotal: 1m 5s\tremaining: 41.3s\n",
            "1840:\tlearn: 0.3093318\ttotal: 1m 5s\tremaining: 41.3s\n",
            "1841:\tlearn: 0.3092991\ttotal: 1m 5s\tremaining: 41.3s\n",
            "1842:\tlearn: 0.3092673\ttotal: 1m 5s\tremaining: 41.2s\n",
            "1843:\tlearn: 0.3092378\ttotal: 1m 5s\tremaining: 41.2s\n",
            "1844:\tlearn: 0.3092345\ttotal: 1m 5s\tremaining: 41.1s\n",
            "1845:\tlearn: 0.3091914\ttotal: 1m 5s\tremaining: 41.1s\n",
            "1846:\tlearn: 0.3091886\ttotal: 1m 5s\tremaining: 41.1s\n",
            "1847:\tlearn: 0.3091511\ttotal: 1m 5s\tremaining: 41s\n",
            "1848:\tlearn: 0.3091338\ttotal: 1m 5s\tremaining: 41s\n",
            "1849:\tlearn: 0.3090956\ttotal: 1m 5s\tremaining: 41s\n",
            "1850:\tlearn: 0.3090596\ttotal: 1m 5s\tremaining: 40.9s\n",
            "1851:\tlearn: 0.3090232\ttotal: 1m 5s\tremaining: 40.9s\n",
            "1852:\tlearn: 0.3089717\ttotal: 1m 5s\tremaining: 40.8s\n",
            "1853:\tlearn: 0.3089363\ttotal: 1m 6s\tremaining: 40.8s\n",
            "1854:\tlearn: 0.3089001\ttotal: 1m 6s\tremaining: 40.8s\n",
            "1855:\tlearn: 0.3088597\ttotal: 1m 6s\tremaining: 40.7s\n",
            "1856:\tlearn: 0.3088307\ttotal: 1m 6s\tremaining: 40.7s\n",
            "1857:\tlearn: 0.3087742\ttotal: 1m 6s\tremaining: 40.6s\n",
            "1858:\tlearn: 0.3087377\ttotal: 1m 6s\tremaining: 40.6s\n",
            "1859:\tlearn: 0.3086787\ttotal: 1m 6s\tremaining: 40.6s\n",
            "1860:\tlearn: 0.3086283\ttotal: 1m 6s\tremaining: 40.5s\n",
            "1861:\tlearn: 0.3085777\ttotal: 1m 6s\tremaining: 40.5s\n",
            "1862:\tlearn: 0.3085447\ttotal: 1m 6s\tremaining: 40.4s\n",
            "1863:\tlearn: 0.3085021\ttotal: 1m 6s\tremaining: 40.4s\n",
            "1864:\tlearn: 0.3084656\ttotal: 1m 6s\tremaining: 40.4s\n",
            "1865:\tlearn: 0.3084406\ttotal: 1m 6s\tremaining: 40.3s\n",
            "1866:\tlearn: 0.3084078\ttotal: 1m 6s\tremaining: 40.3s\n",
            "1867:\tlearn: 0.3083589\ttotal: 1m 6s\tremaining: 40.3s\n",
            "1868:\tlearn: 0.3082905\ttotal: 1m 6s\tremaining: 40.2s\n",
            "1869:\tlearn: 0.3082588\ttotal: 1m 6s\tremaining: 40.2s\n",
            "1870:\tlearn: 0.3082401\ttotal: 1m 6s\tremaining: 40.1s\n",
            "1871:\tlearn: 0.3081641\ttotal: 1m 6s\tremaining: 40.1s\n",
            "1872:\tlearn: 0.3081357\ttotal: 1m 6s\tremaining: 40.1s\n",
            "1873:\tlearn: 0.3080667\ttotal: 1m 6s\tremaining: 40s\n",
            "1874:\tlearn: 0.3080306\ttotal: 1m 6s\tremaining: 40s\n",
            "1875:\tlearn: 0.3080041\ttotal: 1m 6s\tremaining: 39.9s\n",
            "1876:\tlearn: 0.3079508\ttotal: 1m 6s\tremaining: 39.9s\n",
            "1877:\tlearn: 0.3079133\ttotal: 1m 6s\tremaining: 39.9s\n",
            "1878:\tlearn: 0.3079067\ttotal: 1m 6s\tremaining: 39.8s\n",
            "1879:\tlearn: 0.3078470\ttotal: 1m 6s\tremaining: 39.8s\n",
            "1880:\tlearn: 0.3078202\ttotal: 1m 6s\tremaining: 39.8s\n",
            "1881:\tlearn: 0.3077519\ttotal: 1m 6s\tremaining: 39.7s\n",
            "1882:\tlearn: 0.3076857\ttotal: 1m 6s\tremaining: 39.7s\n",
            "1883:\tlearn: 0.3076481\ttotal: 1m 6s\tremaining: 39.6s\n",
            "1884:\tlearn: 0.3076352\ttotal: 1m 6s\tremaining: 39.6s\n",
            "1885:\tlearn: 0.3076056\ttotal: 1m 6s\tremaining: 39.6s\n",
            "1886:\tlearn: 0.3075469\ttotal: 1m 7s\tremaining: 39.5s\n",
            "1887:\tlearn: 0.3075050\ttotal: 1m 7s\tremaining: 39.5s\n",
            "1888:\tlearn: 0.3074516\ttotal: 1m 7s\tremaining: 39.4s\n",
            "1889:\tlearn: 0.3074172\ttotal: 1m 7s\tremaining: 39.4s\n",
            "1890:\tlearn: 0.3073645\ttotal: 1m 7s\tremaining: 39.4s\n",
            "1891:\tlearn: 0.3073203\ttotal: 1m 7s\tremaining: 39.3s\n",
            "1892:\tlearn: 0.3073004\ttotal: 1m 7s\tremaining: 39.3s\n",
            "1893:\tlearn: 0.3072624\ttotal: 1m 7s\tremaining: 39.3s\n",
            "1894:\tlearn: 0.3072029\ttotal: 1m 7s\tremaining: 39.2s\n",
            "1895:\tlearn: 0.3071774\ttotal: 1m 7s\tremaining: 39.2s\n",
            "1896:\tlearn: 0.3071386\ttotal: 1m 7s\tremaining: 39.1s\n",
            "1897:\tlearn: 0.3071054\ttotal: 1m 7s\tremaining: 39.1s\n",
            "1898:\tlearn: 0.3070580\ttotal: 1m 7s\tremaining: 39.1s\n",
            "1899:\tlearn: 0.3070310\ttotal: 1m 7s\tremaining: 39s\n",
            "1900:\tlearn: 0.3070048\ttotal: 1m 7s\tremaining: 39s\n",
            "1901:\tlearn: 0.3069772\ttotal: 1m 7s\tremaining: 39s\n",
            "1902:\tlearn: 0.3069643\ttotal: 1m 7s\tremaining: 38.9s\n",
            "1903:\tlearn: 0.3069206\ttotal: 1m 7s\tremaining: 38.9s\n",
            "1904:\tlearn: 0.3068895\ttotal: 1m 7s\tremaining: 38.8s\n",
            "1905:\tlearn: 0.3068412\ttotal: 1m 7s\tremaining: 38.8s\n",
            "1906:\tlearn: 0.3067772\ttotal: 1m 7s\tremaining: 38.8s\n",
            "1907:\tlearn: 0.3067361\ttotal: 1m 7s\tremaining: 38.7s\n",
            "1908:\tlearn: 0.3066816\ttotal: 1m 7s\tremaining: 38.7s\n",
            "1909:\tlearn: 0.3066507\ttotal: 1m 7s\tremaining: 38.6s\n",
            "1910:\tlearn: 0.3066169\ttotal: 1m 7s\tremaining: 38.6s\n",
            "1911:\tlearn: 0.3065741\ttotal: 1m 7s\tremaining: 38.6s\n",
            "1912:\tlearn: 0.3065203\ttotal: 1m 7s\tremaining: 38.5s\n",
            "1913:\tlearn: 0.3064850\ttotal: 1m 7s\tremaining: 38.5s\n",
            "1914:\tlearn: 0.3064540\ttotal: 1m 7s\tremaining: 38.4s\n",
            "1915:\tlearn: 0.3064112\ttotal: 1m 7s\tremaining: 38.4s\n",
            "1916:\tlearn: 0.3063836\ttotal: 1m 7s\tremaining: 38.4s\n",
            "1917:\tlearn: 0.3063411\ttotal: 1m 7s\tremaining: 38.3s\n",
            "1918:\tlearn: 0.3062874\ttotal: 1m 7s\tremaining: 38.3s\n",
            "1919:\tlearn: 0.3062309\ttotal: 1m 8s\tremaining: 38.3s\n",
            "1920:\tlearn: 0.3062012\ttotal: 1m 8s\tremaining: 38.2s\n",
            "1921:\tlearn: 0.3061563\ttotal: 1m 8s\tremaining: 38.2s\n",
            "1922:\tlearn: 0.3061008\ttotal: 1m 8s\tremaining: 38.1s\n",
            "1923:\tlearn: 0.3060760\ttotal: 1m 8s\tremaining: 38.1s\n",
            "1924:\tlearn: 0.3060413\ttotal: 1m 8s\tremaining: 38.1s\n",
            "1925:\tlearn: 0.3059988\ttotal: 1m 8s\tremaining: 38s\n",
            "1926:\tlearn: 0.3059470\ttotal: 1m 8s\tremaining: 38s\n",
            "1927:\tlearn: 0.3059189\ttotal: 1m 8s\tremaining: 37.9s\n",
            "1928:\tlearn: 0.3058988\ttotal: 1m 8s\tremaining: 37.9s\n",
            "1929:\tlearn: 0.3058173\ttotal: 1m 8s\tremaining: 37.9s\n",
            "1930:\tlearn: 0.3057795\ttotal: 1m 8s\tremaining: 37.8s\n",
            "1931:\tlearn: 0.3057549\ttotal: 1m 8s\tremaining: 37.8s\n",
            "1932:\tlearn: 0.3057362\ttotal: 1m 8s\tremaining: 37.8s\n",
            "1933:\tlearn: 0.3057035\ttotal: 1m 8s\tremaining: 37.7s\n",
            "1934:\tlearn: 0.3056513\ttotal: 1m 8s\tremaining: 37.7s\n",
            "1935:\tlearn: 0.3055933\ttotal: 1m 8s\tremaining: 37.7s\n",
            "1936:\tlearn: 0.3055422\ttotal: 1m 8s\tremaining: 37.6s\n",
            "1937:\tlearn: 0.3055122\ttotal: 1m 8s\tremaining: 37.6s\n",
            "1938:\tlearn: 0.3054858\ttotal: 1m 8s\tremaining: 37.6s\n",
            "1939:\tlearn: 0.3054641\ttotal: 1m 8s\tremaining: 37.6s\n",
            "1940:\tlearn: 0.3054238\ttotal: 1m 8s\tremaining: 37.5s\n",
            "1941:\tlearn: 0.3054029\ttotal: 1m 8s\tremaining: 37.5s\n",
            "1942:\tlearn: 0.3053529\ttotal: 1m 8s\tremaining: 37.5s\n",
            "1943:\tlearn: 0.3053066\ttotal: 1m 8s\tremaining: 37.5s\n",
            "1944:\tlearn: 0.3052477\ttotal: 1m 9s\tremaining: 37.5s\n",
            "1945:\tlearn: 0.3052073\ttotal: 1m 9s\tremaining: 37.4s\n",
            "1946:\tlearn: 0.3051698\ttotal: 1m 9s\tremaining: 37.4s\n",
            "1947:\tlearn: 0.3051052\ttotal: 1m 9s\tremaining: 37.4s\n",
            "1948:\tlearn: 0.3050678\ttotal: 1m 9s\tremaining: 37.4s\n",
            "1949:\tlearn: 0.3050303\ttotal: 1m 9s\tremaining: 37.3s\n",
            "1950:\tlearn: 0.3049782\ttotal: 1m 9s\tremaining: 37.3s\n",
            "1951:\tlearn: 0.3049418\ttotal: 1m 9s\tremaining: 37.3s\n",
            "1952:\tlearn: 0.3048893\ttotal: 1m 9s\tremaining: 37.3s\n",
            "1953:\tlearn: 0.3048506\ttotal: 1m 9s\tremaining: 37.2s\n",
            "1954:\tlearn: 0.3048052\ttotal: 1m 9s\tremaining: 37.2s\n",
            "1955:\tlearn: 0.3047788\ttotal: 1m 9s\tremaining: 37.2s\n",
            "1956:\tlearn: 0.3047386\ttotal: 1m 9s\tremaining: 37.2s\n",
            "1957:\tlearn: 0.3046822\ttotal: 1m 9s\tremaining: 37.1s\n",
            "1958:\tlearn: 0.3046331\ttotal: 1m 9s\tremaining: 37.1s\n",
            "1959:\tlearn: 0.3046253\ttotal: 1m 9s\tremaining: 37.1s\n",
            "1960:\tlearn: 0.3045539\ttotal: 1m 9s\tremaining: 37s\n",
            "1961:\tlearn: 0.3045209\ttotal: 1m 9s\tremaining: 37s\n",
            "1962:\tlearn: 0.3044669\ttotal: 1m 10s\tremaining: 37s\n",
            "1963:\tlearn: 0.3044335\ttotal: 1m 10s\tremaining: 37s\n",
            "1964:\tlearn: 0.3043814\ttotal: 1m 10s\tremaining: 36.9s\n",
            "1965:\tlearn: 0.3043385\ttotal: 1m 10s\tremaining: 36.9s\n",
            "1966:\tlearn: 0.3042921\ttotal: 1m 10s\tremaining: 36.9s\n",
            "1967:\tlearn: 0.3042492\ttotal: 1m 10s\tremaining: 36.9s\n",
            "1968:\tlearn: 0.3042103\ttotal: 1m 10s\tremaining: 36.9s\n",
            "1969:\tlearn: 0.3041726\ttotal: 1m 10s\tremaining: 36.8s\n",
            "1970:\tlearn: 0.3041410\ttotal: 1m 10s\tremaining: 36.8s\n",
            "1971:\tlearn: 0.3041382\ttotal: 1m 10s\tremaining: 36.8s\n",
            "1972:\tlearn: 0.3040996\ttotal: 1m 10s\tremaining: 36.8s\n",
            "1973:\tlearn: 0.3040294\ttotal: 1m 10s\tremaining: 36.7s\n",
            "1974:\tlearn: 0.3039817\ttotal: 1m 10s\tremaining: 36.7s\n",
            "1975:\tlearn: 0.3039647\ttotal: 1m 10s\tremaining: 36.7s\n",
            "1976:\tlearn: 0.3039469\ttotal: 1m 10s\tremaining: 36.7s\n",
            "1977:\tlearn: 0.3039303\ttotal: 1m 10s\tremaining: 36.6s\n",
            "1978:\tlearn: 0.3038884\ttotal: 1m 10s\tremaining: 36.6s\n",
            "1979:\tlearn: 0.3038279\ttotal: 1m 11s\tremaining: 36.6s\n",
            "1980:\tlearn: 0.3037937\ttotal: 1m 11s\tremaining: 36.6s\n",
            "1981:\tlearn: 0.3037622\ttotal: 1m 11s\tremaining: 36.6s\n",
            "1982:\tlearn: 0.3037273\ttotal: 1m 11s\tremaining: 36.5s\n",
            "1983:\tlearn: 0.3036874\ttotal: 1m 11s\tremaining: 36.5s\n",
            "1984:\tlearn: 0.3036147\ttotal: 1m 11s\tremaining: 36.5s\n",
            "1985:\tlearn: 0.3035450\ttotal: 1m 11s\tremaining: 36.5s\n",
            "1986:\tlearn: 0.3034987\ttotal: 1m 11s\tremaining: 36.4s\n",
            "1987:\tlearn: 0.3034553\ttotal: 1m 11s\tremaining: 36.4s\n",
            "1988:\tlearn: 0.3033933\ttotal: 1m 11s\tremaining: 36.4s\n",
            "1989:\tlearn: 0.3033865\ttotal: 1m 11s\tremaining: 36.4s\n",
            "1990:\tlearn: 0.3033682\ttotal: 1m 11s\tremaining: 36.4s\n",
            "1991:\tlearn: 0.3033334\ttotal: 1m 11s\tremaining: 36.3s\n",
            "1992:\tlearn: 0.3032744\ttotal: 1m 11s\tremaining: 36.3s\n",
            "1993:\tlearn: 0.3032265\ttotal: 1m 11s\tremaining: 36.3s\n",
            "1994:\tlearn: 0.3031797\ttotal: 1m 11s\tremaining: 36.3s\n",
            "1995:\tlearn: 0.3031346\ttotal: 1m 12s\tremaining: 36.2s\n",
            "1996:\tlearn: 0.3030826\ttotal: 1m 12s\tremaining: 36.2s\n",
            "1997:\tlearn: 0.3030484\ttotal: 1m 12s\tremaining: 36.2s\n",
            "1998:\tlearn: 0.3030091\ttotal: 1m 12s\tremaining: 36.2s\n",
            "1999:\tlearn: 0.3029733\ttotal: 1m 12s\tremaining: 36.1s\n",
            "2000:\tlearn: 0.3029317\ttotal: 1m 12s\tremaining: 36.1s\n",
            "2001:\tlearn: 0.3028983\ttotal: 1m 12s\tremaining: 36.1s\n",
            "2002:\tlearn: 0.3028526\ttotal: 1m 12s\tremaining: 36.1s\n",
            "2003:\tlearn: 0.3028259\ttotal: 1m 12s\tremaining: 36s\n",
            "2004:\tlearn: 0.3027576\ttotal: 1m 12s\tremaining: 36s\n",
            "2005:\tlearn: 0.3027298\ttotal: 1m 12s\tremaining: 36s\n",
            "2006:\tlearn: 0.3026796\ttotal: 1m 12s\tremaining: 36s\n",
            "2007:\tlearn: 0.3026344\ttotal: 1m 12s\tremaining: 35.9s\n",
            "2008:\tlearn: 0.3026144\ttotal: 1m 12s\tremaining: 35.9s\n",
            "2009:\tlearn: 0.3026009\ttotal: 1m 12s\tremaining: 35.9s\n",
            "2010:\tlearn: 0.3025569\ttotal: 1m 12s\tremaining: 35.9s\n",
            "2011:\tlearn: 0.3025281\ttotal: 1m 12s\tremaining: 35.8s\n",
            "2012:\tlearn: 0.3024808\ttotal: 1m 13s\tremaining: 35.8s\n",
            "2013:\tlearn: 0.3024250\ttotal: 1m 13s\tremaining: 35.8s\n",
            "2014:\tlearn: 0.3023885\ttotal: 1m 13s\tremaining: 35.8s\n",
            "2015:\tlearn: 0.3023555\ttotal: 1m 13s\tremaining: 35.7s\n",
            "2016:\tlearn: 0.3022892\ttotal: 1m 13s\tremaining: 35.7s\n",
            "2017:\tlearn: 0.3022369\ttotal: 1m 13s\tremaining: 35.7s\n",
            "2018:\tlearn: 0.3022252\ttotal: 1m 13s\tremaining: 35.7s\n",
            "2019:\tlearn: 0.3021604\ttotal: 1m 13s\tremaining: 35.6s\n",
            "2020:\tlearn: 0.3021033\ttotal: 1m 13s\tremaining: 35.6s\n",
            "2021:\tlearn: 0.3020750\ttotal: 1m 13s\tremaining: 35.6s\n",
            "2022:\tlearn: 0.3020196\ttotal: 1m 13s\tremaining: 35.6s\n",
            "2023:\tlearn: 0.3019692\ttotal: 1m 13s\tremaining: 35.5s\n",
            "2024:\tlearn: 0.3019408\ttotal: 1m 13s\tremaining: 35.5s\n",
            "2025:\tlearn: 0.3019013\ttotal: 1m 13s\tremaining: 35.5s\n",
            "2026:\tlearn: 0.3018688\ttotal: 1m 13s\tremaining: 35.5s\n",
            "2027:\tlearn: 0.3018523\ttotal: 1m 13s\tremaining: 35.4s\n",
            "2028:\tlearn: 0.3017824\ttotal: 1m 13s\tremaining: 35.4s\n",
            "2029:\tlearn: 0.3017332\ttotal: 1m 14s\tremaining: 35.4s\n",
            "2030:\tlearn: 0.3017132\ttotal: 1m 14s\tremaining: 35.3s\n",
            "2031:\tlearn: 0.3016705\ttotal: 1m 14s\tremaining: 35.3s\n",
            "2032:\tlearn: 0.3016436\ttotal: 1m 14s\tremaining: 35.3s\n",
            "2033:\tlearn: 0.3016016\ttotal: 1m 14s\tremaining: 35.3s\n",
            "2034:\tlearn: 0.3015586\ttotal: 1m 14s\tremaining: 35.2s\n",
            "2035:\tlearn: 0.3015304\ttotal: 1m 14s\tremaining: 35.2s\n",
            "2036:\tlearn: 0.3014883\ttotal: 1m 14s\tremaining: 35.2s\n",
            "2037:\tlearn: 0.3014325\ttotal: 1m 14s\tremaining: 35.1s\n",
            "2038:\tlearn: 0.3013802\ttotal: 1m 14s\tremaining: 35.1s\n",
            "2039:\tlearn: 0.3013548\ttotal: 1m 14s\tremaining: 35.1s\n",
            "2040:\tlearn: 0.3013127\ttotal: 1m 14s\tremaining: 35.1s\n",
            "2041:\tlearn: 0.3012770\ttotal: 1m 14s\tremaining: 35s\n",
            "2042:\tlearn: 0.3012051\ttotal: 1m 14s\tremaining: 35s\n",
            "2043:\tlearn: 0.3011406\ttotal: 1m 14s\tremaining: 35s\n",
            "2044:\tlearn: 0.3010970\ttotal: 1m 14s\tremaining: 35s\n",
            "2045:\tlearn: 0.3010830\ttotal: 1m 14s\tremaining: 34.9s\n",
            "2046:\tlearn: 0.3010395\ttotal: 1m 14s\tremaining: 34.9s\n",
            "2047:\tlearn: 0.3010086\ttotal: 1m 15s\tremaining: 34.9s\n",
            "2048:\tlearn: 0.3009758\ttotal: 1m 15s\tremaining: 34.9s\n",
            "2049:\tlearn: 0.3009316\ttotal: 1m 15s\tremaining: 34.8s\n",
            "2050:\tlearn: 0.3008706\ttotal: 1m 15s\tremaining: 34.8s\n",
            "2051:\tlearn: 0.3008226\ttotal: 1m 15s\tremaining: 34.8s\n",
            "2052:\tlearn: 0.3008024\ttotal: 1m 15s\tremaining: 34.7s\n",
            "2053:\tlearn: 0.3007783\ttotal: 1m 15s\tremaining: 34.7s\n",
            "2054:\tlearn: 0.3007481\ttotal: 1m 15s\tremaining: 34.7s\n",
            "2055:\tlearn: 0.3007131\ttotal: 1m 15s\tremaining: 34.7s\n",
            "2056:\tlearn: 0.3006703\ttotal: 1m 15s\tremaining: 34.6s\n",
            "2057:\tlearn: 0.3006676\ttotal: 1m 15s\tremaining: 34.6s\n",
            "2058:\tlearn: 0.3006238\ttotal: 1m 15s\tremaining: 34.6s\n",
            "2059:\tlearn: 0.3005859\ttotal: 1m 15s\tremaining: 34.6s\n",
            "2060:\tlearn: 0.3005326\ttotal: 1m 15s\tremaining: 34.5s\n",
            "2061:\tlearn: 0.3004839\ttotal: 1m 15s\tremaining: 34.5s\n",
            "2062:\tlearn: 0.3004734\ttotal: 1m 15s\tremaining: 34.5s\n",
            "2063:\tlearn: 0.3004300\ttotal: 1m 15s\tremaining: 34.4s\n",
            "2064:\tlearn: 0.3004118\ttotal: 1m 15s\tremaining: 34.4s\n",
            "2065:\tlearn: 0.3003919\ttotal: 1m 15s\tremaining: 34.3s\n",
            "2066:\tlearn: 0.3003420\ttotal: 1m 15s\tremaining: 34.3s\n",
            "2067:\tlearn: 0.3003023\ttotal: 1m 16s\tremaining: 34.3s\n",
            "2068:\tlearn: 0.3002687\ttotal: 1m 16s\tremaining: 34.2s\n",
            "2069:\tlearn: 0.3002396\ttotal: 1m 16s\tremaining: 34.2s\n",
            "2070:\tlearn: 0.3001969\ttotal: 1m 16s\tremaining: 34.1s\n",
            "2071:\tlearn: 0.3001669\ttotal: 1m 16s\tremaining: 34.1s\n",
            "2072:\tlearn: 0.3001213\ttotal: 1m 16s\tremaining: 34.1s\n",
            "2073:\tlearn: 0.3000710\ttotal: 1m 16s\tremaining: 34s\n",
            "2074:\tlearn: 0.3000335\ttotal: 1m 16s\tremaining: 34s\n",
            "2075:\tlearn: 0.3000026\ttotal: 1m 16s\tremaining: 33.9s\n",
            "2076:\tlearn: 0.2999570\ttotal: 1m 16s\tremaining: 33.9s\n",
            "2077:\tlearn: 0.2999140\ttotal: 1m 16s\tremaining: 33.9s\n",
            "2078:\tlearn: 0.2998608\ttotal: 1m 16s\tremaining: 33.8s\n",
            "2079:\tlearn: 0.2998194\ttotal: 1m 16s\tremaining: 33.8s\n",
            "2080:\tlearn: 0.2997709\ttotal: 1m 16s\tremaining: 33.7s\n",
            "2081:\tlearn: 0.2997519\ttotal: 1m 16s\tremaining: 33.7s\n",
            "2082:\tlearn: 0.2997172\ttotal: 1m 16s\tremaining: 33.7s\n",
            "2083:\tlearn: 0.2996687\ttotal: 1m 16s\tremaining: 33.6s\n",
            "2084:\tlearn: 0.2996448\ttotal: 1m 16s\tremaining: 33.6s\n",
            "2085:\tlearn: 0.2995908\ttotal: 1m 16s\tremaining: 33.5s\n",
            "2086:\tlearn: 0.2995606\ttotal: 1m 16s\tremaining: 33.5s\n",
            "2087:\tlearn: 0.2995220\ttotal: 1m 16s\tremaining: 33.5s\n",
            "2088:\tlearn: 0.2994859\ttotal: 1m 16s\tremaining: 33.4s\n",
            "2089:\tlearn: 0.2994500\ttotal: 1m 16s\tremaining: 33.4s\n",
            "2090:\tlearn: 0.2994137\ttotal: 1m 16s\tremaining: 33.3s\n",
            "2091:\tlearn: 0.2993576\ttotal: 1m 16s\tremaining: 33.3s\n",
            "2092:\tlearn: 0.2993162\ttotal: 1m 16s\tremaining: 33.3s\n",
            "2093:\tlearn: 0.2992881\ttotal: 1m 16s\tremaining: 33.2s\n",
            "2094:\tlearn: 0.2992488\ttotal: 1m 16s\tremaining: 33.2s\n",
            "2095:\tlearn: 0.2992287\ttotal: 1m 16s\tremaining: 33.1s\n",
            "2096:\tlearn: 0.2991937\ttotal: 1m 16s\tremaining: 33.1s\n",
            "2097:\tlearn: 0.2991576\ttotal: 1m 16s\tremaining: 33.1s\n",
            "2098:\tlearn: 0.2991044\ttotal: 1m 16s\tremaining: 33s\n",
            "2099:\tlearn: 0.2990668\ttotal: 1m 16s\tremaining: 33s\n",
            "2100:\tlearn: 0.2990470\ttotal: 1m 17s\tremaining: 33s\n",
            "2101:\tlearn: 0.2989907\ttotal: 1m 17s\tremaining: 32.9s\n",
            "2102:\tlearn: 0.2989421\ttotal: 1m 17s\tremaining: 32.9s\n",
            "2103:\tlearn: 0.2989158\ttotal: 1m 17s\tremaining: 32.8s\n",
            "2104:\tlearn: 0.2988793\ttotal: 1m 17s\tremaining: 32.8s\n",
            "2105:\tlearn: 0.2988394\ttotal: 1m 17s\tremaining: 32.8s\n",
            "2106:\tlearn: 0.2988195\ttotal: 1m 17s\tremaining: 32.7s\n",
            "2107:\tlearn: 0.2988059\ttotal: 1m 17s\tremaining: 32.7s\n",
            "2108:\tlearn: 0.2987451\ttotal: 1m 17s\tremaining: 32.6s\n",
            "2109:\tlearn: 0.2987213\ttotal: 1m 17s\tremaining: 32.6s\n",
            "2110:\tlearn: 0.2986699\ttotal: 1m 17s\tremaining: 32.6s\n",
            "2111:\tlearn: 0.2986327\ttotal: 1m 17s\tremaining: 32.5s\n",
            "2112:\tlearn: 0.2986150\ttotal: 1m 17s\tremaining: 32.5s\n",
            "2113:\tlearn: 0.2985708\ttotal: 1m 17s\tremaining: 32.4s\n",
            "2114:\tlearn: 0.2985387\ttotal: 1m 17s\tremaining: 32.4s\n",
            "2115:\tlearn: 0.2984941\ttotal: 1m 17s\tremaining: 32.4s\n",
            "2116:\tlearn: 0.2984761\ttotal: 1m 17s\tremaining: 32.3s\n",
            "2117:\tlearn: 0.2984447\ttotal: 1m 17s\tremaining: 32.3s\n",
            "2118:\tlearn: 0.2984068\ttotal: 1m 17s\tremaining: 32.2s\n",
            "2119:\tlearn: 0.2983673\ttotal: 1m 17s\tremaining: 32.2s\n",
            "2120:\tlearn: 0.2983437\ttotal: 1m 17s\tremaining: 32.2s\n",
            "2121:\tlearn: 0.2983299\ttotal: 1m 17s\tremaining: 32.1s\n",
            "2122:\tlearn: 0.2983111\ttotal: 1m 17s\tremaining: 32.1s\n",
            "2123:\tlearn: 0.2982352\ttotal: 1m 17s\tremaining: 32.1s\n",
            "2124:\tlearn: 0.2982112\ttotal: 1m 17s\tremaining: 32s\n",
            "2125:\tlearn: 0.2981836\ttotal: 1m 17s\tremaining: 32s\n",
            "2126:\tlearn: 0.2981586\ttotal: 1m 17s\tremaining: 31.9s\n",
            "2127:\tlearn: 0.2981171\ttotal: 1m 17s\tremaining: 31.9s\n",
            "2128:\tlearn: 0.2980800\ttotal: 1m 17s\tremaining: 31.9s\n",
            "2129:\tlearn: 0.2980489\ttotal: 1m 17s\tremaining: 31.8s\n",
            "2130:\tlearn: 0.2980065\ttotal: 1m 17s\tremaining: 31.8s\n",
            "2131:\tlearn: 0.2979810\ttotal: 1m 17s\tremaining: 31.7s\n",
            "2132:\tlearn: 0.2979515\ttotal: 1m 17s\tremaining: 31.7s\n",
            "2133:\tlearn: 0.2979249\ttotal: 1m 18s\tremaining: 31.7s\n",
            "2134:\tlearn: 0.2978818\ttotal: 1m 18s\tremaining: 31.6s\n",
            "2135:\tlearn: 0.2978541\ttotal: 1m 18s\tremaining: 31.6s\n",
            "2136:\tlearn: 0.2978241\ttotal: 1m 18s\tremaining: 31.5s\n",
            "2137:\tlearn: 0.2977808\ttotal: 1m 18s\tremaining: 31.5s\n",
            "2138:\tlearn: 0.2977747\ttotal: 1m 18s\tremaining: 31.5s\n",
            "2139:\tlearn: 0.2977411\ttotal: 1m 18s\tremaining: 31.4s\n",
            "2140:\tlearn: 0.2977231\ttotal: 1m 18s\tremaining: 31.4s\n",
            "2141:\tlearn: 0.2976740\ttotal: 1m 18s\tremaining: 31.3s\n",
            "2142:\tlearn: 0.2976288\ttotal: 1m 18s\tremaining: 31.3s\n",
            "2143:\tlearn: 0.2975952\ttotal: 1m 18s\tremaining: 31.3s\n",
            "2144:\tlearn: 0.2975808\ttotal: 1m 18s\tremaining: 31.2s\n",
            "2145:\tlearn: 0.2975222\ttotal: 1m 18s\tremaining: 31.2s\n",
            "2146:\tlearn: 0.2974699\ttotal: 1m 18s\tremaining: 31.2s\n",
            "2147:\tlearn: 0.2974379\ttotal: 1m 18s\tremaining: 31.1s\n",
            "2148:\tlearn: 0.2974181\ttotal: 1m 18s\tremaining: 31.1s\n",
            "2149:\tlearn: 0.2973814\ttotal: 1m 18s\tremaining: 31s\n",
            "2150:\tlearn: 0.2973364\ttotal: 1m 18s\tremaining: 31s\n",
            "2151:\tlearn: 0.2972927\ttotal: 1m 18s\tremaining: 31s\n",
            "2152:\tlearn: 0.2972360\ttotal: 1m 18s\tremaining: 30.9s\n",
            "2153:\tlearn: 0.2971805\ttotal: 1m 18s\tremaining: 30.9s\n",
            "2154:\tlearn: 0.2971533\ttotal: 1m 18s\tremaining: 30.8s\n",
            "2155:\tlearn: 0.2971362\ttotal: 1m 18s\tremaining: 30.8s\n",
            "2156:\tlearn: 0.2971133\ttotal: 1m 18s\tremaining: 30.8s\n",
            "2157:\tlearn: 0.2970403\ttotal: 1m 18s\tremaining: 30.7s\n",
            "2158:\tlearn: 0.2969961\ttotal: 1m 18s\tremaining: 30.7s\n",
            "2159:\tlearn: 0.2969459\ttotal: 1m 18s\tremaining: 30.6s\n",
            "2160:\tlearn: 0.2968867\ttotal: 1m 18s\tremaining: 30.6s\n",
            "2161:\tlearn: 0.2968337\ttotal: 1m 18s\tremaining: 30.6s\n",
            "2162:\tlearn: 0.2967981\ttotal: 1m 18s\tremaining: 30.5s\n",
            "2163:\tlearn: 0.2967325\ttotal: 1m 18s\tremaining: 30.5s\n",
            "2164:\tlearn: 0.2966926\ttotal: 1m 18s\tremaining: 30.5s\n",
            "2165:\tlearn: 0.2966443\ttotal: 1m 19s\tremaining: 30.4s\n",
            "2166:\tlearn: 0.2966277\ttotal: 1m 19s\tremaining: 30.4s\n",
            "2167:\tlearn: 0.2965880\ttotal: 1m 19s\tremaining: 30.3s\n",
            "2168:\tlearn: 0.2965600\ttotal: 1m 19s\tremaining: 30.3s\n",
            "2169:\tlearn: 0.2965236\ttotal: 1m 19s\tremaining: 30.3s\n",
            "2170:\tlearn: 0.2965009\ttotal: 1m 19s\tremaining: 30.2s\n",
            "2171:\tlearn: 0.2964891\ttotal: 1m 19s\tremaining: 30.2s\n",
            "2172:\tlearn: 0.2964629\ttotal: 1m 19s\tremaining: 30.1s\n",
            "2173:\tlearn: 0.2964293\ttotal: 1m 19s\tremaining: 30.1s\n",
            "2174:\tlearn: 0.2963856\ttotal: 1m 19s\tremaining: 30.1s\n",
            "2175:\tlearn: 0.2963586\ttotal: 1m 19s\tremaining: 30s\n",
            "2176:\tlearn: 0.2963087\ttotal: 1m 19s\tremaining: 30s\n",
            "2177:\tlearn: 0.2962528\ttotal: 1m 19s\tremaining: 30s\n",
            "2178:\tlearn: 0.2962159\ttotal: 1m 19s\tremaining: 29.9s\n",
            "2179:\tlearn: 0.2961289\ttotal: 1m 19s\tremaining: 29.9s\n",
            "2180:\tlearn: 0.2961099\ttotal: 1m 19s\tremaining: 29.8s\n",
            "2181:\tlearn: 0.2960787\ttotal: 1m 19s\tremaining: 29.8s\n",
            "2182:\tlearn: 0.2960624\ttotal: 1m 19s\tremaining: 29.8s\n",
            "2183:\tlearn: 0.2960018\ttotal: 1m 19s\tremaining: 29.7s\n",
            "2184:\tlearn: 0.2959759\ttotal: 1m 19s\tremaining: 29.7s\n",
            "2185:\tlearn: 0.2959446\ttotal: 1m 19s\tremaining: 29.7s\n",
            "2186:\tlearn: 0.2959216\ttotal: 1m 19s\tremaining: 29.6s\n",
            "2187:\tlearn: 0.2958843\ttotal: 1m 19s\tremaining: 29.6s\n",
            "2188:\tlearn: 0.2958637\ttotal: 1m 19s\tremaining: 29.5s\n",
            "2189:\tlearn: 0.2958096\ttotal: 1m 19s\tremaining: 29.5s\n",
            "2190:\tlearn: 0.2957818\ttotal: 1m 19s\tremaining: 29.5s\n",
            "2191:\tlearn: 0.2957195\ttotal: 1m 19s\tremaining: 29.4s\n",
            "2192:\tlearn: 0.2956967\ttotal: 1m 19s\tremaining: 29.4s\n",
            "2193:\tlearn: 0.2956756\ttotal: 1m 19s\tremaining: 29.3s\n",
            "2194:\tlearn: 0.2956519\ttotal: 1m 19s\tremaining: 29.3s\n",
            "2195:\tlearn: 0.2956270\ttotal: 1m 19s\tremaining: 29.3s\n",
            "2196:\tlearn: 0.2955718\ttotal: 1m 19s\tremaining: 29.2s\n",
            "2197:\tlearn: 0.2955545\ttotal: 1m 20s\tremaining: 29.2s\n",
            "2198:\tlearn: 0.2955345\ttotal: 1m 20s\tremaining: 29.2s\n",
            "2199:\tlearn: 0.2954985\ttotal: 1m 20s\tremaining: 29.1s\n",
            "2200:\tlearn: 0.2954711\ttotal: 1m 20s\tremaining: 29.1s\n",
            "2201:\tlearn: 0.2954350\ttotal: 1m 20s\tremaining: 29s\n",
            "2202:\tlearn: 0.2954160\ttotal: 1m 20s\tremaining: 29s\n",
            "2203:\tlearn: 0.2954131\ttotal: 1m 20s\tremaining: 29s\n",
            "2204:\tlearn: 0.2953997\ttotal: 1m 20s\tremaining: 28.9s\n",
            "2205:\tlearn: 0.2953423\ttotal: 1m 20s\tremaining: 28.9s\n",
            "2206:\tlearn: 0.2953182\ttotal: 1m 20s\tremaining: 28.9s\n",
            "2207:\tlearn: 0.2952727\ttotal: 1m 20s\tremaining: 28.8s\n",
            "2208:\tlearn: 0.2952434\ttotal: 1m 20s\tremaining: 28.8s\n",
            "2209:\tlearn: 0.2952149\ttotal: 1m 20s\tremaining: 28.7s\n",
            "2210:\tlearn: 0.2952011\ttotal: 1m 20s\tremaining: 28.7s\n",
            "2211:\tlearn: 0.2951487\ttotal: 1m 20s\tremaining: 28.7s\n",
            "2212:\tlearn: 0.2951419\ttotal: 1m 20s\tremaining: 28.6s\n",
            "2213:\tlearn: 0.2951148\ttotal: 1m 20s\tremaining: 28.6s\n",
            "2214:\tlearn: 0.2950724\ttotal: 1m 20s\tremaining: 28.5s\n",
            "2215:\tlearn: 0.2950484\ttotal: 1m 20s\tremaining: 28.5s\n",
            "2216:\tlearn: 0.2950115\ttotal: 1m 20s\tremaining: 28.5s\n",
            "2217:\tlearn: 0.2949528\ttotal: 1m 20s\tremaining: 28.4s\n",
            "2218:\tlearn: 0.2949178\ttotal: 1m 20s\tremaining: 28.4s\n",
            "2219:\tlearn: 0.2948539\ttotal: 1m 20s\tremaining: 28.3s\n",
            "2220:\tlearn: 0.2948073\ttotal: 1m 20s\tremaining: 28.3s\n",
            "2221:\tlearn: 0.2947362\ttotal: 1m 20s\tremaining: 28.3s\n",
            "2222:\tlearn: 0.2946871\ttotal: 1m 20s\tremaining: 28.2s\n",
            "2223:\tlearn: 0.2946308\ttotal: 1m 20s\tremaining: 28.2s\n",
            "2224:\tlearn: 0.2946257\ttotal: 1m 20s\tremaining: 28.2s\n",
            "2225:\tlearn: 0.2945873\ttotal: 1m 20s\tremaining: 28.1s\n",
            "2226:\tlearn: 0.2945428\ttotal: 1m 20s\tremaining: 28.1s\n",
            "2227:\tlearn: 0.2945135\ttotal: 1m 20s\tremaining: 28.1s\n",
            "2228:\tlearn: 0.2944570\ttotal: 1m 20s\tremaining: 28s\n",
            "2229:\tlearn: 0.2944090\ttotal: 1m 21s\tremaining: 28s\n",
            "2230:\tlearn: 0.2943633\ttotal: 1m 21s\tremaining: 27.9s\n",
            "2231:\tlearn: 0.2943140\ttotal: 1m 21s\tremaining: 27.9s\n",
            "2232:\tlearn: 0.2942949\ttotal: 1m 21s\tremaining: 27.9s\n",
            "2233:\tlearn: 0.2942691\ttotal: 1m 21s\tremaining: 27.8s\n",
            "2234:\tlearn: 0.2942254\ttotal: 1m 21s\tremaining: 27.8s\n",
            "2235:\tlearn: 0.2941793\ttotal: 1m 21s\tremaining: 27.7s\n",
            "2236:\tlearn: 0.2941541\ttotal: 1m 21s\tremaining: 27.7s\n",
            "2237:\tlearn: 0.2941088\ttotal: 1m 21s\tremaining: 27.7s\n",
            "2238:\tlearn: 0.2940852\ttotal: 1m 21s\tremaining: 27.6s\n",
            "2239:\tlearn: 0.2940622\ttotal: 1m 21s\tremaining: 27.6s\n",
            "2240:\tlearn: 0.2940307\ttotal: 1m 21s\tremaining: 27.6s\n",
            "2241:\tlearn: 0.2940105\ttotal: 1m 21s\tremaining: 27.5s\n",
            "2242:\tlearn: 0.2939760\ttotal: 1m 21s\tremaining: 27.5s\n",
            "2243:\tlearn: 0.2939226\ttotal: 1m 21s\tremaining: 27.4s\n",
            "2244:\tlearn: 0.2938970\ttotal: 1m 21s\tremaining: 27.4s\n",
            "2245:\tlearn: 0.2938614\ttotal: 1m 21s\tremaining: 27.4s\n",
            "2246:\tlearn: 0.2938214\ttotal: 1m 21s\tremaining: 27.3s\n",
            "2247:\tlearn: 0.2937792\ttotal: 1m 21s\tremaining: 27.3s\n",
            "2248:\tlearn: 0.2937660\ttotal: 1m 21s\tremaining: 27.2s\n",
            "2249:\tlearn: 0.2937410\ttotal: 1m 21s\tremaining: 27.2s\n",
            "2250:\tlearn: 0.2936722\ttotal: 1m 21s\tremaining: 27.2s\n",
            "2251:\tlearn: 0.2936215\ttotal: 1m 21s\tremaining: 27.1s\n",
            "2252:\tlearn: 0.2935985\ttotal: 1m 21s\tremaining: 27.1s\n",
            "2253:\tlearn: 0.2935816\ttotal: 1m 21s\tremaining: 27.1s\n",
            "2254:\tlearn: 0.2935660\ttotal: 1m 21s\tremaining: 27s\n",
            "2255:\tlearn: 0.2935316\ttotal: 1m 21s\tremaining: 27s\n",
            "2256:\tlearn: 0.2934757\ttotal: 1m 21s\tremaining: 26.9s\n",
            "2257:\tlearn: 0.2934729\ttotal: 1m 21s\tremaining: 26.9s\n",
            "2258:\tlearn: 0.2934332\ttotal: 1m 21s\tremaining: 26.9s\n",
            "2259:\tlearn: 0.2933942\ttotal: 1m 21s\tremaining: 26.8s\n",
            "2260:\tlearn: 0.2933568\ttotal: 1m 21s\tremaining: 26.8s\n",
            "2261:\tlearn: 0.2933207\ttotal: 1m 22s\tremaining: 26.8s\n",
            "2262:\tlearn: 0.2932869\ttotal: 1m 22s\tremaining: 26.7s\n",
            "2263:\tlearn: 0.2932327\ttotal: 1m 22s\tremaining: 26.7s\n",
            "2264:\tlearn: 0.2932028\ttotal: 1m 22s\tremaining: 26.6s\n",
            "2265:\tlearn: 0.2931555\ttotal: 1m 22s\tremaining: 26.6s\n",
            "2266:\tlearn: 0.2931227\ttotal: 1m 22s\tremaining: 26.6s\n",
            "2267:\tlearn: 0.2930821\ttotal: 1m 22s\tremaining: 26.5s\n",
            "2268:\tlearn: 0.2930325\ttotal: 1m 22s\tremaining: 26.5s\n",
            "2269:\tlearn: 0.2930025\ttotal: 1m 22s\tremaining: 26.5s\n",
            "2270:\tlearn: 0.2929875\ttotal: 1m 22s\tremaining: 26.4s\n",
            "2271:\tlearn: 0.2929507\ttotal: 1m 22s\tremaining: 26.4s\n",
            "2272:\tlearn: 0.2929245\ttotal: 1m 22s\tremaining: 26.3s\n",
            "2273:\tlearn: 0.2928782\ttotal: 1m 22s\tremaining: 26.3s\n",
            "2274:\tlearn: 0.2928536\ttotal: 1m 22s\tremaining: 26.3s\n",
            "2275:\tlearn: 0.2928209\ttotal: 1m 22s\tremaining: 26.2s\n",
            "2276:\tlearn: 0.2927898\ttotal: 1m 22s\tremaining: 26.2s\n",
            "2277:\tlearn: 0.2927712\ttotal: 1m 22s\tremaining: 26.1s\n",
            "2278:\tlearn: 0.2927521\ttotal: 1m 22s\tremaining: 26.1s\n",
            "2279:\tlearn: 0.2926978\ttotal: 1m 22s\tremaining: 26.1s\n",
            "2280:\tlearn: 0.2926522\ttotal: 1m 22s\tremaining: 26s\n",
            "2281:\tlearn: 0.2926097\ttotal: 1m 22s\tremaining: 26s\n",
            "2282:\tlearn: 0.2925736\ttotal: 1m 22s\tremaining: 26s\n",
            "2283:\tlearn: 0.2925561\ttotal: 1m 22s\tremaining: 25.9s\n",
            "2284:\tlearn: 0.2925151\ttotal: 1m 22s\tremaining: 25.9s\n",
            "2285:\tlearn: 0.2924867\ttotal: 1m 22s\tremaining: 25.8s\n",
            "2286:\tlearn: 0.2924652\ttotal: 1m 22s\tremaining: 25.8s\n",
            "2287:\tlearn: 0.2923958\ttotal: 1m 22s\tremaining: 25.8s\n",
            "2288:\tlearn: 0.2923703\ttotal: 1m 22s\tremaining: 25.7s\n",
            "2289:\tlearn: 0.2923599\ttotal: 1m 22s\tremaining: 25.7s\n",
            "2290:\tlearn: 0.2923343\ttotal: 1m 22s\tremaining: 25.6s\n",
            "2291:\tlearn: 0.2922957\ttotal: 1m 22s\tremaining: 25.6s\n",
            "2292:\tlearn: 0.2922630\ttotal: 1m 22s\tremaining: 25.6s\n",
            "2293:\tlearn: 0.2922249\ttotal: 1m 22s\tremaining: 25.5s\n",
            "2294:\tlearn: 0.2921754\ttotal: 1m 23s\tremaining: 25.5s\n",
            "2295:\tlearn: 0.2921441\ttotal: 1m 23s\tremaining: 25.5s\n",
            "2296:\tlearn: 0.2920890\ttotal: 1m 23s\tremaining: 25.4s\n",
            "2297:\tlearn: 0.2920632\ttotal: 1m 23s\tremaining: 25.4s\n",
            "2298:\tlearn: 0.2920006\ttotal: 1m 23s\tremaining: 25.4s\n",
            "2299:\tlearn: 0.2919727\ttotal: 1m 23s\tremaining: 25.3s\n",
            "2300:\tlearn: 0.2919392\ttotal: 1m 23s\tremaining: 25.3s\n",
            "2301:\tlearn: 0.2919000\ttotal: 1m 23s\tremaining: 25.2s\n",
            "2302:\tlearn: 0.2918794\ttotal: 1m 23s\tremaining: 25.2s\n",
            "2303:\tlearn: 0.2918424\ttotal: 1m 23s\tremaining: 25.2s\n",
            "2304:\tlearn: 0.2917929\ttotal: 1m 23s\tremaining: 25.1s\n",
            "2305:\tlearn: 0.2917564\ttotal: 1m 23s\tremaining: 25.1s\n",
            "2306:\tlearn: 0.2917169\ttotal: 1m 23s\tremaining: 25s\n",
            "2307:\tlearn: 0.2916727\ttotal: 1m 23s\tremaining: 25s\n",
            "2308:\tlearn: 0.2916318\ttotal: 1m 23s\tremaining: 25s\n",
            "2309:\tlearn: 0.2916084\ttotal: 1m 23s\tremaining: 24.9s\n",
            "2310:\tlearn: 0.2915841\ttotal: 1m 23s\tremaining: 24.9s\n",
            "2311:\tlearn: 0.2915299\ttotal: 1m 23s\tremaining: 24.9s\n",
            "2312:\tlearn: 0.2914948\ttotal: 1m 23s\tremaining: 24.8s\n",
            "2313:\tlearn: 0.2914377\ttotal: 1m 23s\tremaining: 24.8s\n",
            "2314:\tlearn: 0.2914179\ttotal: 1m 23s\tremaining: 24.7s\n",
            "2315:\tlearn: 0.2913699\ttotal: 1m 23s\tremaining: 24.7s\n",
            "2316:\tlearn: 0.2913616\ttotal: 1m 23s\tremaining: 24.7s\n",
            "2317:\tlearn: 0.2913136\ttotal: 1m 23s\tremaining: 24.6s\n",
            "2318:\tlearn: 0.2912785\ttotal: 1m 23s\tremaining: 24.6s\n",
            "2319:\tlearn: 0.2912556\ttotal: 1m 23s\tremaining: 24.6s\n",
            "2320:\tlearn: 0.2912094\ttotal: 1m 23s\tremaining: 24.6s\n",
            "2321:\tlearn: 0.2911712\ttotal: 1m 23s\tremaining: 24.5s\n",
            "2322:\tlearn: 0.2911424\ttotal: 1m 24s\tremaining: 24.5s\n",
            "2323:\tlearn: 0.2911194\ttotal: 1m 24s\tremaining: 24.5s\n",
            "2324:\tlearn: 0.2910779\ttotal: 1m 24s\tremaining: 24.4s\n",
            "2325:\tlearn: 0.2910420\ttotal: 1m 24s\tremaining: 24.4s\n",
            "2326:\tlearn: 0.2910018\ttotal: 1m 24s\tremaining: 24.4s\n",
            "2327:\tlearn: 0.2909748\ttotal: 1m 24s\tremaining: 24.3s\n",
            "2328:\tlearn: 0.2909667\ttotal: 1m 24s\tremaining: 24.3s\n",
            "2329:\tlearn: 0.2909291\ttotal: 1m 24s\tremaining: 24.3s\n",
            "2330:\tlearn: 0.2909266\ttotal: 1m 24s\tremaining: 24.2s\n",
            "2331:\tlearn: 0.2909000\ttotal: 1m 24s\tremaining: 24.2s\n",
            "2332:\tlearn: 0.2908441\ttotal: 1m 24s\tremaining: 24.2s\n",
            "2333:\tlearn: 0.2908094\ttotal: 1m 24s\tremaining: 24.2s\n",
            "2334:\tlearn: 0.2907858\ttotal: 1m 24s\tremaining: 24.1s\n",
            "2335:\tlearn: 0.2907608\ttotal: 1m 24s\tremaining: 24.1s\n",
            "2336:\tlearn: 0.2907315\ttotal: 1m 24s\tremaining: 24.1s\n",
            "2337:\tlearn: 0.2906784\ttotal: 1m 24s\tremaining: 24s\n",
            "2338:\tlearn: 0.2906401\ttotal: 1m 24s\tremaining: 24s\n",
            "2339:\tlearn: 0.2906170\ttotal: 1m 24s\tremaining: 24s\n",
            "2340:\tlearn: 0.2905601\ttotal: 1m 25s\tremaining: 23.9s\n",
            "2341:\tlearn: 0.2905196\ttotal: 1m 25s\tremaining: 23.9s\n",
            "2342:\tlearn: 0.2904705\ttotal: 1m 25s\tremaining: 23.9s\n",
            "2343:\tlearn: 0.2904202\ttotal: 1m 25s\tremaining: 23.8s\n",
            "2344:\tlearn: 0.2904023\ttotal: 1m 25s\tremaining: 23.8s\n",
            "2345:\tlearn: 0.2903602\ttotal: 1m 25s\tremaining: 23.8s\n",
            "2346:\tlearn: 0.2903211\ttotal: 1m 25s\tremaining: 23.7s\n",
            "2347:\tlearn: 0.2902930\ttotal: 1m 25s\tremaining: 23.7s\n",
            "2348:\tlearn: 0.2902488\ttotal: 1m 25s\tremaining: 23.7s\n",
            "2349:\tlearn: 0.2902169\ttotal: 1m 25s\tremaining: 23.7s\n",
            "2350:\tlearn: 0.2901600\ttotal: 1m 25s\tremaining: 23.6s\n",
            "2351:\tlearn: 0.2901300\ttotal: 1m 25s\tremaining: 23.6s\n",
            "2352:\tlearn: 0.2900975\ttotal: 1m 25s\tremaining: 23.6s\n",
            "2353:\tlearn: 0.2900542\ttotal: 1m 25s\tremaining: 23.5s\n",
            "2354:\tlearn: 0.2900089\ttotal: 1m 25s\tremaining: 23.5s\n",
            "2355:\tlearn: 0.2899718\ttotal: 1m 25s\tremaining: 23.5s\n",
            "2356:\tlearn: 0.2899458\ttotal: 1m 25s\tremaining: 23.4s\n",
            "2357:\tlearn: 0.2899133\ttotal: 1m 25s\tremaining: 23.4s\n",
            "2358:\tlearn: 0.2898582\ttotal: 1m 25s\tremaining: 23.4s\n",
            "2359:\tlearn: 0.2898285\ttotal: 1m 26s\tremaining: 23.3s\n",
            "2360:\tlearn: 0.2898213\ttotal: 1m 26s\tremaining: 23.3s\n",
            "2361:\tlearn: 0.2897810\ttotal: 1m 26s\tremaining: 23.3s\n",
            "2362:\tlearn: 0.2897289\ttotal: 1m 26s\tremaining: 23.2s\n",
            "2363:\tlearn: 0.2897102\ttotal: 1m 26s\tremaining: 23.2s\n",
            "2364:\tlearn: 0.2896790\ttotal: 1m 26s\tremaining: 23.2s\n",
            "2365:\tlearn: 0.2896486\ttotal: 1m 26s\tremaining: 23.1s\n",
            "2366:\tlearn: 0.2895926\ttotal: 1m 26s\tremaining: 23.1s\n",
            "2367:\tlearn: 0.2895795\ttotal: 1m 26s\tremaining: 23.1s\n",
            "2368:\tlearn: 0.2895218\ttotal: 1m 26s\tremaining: 23s\n",
            "2369:\tlearn: 0.2894839\ttotal: 1m 26s\tremaining: 23s\n",
            "2370:\tlearn: 0.2894667\ttotal: 1m 26s\tremaining: 23s\n",
            "2371:\tlearn: 0.2894245\ttotal: 1m 26s\tremaining: 22.9s\n",
            "2372:\tlearn: 0.2893888\ttotal: 1m 26s\tremaining: 22.9s\n",
            "2373:\tlearn: 0.2893617\ttotal: 1m 26s\tremaining: 22.9s\n",
            "2374:\tlearn: 0.2893471\ttotal: 1m 26s\tremaining: 22.8s\n",
            "2375:\tlearn: 0.2893348\ttotal: 1m 26s\tremaining: 22.8s\n",
            "2376:\tlearn: 0.2892985\ttotal: 1m 26s\tremaining: 22.8s\n",
            "2377:\tlearn: 0.2892498\ttotal: 1m 26s\tremaining: 22.7s\n",
            "2378:\tlearn: 0.2892172\ttotal: 1m 27s\tremaining: 22.7s\n",
            "2379:\tlearn: 0.2891957\ttotal: 1m 27s\tremaining: 22.7s\n",
            "2380:\tlearn: 0.2891679\ttotal: 1m 27s\tremaining: 22.6s\n",
            "2381:\tlearn: 0.2891597\ttotal: 1m 27s\tremaining: 22.6s\n",
            "2382:\tlearn: 0.2891284\ttotal: 1m 27s\tremaining: 22.6s\n",
            "2383:\tlearn: 0.2890952\ttotal: 1m 27s\tremaining: 22.6s\n",
            "2384:\tlearn: 0.2890442\ttotal: 1m 27s\tremaining: 22.5s\n",
            "2385:\tlearn: 0.2890369\ttotal: 1m 27s\tremaining: 22.5s\n",
            "2386:\tlearn: 0.2889962\ttotal: 1m 27s\tremaining: 22.5s\n",
            "2387:\tlearn: 0.2889623\ttotal: 1m 27s\tremaining: 22.4s\n",
            "2388:\tlearn: 0.2889384\ttotal: 1m 27s\tremaining: 22.4s\n",
            "2389:\tlearn: 0.2889204\ttotal: 1m 27s\tremaining: 22.3s\n",
            "2390:\tlearn: 0.2889105\ttotal: 1m 27s\tremaining: 22.3s\n",
            "2391:\tlearn: 0.2888727\ttotal: 1m 27s\tremaining: 22.3s\n",
            "2392:\tlearn: 0.2888525\ttotal: 1m 27s\tremaining: 22.2s\n",
            "2393:\tlearn: 0.2888260\ttotal: 1m 27s\tremaining: 22.2s\n",
            "2394:\tlearn: 0.2888008\ttotal: 1m 27s\tremaining: 22.2s\n",
            "2395:\tlearn: 0.2887760\ttotal: 1m 27s\tremaining: 22.1s\n",
            "2396:\tlearn: 0.2887389\ttotal: 1m 27s\tremaining: 22.1s\n",
            "2397:\tlearn: 0.2887037\ttotal: 1m 27s\tremaining: 22s\n",
            "2398:\tlearn: 0.2886685\ttotal: 1m 27s\tremaining: 22s\n",
            "2399:\tlearn: 0.2886554\ttotal: 1m 27s\tremaining: 22s\n",
            "2400:\tlearn: 0.2886273\ttotal: 1m 27s\tremaining: 21.9s\n",
            "2401:\tlearn: 0.2886064\ttotal: 1m 27s\tremaining: 21.9s\n",
            "2402:\tlearn: 0.2885989\ttotal: 1m 27s\tremaining: 21.9s\n",
            "2403:\tlearn: 0.2885573\ttotal: 1m 27s\tremaining: 21.8s\n",
            "2404:\tlearn: 0.2885216\ttotal: 1m 28s\tremaining: 21.8s\n",
            "2405:\tlearn: 0.2884878\ttotal: 1m 28s\tremaining: 21.7s\n",
            "2406:\tlearn: 0.2884451\ttotal: 1m 28s\tremaining: 21.7s\n",
            "2407:\tlearn: 0.2884099\ttotal: 1m 28s\tremaining: 21.7s\n",
            "2408:\tlearn: 0.2883667\ttotal: 1m 28s\tremaining: 21.6s\n",
            "2409:\tlearn: 0.2883342\ttotal: 1m 28s\tremaining: 21.6s\n",
            "2410:\tlearn: 0.2883119\ttotal: 1m 28s\tremaining: 21.5s\n",
            "2411:\tlearn: 0.2882794\ttotal: 1m 28s\tremaining: 21.5s\n",
            "2412:\tlearn: 0.2882356\ttotal: 1m 28s\tremaining: 21.5s\n",
            "2413:\tlearn: 0.2882056\ttotal: 1m 28s\tremaining: 21.4s\n",
            "2414:\tlearn: 0.2881653\ttotal: 1m 28s\tremaining: 21.4s\n",
            "2415:\tlearn: 0.2881383\ttotal: 1m 28s\tremaining: 21.4s\n",
            "2416:\tlearn: 0.2881145\ttotal: 1m 28s\tremaining: 21.3s\n",
            "2417:\tlearn: 0.2880773\ttotal: 1m 28s\tremaining: 21.3s\n",
            "2418:\tlearn: 0.2880375\ttotal: 1m 28s\tremaining: 21.2s\n",
            "2419:\tlearn: 0.2879934\ttotal: 1m 28s\tremaining: 21.2s\n",
            "2420:\tlearn: 0.2879606\ttotal: 1m 28s\tremaining: 21.2s\n",
            "2421:\tlearn: 0.2879046\ttotal: 1m 28s\tremaining: 21.1s\n",
            "2422:\tlearn: 0.2878949\ttotal: 1m 28s\tremaining: 21.1s\n",
            "2423:\tlearn: 0.2878616\ttotal: 1m 28s\tremaining: 21.1s\n",
            "2424:\tlearn: 0.2878215\ttotal: 1m 28s\tremaining: 21s\n",
            "2425:\tlearn: 0.2878086\ttotal: 1m 28s\tremaining: 21s\n",
            "2426:\tlearn: 0.2877816\ttotal: 1m 28s\tremaining: 20.9s\n",
            "2427:\tlearn: 0.2877570\ttotal: 1m 28s\tremaining: 20.9s\n",
            "2428:\tlearn: 0.2877142\ttotal: 1m 28s\tremaining: 20.9s\n",
            "2429:\tlearn: 0.2877013\ttotal: 1m 28s\tremaining: 20.8s\n",
            "2430:\tlearn: 0.2876705\ttotal: 1m 28s\tremaining: 20.8s\n",
            "2431:\tlearn: 0.2876411\ttotal: 1m 28s\tremaining: 20.7s\n",
            "2432:\tlearn: 0.2876333\ttotal: 1m 28s\tremaining: 20.7s\n",
            "2433:\tlearn: 0.2875819\ttotal: 1m 28s\tremaining: 20.7s\n",
            "2434:\tlearn: 0.2875612\ttotal: 1m 28s\tremaining: 20.6s\n",
            "2435:\tlearn: 0.2875310\ttotal: 1m 28s\tremaining: 20.6s\n",
            "2436:\tlearn: 0.2874999\ttotal: 1m 28s\tremaining: 20.6s\n",
            "2437:\tlearn: 0.2874878\ttotal: 1m 29s\tremaining: 20.5s\n",
            "2438:\tlearn: 0.2874578\ttotal: 1m 29s\tremaining: 20.5s\n",
            "2439:\tlearn: 0.2874510\ttotal: 1m 29s\tremaining: 20.4s\n",
            "2440:\tlearn: 0.2874344\ttotal: 1m 29s\tremaining: 20.4s\n",
            "2441:\tlearn: 0.2874112\ttotal: 1m 29s\tremaining: 20.4s\n",
            "2442:\tlearn: 0.2873874\ttotal: 1m 29s\tremaining: 20.3s\n",
            "2443:\tlearn: 0.2873488\ttotal: 1m 29s\tremaining: 20.3s\n",
            "2444:\tlearn: 0.2873076\ttotal: 1m 29s\tremaining: 20.3s\n",
            "2445:\tlearn: 0.2872713\ttotal: 1m 29s\tremaining: 20.2s\n",
            "2446:\tlearn: 0.2872345\ttotal: 1m 29s\tremaining: 20.2s\n",
            "2447:\tlearn: 0.2872058\ttotal: 1m 29s\tremaining: 20.1s\n",
            "2448:\tlearn: 0.2871619\ttotal: 1m 29s\tremaining: 20.1s\n",
            "2449:\tlearn: 0.2871163\ttotal: 1m 29s\tremaining: 20.1s\n",
            "2450:\tlearn: 0.2870983\ttotal: 1m 29s\tremaining: 20s\n",
            "2451:\tlearn: 0.2870736\ttotal: 1m 29s\tremaining: 20s\n",
            "2452:\tlearn: 0.2870413\ttotal: 1m 29s\tremaining: 19.9s\n",
            "2453:\tlearn: 0.2869997\ttotal: 1m 29s\tremaining: 19.9s\n",
            "2454:\tlearn: 0.2869802\ttotal: 1m 29s\tremaining: 19.9s\n",
            "2455:\tlearn: 0.2869432\ttotal: 1m 29s\tremaining: 19.8s\n",
            "2456:\tlearn: 0.2869083\ttotal: 1m 29s\tremaining: 19.8s\n",
            "2457:\tlearn: 0.2868887\ttotal: 1m 29s\tremaining: 19.8s\n",
            "2458:\tlearn: 0.2868764\ttotal: 1m 29s\tremaining: 19.7s\n",
            "2459:\tlearn: 0.2868649\ttotal: 1m 29s\tremaining: 19.7s\n",
            "2460:\tlearn: 0.2868316\ttotal: 1m 29s\tremaining: 19.6s\n",
            "2461:\tlearn: 0.2867989\ttotal: 1m 29s\tremaining: 19.6s\n",
            "2462:\tlearn: 0.2867555\ttotal: 1m 29s\tremaining: 19.6s\n",
            "2463:\tlearn: 0.2867301\ttotal: 1m 29s\tremaining: 19.5s\n",
            "2464:\tlearn: 0.2866867\ttotal: 1m 29s\tremaining: 19.5s\n",
            "2465:\tlearn: 0.2866592\ttotal: 1m 29s\tremaining: 19.5s\n",
            "2466:\tlearn: 0.2866202\ttotal: 1m 29s\tremaining: 19.4s\n",
            "2467:\tlearn: 0.2865901\ttotal: 1m 29s\tremaining: 19.4s\n",
            "2468:\tlearn: 0.2865469\ttotal: 1m 29s\tremaining: 19.3s\n",
            "2469:\tlearn: 0.2865250\ttotal: 1m 29s\tremaining: 19.3s\n",
            "2470:\tlearn: 0.2864714\ttotal: 1m 30s\tremaining: 19.3s\n",
            "2471:\tlearn: 0.2864302\ttotal: 1m 30s\tremaining: 19.2s\n",
            "2472:\tlearn: 0.2864274\ttotal: 1m 30s\tremaining: 19.2s\n",
            "2473:\tlearn: 0.2864015\ttotal: 1m 30s\tremaining: 19.2s\n",
            "2474:\tlearn: 0.2863923\ttotal: 1m 30s\tremaining: 19.1s\n",
            "2475:\tlearn: 0.2863575\ttotal: 1m 30s\tremaining: 19.1s\n",
            "2476:\tlearn: 0.2863386\ttotal: 1m 30s\tremaining: 19s\n",
            "2477:\tlearn: 0.2863073\ttotal: 1m 30s\tremaining: 19s\n",
            "2478:\tlearn: 0.2862758\ttotal: 1m 30s\tremaining: 19s\n",
            "2479:\tlearn: 0.2862329\ttotal: 1m 30s\tremaining: 18.9s\n",
            "2480:\tlearn: 0.2862025\ttotal: 1m 30s\tremaining: 18.9s\n",
            "2481:\tlearn: 0.2861540\ttotal: 1m 30s\tremaining: 18.9s\n",
            "2482:\tlearn: 0.2861154\ttotal: 1m 30s\tremaining: 18.8s\n",
            "2483:\tlearn: 0.2861104\ttotal: 1m 30s\tremaining: 18.8s\n",
            "2484:\tlearn: 0.2860959\ttotal: 1m 30s\tremaining: 18.7s\n",
            "2485:\tlearn: 0.2860643\ttotal: 1m 30s\tremaining: 18.7s\n",
            "2486:\tlearn: 0.2860481\ttotal: 1m 30s\tremaining: 18.7s\n",
            "2487:\tlearn: 0.2860005\ttotal: 1m 30s\tremaining: 18.6s\n",
            "2488:\tlearn: 0.2859686\ttotal: 1m 30s\tremaining: 18.6s\n",
            "2489:\tlearn: 0.2859485\ttotal: 1m 30s\tremaining: 18.6s\n",
            "2490:\tlearn: 0.2859119\ttotal: 1m 30s\tremaining: 18.5s\n",
            "2491:\tlearn: 0.2858359\ttotal: 1m 30s\tremaining: 18.5s\n",
            "2492:\tlearn: 0.2858199\ttotal: 1m 30s\tremaining: 18.4s\n",
            "2493:\tlearn: 0.2857826\ttotal: 1m 30s\tremaining: 18.4s\n",
            "2494:\tlearn: 0.2857375\ttotal: 1m 30s\tremaining: 18.4s\n",
            "2495:\tlearn: 0.2857177\ttotal: 1m 30s\tremaining: 18.3s\n",
            "2496:\tlearn: 0.2856845\ttotal: 1m 30s\tremaining: 18.3s\n",
            "2497:\tlearn: 0.2856509\ttotal: 1m 30s\tremaining: 18.3s\n",
            "2498:\tlearn: 0.2856232\ttotal: 1m 30s\tremaining: 18.2s\n",
            "2499:\tlearn: 0.2856103\ttotal: 1m 30s\tremaining: 18.2s\n",
            "2500:\tlearn: 0.2855912\ttotal: 1m 30s\tremaining: 18.1s\n",
            "2501:\tlearn: 0.2855601\ttotal: 1m 30s\tremaining: 18.1s\n",
            "2502:\tlearn: 0.2855329\ttotal: 1m 30s\tremaining: 18.1s\n",
            "2503:\tlearn: 0.2854738\ttotal: 1m 31s\tremaining: 18s\n",
            "2504:\tlearn: 0.2854237\ttotal: 1m 31s\tremaining: 18s\n",
            "2505:\tlearn: 0.2854102\ttotal: 1m 31s\tremaining: 18s\n",
            "2506:\tlearn: 0.2853805\ttotal: 1m 31s\tremaining: 17.9s\n",
            "2507:\tlearn: 0.2853467\ttotal: 1m 31s\tremaining: 17.9s\n",
            "2508:\tlearn: 0.2852948\ttotal: 1m 31s\tremaining: 17.8s\n",
            "2509:\tlearn: 0.2852819\ttotal: 1m 31s\tremaining: 17.8s\n",
            "2510:\tlearn: 0.2852298\ttotal: 1m 31s\tremaining: 17.8s\n",
            "2511:\tlearn: 0.2852221\ttotal: 1m 31s\tremaining: 17.7s\n",
            "2512:\tlearn: 0.2851900\ttotal: 1m 31s\tremaining: 17.7s\n",
            "2513:\tlearn: 0.2851641\ttotal: 1m 31s\tremaining: 17.7s\n",
            "2514:\tlearn: 0.2851275\ttotal: 1m 31s\tremaining: 17.6s\n",
            "2515:\tlearn: 0.2850912\ttotal: 1m 31s\tremaining: 17.6s\n",
            "2516:\tlearn: 0.2850624\ttotal: 1m 31s\tremaining: 17.5s\n",
            "2517:\tlearn: 0.2850300\ttotal: 1m 31s\tremaining: 17.5s\n",
            "2518:\tlearn: 0.2849992\ttotal: 1m 31s\tremaining: 17.5s\n",
            "2519:\tlearn: 0.2849534\ttotal: 1m 31s\tremaining: 17.4s\n",
            "2520:\tlearn: 0.2849090\ttotal: 1m 31s\tremaining: 17.4s\n",
            "2521:\tlearn: 0.2848878\ttotal: 1m 31s\tremaining: 17.4s\n",
            "2522:\tlearn: 0.2848457\ttotal: 1m 31s\tremaining: 17.3s\n",
            "2523:\tlearn: 0.2848128\ttotal: 1m 31s\tremaining: 17.3s\n",
            "2524:\tlearn: 0.2847773\ttotal: 1m 31s\tremaining: 17.2s\n",
            "2525:\tlearn: 0.2847386\ttotal: 1m 31s\tremaining: 17.2s\n",
            "2526:\tlearn: 0.2846921\ttotal: 1m 31s\tremaining: 17.2s\n",
            "2527:\tlearn: 0.2846674\ttotal: 1m 31s\tremaining: 17.1s\n",
            "2528:\tlearn: 0.2846424\ttotal: 1m 31s\tremaining: 17.1s\n",
            "2529:\tlearn: 0.2846167\ttotal: 1m 31s\tremaining: 17.1s\n",
            "2530:\tlearn: 0.2845866\ttotal: 1m 31s\tremaining: 17s\n",
            "2531:\tlearn: 0.2845579\ttotal: 1m 31s\tremaining: 17s\n",
            "2532:\tlearn: 0.2845341\ttotal: 1m 31s\tremaining: 16.9s\n",
            "2533:\tlearn: 0.2845077\ttotal: 1m 31s\tremaining: 16.9s\n",
            "2534:\tlearn: 0.2844738\ttotal: 1m 31s\tremaining: 16.9s\n",
            "2535:\tlearn: 0.2844323\ttotal: 1m 32s\tremaining: 16.8s\n",
            "2536:\tlearn: 0.2844226\ttotal: 1m 32s\tremaining: 16.8s\n",
            "2537:\tlearn: 0.2844101\ttotal: 1m 32s\tremaining: 16.8s\n",
            "2538:\tlearn: 0.2843935\ttotal: 1m 32s\tremaining: 16.7s\n",
            "2539:\tlearn: 0.2843481\ttotal: 1m 32s\tremaining: 16.7s\n",
            "2540:\tlearn: 0.2843365\ttotal: 1m 32s\tremaining: 16.6s\n",
            "2541:\tlearn: 0.2842785\ttotal: 1m 32s\tremaining: 16.6s\n",
            "2542:\tlearn: 0.2842339\ttotal: 1m 32s\tremaining: 16.6s\n",
            "2543:\tlearn: 0.2841821\ttotal: 1m 32s\tremaining: 16.5s\n",
            "2544:\tlearn: 0.2841519\ttotal: 1m 32s\tremaining: 16.5s\n",
            "2545:\tlearn: 0.2841419\ttotal: 1m 32s\tremaining: 16.5s\n",
            "2546:\tlearn: 0.2841184\ttotal: 1m 32s\tremaining: 16.4s\n",
            "2547:\tlearn: 0.2840837\ttotal: 1m 32s\tremaining: 16.4s\n",
            "2548:\tlearn: 0.2840756\ttotal: 1m 32s\tremaining: 16.3s\n",
            "2549:\tlearn: 0.2840555\ttotal: 1m 32s\tremaining: 16.3s\n",
            "2550:\tlearn: 0.2840338\ttotal: 1m 32s\tremaining: 16.3s\n",
            "2551:\tlearn: 0.2840068\ttotal: 1m 32s\tremaining: 16.2s\n",
            "2552:\tlearn: 0.2839623\ttotal: 1m 32s\tremaining: 16.2s\n",
            "2553:\tlearn: 0.2839474\ttotal: 1m 32s\tremaining: 16.2s\n",
            "2554:\tlearn: 0.2839197\ttotal: 1m 32s\tremaining: 16.1s\n",
            "2555:\tlearn: 0.2838983\ttotal: 1m 32s\tremaining: 16.1s\n",
            "2556:\tlearn: 0.2838939\ttotal: 1m 32s\tremaining: 16.1s\n",
            "2557:\tlearn: 0.2838713\ttotal: 1m 32s\tremaining: 16s\n",
            "2558:\tlearn: 0.2838437\ttotal: 1m 32s\tremaining: 16s\n",
            "2559:\tlearn: 0.2838226\ttotal: 1m 32s\tremaining: 15.9s\n",
            "2560:\tlearn: 0.2837969\ttotal: 1m 32s\tremaining: 15.9s\n",
            "2561:\tlearn: 0.2837591\ttotal: 1m 32s\tremaining: 15.9s\n",
            "2562:\tlearn: 0.2837207\ttotal: 1m 32s\tremaining: 15.8s\n",
            "2563:\tlearn: 0.2837060\ttotal: 1m 32s\tremaining: 15.8s\n",
            "2564:\tlearn: 0.2836906\ttotal: 1m 32s\tremaining: 15.8s\n",
            "2565:\tlearn: 0.2836592\ttotal: 1m 32s\tremaining: 15.7s\n",
            "2566:\tlearn: 0.2836292\ttotal: 1m 32s\tremaining: 15.7s\n",
            "2567:\tlearn: 0.2835694\ttotal: 1m 32s\tremaining: 15.6s\n",
            "2568:\tlearn: 0.2835362\ttotal: 1m 33s\tremaining: 15.6s\n",
            "2569:\tlearn: 0.2835301\ttotal: 1m 33s\tremaining: 15.6s\n",
            "2570:\tlearn: 0.2834854\ttotal: 1m 33s\tremaining: 15.5s\n",
            "2571:\tlearn: 0.2834550\ttotal: 1m 33s\tremaining: 15.5s\n",
            "2572:\tlearn: 0.2834276\ttotal: 1m 33s\tremaining: 15.5s\n",
            "2573:\tlearn: 0.2833992\ttotal: 1m 33s\tremaining: 15.4s\n",
            "2574:\tlearn: 0.2833815\ttotal: 1m 33s\tremaining: 15.4s\n",
            "2575:\tlearn: 0.2833588\ttotal: 1m 33s\tremaining: 15.3s\n",
            "2576:\tlearn: 0.2833560\ttotal: 1m 33s\tremaining: 15.3s\n",
            "2577:\tlearn: 0.2833550\ttotal: 1m 33s\tremaining: 15.3s\n",
            "2578:\tlearn: 0.2833243\ttotal: 1m 33s\tremaining: 15.2s\n",
            "2579:\tlearn: 0.2833003\ttotal: 1m 33s\tremaining: 15.2s\n",
            "2580:\tlearn: 0.2832718\ttotal: 1m 33s\tremaining: 15.2s\n",
            "2581:\tlearn: 0.2832128\ttotal: 1m 33s\tremaining: 15.1s\n",
            "2582:\tlearn: 0.2831865\ttotal: 1m 33s\tremaining: 15.1s\n",
            "2583:\tlearn: 0.2831616\ttotal: 1m 33s\tremaining: 15s\n",
            "2584:\tlearn: 0.2831381\ttotal: 1m 33s\tremaining: 15s\n",
            "2585:\tlearn: 0.2831079\ttotal: 1m 33s\tremaining: 15s\n",
            "2586:\tlearn: 0.2830754\ttotal: 1m 33s\tremaining: 14.9s\n",
            "2587:\tlearn: 0.2830619\ttotal: 1m 33s\tremaining: 14.9s\n",
            "2588:\tlearn: 0.2830306\ttotal: 1m 33s\tremaining: 14.9s\n",
            "2589:\tlearn: 0.2830043\ttotal: 1m 33s\tremaining: 14.8s\n",
            "2590:\tlearn: 0.2829872\ttotal: 1m 33s\tremaining: 14.8s\n",
            "2591:\tlearn: 0.2829700\ttotal: 1m 33s\tremaining: 14.7s\n",
            "2592:\tlearn: 0.2829312\ttotal: 1m 33s\tremaining: 14.7s\n",
            "2593:\tlearn: 0.2828956\ttotal: 1m 33s\tremaining: 14.7s\n",
            "2594:\tlearn: 0.2828860\ttotal: 1m 33s\tremaining: 14.6s\n",
            "2595:\tlearn: 0.2828412\ttotal: 1m 33s\tremaining: 14.6s\n",
            "2596:\tlearn: 0.2827952\ttotal: 1m 33s\tremaining: 14.6s\n",
            "2597:\tlearn: 0.2827775\ttotal: 1m 33s\tremaining: 14.5s\n",
            "2598:\tlearn: 0.2827477\ttotal: 1m 33s\tremaining: 14.5s\n",
            "2599:\tlearn: 0.2827200\ttotal: 1m 33s\tremaining: 14.4s\n",
            "2600:\tlearn: 0.2826719\ttotal: 1m 33s\tremaining: 14.4s\n",
            "2601:\tlearn: 0.2826464\ttotal: 1m 33s\tremaining: 14.4s\n",
            "2602:\tlearn: 0.2826132\ttotal: 1m 34s\tremaining: 14.3s\n",
            "2603:\tlearn: 0.2825653\ttotal: 1m 34s\tremaining: 14.3s\n",
            "2604:\tlearn: 0.2825385\ttotal: 1m 34s\tremaining: 14.3s\n",
            "2605:\tlearn: 0.2825066\ttotal: 1m 34s\tremaining: 14.2s\n",
            "2606:\tlearn: 0.2824849\ttotal: 1m 34s\tremaining: 14.2s\n",
            "2607:\tlearn: 0.2824371\ttotal: 1m 34s\tremaining: 14.2s\n",
            "2608:\tlearn: 0.2824126\ttotal: 1m 34s\tremaining: 14.1s\n",
            "2609:\tlearn: 0.2823893\ttotal: 1m 34s\tremaining: 14.1s\n",
            "2610:\tlearn: 0.2823674\ttotal: 1m 34s\tremaining: 14s\n",
            "2611:\tlearn: 0.2823232\ttotal: 1m 34s\tremaining: 14s\n",
            "2612:\tlearn: 0.2822909\ttotal: 1m 34s\tremaining: 14s\n",
            "2613:\tlearn: 0.2822501\ttotal: 1m 34s\tremaining: 13.9s\n",
            "2614:\tlearn: 0.2822169\ttotal: 1m 34s\tremaining: 13.9s\n",
            "2615:\tlearn: 0.2821918\ttotal: 1m 34s\tremaining: 13.9s\n",
            "2616:\tlearn: 0.2821519\ttotal: 1m 34s\tremaining: 13.8s\n",
            "2617:\tlearn: 0.2821094\ttotal: 1m 34s\tremaining: 13.8s\n",
            "2618:\tlearn: 0.2820797\ttotal: 1m 34s\tremaining: 13.7s\n",
            "2619:\tlearn: 0.2820492\ttotal: 1m 34s\tremaining: 13.7s\n",
            "2620:\tlearn: 0.2820060\ttotal: 1m 34s\tremaining: 13.7s\n",
            "2621:\tlearn: 0.2819647\ttotal: 1m 34s\tremaining: 13.6s\n",
            "2622:\tlearn: 0.2819277\ttotal: 1m 34s\tremaining: 13.6s\n",
            "2623:\tlearn: 0.2819060\ttotal: 1m 34s\tremaining: 13.6s\n",
            "2624:\tlearn: 0.2818916\ttotal: 1m 34s\tremaining: 13.5s\n",
            "2625:\tlearn: 0.2818646\ttotal: 1m 34s\tremaining: 13.5s\n",
            "2626:\tlearn: 0.2818463\ttotal: 1m 34s\tremaining: 13.5s\n",
            "2627:\tlearn: 0.2818261\ttotal: 1m 34s\tremaining: 13.4s\n",
            "2628:\tlearn: 0.2817937\ttotal: 1m 34s\tremaining: 13.4s\n",
            "2629:\tlearn: 0.2817702\ttotal: 1m 34s\tremaining: 13.3s\n",
            "2630:\tlearn: 0.2817381\ttotal: 1m 34s\tremaining: 13.3s\n",
            "2631:\tlearn: 0.2817084\ttotal: 1m 34s\tremaining: 13.3s\n",
            "2632:\tlearn: 0.2816678\ttotal: 1m 34s\tremaining: 13.2s\n",
            "2633:\tlearn: 0.2816479\ttotal: 1m 34s\tremaining: 13.2s\n",
            "2634:\tlearn: 0.2815942\ttotal: 1m 34s\tremaining: 13.2s\n",
            "2635:\tlearn: 0.2815831\ttotal: 1m 35s\tremaining: 13.1s\n",
            "2636:\tlearn: 0.2815407\ttotal: 1m 35s\tremaining: 13.1s\n",
            "2637:\tlearn: 0.2815149\ttotal: 1m 35s\tremaining: 13s\n",
            "2638:\tlearn: 0.2814885\ttotal: 1m 35s\tremaining: 13s\n",
            "2639:\tlearn: 0.2814693\ttotal: 1m 35s\tremaining: 13s\n",
            "2640:\tlearn: 0.2814378\ttotal: 1m 35s\tremaining: 12.9s\n",
            "2641:\tlearn: 0.2814167\ttotal: 1m 35s\tremaining: 12.9s\n",
            "2642:\tlearn: 0.2813903\ttotal: 1m 35s\tremaining: 12.9s\n",
            "2643:\tlearn: 0.2813573\ttotal: 1m 35s\tremaining: 12.8s\n",
            "2644:\tlearn: 0.2813285\ttotal: 1m 35s\tremaining: 12.8s\n",
            "2645:\tlearn: 0.2812879\ttotal: 1m 35s\tremaining: 12.8s\n",
            "2646:\tlearn: 0.2812609\ttotal: 1m 35s\tremaining: 12.7s\n",
            "2647:\tlearn: 0.2812163\ttotal: 1m 35s\tremaining: 12.7s\n",
            "2648:\tlearn: 0.2811786\ttotal: 1m 35s\tremaining: 12.6s\n",
            "2649:\tlearn: 0.2811458\ttotal: 1m 35s\tremaining: 12.6s\n",
            "2650:\tlearn: 0.2811178\ttotal: 1m 35s\tremaining: 12.6s\n",
            "2651:\tlearn: 0.2810938\ttotal: 1m 35s\tremaining: 12.5s\n",
            "2652:\tlearn: 0.2810386\ttotal: 1m 35s\tremaining: 12.5s\n",
            "2653:\tlearn: 0.2810113\ttotal: 1m 35s\tremaining: 12.5s\n",
            "2654:\tlearn: 0.2809715\ttotal: 1m 35s\tremaining: 12.4s\n",
            "2655:\tlearn: 0.2809250\ttotal: 1m 35s\tremaining: 12.4s\n",
            "2656:\tlearn: 0.2808917\ttotal: 1m 35s\tremaining: 12.4s\n",
            "2657:\tlearn: 0.2808585\ttotal: 1m 35s\tremaining: 12.3s\n",
            "2658:\tlearn: 0.2808289\ttotal: 1m 35s\tremaining: 12.3s\n",
            "2659:\tlearn: 0.2807995\ttotal: 1m 35s\tremaining: 12.2s\n",
            "2660:\tlearn: 0.2807635\ttotal: 1m 35s\tremaining: 12.2s\n",
            "2661:\tlearn: 0.2807361\ttotal: 1m 35s\tremaining: 12.2s\n",
            "2662:\tlearn: 0.2806915\ttotal: 1m 35s\tremaining: 12.1s\n",
            "2663:\tlearn: 0.2806489\ttotal: 1m 35s\tremaining: 12.1s\n",
            "2664:\tlearn: 0.2806145\ttotal: 1m 35s\tremaining: 12.1s\n",
            "2665:\tlearn: 0.2806002\ttotal: 1m 35s\tremaining: 12s\n",
            "2666:\tlearn: 0.2805941\ttotal: 1m 35s\tremaining: 12s\n",
            "2667:\tlearn: 0.2805568\ttotal: 1m 36s\tremaining: 11.9s\n",
            "2668:\tlearn: 0.2805139\ttotal: 1m 36s\tremaining: 11.9s\n",
            "2669:\tlearn: 0.2804749\ttotal: 1m 36s\tremaining: 11.9s\n",
            "2670:\tlearn: 0.2804727\ttotal: 1m 36s\tremaining: 11.8s\n",
            "2671:\tlearn: 0.2804205\ttotal: 1m 36s\tremaining: 11.8s\n",
            "2672:\tlearn: 0.2804190\ttotal: 1m 36s\tremaining: 11.8s\n",
            "2673:\tlearn: 0.2803831\ttotal: 1m 36s\tremaining: 11.7s\n",
            "2674:\tlearn: 0.2803472\ttotal: 1m 36s\tremaining: 11.7s\n",
            "2675:\tlearn: 0.2802852\ttotal: 1m 36s\tremaining: 11.7s\n",
            "2676:\tlearn: 0.2802477\ttotal: 1m 36s\tremaining: 11.6s\n",
            "2677:\tlearn: 0.2802198\ttotal: 1m 36s\tremaining: 11.6s\n",
            "2678:\tlearn: 0.2802021\ttotal: 1m 36s\tremaining: 11.5s\n",
            "2679:\tlearn: 0.2801453\ttotal: 1m 36s\tremaining: 11.5s\n",
            "2680:\tlearn: 0.2801078\ttotal: 1m 36s\tremaining: 11.5s\n",
            "2681:\tlearn: 0.2800806\ttotal: 1m 36s\tremaining: 11.4s\n",
            "2682:\tlearn: 0.2800451\ttotal: 1m 36s\tremaining: 11.4s\n",
            "2683:\tlearn: 0.2800332\ttotal: 1m 36s\tremaining: 11.4s\n",
            "2684:\tlearn: 0.2799910\ttotal: 1m 36s\tremaining: 11.3s\n",
            "2685:\tlearn: 0.2799595\ttotal: 1m 36s\tremaining: 11.3s\n",
            "2686:\tlearn: 0.2799002\ttotal: 1m 36s\tremaining: 11.3s\n",
            "2687:\tlearn: 0.2798687\ttotal: 1m 36s\tremaining: 11.2s\n",
            "2688:\tlearn: 0.2798406\ttotal: 1m 36s\tremaining: 11.2s\n",
            "2689:\tlearn: 0.2798082\ttotal: 1m 36s\tremaining: 11.1s\n",
            "2690:\tlearn: 0.2797855\ttotal: 1m 36s\tremaining: 11.1s\n",
            "2691:\tlearn: 0.2797504\ttotal: 1m 36s\tremaining: 11.1s\n",
            "2692:\tlearn: 0.2797148\ttotal: 1m 36s\tremaining: 11s\n",
            "2693:\tlearn: 0.2796904\ttotal: 1m 36s\tremaining: 11s\n",
            "2694:\tlearn: 0.2796433\ttotal: 1m 36s\tremaining: 11s\n",
            "2695:\tlearn: 0.2796124\ttotal: 1m 36s\tremaining: 10.9s\n",
            "2696:\tlearn: 0.2795911\ttotal: 1m 36s\tremaining: 10.9s\n",
            "2697:\tlearn: 0.2795554\ttotal: 1m 36s\tremaining: 10.9s\n",
            "2698:\tlearn: 0.2795191\ttotal: 1m 36s\tremaining: 10.8s\n",
            "2699:\tlearn: 0.2794856\ttotal: 1m 36s\tremaining: 10.8s\n",
            "2700:\tlearn: 0.2794556\ttotal: 1m 37s\tremaining: 10.7s\n",
            "2701:\tlearn: 0.2794304\ttotal: 1m 37s\tremaining: 10.7s\n",
            "2702:\tlearn: 0.2793891\ttotal: 1m 37s\tremaining: 10.7s\n",
            "2703:\tlearn: 0.2793571\ttotal: 1m 37s\tremaining: 10.6s\n",
            "2704:\tlearn: 0.2793202\ttotal: 1m 37s\tremaining: 10.6s\n",
            "2705:\tlearn: 0.2792967\ttotal: 1m 37s\tremaining: 10.6s\n",
            "2706:\tlearn: 0.2792807\ttotal: 1m 37s\tremaining: 10.5s\n",
            "2707:\tlearn: 0.2792506\ttotal: 1m 37s\tremaining: 10.5s\n",
            "2708:\tlearn: 0.2792197\ttotal: 1m 37s\tremaining: 10.4s\n",
            "2709:\tlearn: 0.2791763\ttotal: 1m 37s\tremaining: 10.4s\n",
            "2710:\tlearn: 0.2791370\ttotal: 1m 37s\tremaining: 10.4s\n",
            "2711:\tlearn: 0.2790941\ttotal: 1m 37s\tremaining: 10.3s\n",
            "2712:\tlearn: 0.2790654\ttotal: 1m 37s\tremaining: 10.3s\n",
            "2713:\tlearn: 0.2790327\ttotal: 1m 37s\tremaining: 10.3s\n",
            "2714:\tlearn: 0.2790052\ttotal: 1m 37s\tremaining: 10.2s\n",
            "2715:\tlearn: 0.2789648\ttotal: 1m 37s\tremaining: 10.2s\n",
            "2716:\tlearn: 0.2789312\ttotal: 1m 37s\tremaining: 10.2s\n",
            "2717:\tlearn: 0.2788985\ttotal: 1m 37s\tremaining: 10.1s\n",
            "2718:\tlearn: 0.2788847\ttotal: 1m 37s\tremaining: 10.1s\n",
            "2719:\tlearn: 0.2788733\ttotal: 1m 37s\tremaining: 10.1s\n",
            "2720:\tlearn: 0.2788532\ttotal: 1m 37s\tremaining: 10s\n",
            "2721:\tlearn: 0.2788254\ttotal: 1m 37s\tremaining: 10s\n",
            "2722:\tlearn: 0.2788011\ttotal: 1m 37s\tremaining: 9.97s\n",
            "2723:\tlearn: 0.2787558\ttotal: 1m 38s\tremaining: 9.93s\n",
            "2724:\tlearn: 0.2787299\ttotal: 1m 38s\tremaining: 9.9s\n",
            "2725:\tlearn: 0.2787143\ttotal: 1m 38s\tremaining: 9.87s\n",
            "2726:\tlearn: 0.2786833\ttotal: 1m 38s\tremaining: 9.83s\n",
            "2727:\tlearn: 0.2786560\ttotal: 1m 38s\tremaining: 9.8s\n",
            "2728:\tlearn: 0.2786087\ttotal: 1m 38s\tremaining: 9.76s\n",
            "2729:\tlearn: 0.2785614\ttotal: 1m 38s\tremaining: 9.73s\n",
            "2730:\tlearn: 0.2785524\ttotal: 1m 38s\tremaining: 9.7s\n",
            "2731:\tlearn: 0.2785328\ttotal: 1m 38s\tremaining: 9.66s\n",
            "2732:\tlearn: 0.2785136\ttotal: 1m 38s\tremaining: 9.63s\n",
            "2733:\tlearn: 0.2784723\ttotal: 1m 38s\tremaining: 9.59s\n",
            "2734:\tlearn: 0.2784472\ttotal: 1m 38s\tremaining: 9.56s\n",
            "2735:\tlearn: 0.2784170\ttotal: 1m 38s\tremaining: 9.52s\n",
            "2736:\tlearn: 0.2783794\ttotal: 1m 38s\tremaining: 9.49s\n",
            "2737:\tlearn: 0.2783346\ttotal: 1m 38s\tremaining: 9.46s\n",
            "2738:\tlearn: 0.2782867\ttotal: 1m 38s\tremaining: 9.42s\n",
            "2739:\tlearn: 0.2782498\ttotal: 1m 38s\tremaining: 9.39s\n",
            "2740:\tlearn: 0.2782228\ttotal: 1m 38s\tremaining: 9.35s\n",
            "2741:\tlearn: 0.2781961\ttotal: 1m 39s\tremaining: 9.32s\n",
            "2742:\tlearn: 0.2781363\ttotal: 1m 39s\tremaining: 9.28s\n",
            "2743:\tlearn: 0.2781172\ttotal: 1m 39s\tremaining: 9.25s\n",
            "2744:\tlearn: 0.2780923\ttotal: 1m 39s\tremaining: 9.21s\n",
            "2745:\tlearn: 0.2780752\ttotal: 1m 39s\tremaining: 9.18s\n",
            "2746:\tlearn: 0.2780524\ttotal: 1m 39s\tremaining: 9.15s\n",
            "2747:\tlearn: 0.2780239\ttotal: 1m 39s\tremaining: 9.11s\n",
            "2748:\tlearn: 0.2779840\ttotal: 1m 39s\tremaining: 9.08s\n",
            "2749:\tlearn: 0.2779494\ttotal: 1m 39s\tremaining: 9.04s\n",
            "2750:\tlearn: 0.2779151\ttotal: 1m 39s\tremaining: 9.01s\n",
            "2751:\tlearn: 0.2778779\ttotal: 1m 39s\tremaining: 8.97s\n",
            "2752:\tlearn: 0.2778212\ttotal: 1m 39s\tremaining: 8.94s\n",
            "2753:\tlearn: 0.2777837\ttotal: 1m 39s\tremaining: 8.9s\n",
            "2754:\tlearn: 0.2777551\ttotal: 1m 39s\tremaining: 8.87s\n",
            "2755:\tlearn: 0.2777367\ttotal: 1m 39s\tremaining: 8.83s\n",
            "2756:\tlearn: 0.2777174\ttotal: 1m 39s\tremaining: 8.8s\n",
            "2757:\tlearn: 0.2776730\ttotal: 1m 39s\tremaining: 8.76s\n",
            "2758:\tlearn: 0.2776249\ttotal: 1m 39s\tremaining: 8.73s\n",
            "2759:\tlearn: 0.2775736\ttotal: 1m 39s\tremaining: 8.7s\n",
            "2760:\tlearn: 0.2775473\ttotal: 1m 40s\tremaining: 8.66s\n",
            "2761:\tlearn: 0.2775093\ttotal: 1m 40s\tremaining: 8.63s\n",
            "2762:\tlearn: 0.2774950\ttotal: 1m 40s\tremaining: 8.59s\n",
            "2763:\tlearn: 0.2774626\ttotal: 1m 40s\tremaining: 8.56s\n",
            "2764:\tlearn: 0.2774302\ttotal: 1m 40s\tremaining: 8.54s\n",
            "2765:\tlearn: 0.2774122\ttotal: 1m 40s\tremaining: 8.52s\n",
            "2766:\tlearn: 0.2773852\ttotal: 1m 40s\tremaining: 8.49s\n",
            "2767:\tlearn: 0.2773624\ttotal: 1m 40s\tremaining: 8.46s\n",
            "2768:\tlearn: 0.2773406\ttotal: 1m 41s\tremaining: 8.43s\n",
            "2769:\tlearn: 0.2773140\ttotal: 1m 41s\tremaining: 8.39s\n",
            "2770:\tlearn: 0.2772803\ttotal: 1m 41s\tremaining: 8.36s\n",
            "2771:\tlearn: 0.2772580\ttotal: 1m 41s\tremaining: 8.33s\n",
            "2772:\tlearn: 0.2772276\ttotal: 1m 41s\tremaining: 8.29s\n",
            "2773:\tlearn: 0.2772121\ttotal: 1m 41s\tremaining: 8.25s\n",
            "2774:\tlearn: 0.2771769\ttotal: 1m 41s\tremaining: 8.22s\n",
            "2775:\tlearn: 0.2771552\ttotal: 1m 41s\tremaining: 8.18s\n",
            "2776:\tlearn: 0.2771061\ttotal: 1m 41s\tremaining: 8.14s\n",
            "2777:\tlearn: 0.2770696\ttotal: 1m 41s\tremaining: 8.11s\n",
            "2778:\tlearn: 0.2770451\ttotal: 1m 41s\tremaining: 8.07s\n",
            "2779:\tlearn: 0.2770075\ttotal: 1m 41s\tremaining: 8.03s\n",
            "2780:\tlearn: 0.2769726\ttotal: 1m 41s\tremaining: 8s\n",
            "2781:\tlearn: 0.2769529\ttotal: 1m 41s\tremaining: 7.96s\n",
            "2782:\tlearn: 0.2769363\ttotal: 1m 41s\tremaining: 7.92s\n",
            "2783:\tlearn: 0.2769167\ttotal: 1m 41s\tremaining: 7.88s\n",
            "2784:\tlearn: 0.2768875\ttotal: 1m 41s\tremaining: 7.85s\n",
            "2785:\tlearn: 0.2768560\ttotal: 1m 41s\tremaining: 7.81s\n",
            "2786:\tlearn: 0.2768079\ttotal: 1m 41s\tremaining: 7.77s\n",
            "2787:\tlearn: 0.2767904\ttotal: 1m 41s\tremaining: 7.74s\n",
            "2788:\tlearn: 0.2767655\ttotal: 1m 41s\tremaining: 7.7s\n",
            "2789:\tlearn: 0.2767228\ttotal: 1m 41s\tremaining: 7.66s\n",
            "2790:\tlearn: 0.2766647\ttotal: 1m 41s\tremaining: 7.63s\n",
            "2791:\tlearn: 0.2766270\ttotal: 1m 41s\tremaining: 7.59s\n",
            "2792:\tlearn: 0.2765878\ttotal: 1m 41s\tremaining: 7.55s\n",
            "2793:\tlearn: 0.2765575\ttotal: 1m 41s\tremaining: 7.52s\n",
            "2794:\tlearn: 0.2765071\ttotal: 1m 41s\tremaining: 7.48s\n",
            "2795:\tlearn: 0.2764684\ttotal: 1m 41s\tremaining: 7.44s\n",
            "2796:\tlearn: 0.2764251\ttotal: 1m 42s\tremaining: 7.4s\n",
            "2797:\tlearn: 0.2763916\ttotal: 1m 42s\tremaining: 7.37s\n",
            "2798:\tlearn: 0.2763775\ttotal: 1m 42s\tremaining: 7.33s\n",
            "2799:\tlearn: 0.2763564\ttotal: 1m 42s\tremaining: 7.29s\n",
            "2800:\tlearn: 0.2763291\ttotal: 1m 42s\tremaining: 7.26s\n",
            "2801:\tlearn: 0.2763049\ttotal: 1m 42s\tremaining: 7.22s\n",
            "2802:\tlearn: 0.2762743\ttotal: 1m 42s\tremaining: 7.18s\n",
            "2803:\tlearn: 0.2762466\ttotal: 1m 42s\tremaining: 7.15s\n",
            "2804:\tlearn: 0.2762120\ttotal: 1m 42s\tremaining: 7.11s\n",
            "2805:\tlearn: 0.2761903\ttotal: 1m 42s\tremaining: 7.07s\n",
            "2806:\tlearn: 0.2761528\ttotal: 1m 42s\tremaining: 7.04s\n",
            "2807:\tlearn: 0.2761269\ttotal: 1m 42s\tremaining: 7s\n",
            "2808:\tlearn: 0.2760969\ttotal: 1m 42s\tremaining: 6.96s\n",
            "2809:\tlearn: 0.2760675\ttotal: 1m 42s\tremaining: 6.92s\n",
            "2810:\tlearn: 0.2760261\ttotal: 1m 42s\tremaining: 6.89s\n",
            "2811:\tlearn: 0.2759873\ttotal: 1m 42s\tremaining: 6.85s\n",
            "2812:\tlearn: 0.2759674\ttotal: 1m 42s\tremaining: 6.82s\n",
            "2813:\tlearn: 0.2759402\ttotal: 1m 42s\tremaining: 6.78s\n",
            "2814:\tlearn: 0.2759010\ttotal: 1m 42s\tremaining: 6.74s\n",
            "2815:\tlearn: 0.2758807\ttotal: 1m 42s\tremaining: 6.71s\n",
            "2816:\tlearn: 0.2758541\ttotal: 1m 42s\tremaining: 6.67s\n",
            "2817:\tlearn: 0.2758007\ttotal: 1m 42s\tremaining: 6.63s\n",
            "2818:\tlearn: 0.2757788\ttotal: 1m 42s\tremaining: 6.59s\n",
            "2819:\tlearn: 0.2757449\ttotal: 1m 42s\tremaining: 6.56s\n",
            "2820:\tlearn: 0.2757057\ttotal: 1m 42s\tremaining: 6.52s\n",
            "2821:\tlearn: 0.2756914\ttotal: 1m 42s\tremaining: 6.48s\n",
            "2822:\tlearn: 0.2756723\ttotal: 1m 42s\tremaining: 6.45s\n",
            "2823:\tlearn: 0.2756344\ttotal: 1m 42s\tremaining: 6.41s\n",
            "2824:\tlearn: 0.2756009\ttotal: 1m 42s\tremaining: 6.37s\n",
            "2825:\tlearn: 0.2755660\ttotal: 1m 42s\tremaining: 6.34s\n",
            "2826:\tlearn: 0.2755425\ttotal: 1m 42s\tremaining: 6.3s\n",
            "2827:\tlearn: 0.2755081\ttotal: 1m 42s\tremaining: 6.26s\n",
            "2828:\tlearn: 0.2754860\ttotal: 1m 43s\tremaining: 6.23s\n",
            "2829:\tlearn: 0.2754684\ttotal: 1m 43s\tremaining: 6.19s\n",
            "2830:\tlearn: 0.2754204\ttotal: 1m 43s\tremaining: 6.15s\n",
            "2831:\tlearn: 0.2753938\ttotal: 1m 43s\tremaining: 6.12s\n",
            "2832:\tlearn: 0.2753628\ttotal: 1m 43s\tremaining: 6.08s\n",
            "2833:\tlearn: 0.2753415\ttotal: 1m 43s\tremaining: 6.04s\n",
            "2834:\tlearn: 0.2752876\ttotal: 1m 43s\tremaining: 6.01s\n",
            "2835:\tlearn: 0.2752692\ttotal: 1m 43s\tremaining: 5.97s\n",
            "2836:\tlearn: 0.2752292\ttotal: 1m 43s\tremaining: 5.93s\n",
            "2837:\tlearn: 0.2751912\ttotal: 1m 43s\tremaining: 5.89s\n",
            "2838:\tlearn: 0.2751350\ttotal: 1m 43s\tremaining: 5.86s\n",
            "2839:\tlearn: 0.2751104\ttotal: 1m 43s\tremaining: 5.82s\n",
            "2840:\tlearn: 0.2750658\ttotal: 1m 43s\tremaining: 5.79s\n",
            "2841:\tlearn: 0.2750349\ttotal: 1m 43s\tremaining: 5.75s\n",
            "2842:\tlearn: 0.2749925\ttotal: 1m 43s\tremaining: 5.71s\n",
            "2843:\tlearn: 0.2749685\ttotal: 1m 43s\tremaining: 5.68s\n",
            "2844:\tlearn: 0.2749271\ttotal: 1m 43s\tremaining: 5.64s\n",
            "2845:\tlearn: 0.2748979\ttotal: 1m 43s\tremaining: 5.6s\n",
            "2846:\tlearn: 0.2748793\ttotal: 1m 43s\tremaining: 5.57s\n",
            "2847:\tlearn: 0.2748560\ttotal: 1m 43s\tremaining: 5.53s\n",
            "2848:\tlearn: 0.2748278\ttotal: 1m 43s\tremaining: 5.49s\n",
            "2849:\tlearn: 0.2747966\ttotal: 1m 43s\tremaining: 5.46s\n",
            "2850:\tlearn: 0.2747881\ttotal: 1m 43s\tremaining: 5.42s\n",
            "2851:\tlearn: 0.2747396\ttotal: 1m 43s\tremaining: 5.38s\n",
            "2852:\tlearn: 0.2747149\ttotal: 1m 43s\tremaining: 5.35s\n",
            "2853:\tlearn: 0.2746737\ttotal: 1m 43s\tremaining: 5.31s\n",
            "2854:\tlearn: 0.2746356\ttotal: 1m 43s\tremaining: 5.27s\n",
            "2855:\tlearn: 0.2746081\ttotal: 1m 43s\tremaining: 5.24s\n",
            "2856:\tlearn: 0.2745889\ttotal: 1m 43s\tremaining: 5.2s\n",
            "2857:\tlearn: 0.2745529\ttotal: 1m 43s\tremaining: 5.16s\n",
            "2858:\tlearn: 0.2745265\ttotal: 1m 43s\tremaining: 5.13s\n",
            "2859:\tlearn: 0.2744948\ttotal: 1m 43s\tremaining: 5.09s\n",
            "2860:\tlearn: 0.2744615\ttotal: 1m 44s\tremaining: 5.05s\n",
            "2861:\tlearn: 0.2744200\ttotal: 1m 44s\tremaining: 5.02s\n",
            "2862:\tlearn: 0.2743807\ttotal: 1m 44s\tremaining: 4.98s\n",
            "2863:\tlearn: 0.2743352\ttotal: 1m 44s\tremaining: 4.94s\n",
            "2864:\tlearn: 0.2743076\ttotal: 1m 44s\tremaining: 4.91s\n",
            "2865:\tlearn: 0.2743028\ttotal: 1m 44s\tremaining: 4.87s\n",
            "2866:\tlearn: 0.2742706\ttotal: 1m 44s\tremaining: 4.83s\n",
            "2867:\tlearn: 0.2742366\ttotal: 1m 44s\tremaining: 4.8s\n",
            "2868:\tlearn: 0.2742314\ttotal: 1m 44s\tremaining: 4.76s\n",
            "2869:\tlearn: 0.2742009\ttotal: 1m 44s\tremaining: 4.72s\n",
            "2870:\tlearn: 0.2741876\ttotal: 1m 44s\tremaining: 4.69s\n",
            "2871:\tlearn: 0.2741712\ttotal: 1m 44s\tremaining: 4.65s\n",
            "2872:\tlearn: 0.2741473\ttotal: 1m 44s\tremaining: 4.61s\n",
            "2873:\tlearn: 0.2741345\ttotal: 1m 44s\tremaining: 4.58s\n",
            "2874:\tlearn: 0.2741019\ttotal: 1m 44s\tremaining: 4.54s\n",
            "2875:\tlearn: 0.2740768\ttotal: 1m 44s\tremaining: 4.5s\n",
            "2876:\tlearn: 0.2740494\ttotal: 1m 44s\tremaining: 4.47s\n",
            "2877:\tlearn: 0.2740318\ttotal: 1m 44s\tremaining: 4.43s\n",
            "2878:\tlearn: 0.2740104\ttotal: 1m 44s\tremaining: 4.39s\n",
            "2879:\tlearn: 0.2739756\ttotal: 1m 44s\tremaining: 4.36s\n",
            "2880:\tlearn: 0.2739423\ttotal: 1m 44s\tremaining: 4.32s\n",
            "2881:\tlearn: 0.2739272\ttotal: 1m 44s\tremaining: 4.29s\n",
            "2882:\tlearn: 0.2738896\ttotal: 1m 44s\tremaining: 4.25s\n",
            "2883:\tlearn: 0.2738734\ttotal: 1m 44s\tremaining: 4.21s\n",
            "2884:\tlearn: 0.2738330\ttotal: 1m 44s\tremaining: 4.17s\n",
            "2885:\tlearn: 0.2738094\ttotal: 1m 44s\tremaining: 4.14s\n",
            "2886:\tlearn: 0.2738004\ttotal: 1m 44s\tremaining: 4.1s\n",
            "2887:\tlearn: 0.2737686\ttotal: 1m 44s\tremaining: 4.07s\n",
            "2888:\tlearn: 0.2737260\ttotal: 1m 44s\tremaining: 4.03s\n",
            "2889:\tlearn: 0.2737018\ttotal: 1m 44s\tremaining: 3.99s\n",
            "2890:\tlearn: 0.2736611\ttotal: 1m 44s\tremaining: 3.96s\n",
            "2891:\tlearn: 0.2736399\ttotal: 1m 44s\tremaining: 3.92s\n",
            "2892:\tlearn: 0.2736281\ttotal: 1m 44s\tremaining: 3.88s\n",
            "2893:\tlearn: 0.2736046\ttotal: 1m 45s\tremaining: 3.85s\n",
            "2894:\tlearn: 0.2735701\ttotal: 1m 45s\tremaining: 3.81s\n",
            "2895:\tlearn: 0.2735418\ttotal: 1m 45s\tremaining: 3.77s\n",
            "2896:\tlearn: 0.2735207\ttotal: 1m 45s\tremaining: 3.74s\n",
            "2897:\tlearn: 0.2735187\ttotal: 1m 45s\tremaining: 3.7s\n",
            "2898:\tlearn: 0.2734944\ttotal: 1m 45s\tremaining: 3.66s\n",
            "2899:\tlearn: 0.2734730\ttotal: 1m 45s\tremaining: 3.63s\n",
            "2900:\tlearn: 0.2734548\ttotal: 1m 45s\tremaining: 3.59s\n",
            "2901:\tlearn: 0.2734059\ttotal: 1m 45s\tremaining: 3.55s\n",
            "2902:\tlearn: 0.2733499\ttotal: 1m 45s\tremaining: 3.52s\n",
            "2903:\tlearn: 0.2733015\ttotal: 1m 45s\tremaining: 3.48s\n",
            "2904:\tlearn: 0.2732798\ttotal: 1m 45s\tremaining: 3.44s\n",
            "2905:\tlearn: 0.2732680\ttotal: 1m 45s\tremaining: 3.41s\n",
            "2906:\tlearn: 0.2732480\ttotal: 1m 45s\tremaining: 3.37s\n",
            "2907:\tlearn: 0.2732157\ttotal: 1m 45s\tremaining: 3.33s\n",
            "2908:\tlearn: 0.2731951\ttotal: 1m 45s\tremaining: 3.3s\n",
            "2909:\tlearn: 0.2731775\ttotal: 1m 45s\tremaining: 3.26s\n",
            "2910:\tlearn: 0.2731488\ttotal: 1m 45s\tremaining: 3.23s\n",
            "2911:\tlearn: 0.2731133\ttotal: 1m 45s\tremaining: 3.19s\n",
            "2912:\tlearn: 0.2730735\ttotal: 1m 45s\tremaining: 3.15s\n",
            "2913:\tlearn: 0.2730374\ttotal: 1m 45s\tremaining: 3.12s\n",
            "2914:\tlearn: 0.2730012\ttotal: 1m 45s\tremaining: 3.08s\n",
            "2915:\tlearn: 0.2729684\ttotal: 1m 45s\tremaining: 3.04s\n",
            "2916:\tlearn: 0.2729303\ttotal: 1m 45s\tremaining: 3.01s\n",
            "2917:\tlearn: 0.2729141\ttotal: 1m 45s\tremaining: 2.97s\n",
            "2918:\tlearn: 0.2728888\ttotal: 1m 45s\tremaining: 2.94s\n",
            "2919:\tlearn: 0.2728599\ttotal: 1m 45s\tremaining: 2.9s\n",
            "2920:\tlearn: 0.2728421\ttotal: 1m 45s\tremaining: 2.86s\n",
            "2921:\tlearn: 0.2728218\ttotal: 1m 45s\tremaining: 2.83s\n",
            "2922:\tlearn: 0.2728007\ttotal: 1m 45s\tremaining: 2.79s\n",
            "2923:\tlearn: 0.2727812\ttotal: 1m 45s\tremaining: 2.75s\n",
            "2924:\tlearn: 0.2727457\ttotal: 1m 45s\tremaining: 2.72s\n",
            "2925:\tlearn: 0.2727187\ttotal: 1m 46s\tremaining: 2.68s\n",
            "2926:\tlearn: 0.2726987\ttotal: 1m 46s\tremaining: 2.64s\n",
            "2927:\tlearn: 0.2726706\ttotal: 1m 46s\tremaining: 2.61s\n",
            "2928:\tlearn: 0.2726485\ttotal: 1m 46s\tremaining: 2.57s\n",
            "2929:\tlearn: 0.2726282\ttotal: 1m 46s\tremaining: 2.54s\n",
            "2930:\tlearn: 0.2726009\ttotal: 1m 46s\tremaining: 2.5s\n",
            "2931:\tlearn: 0.2725833\ttotal: 1m 46s\tremaining: 2.46s\n",
            "2932:\tlearn: 0.2725608\ttotal: 1m 46s\tremaining: 2.43s\n",
            "2933:\tlearn: 0.2725231\ttotal: 1m 46s\tremaining: 2.39s\n",
            "2934:\tlearn: 0.2724941\ttotal: 1m 46s\tremaining: 2.35s\n",
            "2935:\tlearn: 0.2724729\ttotal: 1m 46s\tremaining: 2.32s\n",
            "2936:\tlearn: 0.2724414\ttotal: 1m 46s\tremaining: 2.28s\n",
            "2937:\tlearn: 0.2724328\ttotal: 1m 46s\tremaining: 2.24s\n",
            "2938:\tlearn: 0.2724196\ttotal: 1m 46s\tremaining: 2.21s\n",
            "2939:\tlearn: 0.2723624\ttotal: 1m 46s\tremaining: 2.17s\n",
            "2940:\tlearn: 0.2723223\ttotal: 1m 46s\tremaining: 2.13s\n",
            "2941:\tlearn: 0.2722949\ttotal: 1m 46s\tremaining: 2.1s\n",
            "2942:\tlearn: 0.2722712\ttotal: 1m 46s\tremaining: 2.06s\n",
            "2943:\tlearn: 0.2722434\ttotal: 1m 46s\tremaining: 2.03s\n",
            "2944:\tlearn: 0.2722234\ttotal: 1m 46s\tremaining: 1.99s\n",
            "2945:\tlearn: 0.2721968\ttotal: 1m 46s\tremaining: 1.95s\n",
            "2946:\tlearn: 0.2721561\ttotal: 1m 46s\tremaining: 1.92s\n",
            "2947:\tlearn: 0.2721258\ttotal: 1m 46s\tremaining: 1.88s\n",
            "2948:\tlearn: 0.2720880\ttotal: 1m 46s\tremaining: 1.84s\n",
            "2949:\tlearn: 0.2720728\ttotal: 1m 46s\tremaining: 1.81s\n",
            "2950:\tlearn: 0.2720366\ttotal: 1m 46s\tremaining: 1.77s\n",
            "2951:\tlearn: 0.2720259\ttotal: 1m 46s\tremaining: 1.74s\n",
            "2952:\tlearn: 0.2720085\ttotal: 1m 46s\tremaining: 1.7s\n",
            "2953:\tlearn: 0.2720066\ttotal: 1m 46s\tremaining: 1.66s\n",
            "2954:\tlearn: 0.2720012\ttotal: 1m 46s\tremaining: 1.63s\n",
            "2955:\tlearn: 0.2719597\ttotal: 1m 46s\tremaining: 1.59s\n",
            "2956:\tlearn: 0.2719406\ttotal: 1m 46s\tremaining: 1.55s\n",
            "2957:\tlearn: 0.2719337\ttotal: 1m 46s\tremaining: 1.52s\n",
            "2958:\tlearn: 0.2719261\ttotal: 1m 47s\tremaining: 1.48s\n",
            "2959:\tlearn: 0.2719103\ttotal: 1m 47s\tremaining: 1.45s\n",
            "2960:\tlearn: 0.2718897\ttotal: 1m 47s\tremaining: 1.41s\n",
            "2961:\tlearn: 0.2718514\ttotal: 1m 47s\tremaining: 1.37s\n",
            "2962:\tlearn: 0.2718249\ttotal: 1m 47s\tremaining: 1.34s\n",
            "2963:\tlearn: 0.2718054\ttotal: 1m 47s\tremaining: 1.3s\n",
            "2964:\tlearn: 0.2717885\ttotal: 1m 47s\tremaining: 1.26s\n",
            "2965:\tlearn: 0.2717650\ttotal: 1m 47s\tremaining: 1.23s\n",
            "2966:\tlearn: 0.2717437\ttotal: 1m 47s\tremaining: 1.19s\n",
            "2967:\tlearn: 0.2717242\ttotal: 1m 47s\tremaining: 1.16s\n",
            "2968:\tlearn: 0.2716992\ttotal: 1m 47s\tremaining: 1.12s\n",
            "2969:\tlearn: 0.2716661\ttotal: 1m 47s\tremaining: 1.08s\n",
            "2970:\tlearn: 0.2716209\ttotal: 1m 47s\tremaining: 1.05s\n",
            "2971:\tlearn: 0.2715905\ttotal: 1m 47s\tremaining: 1.01s\n",
            "2972:\tlearn: 0.2715625\ttotal: 1m 47s\tremaining: 976ms\n",
            "2973:\tlearn: 0.2715522\ttotal: 1m 47s\tremaining: 939ms\n",
            "2974:\tlearn: 0.2715292\ttotal: 1m 47s\tremaining: 903ms\n",
            "2975:\tlearn: 0.2714977\ttotal: 1m 47s\tremaining: 867ms\n",
            "2976:\tlearn: 0.2714760\ttotal: 1m 47s\tremaining: 831ms\n",
            "2977:\tlearn: 0.2714579\ttotal: 1m 47s\tremaining: 795ms\n",
            "2978:\tlearn: 0.2714304\ttotal: 1m 47s\tremaining: 759ms\n",
            "2979:\tlearn: 0.2714072\ttotal: 1m 47s\tremaining: 723ms\n",
            "2980:\tlearn: 0.2713715\ttotal: 1m 47s\tremaining: 686ms\n",
            "2981:\tlearn: 0.2713607\ttotal: 1m 47s\tremaining: 650ms\n",
            "2982:\tlearn: 0.2713366\ttotal: 1m 47s\tremaining: 614ms\n",
            "2983:\tlearn: 0.2713016\ttotal: 1m 47s\tremaining: 578ms\n",
            "2984:\tlearn: 0.2712718\ttotal: 1m 47s\tremaining: 542ms\n",
            "2985:\tlearn: 0.2712522\ttotal: 1m 47s\tremaining: 506ms\n",
            "2986:\tlearn: 0.2712203\ttotal: 1m 47s\tremaining: 469ms\n",
            "2987:\tlearn: 0.2712012\ttotal: 1m 47s\tremaining: 433ms\n",
            "2988:\tlearn: 0.2711637\ttotal: 1m 47s\tremaining: 397ms\n",
            "2989:\tlearn: 0.2711312\ttotal: 1m 47s\tremaining: 361ms\n",
            "2990:\tlearn: 0.2710977\ttotal: 1m 47s\tremaining: 325ms\n",
            "2991:\tlearn: 0.2710549\ttotal: 1m 48s\tremaining: 289ms\n",
            "2992:\tlearn: 0.2710413\ttotal: 1m 48s\tremaining: 253ms\n",
            "2993:\tlearn: 0.2710094\ttotal: 1m 48s\tremaining: 217ms\n",
            "2994:\tlearn: 0.2709994\ttotal: 1m 48s\tremaining: 180ms\n",
            "2995:\tlearn: 0.2709618\ttotal: 1m 48s\tremaining: 144ms\n",
            "2996:\tlearn: 0.2709336\ttotal: 1m 48s\tremaining: 108ms\n",
            "2997:\tlearn: 0.2709320\ttotal: 1m 48s\tremaining: 72.2ms\n",
            "2998:\tlearn: 0.2709145\ttotal: 1m 48s\tremaining: 36.1ms\n",
            "2999:\tlearn: 0.2708792\ttotal: 1m 48s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7c3155889c60>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model2 = CatBoostClassifier(iterations=3000, loss_function=\"MultiClassOneVsAll\", learning_rate=0.05, l2_leaf_reg=14, max_depth=8)\n",
        "model2.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T08:57:49.013206Z",
          "end_time": "2024-05-13T08:58:03.446624Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYFWT4SvlLVZ",
        "outputId": "bd05abb0-9e45-4955-ac56-7a60cbc04652"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.434\n",
            "R2: 0.182\n",
            "ROC AUC: 0.79222\n",
            "Score: 0.8119\n",
            "Local Score: 0.9090\n",
            "Best params:  {'iterations': 3000, 'learning_rate': 0.05, 'l2_leaf_reg': 14, 'loss_function': 'MultiClassOneVsAll', 'max_depth': 8}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model2)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T08:58:04.907344Z",
          "end_time": "2024-05-13T08:58:04.932422Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4FMUU75lLVZ",
        "outputId": "738d6bcf-4c3b-4cbd-aa2b-10f223e74e07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LightGBM"
      ],
      "metadata": {
        "collapsed": false,
        "id": "bJduZpxmlLVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:41:45.651982Z",
          "end_time": "2024-05-14T18:41:45.918318Z"
        },
        "id": "Tk4331__lLVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2217, number of negative: 3698\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3745\n",
            "[LightGBM] [Info] Number of data points in the train set: 5915, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374810 -> initscore=-0.511637\n",
            "[LightGBM] [Info] Start training from score -0.511637\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model3 = LGBMClassifier()\n",
        "model3.fit(x_train.to_numpy(), y_train.to_numpy())"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T08:59:23.892500Z",
          "end_time": "2024-05-13T08:59:24.509467Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "6dr-EslulLVZ",
        "outputId": "b3f0acd4-4770-49b1-b7a3-08d8f3c8b7ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.436\n",
            "R2: 0.173\n",
            "ROC AUC: 0.79064\n",
            "Score: 0.8098\n",
            "Local Score: 0.8898\n",
            "Best params:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 31, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model3)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T08:59:29.566396Z",
          "end_time": "2024-05-13T08:59:29.592662Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob45C9pLlLVZ",
        "outputId": "83603361-42d6-418e-9619-4ff60820ccae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XgBoost"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Bc9GY54VlLVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:13:52.158207Z",
          "end_time": "2024-05-13T22:13:52.220821Z"
        },
        "id": "wPDi4D2XlLVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model4 = XGBClassifier()\n",
        "model4.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:13:53.411188Z",
          "end_time": "2024-05-13T22:13:53.573250Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "c-r5A1oHlLVa",
        "outputId": "a1be07a0-3a7e-4a37-946a-d48a6d670667"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.452\n",
            "R2: 0.114\n",
            "ROC AUC: 0.77435\n",
            "Score: 0.7961\n",
            "Local Score: 0.9319\n",
            "Best params:  {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model4)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:13:56.077641Z",
          "end_time": "2024-05-13T22:13:56.138055Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSYKjO4QlLVa",
        "outputId": "3aca0f0c-8356-441f-9054-f8ca245c0567"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Подбор параметров с GridSearch"
      ],
      "metadata": {
        "collapsed": false,
        "id": "B_NDA1NOlLVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:28:44.261276Z",
          "end_time": "2024-05-13T21:28:47.264744Z"
        },
        "id": "kMBMQlXUlLVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "200:\tlearn: 0.1561242\ttotal: 11.5s\tremaining: 45.7s\n",
            "201:\tlearn: 0.1556093\ttotal: 11.6s\tremaining: 45.6s\n",
            "202:\tlearn: 0.1551172\ttotal: 11.6s\tremaining: 45.6s\n",
            "203:\tlearn: 0.1546893\ttotal: 11.6s\tremaining: 45.5s\n",
            "204:\tlearn: 0.1543465\ttotal: 11.7s\tremaining: 45.4s\n",
            "205:\tlearn: 0.1536584\ttotal: 11.8s\tremaining: 45.3s\n",
            "206:\tlearn: 0.1533774\ttotal: 11.8s\tremaining: 45.2s\n",
            "207:\tlearn: 0.1528047\ttotal: 11.8s\tremaining: 45.1s\n",
            "208:\tlearn: 0.1526940\ttotal: 11.9s\tremaining: 45s\n",
            "209:\tlearn: 0.1525546\ttotal: 11.9s\tremaining: 45s\n",
            "210:\tlearn: 0.1520329\ttotal: 12s\tremaining: 44.9s\n",
            "211:\tlearn: 0.1514918\ttotal: 12s\tremaining: 44.8s\n",
            "212:\tlearn: 0.1508919\ttotal: 12.1s\tremaining: 44.7s\n",
            "213:\tlearn: 0.1503103\ttotal: 12.1s\tremaining: 44.6s\n",
            "214:\tlearn: 0.1499454\ttotal: 12.2s\tremaining: 44.6s\n",
            "215:\tlearn: 0.1493076\ttotal: 12.2s\tremaining: 44.5s\n",
            "216:\tlearn: 0.1487388\ttotal: 12.3s\tremaining: 44.4s\n",
            "217:\tlearn: 0.1484673\ttotal: 12.3s\tremaining: 44.3s\n",
            "218:\tlearn: 0.1481602\ttotal: 12.4s\tremaining: 44.2s\n",
            "219:\tlearn: 0.1478725\ttotal: 12.5s\tremaining: 44.2s\n",
            "220:\tlearn: 0.1475493\ttotal: 12.5s\tremaining: 44.1s\n",
            "221:\tlearn: 0.1473230\ttotal: 12.6s\tremaining: 44s\n",
            "222:\tlearn: 0.1469890\ttotal: 12.6s\tremaining: 43.9s\n",
            "223:\tlearn: 0.1466854\ttotal: 12.7s\tremaining: 43.9s\n",
            "224:\tlearn: 0.1465448\ttotal: 12.7s\tremaining: 43.8s\n",
            "225:\tlearn: 0.1461073\ttotal: 12.8s\tremaining: 43.7s\n",
            "226:\tlearn: 0.1457743\ttotal: 12.8s\tremaining: 43.6s\n",
            "227:\tlearn: 0.1455330\ttotal: 12.9s\tremaining: 43.6s\n",
            "228:\tlearn: 0.1451848\ttotal: 12.9s\tremaining: 43.5s\n",
            "229:\tlearn: 0.1448276\ttotal: 13s\tremaining: 43.4s\n",
            "230:\tlearn: 0.1444640\ttotal: 13s\tremaining: 43.3s\n",
            "231:\tlearn: 0.1440625\ttotal: 13.1s\tremaining: 43.2s\n",
            "232:\tlearn: 0.1438237\ttotal: 13.1s\tremaining: 43.2s\n",
            "233:\tlearn: 0.1435851\ttotal: 13.2s\tremaining: 43.1s\n",
            "234:\tlearn: 0.1433233\ttotal: 13.2s\tremaining: 43s\n",
            "235:\tlearn: 0.1428806\ttotal: 13.3s\tremaining: 43s\n",
            "236:\tlearn: 0.1426456\ttotal: 13.3s\tremaining: 42.9s\n",
            "237:\tlearn: 0.1423299\ttotal: 13.4s\tremaining: 42.8s\n",
            "238:\tlearn: 0.1421762\ttotal: 13.4s\tremaining: 42.7s\n",
            "239:\tlearn: 0.1418156\ttotal: 13.5s\tremaining: 42.7s\n",
            "240:\tlearn: 0.1416302\ttotal: 13.5s\tremaining: 42.6s\n",
            "241:\tlearn: 0.1414153\ttotal: 13.6s\tremaining: 42.5s\n",
            "242:\tlearn: 0.1413313\ttotal: 13.6s\tremaining: 42.4s\n",
            "243:\tlearn: 0.1411569\ttotal: 13.7s\tremaining: 42.4s\n",
            "244:\tlearn: 0.1409870\ttotal: 13.7s\tremaining: 42.3s\n",
            "245:\tlearn: 0.1406297\ttotal: 13.8s\tremaining: 42.2s\n",
            "246:\tlearn: 0.1404500\ttotal: 13.8s\tremaining: 42.1s\n",
            "247:\tlearn: 0.1402240\ttotal: 13.9s\tremaining: 42s\n",
            "248:\tlearn: 0.1401516\ttotal: 13.9s\tremaining: 42s\n",
            "249:\tlearn: 0.1397605\ttotal: 14s\tremaining: 41.9s\n",
            "250:\tlearn: 0.1394326\ttotal: 14s\tremaining: 41.8s\n",
            "251:\tlearn: 0.1393511\ttotal: 14.1s\tremaining: 41.7s\n",
            "252:\tlearn: 0.1388131\ttotal: 14.1s\tremaining: 41.7s\n",
            "253:\tlearn: 0.1384778\ttotal: 14.2s\tremaining: 41.6s\n",
            "254:\tlearn: 0.1380070\ttotal: 14.2s\tremaining: 41.5s\n",
            "255:\tlearn: 0.1378170\ttotal: 14.3s\tremaining: 41.5s\n",
            "256:\tlearn: 0.1377075\ttotal: 14.3s\tremaining: 41.4s\n",
            "257:\tlearn: 0.1375438\ttotal: 14.4s\tremaining: 41.3s\n",
            "258:\tlearn: 0.1372559\ttotal: 14.4s\tremaining: 41.2s\n",
            "259:\tlearn: 0.1370835\ttotal: 14.5s\tremaining: 41.2s\n",
            "260:\tlearn: 0.1369990\ttotal: 14.5s\tremaining: 41.1s\n",
            "261:\tlearn: 0.1368337\ttotal: 14.6s\tremaining: 41s\n",
            "262:\tlearn: 0.1364993\ttotal: 14.6s\tremaining: 41s\n",
            "263:\tlearn: 0.1362222\ttotal: 14.7s\tremaining: 40.9s\n",
            "264:\tlearn: 0.1360602\ttotal: 14.7s\tremaining: 40.8s\n",
            "265:\tlearn: 0.1360281\ttotal: 14.8s\tremaining: 40.7s\n",
            "266:\tlearn: 0.1359977\ttotal: 14.8s\tremaining: 40.6s\n",
            "267:\tlearn: 0.1358211\ttotal: 14.9s\tremaining: 40.6s\n",
            "268:\tlearn: 0.1355785\ttotal: 14.9s\tremaining: 40.5s\n",
            "269:\tlearn: 0.1352890\ttotal: 14.9s\tremaining: 40.4s\n",
            "270:\tlearn: 0.1350307\ttotal: 15s\tremaining: 40.4s\n",
            "271:\tlearn: 0.1348943\ttotal: 15.1s\tremaining: 40.3s\n",
            "272:\tlearn: 0.1346185\ttotal: 15.1s\tremaining: 40.2s\n",
            "273:\tlearn: 0.1343167\ttotal: 15.1s\tremaining: 40.1s\n",
            "274:\tlearn: 0.1340834\ttotal: 15.2s\tremaining: 40s\n",
            "275:\tlearn: 0.1339151\ttotal: 15.3s\tremaining: 40s\n",
            "276:\tlearn: 0.1337362\ttotal: 15.3s\tremaining: 40s\n",
            "277:\tlearn: 0.1334455\ttotal: 15.4s\tremaining: 39.9s\n",
            "278:\tlearn: 0.1331762\ttotal: 15.4s\tremaining: 39.8s\n",
            "279:\tlearn: 0.1329431\ttotal: 15.4s\tremaining: 39.7s\n",
            "280:\tlearn: 0.1328483\ttotal: 15.5s\tremaining: 39.7s\n",
            "281:\tlearn: 0.1323151\ttotal: 15.6s\tremaining: 39.6s\n",
            "282:\tlearn: 0.1320696\ttotal: 15.6s\tremaining: 39.5s\n",
            "283:\tlearn: 0.1318120\ttotal: 15.6s\tremaining: 39.4s\n",
            "284:\tlearn: 0.1316521\ttotal: 15.7s\tremaining: 39.4s\n",
            "285:\tlearn: 0.1313522\ttotal: 15.7s\tremaining: 39.3s\n",
            "286:\tlearn: 0.1311351\ttotal: 15.8s\tremaining: 39.2s\n",
            "287:\tlearn: 0.1307853\ttotal: 15.8s\tremaining: 39.2s\n",
            "288:\tlearn: 0.1306296\ttotal: 15.9s\tremaining: 39.1s\n",
            "289:\tlearn: 0.1305163\ttotal: 15.9s\tremaining: 39s\n",
            "290:\tlearn: 0.1302918\ttotal: 16s\tremaining: 38.9s\n",
            "291:\tlearn: 0.1301573\ttotal: 16s\tremaining: 38.9s\n",
            "292:\tlearn: 0.1299536\ttotal: 16.1s\tremaining: 38.8s\n",
            "293:\tlearn: 0.1297610\ttotal: 16.1s\tremaining: 38.7s\n",
            "294:\tlearn: 0.1296018\ttotal: 16.2s\tremaining: 38.6s\n",
            "295:\tlearn: 0.1295244\ttotal: 16.2s\tremaining: 38.6s\n",
            "296:\tlearn: 0.1293868\ttotal: 16.3s\tremaining: 38.6s\n",
            "297:\tlearn: 0.1292456\ttotal: 16.3s\tremaining: 38.5s\n",
            "298:\tlearn: 0.1290308\ttotal: 16.4s\tremaining: 38.4s\n",
            "299:\tlearn: 0.1289627\ttotal: 16.4s\tremaining: 38.4s\n",
            "300:\tlearn: 0.1288779\ttotal: 16.5s\tremaining: 38.3s\n",
            "301:\tlearn: 0.1286556\ttotal: 16.5s\tremaining: 38.2s\n",
            "302:\tlearn: 0.1285557\ttotal: 16.6s\tremaining: 38.1s\n",
            "303:\tlearn: 0.1283984\ttotal: 16.6s\tremaining: 38.1s\n",
            "304:\tlearn: 0.1279662\ttotal: 16.7s\tremaining: 38s\n",
            "305:\tlearn: 0.1278123\ttotal: 16.7s\tremaining: 38s\n",
            "306:\tlearn: 0.1275501\ttotal: 16.8s\tremaining: 37.9s\n",
            "307:\tlearn: 0.1272840\ttotal: 16.8s\tremaining: 37.8s\n",
            "308:\tlearn: 0.1271741\ttotal: 16.9s\tremaining: 37.7s\n",
            "309:\tlearn: 0.1270034\ttotal: 16.9s\tremaining: 37.7s\n",
            "310:\tlearn: 0.1269556\ttotal: 17s\tremaining: 37.6s\n",
            "311:\tlearn: 0.1269480\ttotal: 17s\tremaining: 37.5s\n",
            "312:\tlearn: 0.1267970\ttotal: 17s\tremaining: 37.4s\n",
            "313:\tlearn: 0.1264519\ttotal: 17.1s\tremaining: 37.3s\n",
            "314:\tlearn: 0.1263124\ttotal: 17.1s\tremaining: 37.3s\n",
            "315:\tlearn: 0.1262521\ttotal: 17.2s\tremaining: 37.2s\n",
            "316:\tlearn: 0.1261898\ttotal: 17.2s\tremaining: 37.1s\n",
            "317:\tlearn: 0.1260244\ttotal: 17.3s\tremaining: 37.1s\n",
            "318:\tlearn: 0.1257587\ttotal: 17.4s\tremaining: 37s\n",
            "319:\tlearn: 0.1256502\ttotal: 17.4s\tremaining: 37s\n",
            "320:\tlearn: 0.1256310\ttotal: 17.5s\tremaining: 36.9s\n",
            "321:\tlearn: 0.1256270\ttotal: 17.5s\tremaining: 36.9s\n",
            "322:\tlearn: 0.1255109\ttotal: 17.6s\tremaining: 36.8s\n",
            "323:\tlearn: 0.1251142\ttotal: 17.6s\tremaining: 36.7s\n",
            "324:\tlearn: 0.1250972\ttotal: 17.7s\tremaining: 36.7s\n",
            "325:\tlearn: 0.1247929\ttotal: 17.7s\tremaining: 36.6s\n",
            "326:\tlearn: 0.1246121\ttotal: 17.8s\tremaining: 36.5s\n",
            "327:\tlearn: 0.1243295\ttotal: 17.8s\tremaining: 36.5s\n",
            "328:\tlearn: 0.1242059\ttotal: 17.9s\tremaining: 36.4s\n",
            "329:\tlearn: 0.1241051\ttotal: 17.9s\tremaining: 36.4s\n",
            "330:\tlearn: 0.1239575\ttotal: 18s\tremaining: 36.3s\n",
            "331:\tlearn: 0.1239517\ttotal: 18s\tremaining: 36.1s\n",
            "332:\tlearn: 0.1236496\ttotal: 18s\tremaining: 36.1s\n",
            "333:\tlearn: 0.1233443\ttotal: 18.1s\tremaining: 36s\n",
            "334:\tlearn: 0.1231922\ttotal: 18.1s\tremaining: 35.9s\n",
            "335:\tlearn: 0.1229912\ttotal: 18.2s\tremaining: 35.9s\n",
            "336:\tlearn: 0.1227873\ttotal: 18.2s\tremaining: 35.9s\n",
            "337:\tlearn: 0.1227767\ttotal: 18.3s\tremaining: 35.9s\n",
            "338:\tlearn: 0.1226614\ttotal: 18.4s\tremaining: 35.9s\n",
            "339:\tlearn: 0.1225644\ttotal: 18.5s\tremaining: 35.9s\n",
            "340:\tlearn: 0.1224457\ttotal: 18.6s\tremaining: 36s\n",
            "341:\tlearn: 0.1224055\ttotal: 18.7s\tremaining: 35.9s\n",
            "342:\tlearn: 0.1222219\ttotal: 18.8s\tremaining: 36s\n",
            "343:\tlearn: 0.1222141\ttotal: 18.9s\tremaining: 36s\n",
            "344:\tlearn: 0.1220122\ttotal: 18.9s\tremaining: 36s\n",
            "345:\tlearn: 0.1219507\ttotal: 19s\tremaining: 36s\n",
            "346:\tlearn: 0.1219087\ttotal: 19.1s\tremaining: 36s\n",
            "347:\tlearn: 0.1219085\ttotal: 19.1s\tremaining: 35.9s\n",
            "348:\tlearn: 0.1217595\ttotal: 19.2s\tremaining: 35.9s\n",
            "349:\tlearn: 0.1217261\ttotal: 19.3s\tremaining: 35.9s\n",
            "350:\tlearn: 0.1215946\ttotal: 19.4s\tremaining: 35.9s\n",
            "351:\tlearn: 0.1214977\ttotal: 19.5s\tremaining: 35.9s\n",
            "352:\tlearn: 0.1213712\ttotal: 19.6s\tremaining: 35.9s\n",
            "353:\tlearn: 0.1211085\ttotal: 19.7s\tremaining: 35.9s\n",
            "354:\tlearn: 0.1209967\ttotal: 19.8s\tremaining: 35.9s\n",
            "355:\tlearn: 0.1208881\ttotal: 19.9s\tremaining: 35.9s\n",
            "356:\tlearn: 0.1207487\ttotal: 19.9s\tremaining: 35.9s\n",
            "357:\tlearn: 0.1206291\ttotal: 20s\tremaining: 35.9s\n",
            "358:\tlearn: 0.1204139\ttotal: 20.1s\tremaining: 35.9s\n",
            "359:\tlearn: 0.1202388\ttotal: 20.2s\tremaining: 35.9s\n",
            "360:\tlearn: 0.1200417\ttotal: 20.3s\tremaining: 35.9s\n",
            "361:\tlearn: 0.1198407\ttotal: 20.3s\tremaining: 35.8s\n",
            "362:\tlearn: 0.1196663\ttotal: 20.4s\tremaining: 35.9s\n",
            "363:\tlearn: 0.1194568\ttotal: 20.5s\tremaining: 35.8s\n",
            "364:\tlearn: 0.1192420\ttotal: 20.6s\tremaining: 35.8s\n",
            "365:\tlearn: 0.1191425\ttotal: 20.7s\tremaining: 35.9s\n",
            "366:\tlearn: 0.1189131\ttotal: 20.8s\tremaining: 35.8s\n",
            "367:\tlearn: 0.1187472\ttotal: 20.9s\tremaining: 35.8s\n",
            "368:\tlearn: 0.1186468\ttotal: 21s\tremaining: 35.8s\n",
            "369:\tlearn: 0.1185983\ttotal: 21.1s\tremaining: 35.8s\n",
            "370:\tlearn: 0.1183522\ttotal: 21.1s\tremaining: 35.8s\n",
            "371:\tlearn: 0.1181644\ttotal: 21.2s\tremaining: 35.9s\n",
            "372:\tlearn: 0.1181173\ttotal: 21.3s\tremaining: 35.9s\n",
            "373:\tlearn: 0.1180119\ttotal: 21.4s\tremaining: 35.8s\n",
            "374:\tlearn: 0.1179444\ttotal: 21.5s\tremaining: 35.9s\n",
            "375:\tlearn: 0.1178523\ttotal: 21.6s\tremaining: 35.9s\n",
            "376:\tlearn: 0.1177545\ttotal: 21.7s\tremaining: 35.9s\n",
            "377:\tlearn: 0.1176338\ttotal: 21.8s\tremaining: 35.8s\n",
            "378:\tlearn: 0.1175066\ttotal: 21.8s\tremaining: 35.8s\n",
            "379:\tlearn: 0.1173865\ttotal: 21.9s\tremaining: 35.7s\n",
            "380:\tlearn: 0.1172901\ttotal: 21.9s\tremaining: 35.6s\n",
            "381:\tlearn: 0.1171272\ttotal: 22s\tremaining: 35.5s\n",
            "382:\tlearn: 0.1170158\ttotal: 22s\tremaining: 35.5s\n",
            "383:\tlearn: 0.1168713\ttotal: 22.1s\tremaining: 35.4s\n",
            "384:\tlearn: 0.1167137\ttotal: 22.1s\tremaining: 35.3s\n",
            "385:\tlearn: 0.1165568\ttotal: 22.2s\tremaining: 35.2s\n",
            "386:\tlearn: 0.1163978\ttotal: 22.2s\tremaining: 35.2s\n",
            "387:\tlearn: 0.1163011\ttotal: 22.3s\tremaining: 35.1s\n",
            "388:\tlearn: 0.1162379\ttotal: 22.3s\tremaining: 35s\n",
            "389:\tlearn: 0.1161904\ttotal: 22.4s\tremaining: 35s\n",
            "390:\tlearn: 0.1159831\ttotal: 22.4s\tremaining: 34.9s\n",
            "391:\tlearn: 0.1158856\ttotal: 22.4s\tremaining: 34.8s\n",
            "392:\tlearn: 0.1157642\ttotal: 22.5s\tremaining: 34.8s\n",
            "393:\tlearn: 0.1157337\ttotal: 22.6s\tremaining: 34.7s\n",
            "394:\tlearn: 0.1156225\ttotal: 22.6s\tremaining: 34.6s\n",
            "395:\tlearn: 0.1154882\ttotal: 22.7s\tremaining: 34.6s\n",
            "396:\tlearn: 0.1154101\ttotal: 22.7s\tremaining: 34.5s\n",
            "397:\tlearn: 0.1153450\ttotal: 22.8s\tremaining: 34.4s\n",
            "398:\tlearn: 0.1151677\ttotal: 22.8s\tremaining: 34.4s\n",
            "399:\tlearn: 0.1150574\ttotal: 22.9s\tremaining: 34.3s\n",
            "400:\tlearn: 0.1149599\ttotal: 22.9s\tremaining: 34.2s\n",
            "401:\tlearn: 0.1148288\ttotal: 23s\tremaining: 34.2s\n",
            "402:\tlearn: 0.1148019\ttotal: 23s\tremaining: 34.1s\n",
            "403:\tlearn: 0.1147873\ttotal: 23.1s\tremaining: 34s\n",
            "404:\tlearn: 0.1146298\ttotal: 23.1s\tremaining: 33.9s\n",
            "405:\tlearn: 0.1145320\ttotal: 23.2s\tremaining: 33.9s\n",
            "406:\tlearn: 0.1144147\ttotal: 23.2s\tremaining: 33.8s\n",
            "407:\tlearn: 0.1142581\ttotal: 23.3s\tremaining: 33.7s\n",
            "408:\tlearn: 0.1141921\ttotal: 23.3s\tremaining: 33.7s\n",
            "409:\tlearn: 0.1141123\ttotal: 23.4s\tremaining: 33.6s\n",
            "410:\tlearn: 0.1140266\ttotal: 23.4s\tremaining: 33.5s\n",
            "411:\tlearn: 0.1139823\ttotal: 23.5s\tremaining: 33.5s\n",
            "412:\tlearn: 0.1138753\ttotal: 23.5s\tremaining: 33.4s\n",
            "413:\tlearn: 0.1138124\ttotal: 23.6s\tremaining: 33.4s\n",
            "414:\tlearn: 0.1137275\ttotal: 23.6s\tremaining: 33.3s\n",
            "415:\tlearn: 0.1136797\ttotal: 23.7s\tremaining: 33.2s\n",
            "416:\tlearn: 0.1135511\ttotal: 23.7s\tremaining: 33.1s\n",
            "417:\tlearn: 0.1134130\ttotal: 23.8s\tremaining: 33.1s\n",
            "418:\tlearn: 0.1133811\ttotal: 23.8s\tremaining: 33s\n",
            "419:\tlearn: 0.1131995\ttotal: 23.9s\tremaining: 32.9s\n",
            "420:\tlearn: 0.1131950\ttotal: 23.9s\tremaining: 32.9s\n",
            "421:\tlearn: 0.1131458\ttotal: 23.9s\tremaining: 32.8s\n",
            "422:\tlearn: 0.1129978\ttotal: 24s\tremaining: 32.7s\n",
            "423:\tlearn: 0.1129617\ttotal: 24s\tremaining: 32.7s\n",
            "424:\tlearn: 0.1128870\ttotal: 24.1s\tremaining: 32.6s\n",
            "425:\tlearn: 0.1128444\ttotal: 24.1s\tremaining: 32.5s\n",
            "426:\tlearn: 0.1127617\ttotal: 24.2s\tremaining: 32.5s\n",
            "427:\tlearn: 0.1125940\ttotal: 24.2s\tremaining: 32.4s\n",
            "428:\tlearn: 0.1125873\ttotal: 24.3s\tremaining: 32.3s\n",
            "429:\tlearn: 0.1125564\ttotal: 24.3s\tremaining: 32.3s\n",
            "430:\tlearn: 0.1124326\ttotal: 24.4s\tremaining: 32.2s\n",
            "431:\tlearn: 0.1123501\ttotal: 24.4s\tremaining: 32.1s\n",
            "432:\tlearn: 0.1122178\ttotal: 24.5s\tremaining: 32.1s\n",
            "433:\tlearn: 0.1120669\ttotal: 24.5s\tremaining: 32s\n",
            "434:\tlearn: 0.1118946\ttotal: 24.6s\tremaining: 31.9s\n",
            "435:\tlearn: 0.1118113\ttotal: 24.6s\tremaining: 31.9s\n",
            "436:\tlearn: 0.1116338\ttotal: 24.7s\tremaining: 31.8s\n",
            "437:\tlearn: 0.1114848\ttotal: 24.7s\tremaining: 31.7s\n",
            "438:\tlearn: 0.1113768\ttotal: 24.8s\tremaining: 31.7s\n",
            "439:\tlearn: 0.1112778\ttotal: 24.8s\tremaining: 31.6s\n",
            "440:\tlearn: 0.1112704\ttotal: 24.9s\tremaining: 31.5s\n",
            "441:\tlearn: 0.1112651\ttotal: 24.9s\tremaining: 31.5s\n",
            "442:\tlearn: 0.1111915\ttotal: 25s\tremaining: 31.4s\n",
            "443:\tlearn: 0.1111069\ttotal: 25s\tremaining: 31.3s\n",
            "444:\tlearn: 0.1110236\ttotal: 25.1s\tremaining: 31.3s\n",
            "445:\tlearn: 0.1110030\ttotal: 25.1s\tremaining: 31.2s\n",
            "446:\tlearn: 0.1108727\ttotal: 25.2s\tremaining: 31.1s\n",
            "447:\tlearn: 0.1107868\ttotal: 25.2s\tremaining: 31.1s\n",
            "448:\tlearn: 0.1106326\ttotal: 25.3s\tremaining: 31s\n",
            "449:\tlearn: 0.1105547\ttotal: 25.3s\tremaining: 31s\n",
            "450:\tlearn: 0.1104523\ttotal: 25.4s\tremaining: 30.9s\n",
            "451:\tlearn: 0.1103463\ttotal: 25.4s\tremaining: 30.8s\n",
            "452:\tlearn: 0.1103207\ttotal: 25.5s\tremaining: 30.8s\n",
            "453:\tlearn: 0.1103126\ttotal: 25.5s\tremaining: 30.7s\n",
            "454:\tlearn: 0.1102219\ttotal: 25.6s\tremaining: 30.6s\n",
            "455:\tlearn: 0.1101123\ttotal: 25.6s\tremaining: 30.6s\n",
            "456:\tlearn: 0.1101024\ttotal: 25.7s\tremaining: 30.5s\n",
            "457:\tlearn: 0.1100226\ttotal: 25.7s\tremaining: 30.4s\n",
            "458:\tlearn: 0.1098416\ttotal: 25.8s\tremaining: 30.4s\n",
            "459:\tlearn: 0.1097772\ttotal: 25.8s\tremaining: 30.3s\n",
            "460:\tlearn: 0.1097319\ttotal: 25.9s\tremaining: 30.2s\n",
            "461:\tlearn: 0.1096311\ttotal: 25.9s\tremaining: 30.2s\n",
            "462:\tlearn: 0.1096301\ttotal: 25.9s\tremaining: 30.1s\n",
            "463:\tlearn: 0.1095430\ttotal: 26s\tremaining: 30s\n",
            "464:\tlearn: 0.1094765\ttotal: 26s\tremaining: 30s\n",
            "465:\tlearn: 0.1094037\ttotal: 26.1s\tremaining: 29.9s\n",
            "466:\tlearn: 0.1092892\ttotal: 26.1s\tremaining: 29.8s\n",
            "467:\tlearn: 0.1092646\ttotal: 26.2s\tremaining: 29.8s\n",
            "468:\tlearn: 0.1092029\ttotal: 26.2s\tremaining: 29.7s\n",
            "469:\tlearn: 0.1090423\ttotal: 26.3s\tremaining: 29.6s\n",
            "470:\tlearn: 0.1090357\ttotal: 26.3s\tremaining: 29.6s\n",
            "471:\tlearn: 0.1089786\ttotal: 26.4s\tremaining: 29.5s\n",
            "472:\tlearn: 0.1089153\ttotal: 26.4s\tremaining: 29.5s\n",
            "473:\tlearn: 0.1088598\ttotal: 26.5s\tremaining: 29.4s\n",
            "474:\tlearn: 0.1088083\ttotal: 26.5s\tremaining: 29.3s\n",
            "475:\tlearn: 0.1087305\ttotal: 26.6s\tremaining: 29.3s\n",
            "476:\tlearn: 0.1086509\ttotal: 26.6s\tremaining: 29.2s\n",
            "477:\tlearn: 0.1086066\ttotal: 26.7s\tremaining: 29.1s\n",
            "478:\tlearn: 0.1085885\ttotal: 26.7s\tremaining: 29.1s\n",
            "479:\tlearn: 0.1085432\ttotal: 26.8s\tremaining: 29s\n",
            "480:\tlearn: 0.1084571\ttotal: 26.8s\tremaining: 29s\n",
            "481:\tlearn: 0.1084067\ttotal: 26.9s\tremaining: 28.9s\n",
            "482:\tlearn: 0.1083681\ttotal: 26.9s\tremaining: 28.8s\n",
            "483:\tlearn: 0.1082744\ttotal: 27s\tremaining: 28.8s\n",
            "484:\tlearn: 0.1081684\ttotal: 27s\tremaining: 28.7s\n",
            "485:\tlearn: 0.1081327\ttotal: 27.1s\tremaining: 28.6s\n",
            "486:\tlearn: 0.1081053\ttotal: 27.1s\tremaining: 28.6s\n",
            "487:\tlearn: 0.1080648\ttotal: 27.2s\tremaining: 28.5s\n",
            "488:\tlearn: 0.1080108\ttotal: 27.2s\tremaining: 28.4s\n",
            "489:\tlearn: 0.1079963\ttotal: 27.3s\tremaining: 28.4s\n",
            "490:\tlearn: 0.1079952\ttotal: 27.3s\tremaining: 28.3s\n",
            "491:\tlearn: 0.1079335\ttotal: 27.4s\tremaining: 28.3s\n",
            "492:\tlearn: 0.1078333\ttotal: 27.4s\tremaining: 28.2s\n",
            "493:\tlearn: 0.1076775\ttotal: 27.5s\tremaining: 28.1s\n",
            "494:\tlearn: 0.1076560\ttotal: 27.5s\tremaining: 28.1s\n",
            "495:\tlearn: 0.1076559\ttotal: 27.6s\tremaining: 28s\n",
            "496:\tlearn: 0.1075521\ttotal: 27.6s\tremaining: 27.9s\n",
            "497:\tlearn: 0.1074827\ttotal: 27.7s\tremaining: 27.9s\n",
            "498:\tlearn: 0.1074294\ttotal: 27.7s\tremaining: 27.8s\n",
            "499:\tlearn: 0.1073383\ttotal: 27.8s\tremaining: 27.8s\n",
            "500:\tlearn: 0.1072455\ttotal: 27.8s\tremaining: 27.7s\n",
            "501:\tlearn: 0.1072202\ttotal: 27.9s\tremaining: 27.6s\n",
            "502:\tlearn: 0.1072166\ttotal: 27.9s\tremaining: 27.6s\n",
            "503:\tlearn: 0.1071354\ttotal: 27.9s\tremaining: 27.5s\n",
            "504:\tlearn: 0.1071238\ttotal: 28s\tremaining: 27.4s\n",
            "505:\tlearn: 0.1070530\ttotal: 28s\tremaining: 27.4s\n",
            "506:\tlearn: 0.1070523\ttotal: 28.1s\tremaining: 27.3s\n",
            "507:\tlearn: 0.1069602\ttotal: 28.1s\tremaining: 27.2s\n",
            "508:\tlearn: 0.1068782\ttotal: 28.2s\tremaining: 27.2s\n",
            "509:\tlearn: 0.1068306\ttotal: 28.2s\tremaining: 27.1s\n",
            "510:\tlearn: 0.1068166\ttotal: 28.3s\tremaining: 27.1s\n",
            "511:\tlearn: 0.1068039\ttotal: 28.3s\tremaining: 27s\n",
            "512:\tlearn: 0.1067781\ttotal: 28.4s\tremaining: 26.9s\n",
            "513:\tlearn: 0.1067443\ttotal: 28.4s\tremaining: 26.9s\n",
            "514:\tlearn: 0.1066299\ttotal: 28.5s\tremaining: 26.8s\n",
            "515:\tlearn: 0.1065901\ttotal: 28.5s\tremaining: 26.8s\n",
            "516:\tlearn: 0.1064770\ttotal: 28.6s\tremaining: 26.7s\n",
            "517:\tlearn: 0.1064358\ttotal: 28.6s\tremaining: 26.6s\n",
            "518:\tlearn: 0.1063625\ttotal: 28.7s\tremaining: 26.6s\n",
            "519:\tlearn: 0.1062948\ttotal: 28.7s\tremaining: 26.5s\n",
            "520:\tlearn: 0.1062736\ttotal: 28.8s\tremaining: 26.5s\n",
            "521:\tlearn: 0.1061980\ttotal: 28.8s\tremaining: 26.4s\n",
            "522:\tlearn: 0.1061514\ttotal: 28.9s\tremaining: 26.3s\n",
            "523:\tlearn: 0.1061362\ttotal: 28.9s\tremaining: 26.3s\n",
            "524:\tlearn: 0.1060601\ttotal: 29s\tremaining: 26.2s\n",
            "525:\tlearn: 0.1060144\ttotal: 29s\tremaining: 26.2s\n",
            "526:\tlearn: 0.1059818\ttotal: 29.1s\tremaining: 26.1s\n",
            "527:\tlearn: 0.1059550\ttotal: 29.1s\tremaining: 26s\n",
            "528:\tlearn: 0.1059125\ttotal: 29.2s\tremaining: 26s\n",
            "529:\tlearn: 0.1058491\ttotal: 29.2s\tremaining: 25.9s\n",
            "530:\tlearn: 0.1058279\ttotal: 29.3s\tremaining: 25.8s\n",
            "531:\tlearn: 0.1057857\ttotal: 29.3s\tremaining: 25.8s\n",
            "532:\tlearn: 0.1057173\ttotal: 29.4s\tremaining: 25.7s\n",
            "533:\tlearn: 0.1056777\ttotal: 29.4s\tremaining: 25.7s\n",
            "534:\tlearn: 0.1056246\ttotal: 29.5s\tremaining: 25.6s\n",
            "535:\tlearn: 0.1056098\ttotal: 29.5s\tremaining: 25.5s\n",
            "536:\tlearn: 0.1055442\ttotal: 29.6s\tremaining: 25.5s\n",
            "537:\tlearn: 0.1054465\ttotal: 29.6s\tremaining: 25.4s\n",
            "538:\tlearn: 0.1053625\ttotal: 29.6s\tremaining: 25.4s\n",
            "539:\tlearn: 0.1052832\ttotal: 29.7s\tremaining: 25.3s\n",
            "540:\tlearn: 0.1052778\ttotal: 29.8s\tremaining: 25.2s\n",
            "541:\tlearn: 0.1051596\ttotal: 29.8s\tremaining: 25.2s\n",
            "542:\tlearn: 0.1050918\ttotal: 29.9s\tremaining: 25.1s\n",
            "543:\tlearn: 0.1049885\ttotal: 29.9s\tremaining: 25.1s\n",
            "544:\tlearn: 0.1049483\ttotal: 30s\tremaining: 25s\n",
            "545:\tlearn: 0.1048718\ttotal: 30s\tremaining: 24.9s\n",
            "546:\tlearn: 0.1048224\ttotal: 30s\tremaining: 24.9s\n",
            "547:\tlearn: 0.1047540\ttotal: 30.1s\tremaining: 24.8s\n",
            "548:\tlearn: 0.1046686\ttotal: 30.1s\tremaining: 24.8s\n",
            "549:\tlearn: 0.1046182\ttotal: 30.2s\tremaining: 24.7s\n",
            "550:\tlearn: 0.1045552\ttotal: 30.2s\tremaining: 24.6s\n",
            "551:\tlearn: 0.1044882\ttotal: 30.3s\tremaining: 24.6s\n",
            "552:\tlearn: 0.1044602\ttotal: 30.3s\tremaining: 24.5s\n",
            "553:\tlearn: 0.1044002\ttotal: 30.4s\tremaining: 24.5s\n",
            "554:\tlearn: 0.1043612\ttotal: 30.4s\tremaining: 24.4s\n",
            "555:\tlearn: 0.1042895\ttotal: 30.4s\tremaining: 24.3s\n",
            "556:\tlearn: 0.1041950\ttotal: 30.5s\tremaining: 24.2s\n",
            "557:\tlearn: 0.1041673\ttotal: 30.5s\tremaining: 24.2s\n",
            "558:\tlearn: 0.1041401\ttotal: 30.6s\tremaining: 24.1s\n",
            "559:\tlearn: 0.1041402\ttotal: 30.6s\tremaining: 24.1s\n",
            "560:\tlearn: 0.1040844\ttotal: 30.7s\tremaining: 24s\n",
            "561:\tlearn: 0.1040364\ttotal: 30.7s\tremaining: 24s\n",
            "562:\tlearn: 0.1040159\ttotal: 30.8s\tremaining: 23.9s\n",
            "563:\tlearn: 0.1039613\ttotal: 30.8s\tremaining: 23.8s\n",
            "564:\tlearn: 0.1039120\ttotal: 30.9s\tremaining: 23.8s\n",
            "565:\tlearn: 0.1038346\ttotal: 30.9s\tremaining: 23.7s\n",
            "566:\tlearn: 0.1038011\ttotal: 31s\tremaining: 23.7s\n",
            "567:\tlearn: 0.1037679\ttotal: 31s\tremaining: 23.6s\n",
            "568:\tlearn: 0.1037577\ttotal: 31.1s\tremaining: 23.5s\n",
            "569:\tlearn: 0.1037559\ttotal: 31.1s\tremaining: 23.5s\n",
            "570:\tlearn: 0.1037142\ttotal: 31.2s\tremaining: 23.4s\n",
            "571:\tlearn: 0.1036866\ttotal: 31.2s\tremaining: 23.4s\n",
            "572:\tlearn: 0.1036533\ttotal: 31.3s\tremaining: 23.3s\n",
            "573:\tlearn: 0.1035828\ttotal: 31.3s\tremaining: 23.2s\n",
            "574:\tlearn: 0.1035051\ttotal: 31.4s\tremaining: 23.2s\n",
            "575:\tlearn: 0.1034894\ttotal: 31.4s\tremaining: 23.1s\n",
            "576:\tlearn: 0.1034252\ttotal: 31.5s\tremaining: 23.1s\n",
            "577:\tlearn: 0.1034026\ttotal: 31.5s\tremaining: 23s\n",
            "578:\tlearn: 0.1033847\ttotal: 31.6s\tremaining: 22.9s\n",
            "579:\tlearn: 0.1033473\ttotal: 31.6s\tremaining: 22.9s\n",
            "580:\tlearn: 0.1033096\ttotal: 31.7s\tremaining: 22.8s\n",
            "581:\tlearn: 0.1032806\ttotal: 31.7s\tremaining: 22.8s\n",
            "582:\tlearn: 0.1032160\ttotal: 31.8s\tremaining: 22.8s\n",
            "583:\tlearn: 0.1030946\ttotal: 31.9s\tremaining: 22.7s\n",
            "584:\tlearn: 0.1030061\ttotal: 32s\tremaining: 22.7s\n",
            "585:\tlearn: 0.1029757\ttotal: 32.1s\tremaining: 22.7s\n",
            "586:\tlearn: 0.1029528\ttotal: 32.2s\tremaining: 22.6s\n",
            "587:\tlearn: 0.1029050\ttotal: 32.3s\tremaining: 22.6s\n",
            "588:\tlearn: 0.1028342\ttotal: 32.3s\tremaining: 22.6s\n",
            "589:\tlearn: 0.1028143\ttotal: 32.4s\tremaining: 22.5s\n",
            "590:\tlearn: 0.1027688\ttotal: 32.5s\tremaining: 22.5s\n",
            "591:\tlearn: 0.1027102\ttotal: 32.6s\tremaining: 22.5s\n",
            "592:\tlearn: 0.1026611\ttotal: 32.7s\tremaining: 22.5s\n",
            "593:\tlearn: 0.1026437\ttotal: 32.8s\tremaining: 22.4s\n",
            "594:\tlearn: 0.1025982\ttotal: 32.9s\tremaining: 22.4s\n",
            "595:\tlearn: 0.1025827\ttotal: 33s\tremaining: 22.4s\n",
            "596:\tlearn: 0.1025021\ttotal: 33.1s\tremaining: 22.3s\n",
            "597:\tlearn: 0.1024856\ttotal: 33.2s\tremaining: 22.3s\n",
            "598:\tlearn: 0.1024408\ttotal: 33.3s\tremaining: 22.3s\n",
            "599:\tlearn: 0.1024216\ttotal: 33.3s\tremaining: 22.2s\n",
            "600:\tlearn: 0.1024059\ttotal: 33.4s\tremaining: 22.2s\n",
            "601:\tlearn: 0.1023734\ttotal: 33.5s\tremaining: 22.2s\n",
            "602:\tlearn: 0.1022229\ttotal: 33.6s\tremaining: 22.1s\n",
            "603:\tlearn: 0.1021357\ttotal: 33.7s\tremaining: 22.1s\n",
            "604:\tlearn: 0.1021108\ttotal: 33.8s\tremaining: 22.1s\n",
            "605:\tlearn: 0.1020181\ttotal: 33.9s\tremaining: 22s\n",
            "606:\tlearn: 0.1018966\ttotal: 33.9s\tremaining: 22s\n",
            "607:\tlearn: 0.1018209\ttotal: 34s\tremaining: 21.9s\n",
            "608:\tlearn: 0.1017455\ttotal: 34.1s\tremaining: 21.9s\n",
            "609:\tlearn: 0.1017127\ttotal: 34.2s\tremaining: 21.9s\n",
            "610:\tlearn: 0.1016606\ttotal: 34.3s\tremaining: 21.8s\n",
            "611:\tlearn: 0.1016605\ttotal: 34.4s\tremaining: 21.8s\n",
            "612:\tlearn: 0.1016143\ttotal: 34.5s\tremaining: 21.8s\n",
            "613:\tlearn: 0.1015759\ttotal: 34.6s\tremaining: 21.7s\n",
            "614:\tlearn: 0.1014992\ttotal: 34.6s\tremaining: 21.7s\n",
            "615:\tlearn: 0.1014788\ttotal: 34.7s\tremaining: 21.6s\n",
            "616:\tlearn: 0.1014589\ttotal: 34.8s\tremaining: 21.6s\n",
            "617:\tlearn: 0.1014250\ttotal: 34.9s\tremaining: 21.6s\n",
            "618:\tlearn: 0.1013818\ttotal: 35s\tremaining: 21.5s\n",
            "619:\tlearn: 0.1013487\ttotal: 35.1s\tremaining: 21.5s\n",
            "620:\tlearn: 0.1012914\ttotal: 35.2s\tremaining: 21.5s\n",
            "621:\tlearn: 0.1012559\ttotal: 35.2s\tremaining: 21.4s\n",
            "622:\tlearn: 0.1012211\ttotal: 35.3s\tremaining: 21.4s\n",
            "623:\tlearn: 0.1011600\ttotal: 35.3s\tremaining: 21.3s\n",
            "624:\tlearn: 0.1011542\ttotal: 35.4s\tremaining: 21.2s\n",
            "625:\tlearn: 0.1011180\ttotal: 35.4s\tremaining: 21.2s\n",
            "626:\tlearn: 0.1010629\ttotal: 35.5s\tremaining: 21.1s\n",
            "627:\tlearn: 0.1009817\ttotal: 35.5s\tremaining: 21s\n",
            "628:\tlearn: 0.1009250\ttotal: 35.6s\tremaining: 21s\n",
            "629:\tlearn: 0.1008781\ttotal: 35.6s\tremaining: 20.9s\n",
            "630:\tlearn: 0.1008403\ttotal: 35.7s\tremaining: 20.9s\n",
            "631:\tlearn: 0.1007745\ttotal: 35.7s\tremaining: 20.8s\n",
            "632:\tlearn: 0.1007370\ttotal: 35.8s\tremaining: 20.7s\n",
            "633:\tlearn: 0.1007357\ttotal: 35.8s\tremaining: 20.7s\n",
            "634:\tlearn: 0.1007192\ttotal: 35.9s\tremaining: 20.6s\n",
            "635:\tlearn: 0.1006679\ttotal: 35.9s\tremaining: 20.6s\n",
            "636:\tlearn: 0.1006511\ttotal: 36s\tremaining: 20.5s\n",
            "637:\tlearn: 0.1006018\ttotal: 36s\tremaining: 20.4s\n",
            "638:\tlearn: 0.1005490\ttotal: 36.1s\tremaining: 20.4s\n",
            "639:\tlearn: 0.1005108\ttotal: 36.1s\tremaining: 20.3s\n",
            "640:\tlearn: 0.1004950\ttotal: 36.2s\tremaining: 20.3s\n",
            "641:\tlearn: 0.1004667\ttotal: 36.2s\tremaining: 20.2s\n",
            "642:\tlearn: 0.1004642\ttotal: 36.3s\tremaining: 20.1s\n",
            "643:\tlearn: 0.1004452\ttotal: 36.3s\tremaining: 20.1s\n",
            "644:\tlearn: 0.1004000\ttotal: 36.4s\tremaining: 20s\n",
            "645:\tlearn: 0.1003629\ttotal: 36.4s\tremaining: 20s\n",
            "646:\tlearn: 0.1003629\ttotal: 36.5s\tremaining: 19.9s\n",
            "647:\tlearn: 0.1003222\ttotal: 36.5s\tremaining: 19.8s\n",
            "648:\tlearn: 0.1002656\ttotal: 36.6s\tremaining: 19.8s\n",
            "649:\tlearn: 0.1002129\ttotal: 36.6s\tremaining: 19.7s\n",
            "650:\tlearn: 0.1001658\ttotal: 36.7s\tremaining: 19.7s\n",
            "651:\tlearn: 0.1001156\ttotal: 36.7s\tremaining: 19.6s\n",
            "652:\tlearn: 0.1000702\ttotal: 36.8s\tremaining: 19.5s\n",
            "653:\tlearn: 0.1000179\ttotal: 36.8s\tremaining: 19.5s\n",
            "654:\tlearn: 0.0999978\ttotal: 36.8s\tremaining: 19.4s\n",
            "655:\tlearn: 0.0999569\ttotal: 36.9s\tremaining: 19.3s\n",
            "656:\tlearn: 0.0999246\ttotal: 36.9s\tremaining: 19.3s\n",
            "657:\tlearn: 0.0998998\ttotal: 37s\tremaining: 19.2s\n",
            "658:\tlearn: 0.0998848\ttotal: 37.1s\tremaining: 19.2s\n",
            "659:\tlearn: 0.0998680\ttotal: 37.1s\tremaining: 19.1s\n",
            "660:\tlearn: 0.0998306\ttotal: 37.2s\tremaining: 19.1s\n",
            "661:\tlearn: 0.0997759\ttotal: 37.2s\tremaining: 19s\n",
            "662:\tlearn: 0.0997606\ttotal: 37.3s\tremaining: 18.9s\n",
            "663:\tlearn: 0.0997393\ttotal: 37.3s\tremaining: 18.9s\n",
            "664:\tlearn: 0.0997131\ttotal: 37.4s\tremaining: 18.8s\n",
            "665:\tlearn: 0.0996506\ttotal: 37.4s\tremaining: 18.8s\n",
            "666:\tlearn: 0.0996235\ttotal: 37.4s\tremaining: 18.7s\n",
            "667:\tlearn: 0.0995872\ttotal: 37.5s\tremaining: 18.6s\n",
            "668:\tlearn: 0.0995646\ttotal: 37.6s\tremaining: 18.6s\n",
            "669:\tlearn: 0.0995446\ttotal: 37.6s\tremaining: 18.5s\n",
            "670:\tlearn: 0.0995073\ttotal: 37.7s\tremaining: 18.5s\n",
            "671:\tlearn: 0.0995058\ttotal: 37.7s\tremaining: 18.4s\n",
            "672:\tlearn: 0.0994925\ttotal: 37.8s\tremaining: 18.3s\n",
            "673:\tlearn: 0.0994634\ttotal: 37.8s\tremaining: 18.3s\n",
            "674:\tlearn: 0.0994437\ttotal: 37.8s\tremaining: 18.2s\n",
            "675:\tlearn: 0.0994344\ttotal: 37.9s\tremaining: 18.2s\n",
            "676:\tlearn: 0.0994108\ttotal: 37.9s\tremaining: 18.1s\n",
            "677:\tlearn: 0.0993895\ttotal: 38s\tremaining: 18s\n",
            "678:\tlearn: 0.0993574\ttotal: 38.1s\tremaining: 18s\n",
            "679:\tlearn: 0.0993258\ttotal: 38.1s\tremaining: 17.9s\n",
            "680:\tlearn: 0.0992814\ttotal: 38.2s\tremaining: 17.9s\n",
            "681:\tlearn: 0.0992478\ttotal: 38.2s\tremaining: 17.8s\n",
            "682:\tlearn: 0.0992121\ttotal: 38.3s\tremaining: 17.8s\n",
            "683:\tlearn: 0.0991886\ttotal: 38.3s\tremaining: 17.7s\n",
            "684:\tlearn: 0.0991748\ttotal: 38.3s\tremaining: 17.6s\n",
            "685:\tlearn: 0.0991674\ttotal: 38.4s\tremaining: 17.6s\n",
            "686:\tlearn: 0.0991520\ttotal: 38.4s\tremaining: 17.5s\n",
            "687:\tlearn: 0.0991426\ttotal: 38.5s\tremaining: 17.5s\n",
            "688:\tlearn: 0.0990968\ttotal: 38.5s\tremaining: 17.4s\n",
            "689:\tlearn: 0.0990583\ttotal: 38.6s\tremaining: 17.3s\n",
            "690:\tlearn: 0.0990496\ttotal: 38.6s\tremaining: 17.3s\n",
            "691:\tlearn: 0.0989995\ttotal: 38.7s\tremaining: 17.2s\n",
            "692:\tlearn: 0.0989710\ttotal: 38.7s\tremaining: 17.2s\n",
            "693:\tlearn: 0.0989421\ttotal: 38.8s\tremaining: 17.1s\n",
            "694:\tlearn: 0.0989367\ttotal: 38.8s\tremaining: 17s\n",
            "695:\tlearn: 0.0989366\ttotal: 38.9s\tremaining: 17s\n",
            "696:\tlearn: 0.0989198\ttotal: 38.9s\tremaining: 16.9s\n",
            "697:\tlearn: 0.0988741\ttotal: 39s\tremaining: 16.9s\n",
            "698:\tlearn: 0.0988375\ttotal: 39s\tremaining: 16.8s\n",
            "699:\tlearn: 0.0987848\ttotal: 39.1s\tremaining: 16.7s\n",
            "700:\tlearn: 0.0987109\ttotal: 39.1s\tremaining: 16.7s\n",
            "701:\tlearn: 0.0986693\ttotal: 39.2s\tremaining: 16.6s\n",
            "702:\tlearn: 0.0986321\ttotal: 39.2s\tremaining: 16.6s\n",
            "703:\tlearn: 0.0985969\ttotal: 39.3s\tremaining: 16.5s\n",
            "704:\tlearn: 0.0985729\ttotal: 39.3s\tremaining: 16.5s\n",
            "705:\tlearn: 0.0985342\ttotal: 39.4s\tremaining: 16.4s\n",
            "706:\tlearn: 0.0984997\ttotal: 39.4s\tremaining: 16.3s\n",
            "707:\tlearn: 0.0984656\ttotal: 39.5s\tremaining: 16.3s\n",
            "708:\tlearn: 0.0984190\ttotal: 39.5s\tremaining: 16.2s\n",
            "709:\tlearn: 0.0983941\ttotal: 39.6s\tremaining: 16.2s\n",
            "710:\tlearn: 0.0983600\ttotal: 39.6s\tremaining: 16.1s\n",
            "711:\tlearn: 0.0983359\ttotal: 39.7s\tremaining: 16s\n",
            "712:\tlearn: 0.0983094\ttotal: 39.7s\tremaining: 16s\n",
            "713:\tlearn: 0.0982877\ttotal: 39.8s\tremaining: 15.9s\n",
            "714:\tlearn: 0.0982634\ttotal: 39.8s\tremaining: 15.9s\n",
            "715:\tlearn: 0.0982411\ttotal: 39.9s\tremaining: 15.8s\n",
            "716:\tlearn: 0.0982190\ttotal: 39.9s\tremaining: 15.8s\n",
            "717:\tlearn: 0.0982071\ttotal: 40s\tremaining: 15.7s\n",
            "718:\tlearn: 0.0981501\ttotal: 40s\tremaining: 15.6s\n",
            "719:\tlearn: 0.0981080\ttotal: 40.1s\tremaining: 15.6s\n",
            "720:\tlearn: 0.0980746\ttotal: 40.1s\tremaining: 15.5s\n",
            "721:\tlearn: 0.0980629\ttotal: 40.2s\tremaining: 15.5s\n",
            "722:\tlearn: 0.0980488\ttotal: 40.2s\tremaining: 15.4s\n",
            "723:\tlearn: 0.0980037\ttotal: 40.3s\tremaining: 15.3s\n",
            "724:\tlearn: 0.0979406\ttotal: 40.3s\tremaining: 15.3s\n",
            "725:\tlearn: 0.0979004\ttotal: 40.4s\tremaining: 15.2s\n",
            "726:\tlearn: 0.0978659\ttotal: 40.4s\tremaining: 15.2s\n",
            "727:\tlearn: 0.0978283\ttotal: 40.5s\tremaining: 15.1s\n",
            "728:\tlearn: 0.0978095\ttotal: 40.5s\tremaining: 15.1s\n",
            "729:\tlearn: 0.0978095\ttotal: 40.6s\tremaining: 15s\n",
            "730:\tlearn: 0.0977969\ttotal: 40.6s\tremaining: 14.9s\n",
            "731:\tlearn: 0.0977969\ttotal: 40.7s\tremaining: 14.9s\n",
            "732:\tlearn: 0.0977773\ttotal: 40.7s\tremaining: 14.8s\n",
            "733:\tlearn: 0.0977679\ttotal: 40.8s\tremaining: 14.8s\n",
            "734:\tlearn: 0.0977372\ttotal: 40.8s\tremaining: 14.7s\n",
            "735:\tlearn: 0.0976983\ttotal: 40.9s\tremaining: 14.7s\n",
            "736:\tlearn: 0.0976624\ttotal: 40.9s\tremaining: 14.6s\n",
            "737:\tlearn: 0.0976422\ttotal: 40.9s\tremaining: 14.5s\n",
            "738:\tlearn: 0.0976030\ttotal: 41s\tremaining: 14.5s\n",
            "739:\tlearn: 0.0976027\ttotal: 41s\tremaining: 14.4s\n",
            "740:\tlearn: 0.0975939\ttotal: 41.1s\tremaining: 14.4s\n",
            "741:\tlearn: 0.0975938\ttotal: 41.2s\tremaining: 14.3s\n",
            "742:\tlearn: 0.0975720\ttotal: 41.2s\tremaining: 14.3s\n",
            "743:\tlearn: 0.0975547\ttotal: 41.3s\tremaining: 14.2s\n",
            "744:\tlearn: 0.0975523\ttotal: 41.3s\tremaining: 14.1s\n",
            "745:\tlearn: 0.0975316\ttotal: 41.4s\tremaining: 14.1s\n",
            "746:\tlearn: 0.0975042\ttotal: 41.4s\tremaining: 14s\n",
            "747:\tlearn: 0.0974856\ttotal: 41.5s\tremaining: 14s\n",
            "748:\tlearn: 0.0974407\ttotal: 41.5s\tremaining: 13.9s\n",
            "749:\tlearn: 0.0974120\ttotal: 41.5s\tremaining: 13.8s\n",
            "750:\tlearn: 0.0973724\ttotal: 41.6s\tremaining: 13.8s\n",
            "751:\tlearn: 0.0973629\ttotal: 41.6s\tremaining: 13.7s\n",
            "752:\tlearn: 0.0973370\ttotal: 41.7s\tremaining: 13.7s\n",
            "753:\tlearn: 0.0973250\ttotal: 41.7s\tremaining: 13.6s\n",
            "754:\tlearn: 0.0973062\ttotal: 41.8s\tremaining: 13.6s\n",
            "755:\tlearn: 0.0972660\ttotal: 41.8s\tremaining: 13.5s\n",
            "756:\tlearn: 0.0972427\ttotal: 41.9s\tremaining: 13.4s\n",
            "757:\tlearn: 0.0972294\ttotal: 41.9s\tremaining: 13.4s\n",
            "758:\tlearn: 0.0972274\ttotal: 42s\tremaining: 13.3s\n",
            "759:\tlearn: 0.0972037\ttotal: 42s\tremaining: 13.3s\n",
            "760:\tlearn: 0.0971875\ttotal: 42.1s\tremaining: 13.2s\n",
            "761:\tlearn: 0.0971655\ttotal: 42.1s\tremaining: 13.2s\n",
            "762:\tlearn: 0.0971277\ttotal: 42.2s\tremaining: 13.1s\n",
            "763:\tlearn: 0.0971008\ttotal: 42.2s\tremaining: 13s\n",
            "764:\tlearn: 0.0970474\ttotal: 42.3s\tremaining: 13s\n",
            "765:\tlearn: 0.0970218\ttotal: 42.3s\tremaining: 12.9s\n",
            "766:\tlearn: 0.0969990\ttotal: 42.4s\tremaining: 12.9s\n",
            "767:\tlearn: 0.0969751\ttotal: 42.4s\tremaining: 12.8s\n",
            "768:\tlearn: 0.0969556\ttotal: 42.5s\tremaining: 12.8s\n",
            "769:\tlearn: 0.0969557\ttotal: 42.5s\tremaining: 12.7s\n",
            "770:\tlearn: 0.0969440\ttotal: 42.6s\tremaining: 12.6s\n",
            "771:\tlearn: 0.0969332\ttotal: 42.6s\tremaining: 12.6s\n",
            "772:\tlearn: 0.0969304\ttotal: 42.7s\tremaining: 12.5s\n",
            "773:\tlearn: 0.0969260\ttotal: 42.7s\tremaining: 12.5s\n",
            "774:\tlearn: 0.0969101\ttotal: 42.8s\tremaining: 12.4s\n",
            "775:\tlearn: 0.0968773\ttotal: 42.8s\tremaining: 12.4s\n",
            "776:\tlearn: 0.0968606\ttotal: 42.9s\tremaining: 12.3s\n",
            "777:\tlearn: 0.0968415\ttotal: 42.9s\tremaining: 12.2s\n",
            "778:\tlearn: 0.0968271\ttotal: 42.9s\tremaining: 12.2s\n",
            "779:\tlearn: 0.0967776\ttotal: 43s\tremaining: 12.1s\n",
            "780:\tlearn: 0.0967522\ttotal: 43s\tremaining: 12.1s\n",
            "781:\tlearn: 0.0967090\ttotal: 43.1s\tremaining: 12s\n",
            "782:\tlearn: 0.0966962\ttotal: 43.2s\tremaining: 12s\n",
            "783:\tlearn: 0.0966720\ttotal: 43.2s\tremaining: 11.9s\n",
            "784:\tlearn: 0.0966368\ttotal: 43.2s\tremaining: 11.8s\n",
            "785:\tlearn: 0.0966317\ttotal: 43.3s\tremaining: 11.8s\n",
            "786:\tlearn: 0.0966222\ttotal: 43.3s\tremaining: 11.7s\n",
            "787:\tlearn: 0.0966207\ttotal: 43.4s\tremaining: 11.7s\n",
            "788:\tlearn: 0.0966143\ttotal: 43.4s\tremaining: 11.6s\n",
            "789:\tlearn: 0.0965962\ttotal: 43.5s\tremaining: 11.6s\n",
            "790:\tlearn: 0.0965625\ttotal: 43.5s\tremaining: 11.5s\n",
            "791:\tlearn: 0.0965400\ttotal: 43.6s\tremaining: 11.4s\n",
            "792:\tlearn: 0.0965194\ttotal: 43.6s\tremaining: 11.4s\n",
            "793:\tlearn: 0.0965102\ttotal: 43.7s\tremaining: 11.3s\n",
            "794:\tlearn: 0.0965081\ttotal: 43.7s\tremaining: 11.3s\n",
            "795:\tlearn: 0.0964900\ttotal: 43.8s\tremaining: 11.2s\n",
            "796:\tlearn: 0.0964883\ttotal: 43.8s\tremaining: 11.2s\n",
            "797:\tlearn: 0.0964857\ttotal: 43.9s\tremaining: 11.1s\n",
            "798:\tlearn: 0.0964597\ttotal: 43.9s\tremaining: 11s\n",
            "799:\tlearn: 0.0964464\ttotal: 44s\tremaining: 11s\n",
            "800:\tlearn: 0.0964237\ttotal: 44s\tremaining: 10.9s\n",
            "801:\tlearn: 0.0963747\ttotal: 44.1s\tremaining: 10.9s\n",
            "802:\tlearn: 0.0963541\ttotal: 44.1s\tremaining: 10.8s\n",
            "803:\tlearn: 0.0963541\ttotal: 44.2s\tremaining: 10.8s\n",
            "804:\tlearn: 0.0963402\ttotal: 44.2s\tremaining: 10.7s\n",
            "805:\tlearn: 0.0962882\ttotal: 44.3s\tremaining: 10.7s\n",
            "806:\tlearn: 0.0962572\ttotal: 44.3s\tremaining: 10.6s\n",
            "807:\tlearn: 0.0962292\ttotal: 44.4s\tremaining: 10.5s\n",
            "808:\tlearn: 0.0961925\ttotal: 44.4s\tremaining: 10.5s\n",
            "809:\tlearn: 0.0961742\ttotal: 44.5s\tremaining: 10.4s\n",
            "810:\tlearn: 0.0961251\ttotal: 44.5s\tremaining: 10.4s\n",
            "811:\tlearn: 0.0960846\ttotal: 44.6s\tremaining: 10.3s\n",
            "812:\tlearn: 0.0960506\ttotal: 44.6s\tremaining: 10.3s\n",
            "813:\tlearn: 0.0960251\ttotal: 44.6s\tremaining: 10.2s\n",
            "814:\tlearn: 0.0960109\ttotal: 44.7s\tremaining: 10.1s\n",
            "815:\tlearn: 0.0959857\ttotal: 44.7s\tremaining: 10.1s\n",
            "816:\tlearn: 0.0959371\ttotal: 44.8s\tremaining: 10s\n",
            "817:\tlearn: 0.0958931\ttotal: 44.8s\tremaining: 9.97s\n",
            "818:\tlearn: 0.0958812\ttotal: 44.9s\tremaining: 9.92s\n",
            "819:\tlearn: 0.0958469\ttotal: 44.9s\tremaining: 9.86s\n",
            "820:\tlearn: 0.0958127\ttotal: 45s\tremaining: 9.81s\n",
            "821:\tlearn: 0.0957919\ttotal: 45s\tremaining: 9.75s\n",
            "822:\tlearn: 0.0957537\ttotal: 45.1s\tremaining: 9.7s\n",
            "823:\tlearn: 0.0957384\ttotal: 45.1s\tremaining: 9.64s\n",
            "824:\tlearn: 0.0957012\ttotal: 45.2s\tremaining: 9.59s\n",
            "825:\tlearn: 0.0956640\ttotal: 45.3s\tremaining: 9.54s\n",
            "826:\tlearn: 0.0956374\ttotal: 45.4s\tremaining: 9.49s\n",
            "827:\tlearn: 0.0956065\ttotal: 45.5s\tremaining: 9.44s\n",
            "828:\tlearn: 0.0955788\ttotal: 45.5s\tremaining: 9.39s\n",
            "829:\tlearn: 0.0955557\ttotal: 45.6s\tremaining: 9.34s\n",
            "830:\tlearn: 0.0955267\ttotal: 45.7s\tremaining: 9.3s\n",
            "831:\tlearn: 0.0955007\ttotal: 45.8s\tremaining: 9.25s\n",
            "832:\tlearn: 0.0954794\ttotal: 45.9s\tremaining: 9.2s\n",
            "833:\tlearn: 0.0954563\ttotal: 46s\tremaining: 9.15s\n",
            "834:\tlearn: 0.0954307\ttotal: 46.1s\tremaining: 9.1s\n",
            "835:\tlearn: 0.0954114\ttotal: 46.1s\tremaining: 9.05s\n",
            "836:\tlearn: 0.0953826\ttotal: 46.2s\tremaining: 9s\n",
            "837:\tlearn: 0.0953479\ttotal: 46.3s\tremaining: 8.95s\n",
            "838:\tlearn: 0.0953420\ttotal: 46.4s\tremaining: 8.9s\n",
            "839:\tlearn: 0.0953293\ttotal: 46.5s\tremaining: 8.86s\n",
            "840:\tlearn: 0.0953106\ttotal: 46.6s\tremaining: 8.81s\n",
            "841:\tlearn: 0.0952978\ttotal: 46.7s\tremaining: 8.76s\n",
            "842:\tlearn: 0.0952827\ttotal: 46.8s\tremaining: 8.71s\n",
            "843:\tlearn: 0.0952450\ttotal: 46.8s\tremaining: 8.66s\n",
            "844:\tlearn: 0.0952268\ttotal: 46.9s\tremaining: 8.61s\n",
            "845:\tlearn: 0.0952175\ttotal: 47s\tremaining: 8.56s\n",
            "846:\tlearn: 0.0952175\ttotal: 47.1s\tremaining: 8.51s\n",
            "847:\tlearn: 0.0952035\ttotal: 47.2s\tremaining: 8.46s\n",
            "848:\tlearn: 0.0951949\ttotal: 47.3s\tremaining: 8.41s\n",
            "849:\tlearn: 0.0951784\ttotal: 47.4s\tremaining: 8.36s\n",
            "850:\tlearn: 0.0951454\ttotal: 47.4s\tremaining: 8.3s\n",
            "851:\tlearn: 0.0951329\ttotal: 47.5s\tremaining: 8.26s\n",
            "852:\tlearn: 0.0951049\ttotal: 47.6s\tremaining: 8.2s\n",
            "853:\tlearn: 0.0951049\ttotal: 47.6s\tremaining: 8.14s\n",
            "854:\tlearn: 0.0950919\ttotal: 47.7s\tremaining: 8.09s\n",
            "855:\tlearn: 0.0950872\ttotal: 47.8s\tremaining: 8.04s\n",
            "856:\tlearn: 0.0950702\ttotal: 47.9s\tremaining: 7.99s\n",
            "857:\tlearn: 0.0950701\ttotal: 48s\tremaining: 7.94s\n",
            "858:\tlearn: 0.0950700\ttotal: 48.1s\tremaining: 7.89s\n",
            "859:\tlearn: 0.0950517\ttotal: 48.2s\tremaining: 7.84s\n",
            "860:\tlearn: 0.0950476\ttotal: 48.3s\tremaining: 7.79s\n",
            "861:\tlearn: 0.0950462\ttotal: 48.4s\tremaining: 7.74s\n",
            "862:\tlearn: 0.0950459\ttotal: 48.4s\tremaining: 7.69s\n",
            "863:\tlearn: 0.0950263\ttotal: 48.5s\tremaining: 7.64s\n",
            "864:\tlearn: 0.0950262\ttotal: 48.6s\tremaining: 7.59s\n",
            "865:\tlearn: 0.0950249\ttotal: 48.7s\tremaining: 7.54s\n",
            "866:\tlearn: 0.0949880\ttotal: 48.8s\tremaining: 7.49s\n",
            "867:\tlearn: 0.0949704\ttotal: 48.9s\tremaining: 7.43s\n",
            "868:\tlearn: 0.0949703\ttotal: 48.9s\tremaining: 7.38s\n",
            "869:\tlearn: 0.0949704\ttotal: 49s\tremaining: 7.32s\n",
            "870:\tlearn: 0.0949703\ttotal: 49s\tremaining: 7.26s\n",
            "871:\tlearn: 0.0949653\ttotal: 49.1s\tremaining: 7.2s\n",
            "872:\tlearn: 0.0949651\ttotal: 49.1s\tremaining: 7.15s\n",
            "873:\tlearn: 0.0949482\ttotal: 49.2s\tremaining: 7.09s\n",
            "874:\tlearn: 0.0949482\ttotal: 49.2s\tremaining: 7.03s\n",
            "875:\tlearn: 0.0949171\ttotal: 49.3s\tremaining: 6.97s\n",
            "876:\tlearn: 0.0949006\ttotal: 49.3s\tremaining: 6.92s\n",
            "877:\tlearn: 0.0948810\ttotal: 49.4s\tremaining: 6.86s\n",
            "878:\tlearn: 0.0948809\ttotal: 49.4s\tremaining: 6.8s\n",
            "879:\tlearn: 0.0948569\ttotal: 49.5s\tremaining: 6.75s\n",
            "880:\tlearn: 0.0948412\ttotal: 49.5s\tremaining: 6.69s\n",
            "881:\tlearn: 0.0948329\ttotal: 49.6s\tremaining: 6.63s\n",
            "882:\tlearn: 0.0948330\ttotal: 49.6s\tremaining: 6.58s\n",
            "883:\tlearn: 0.0948172\ttotal: 49.7s\tremaining: 6.52s\n",
            "884:\tlearn: 0.0947977\ttotal: 49.7s\tremaining: 6.46s\n",
            "885:\tlearn: 0.0947727\ttotal: 49.8s\tremaining: 6.41s\n",
            "886:\tlearn: 0.0947593\ttotal: 49.8s\tremaining: 6.35s\n",
            "887:\tlearn: 0.0947583\ttotal: 49.9s\tremaining: 6.29s\n",
            "888:\tlearn: 0.0947470\ttotal: 49.9s\tremaining: 6.23s\n",
            "889:\tlearn: 0.0947461\ttotal: 50s\tremaining: 6.18s\n",
            "890:\tlearn: 0.0947278\ttotal: 50s\tremaining: 6.12s\n",
            "891:\tlearn: 0.0947180\ttotal: 50.1s\tremaining: 6.06s\n",
            "892:\tlearn: 0.0946998\ttotal: 50.1s\tremaining: 6.01s\n",
            "893:\tlearn: 0.0946958\ttotal: 50.2s\tremaining: 5.95s\n",
            "894:\tlearn: 0.0946682\ttotal: 50.2s\tremaining: 5.89s\n",
            "895:\tlearn: 0.0946379\ttotal: 50.3s\tremaining: 5.83s\n",
            "896:\tlearn: 0.0946379\ttotal: 50.3s\tremaining: 5.77s\n",
            "897:\tlearn: 0.0946110\ttotal: 50.3s\tremaining: 5.71s\n",
            "898:\tlearn: 0.0945853\ttotal: 50.4s\tremaining: 5.66s\n",
            "899:\tlearn: 0.0945849\ttotal: 50.4s\tremaining: 5.6s\n",
            "900:\tlearn: 0.0945612\ttotal: 50.5s\tremaining: 5.55s\n",
            "901:\tlearn: 0.0945409\ttotal: 50.5s\tremaining: 5.49s\n",
            "902:\tlearn: 0.0945002\ttotal: 50.6s\tremaining: 5.43s\n",
            "903:\tlearn: 0.0944992\ttotal: 50.6s\tremaining: 5.38s\n",
            "904:\tlearn: 0.0944775\ttotal: 50.7s\tremaining: 5.32s\n",
            "905:\tlearn: 0.0944774\ttotal: 50.7s\tremaining: 5.26s\n",
            "906:\tlearn: 0.0944600\ttotal: 50.8s\tremaining: 5.21s\n",
            "907:\tlearn: 0.0944558\ttotal: 50.8s\tremaining: 5.15s\n",
            "908:\tlearn: 0.0944519\ttotal: 50.9s\tremaining: 5.09s\n",
            "909:\tlearn: 0.0944519\ttotal: 50.9s\tremaining: 5.03s\n",
            "910:\tlearn: 0.0944367\ttotal: 50.9s\tremaining: 4.97s\n",
            "911:\tlearn: 0.0943899\ttotal: 51s\tremaining: 4.92s\n",
            "912:\tlearn: 0.0943637\ttotal: 51s\tremaining: 4.86s\n",
            "913:\tlearn: 0.0943636\ttotal: 51.1s\tremaining: 4.81s\n",
            "914:\tlearn: 0.0943636\ttotal: 51.1s\tremaining: 4.75s\n",
            "915:\tlearn: 0.0943637\ttotal: 51.2s\tremaining: 4.69s\n",
            "916:\tlearn: 0.0943570\ttotal: 51.2s\tremaining: 4.63s\n",
            "917:\tlearn: 0.0943204\ttotal: 51.3s\tremaining: 4.58s\n",
            "918:\tlearn: 0.0943059\ttotal: 51.3s\tremaining: 4.52s\n",
            "919:\tlearn: 0.0943058\ttotal: 51.4s\tremaining: 4.47s\n",
            "920:\tlearn: 0.0943030\ttotal: 51.4s\tremaining: 4.41s\n",
            "921:\tlearn: 0.0943029\ttotal: 51.5s\tremaining: 4.36s\n",
            "922:\tlearn: 0.0942873\ttotal: 51.5s\tremaining: 4.3s\n",
            "923:\tlearn: 0.0942744\ttotal: 51.6s\tremaining: 4.24s\n",
            "924:\tlearn: 0.0942521\ttotal: 51.6s\tremaining: 4.18s\n",
            "925:\tlearn: 0.0942520\ttotal: 51.7s\tremaining: 4.13s\n",
            "926:\tlearn: 0.0942521\ttotal: 51.7s\tremaining: 4.07s\n",
            "927:\tlearn: 0.0942305\ttotal: 51.8s\tremaining: 4.02s\n",
            "928:\tlearn: 0.0942148\ttotal: 51.8s\tremaining: 3.96s\n",
            "929:\tlearn: 0.0941983\ttotal: 51.9s\tremaining: 3.9s\n",
            "930:\tlearn: 0.0941787\ttotal: 51.9s\tremaining: 3.85s\n",
            "931:\tlearn: 0.0941716\ttotal: 52s\tremaining: 3.79s\n",
            "932:\tlearn: 0.0941710\ttotal: 52s\tremaining: 3.73s\n",
            "933:\tlearn: 0.0941641\ttotal: 52.1s\tremaining: 3.68s\n",
            "934:\tlearn: 0.0941476\ttotal: 52.1s\tremaining: 3.62s\n",
            "935:\tlearn: 0.0941476\ttotal: 52.1s\tremaining: 3.56s\n",
            "936:\tlearn: 0.0941461\ttotal: 52.2s\tremaining: 3.51s\n",
            "937:\tlearn: 0.0941460\ttotal: 52.2s\tremaining: 3.45s\n",
            "938:\tlearn: 0.0941433\ttotal: 52.3s\tremaining: 3.4s\n",
            "939:\tlearn: 0.0941138\ttotal: 52.3s\tremaining: 3.34s\n",
            "940:\tlearn: 0.0941126\ttotal: 52.4s\tremaining: 3.28s\n",
            "941:\tlearn: 0.0940960\ttotal: 52.5s\tremaining: 3.23s\n",
            "942:\tlearn: 0.0940959\ttotal: 52.5s\tremaining: 3.17s\n",
            "943:\tlearn: 0.0940761\ttotal: 52.6s\tremaining: 3.12s\n",
            "944:\tlearn: 0.0940761\ttotal: 52.6s\tremaining: 3.06s\n",
            "945:\tlearn: 0.0940692\ttotal: 52.6s\tremaining: 3s\n",
            "946:\tlearn: 0.0940692\ttotal: 52.7s\tremaining: 2.95s\n",
            "947:\tlearn: 0.0940691\ttotal: 52.8s\tremaining: 2.89s\n",
            "948:\tlearn: 0.0940691\ttotal: 52.8s\tremaining: 2.84s\n",
            "949:\tlearn: 0.0940563\ttotal: 52.8s\tremaining: 2.78s\n",
            "950:\tlearn: 0.0940563\ttotal: 52.9s\tremaining: 2.73s\n",
            "951:\tlearn: 0.0940562\ttotal: 52.9s\tremaining: 2.67s\n",
            "952:\tlearn: 0.0940505\ttotal: 53s\tremaining: 2.61s\n",
            "953:\tlearn: 0.0940348\ttotal: 53s\tremaining: 2.56s\n",
            "954:\tlearn: 0.0940346\ttotal: 53.1s\tremaining: 2.5s\n",
            "955:\tlearn: 0.0940330\ttotal: 53.1s\tremaining: 2.44s\n",
            "956:\tlearn: 0.0940328\ttotal: 53.2s\tremaining: 2.39s\n",
            "957:\tlearn: 0.0940182\ttotal: 53.2s\tremaining: 2.33s\n",
            "958:\tlearn: 0.0940160\ttotal: 53.3s\tremaining: 2.28s\n",
            "959:\tlearn: 0.0940159\ttotal: 53.3s\tremaining: 2.22s\n",
            "960:\tlearn: 0.0939979\ttotal: 53.4s\tremaining: 2.17s\n",
            "961:\tlearn: 0.0939809\ttotal: 53.4s\tremaining: 2.11s\n",
            "962:\tlearn: 0.0939793\ttotal: 53.5s\tremaining: 2.05s\n",
            "963:\tlearn: 0.0939696\ttotal: 53.5s\tremaining: 2s\n",
            "964:\tlearn: 0.0939520\ttotal: 53.6s\tremaining: 1.94s\n",
            "965:\tlearn: 0.0939519\ttotal: 53.6s\tremaining: 1.89s\n",
            "966:\tlearn: 0.0939409\ttotal: 53.7s\tremaining: 1.83s\n",
            "967:\tlearn: 0.0939374\ttotal: 53.7s\tremaining: 1.78s\n",
            "968:\tlearn: 0.0939374\ttotal: 53.8s\tremaining: 1.72s\n",
            "969:\tlearn: 0.0939238\ttotal: 53.8s\tremaining: 1.66s\n",
            "970:\tlearn: 0.0939108\ttotal: 53.9s\tremaining: 1.61s\n",
            "971:\tlearn: 0.0938875\ttotal: 53.9s\tremaining: 1.55s\n",
            "972:\tlearn: 0.0938659\ttotal: 54s\tremaining: 1.5s\n",
            "973:\tlearn: 0.0938423\ttotal: 54s\tremaining: 1.44s\n",
            "974:\tlearn: 0.0938241\ttotal: 54.1s\tremaining: 1.39s\n",
            "975:\tlearn: 0.0938144\ttotal: 54.1s\tremaining: 1.33s\n",
            "976:\tlearn: 0.0938120\ttotal: 54.2s\tremaining: 1.27s\n",
            "977:\tlearn: 0.0937959\ttotal: 54.2s\tremaining: 1.22s\n",
            "978:\tlearn: 0.0937678\ttotal: 54.3s\tremaining: 1.16s\n",
            "979:\tlearn: 0.0937459\ttotal: 54.3s\tremaining: 1.11s\n",
            "980:\tlearn: 0.0937459\ttotal: 54.4s\tremaining: 1.05s\n",
            "981:\tlearn: 0.0937418\ttotal: 54.4s\tremaining: 997ms\n",
            "982:\tlearn: 0.0937314\ttotal: 54.5s\tremaining: 942ms\n",
            "983:\tlearn: 0.0937178\ttotal: 54.5s\tremaining: 886ms\n",
            "984:\tlearn: 0.0937179\ttotal: 54.6s\tremaining: 831ms\n",
            "985:\tlearn: 0.0937172\ttotal: 54.6s\tremaining: 776ms\n",
            "986:\tlearn: 0.0937172\ttotal: 54.7s\tremaining: 720ms\n",
            "987:\tlearn: 0.0937155\ttotal: 54.7s\tremaining: 665ms\n",
            "988:\tlearn: 0.0937032\ttotal: 54.8s\tremaining: 609ms\n",
            "989:\tlearn: 0.0936874\ttotal: 54.8s\tremaining: 554ms\n",
            "990:\tlearn: 0.0936721\ttotal: 54.9s\tremaining: 498ms\n",
            "991:\tlearn: 0.0936583\ttotal: 54.9s\tremaining: 443ms\n",
            "992:\tlearn: 0.0936442\ttotal: 55s\tremaining: 387ms\n",
            "993:\tlearn: 0.0936184\ttotal: 55s\tremaining: 332ms\n",
            "994:\tlearn: 0.0936143\ttotal: 55.1s\tremaining: 277ms\n",
            "995:\tlearn: 0.0936143\ttotal: 55.1s\tremaining: 221ms\n",
            "996:\tlearn: 0.0936059\ttotal: 55.1s\tremaining: 166ms\n",
            "997:\tlearn: 0.0936059\ttotal: 55.2s\tremaining: 111ms\n",
            "998:\tlearn: 0.0936005\ttotal: 55.2s\tremaining: 55.3ms\n",
            "999:\tlearn: 0.0935915\ttotal: 55.3s\tremaining: 0us\n",
            "0:\tlearn: 0.5935827\ttotal: 46ms\tremaining: 46s\n",
            "1:\tlearn: 0.5328525\ttotal: 113ms\tremaining: 56.5s\n",
            "2:\tlearn: 0.4883209\ttotal: 168ms\tremaining: 55.7s\n",
            "3:\tlearn: 0.4662972\ttotal: 215ms\tremaining: 53.6s\n",
            "4:\tlearn: 0.4500122\ttotal: 261ms\tremaining: 52s\n",
            "5:\tlearn: 0.4343461\ttotal: 316ms\tremaining: 52.3s\n",
            "6:\tlearn: 0.4230211\ttotal: 372ms\tremaining: 52.8s\n",
            "7:\tlearn: 0.4180346\ttotal: 419ms\tremaining: 51.9s\n",
            "8:\tlearn: 0.4145861\ttotal: 469ms\tremaining: 51.7s\n",
            "9:\tlearn: 0.4063593\ttotal: 517ms\tremaining: 51.2s\n",
            "10:\tlearn: 0.4014308\ttotal: 572ms\tremaining: 51.4s\n",
            "11:\tlearn: 0.3945279\ttotal: 619ms\tremaining: 51s\n",
            "12:\tlearn: 0.3880268\ttotal: 666ms\tremaining: 50.5s\n",
            "13:\tlearn: 0.3841584\ttotal: 711ms\tremaining: 50.1s\n",
            "14:\tlearn: 0.3818376\ttotal: 765ms\tremaining: 50.2s\n",
            "15:\tlearn: 0.3800461\ttotal: 815ms\tremaining: 50.2s\n",
            "16:\tlearn: 0.3766522\ttotal: 864ms\tremaining: 50s\n",
            "17:\tlearn: 0.3716890\ttotal: 911ms\tremaining: 49.7s\n",
            "18:\tlearn: 0.3669392\ttotal: 958ms\tremaining: 49.5s\n",
            "19:\tlearn: 0.3646812\ttotal: 1.01s\tremaining: 49.6s\n",
            "20:\tlearn: 0.3612996\ttotal: 1.06s\tremaining: 49.3s\n",
            "21:\tlearn: 0.3578192\ttotal: 1.1s\tremaining: 49.2s\n",
            "22:\tlearn: 0.3565273\ttotal: 1.17s\tremaining: 49.8s\n",
            "23:\tlearn: 0.3538459\ttotal: 1.22s\tremaining: 49.8s\n",
            "24:\tlearn: 0.3532299\ttotal: 1.27s\tremaining: 49.7s\n",
            "25:\tlearn: 0.3496939\ttotal: 1.32s\tremaining: 49.5s\n",
            "26:\tlearn: 0.3486361\ttotal: 1.37s\tremaining: 49.3s\n",
            "27:\tlearn: 0.3467697\ttotal: 1.41s\tremaining: 49.1s\n",
            "28:\tlearn: 0.3458125\ttotal: 1.48s\tremaining: 49.4s\n",
            "29:\tlearn: 0.3424148\ttotal: 1.52s\tremaining: 49.3s\n",
            "30:\tlearn: 0.3390442\ttotal: 1.57s\tremaining: 49.1s\n",
            "31:\tlearn: 0.3355928\ttotal: 1.62s\tremaining: 49s\n",
            "32:\tlearn: 0.3348900\ttotal: 1.67s\tremaining: 48.9s\n",
            "33:\tlearn: 0.3326544\ttotal: 1.72s\tremaining: 49s\n",
            "34:\tlearn: 0.3306511\ttotal: 1.77s\tremaining: 48.8s\n",
            "35:\tlearn: 0.3277375\ttotal: 1.82s\tremaining: 48.7s\n",
            "36:\tlearn: 0.3272564\ttotal: 1.87s\tremaining: 48.7s\n",
            "37:\tlearn: 0.3255678\ttotal: 1.92s\tremaining: 48.5s\n",
            "38:\tlearn: 0.3223457\ttotal: 1.97s\tremaining: 48.6s\n",
            "39:\tlearn: 0.3209635\ttotal: 2.02s\tremaining: 48.5s\n",
            "40:\tlearn: 0.3168347\ttotal: 2.06s\tremaining: 48.3s\n",
            "41:\tlearn: 0.3159614\ttotal: 2.11s\tremaining: 48.2s\n",
            "42:\tlearn: 0.3129356\ttotal: 2.16s\tremaining: 48.1s\n",
            "43:\tlearn: 0.3113257\ttotal: 2.23s\tremaining: 48.5s\n",
            "44:\tlearn: 0.3096483\ttotal: 2.27s\tremaining: 48.3s\n",
            "45:\tlearn: 0.3083222\ttotal: 2.32s\tremaining: 48.1s\n",
            "46:\tlearn: 0.3073177\ttotal: 2.37s\tremaining: 48.1s\n",
            "47:\tlearn: 0.3046690\ttotal: 2.42s\tremaining: 48s\n",
            "48:\tlearn: 0.3018626\ttotal: 2.48s\tremaining: 48.2s\n",
            "49:\tlearn: 0.2996855\ttotal: 2.53s\tremaining: 48s\n",
            "50:\tlearn: 0.2974596\ttotal: 2.57s\tremaining: 47.9s\n",
            "51:\tlearn: 0.2945766\ttotal: 2.62s\tremaining: 47.8s\n",
            "52:\tlearn: 0.2916684\ttotal: 2.67s\tremaining: 47.7s\n",
            "53:\tlearn: 0.2908164\ttotal: 2.72s\tremaining: 47.7s\n",
            "54:\tlearn: 0.2906264\ttotal: 2.77s\tremaining: 47.5s\n",
            "55:\tlearn: 0.2886514\ttotal: 2.81s\tremaining: 47.4s\n",
            "56:\tlearn: 0.2876355\ttotal: 2.86s\tremaining: 47.4s\n",
            "57:\tlearn: 0.2866120\ttotal: 2.91s\tremaining: 47.2s\n",
            "58:\tlearn: 0.2860015\ttotal: 2.96s\tremaining: 47.3s\n",
            "59:\tlearn: 0.2852399\ttotal: 3.01s\tremaining: 47.2s\n",
            "60:\tlearn: 0.2834256\ttotal: 3.06s\tremaining: 47.1s\n",
            "61:\tlearn: 0.2803630\ttotal: 3.11s\tremaining: 47s\n",
            "62:\tlearn: 0.2788367\ttotal: 3.15s\tremaining: 46.9s\n",
            "63:\tlearn: 0.2771714\ttotal: 3.23s\tremaining: 47.2s\n",
            "64:\tlearn: 0.2752061\ttotal: 3.28s\tremaining: 47.2s\n",
            "65:\tlearn: 0.2741471\ttotal: 3.32s\tremaining: 47s\n",
            "66:\tlearn: 0.2713889\ttotal: 3.37s\tremaining: 46.9s\n",
            "67:\tlearn: 0.2702886\ttotal: 3.44s\tremaining: 47.1s\n",
            "68:\tlearn: 0.2698153\ttotal: 3.52s\tremaining: 47.6s\n",
            "69:\tlearn: 0.2682117\ttotal: 3.62s\tremaining: 48s\n",
            "70:\tlearn: 0.2676893\ttotal: 3.69s\tremaining: 48.3s\n",
            "71:\tlearn: 0.2647046\ttotal: 3.76s\tremaining: 48.5s\n",
            "72:\tlearn: 0.2628936\ttotal: 3.85s\tremaining: 48.9s\n",
            "73:\tlearn: 0.2592498\ttotal: 3.95s\tremaining: 49.4s\n",
            "74:\tlearn: 0.2578951\ttotal: 4.04s\tremaining: 49.8s\n",
            "75:\tlearn: 0.2562373\ttotal: 4.13s\tremaining: 50.2s\n",
            "76:\tlearn: 0.2551810\ttotal: 4.24s\tremaining: 50.8s\n",
            "77:\tlearn: 0.2542007\ttotal: 4.33s\tremaining: 51.1s\n",
            "78:\tlearn: 0.2538894\ttotal: 4.41s\tremaining: 51.4s\n",
            "79:\tlearn: 0.2528795\ttotal: 4.5s\tremaining: 51.8s\n",
            "80:\tlearn: 0.2516682\ttotal: 4.59s\tremaining: 52.1s\n",
            "81:\tlearn: 0.2513933\ttotal: 4.67s\tremaining: 52.3s\n",
            "82:\tlearn: 0.2507376\ttotal: 4.77s\tremaining: 52.7s\n",
            "83:\tlearn: 0.2484628\ttotal: 4.85s\tremaining: 52.9s\n",
            "84:\tlearn: 0.2471982\ttotal: 4.94s\tremaining: 53.2s\n",
            "85:\tlearn: 0.2442652\ttotal: 5.04s\tremaining: 53.6s\n",
            "86:\tlearn: 0.2429680\ttotal: 5.11s\tremaining: 53.6s\n",
            "87:\tlearn: 0.2424694\ttotal: 5.19s\tremaining: 53.8s\n",
            "88:\tlearn: 0.2419714\ttotal: 5.3s\tremaining: 54.3s\n",
            "89:\tlearn: 0.2406492\ttotal: 5.38s\tremaining: 54.4s\n",
            "90:\tlearn: 0.2388278\ttotal: 5.46s\tremaining: 54.5s\n",
            "91:\tlearn: 0.2373215\ttotal: 5.55s\tremaining: 54.8s\n",
            "92:\tlearn: 0.2346819\ttotal: 5.64s\tremaining: 55s\n",
            "93:\tlearn: 0.2330535\ttotal: 5.73s\tremaining: 55.2s\n",
            "94:\tlearn: 0.2323320\ttotal: 5.81s\tremaining: 55.4s\n",
            "95:\tlearn: 0.2319672\ttotal: 5.89s\tremaining: 55.5s\n",
            "96:\tlearn: 0.2318800\ttotal: 5.98s\tremaining: 55.7s\n",
            "97:\tlearn: 0.2296914\ttotal: 6.07s\tremaining: 55.9s\n",
            "98:\tlearn: 0.2273568\ttotal: 6.16s\tremaining: 56.1s\n",
            "99:\tlearn: 0.2260163\ttotal: 6.25s\tremaining: 56.3s\n",
            "100:\tlearn: 0.2250499\ttotal: 6.34s\tremaining: 56.4s\n",
            "101:\tlearn: 0.2248135\ttotal: 6.43s\tremaining: 56.6s\n",
            "102:\tlearn: 0.2227564\ttotal: 6.52s\tremaining: 56.8s\n",
            "103:\tlearn: 0.2216910\ttotal: 6.61s\tremaining: 57s\n",
            "104:\tlearn: 0.2205252\ttotal: 6.69s\tremaining: 57s\n",
            "105:\tlearn: 0.2197243\ttotal: 6.79s\tremaining: 57.2s\n",
            "106:\tlearn: 0.2189744\ttotal: 6.89s\tremaining: 57.5s\n",
            "107:\tlearn: 0.2183697\ttotal: 6.96s\tremaining: 57.5s\n",
            "108:\tlearn: 0.2168536\ttotal: 7.01s\tremaining: 57.3s\n",
            "109:\tlearn: 0.2158081\ttotal: 7.06s\tremaining: 57.1s\n",
            "110:\tlearn: 0.2150066\ttotal: 7.12s\tremaining: 57s\n",
            "111:\tlearn: 0.2148936\ttotal: 7.17s\tremaining: 56.8s\n",
            "112:\tlearn: 0.2136965\ttotal: 7.21s\tremaining: 56.6s\n",
            "113:\tlearn: 0.2127389\ttotal: 7.26s\tremaining: 56.4s\n",
            "114:\tlearn: 0.2126462\ttotal: 7.3s\tremaining: 56.2s\n",
            "115:\tlearn: 0.2116539\ttotal: 7.36s\tremaining: 56.1s\n",
            "116:\tlearn: 0.2100320\ttotal: 7.42s\tremaining: 56s\n",
            "117:\tlearn: 0.2090780\ttotal: 7.47s\tremaining: 55.8s\n",
            "118:\tlearn: 0.2089403\ttotal: 7.51s\tremaining: 55.6s\n",
            "119:\tlearn: 0.2078788\ttotal: 7.56s\tremaining: 55.5s\n",
            "120:\tlearn: 0.2075094\ttotal: 7.62s\tremaining: 55.3s\n",
            "121:\tlearn: 0.2068788\ttotal: 7.67s\tremaining: 55.2s\n",
            "122:\tlearn: 0.2060636\ttotal: 7.71s\tremaining: 55s\n",
            "123:\tlearn: 0.2056337\ttotal: 7.76s\tremaining: 54.8s\n",
            "124:\tlearn: 0.2042432\ttotal: 7.81s\tremaining: 54.7s\n",
            "125:\tlearn: 0.2033972\ttotal: 7.86s\tremaining: 54.5s\n",
            "126:\tlearn: 0.2025302\ttotal: 7.91s\tremaining: 54.4s\n",
            "127:\tlearn: 0.2011790\ttotal: 7.96s\tremaining: 54.2s\n",
            "128:\tlearn: 0.1997968\ttotal: 8s\tremaining: 54s\n",
            "129:\tlearn: 0.1990245\ttotal: 8.05s\tremaining: 53.9s\n",
            "130:\tlearn: 0.1984678\ttotal: 8.11s\tremaining: 53.8s\n",
            "131:\tlearn: 0.1979882\ttotal: 8.16s\tremaining: 53.7s\n",
            "132:\tlearn: 0.1971027\ttotal: 8.21s\tremaining: 53.5s\n",
            "133:\tlearn: 0.1959648\ttotal: 8.26s\tremaining: 53.4s\n",
            "134:\tlearn: 0.1950054\ttotal: 8.3s\tremaining: 53.2s\n",
            "135:\tlearn: 0.1938319\ttotal: 8.36s\tremaining: 53.1s\n",
            "136:\tlearn: 0.1932007\ttotal: 8.42s\tremaining: 53.1s\n",
            "137:\tlearn: 0.1922077\ttotal: 8.47s\tremaining: 52.9s\n",
            "138:\tlearn: 0.1911311\ttotal: 8.52s\tremaining: 52.7s\n",
            "139:\tlearn: 0.1910721\ttotal: 8.57s\tremaining: 52.6s\n",
            "140:\tlearn: 0.1902336\ttotal: 8.62s\tremaining: 52.5s\n",
            "141:\tlearn: 0.1898999\ttotal: 8.67s\tremaining: 52.4s\n",
            "142:\tlearn: 0.1892369\ttotal: 8.71s\tremaining: 52.2s\n",
            "143:\tlearn: 0.1882655\ttotal: 8.76s\tremaining: 52.1s\n",
            "144:\tlearn: 0.1876720\ttotal: 8.82s\tremaining: 52s\n",
            "145:\tlearn: 0.1869906\ttotal: 8.86s\tremaining: 51.8s\n",
            "146:\tlearn: 0.1858828\ttotal: 8.91s\tremaining: 51.7s\n",
            "147:\tlearn: 0.1850881\ttotal: 8.96s\tremaining: 51.6s\n",
            "148:\tlearn: 0.1839337\ttotal: 9s\tremaining: 51.4s\n",
            "149:\tlearn: 0.1832343\ttotal: 9.06s\tremaining: 51.3s\n",
            "150:\tlearn: 0.1825639\ttotal: 9.11s\tremaining: 51.2s\n",
            "151:\tlearn: 0.1820640\ttotal: 9.16s\tremaining: 51.1s\n",
            "152:\tlearn: 0.1814230\ttotal: 9.2s\tremaining: 51s\n",
            "153:\tlearn: 0.1811168\ttotal: 9.25s\tremaining: 50.8s\n",
            "154:\tlearn: 0.1802926\ttotal: 9.31s\tremaining: 50.7s\n",
            "155:\tlearn: 0.1797463\ttotal: 9.35s\tremaining: 50.6s\n",
            "156:\tlearn: 0.1788109\ttotal: 9.4s\tremaining: 50.5s\n",
            "157:\tlearn: 0.1785258\ttotal: 9.46s\tremaining: 50.4s\n",
            "158:\tlearn: 0.1783494\ttotal: 9.51s\tremaining: 50.3s\n",
            "159:\tlearn: 0.1778561\ttotal: 9.57s\tremaining: 50.2s\n",
            "160:\tlearn: 0.1773639\ttotal: 9.61s\tremaining: 50.1s\n",
            "161:\tlearn: 0.1773519\ttotal: 9.66s\tremaining: 50s\n",
            "162:\tlearn: 0.1766717\ttotal: 9.7s\tremaining: 49.8s\n",
            "163:\tlearn: 0.1764546\ttotal: 9.76s\tremaining: 49.8s\n",
            "164:\tlearn: 0.1758939\ttotal: 9.81s\tremaining: 49.6s\n",
            "165:\tlearn: 0.1752307\ttotal: 9.85s\tremaining: 49.5s\n",
            "166:\tlearn: 0.1744879\ttotal: 9.91s\tremaining: 49.4s\n",
            "167:\tlearn: 0.1738465\ttotal: 9.95s\tremaining: 49.3s\n",
            "168:\tlearn: 0.1731158\ttotal: 10s\tremaining: 49.2s\n",
            "169:\tlearn: 0.1729756\ttotal: 10.1s\tremaining: 49.1s\n",
            "170:\tlearn: 0.1718560\ttotal: 10.1s\tremaining: 49s\n",
            "171:\tlearn: 0.1718438\ttotal: 10.2s\tremaining: 48.9s\n",
            "172:\tlearn: 0.1718350\ttotal: 10.2s\tremaining: 48.8s\n",
            "173:\tlearn: 0.1717590\ttotal: 10.3s\tremaining: 48.7s\n",
            "174:\tlearn: 0.1711650\ttotal: 10.3s\tremaining: 48.6s\n",
            "175:\tlearn: 0.1709284\ttotal: 10.4s\tremaining: 48.5s\n",
            "176:\tlearn: 0.1703228\ttotal: 10.4s\tremaining: 48.4s\n",
            "177:\tlearn: 0.1692243\ttotal: 10.5s\tremaining: 48.3s\n",
            "178:\tlearn: 0.1687743\ttotal: 10.5s\tremaining: 48.3s\n",
            "179:\tlearn: 0.1684482\ttotal: 10.6s\tremaining: 48.1s\n",
            "180:\tlearn: 0.1679307\ttotal: 10.6s\tremaining: 48s\n",
            "181:\tlearn: 0.1672171\ttotal: 10.7s\tremaining: 47.9s\n",
            "182:\tlearn: 0.1666520\ttotal: 10.7s\tremaining: 47.9s\n",
            "183:\tlearn: 0.1660705\ttotal: 10.8s\tremaining: 47.8s\n",
            "184:\tlearn: 0.1655366\ttotal: 10.8s\tremaining: 47.6s\n",
            "185:\tlearn: 0.1647772\ttotal: 10.9s\tremaining: 47.6s\n",
            "186:\tlearn: 0.1643327\ttotal: 10.9s\tremaining: 47.4s\n",
            "187:\tlearn: 0.1640007\ttotal: 11s\tremaining: 47.4s\n",
            "188:\tlearn: 0.1637316\ttotal: 11s\tremaining: 47.3s\n",
            "189:\tlearn: 0.1635185\ttotal: 11.1s\tremaining: 47.2s\n",
            "190:\tlearn: 0.1630950\ttotal: 11.1s\tremaining: 47.1s\n",
            "191:\tlearn: 0.1629290\ttotal: 11.2s\tremaining: 47s\n",
            "192:\tlearn: 0.1624269\ttotal: 11.2s\tremaining: 46.9s\n",
            "193:\tlearn: 0.1622498\ttotal: 11.3s\tremaining: 46.8s\n",
            "194:\tlearn: 0.1615700\ttotal: 11.3s\tremaining: 46.7s\n",
            "195:\tlearn: 0.1613167\ttotal: 11.4s\tremaining: 46.6s\n",
            "196:\tlearn: 0.1609313\ttotal: 11.4s\tremaining: 46.5s\n",
            "197:\tlearn: 0.1608641\ttotal: 11.5s\tremaining: 46.5s\n",
            "198:\tlearn: 0.1603372\ttotal: 11.5s\tremaining: 46.4s\n",
            "199:\tlearn: 0.1595705\ttotal: 11.6s\tremaining: 46.3s\n",
            "200:\tlearn: 0.1591466\ttotal: 11.6s\tremaining: 46.2s\n",
            "201:\tlearn: 0.1585411\ttotal: 11.7s\tremaining: 46.1s\n",
            "202:\tlearn: 0.1578503\ttotal: 11.7s\tremaining: 46s\n",
            "203:\tlearn: 0.1571563\ttotal: 11.8s\tremaining: 45.9s\n",
            "204:\tlearn: 0.1570199\ttotal: 11.8s\tremaining: 45.8s\n",
            "205:\tlearn: 0.1563747\ttotal: 11.9s\tremaining: 45.7s\n",
            "206:\tlearn: 0.1562536\ttotal: 11.9s\tremaining: 45.6s\n",
            "207:\tlearn: 0.1559759\ttotal: 12s\tremaining: 45.6s\n",
            "208:\tlearn: 0.1555927\ttotal: 12s\tremaining: 45.5s\n",
            "209:\tlearn: 0.1552704\ttotal: 12.1s\tremaining: 45.4s\n",
            "210:\tlearn: 0.1552052\ttotal: 12.1s\tremaining: 45.3s\n",
            "211:\tlearn: 0.1550834\ttotal: 12.2s\tremaining: 45.2s\n",
            "212:\tlearn: 0.1547233\ttotal: 12.2s\tremaining: 45.2s\n",
            "213:\tlearn: 0.1542450\ttotal: 12.3s\tremaining: 45.1s\n",
            "214:\tlearn: 0.1539608\ttotal: 12.3s\tremaining: 45s\n",
            "215:\tlearn: 0.1536326\ttotal: 12.4s\tremaining: 44.9s\n",
            "216:\tlearn: 0.1531768\ttotal: 12.4s\tremaining: 44.8s\n",
            "217:\tlearn: 0.1528840\ttotal: 12.5s\tremaining: 44.7s\n",
            "218:\tlearn: 0.1527064\ttotal: 12.5s\tremaining: 44.7s\n",
            "219:\tlearn: 0.1523938\ttotal: 12.6s\tremaining: 44.6s\n",
            "220:\tlearn: 0.1522603\ttotal: 12.6s\tremaining: 44.5s\n",
            "221:\tlearn: 0.1518749\ttotal: 12.7s\tremaining: 44.4s\n",
            "222:\tlearn: 0.1513018\ttotal: 12.7s\tremaining: 44.3s\n",
            "223:\tlearn: 0.1508995\ttotal: 12.8s\tremaining: 44.2s\n",
            "224:\tlearn: 0.1505879\ttotal: 12.8s\tremaining: 44.1s\n",
            "225:\tlearn: 0.1504852\ttotal: 12.9s\tremaining: 44.1s\n",
            "226:\tlearn: 0.1500641\ttotal: 12.9s\tremaining: 44s\n",
            "227:\tlearn: 0.1497523\ttotal: 13s\tremaining: 43.9s\n",
            "228:\tlearn: 0.1494337\ttotal: 13s\tremaining: 43.8s\n",
            "229:\tlearn: 0.1492634\ttotal: 13.1s\tremaining: 43.7s\n",
            "230:\tlearn: 0.1489778\ttotal: 13.1s\tremaining: 43.7s\n",
            "231:\tlearn: 0.1487891\ttotal: 13.2s\tremaining: 43.6s\n",
            "232:\tlearn: 0.1482975\ttotal: 13.2s\tremaining: 43.5s\n",
            "233:\tlearn: 0.1482934\ttotal: 13.3s\tremaining: 43.4s\n",
            "234:\tlearn: 0.1481930\ttotal: 13.3s\tremaining: 43.3s\n",
            "235:\tlearn: 0.1480074\ttotal: 13.4s\tremaining: 43.3s\n",
            "236:\tlearn: 0.1474468\ttotal: 13.4s\tremaining: 43.2s\n",
            "237:\tlearn: 0.1472015\ttotal: 13.5s\tremaining: 43.1s\n",
            "238:\tlearn: 0.1470323\ttotal: 13.5s\tremaining: 43s\n",
            "239:\tlearn: 0.1465228\ttotal: 13.6s\tremaining: 43s\n",
            "240:\tlearn: 0.1460984\ttotal: 13.7s\tremaining: 43s\n",
            "241:\tlearn: 0.1457369\ttotal: 13.7s\tremaining: 43.1s\n",
            "242:\tlearn: 0.1454864\ttotal: 13.8s\tremaining: 43.1s\n",
            "243:\tlearn: 0.1451644\ttotal: 13.9s\tremaining: 43.1s\n",
            "244:\tlearn: 0.1448768\ttotal: 14s\tremaining: 43s\n",
            "245:\tlearn: 0.1446847\ttotal: 14s\tremaining: 43s\n",
            "246:\tlearn: 0.1441517\ttotal: 14.1s\tremaining: 42.9s\n",
            "247:\tlearn: 0.1436675\ttotal: 14.1s\tremaining: 42.9s\n",
            "248:\tlearn: 0.1432533\ttotal: 14.2s\tremaining: 42.8s\n",
            "249:\tlearn: 0.1428414\ttotal: 14.2s\tremaining: 42.7s\n",
            "250:\tlearn: 0.1424291\ttotal: 14.3s\tremaining: 42.6s\n",
            "251:\tlearn: 0.1424170\ttotal: 14.3s\tremaining: 42.6s\n",
            "252:\tlearn: 0.1420504\ttotal: 14.4s\tremaining: 42.5s\n",
            "253:\tlearn: 0.1418361\ttotal: 14.4s\tremaining: 42.4s\n",
            "254:\tlearn: 0.1415991\ttotal: 14.5s\tremaining: 42.3s\n",
            "255:\tlearn: 0.1413825\ttotal: 14.5s\tremaining: 42.2s\n",
            "256:\tlearn: 0.1411846\ttotal: 14.6s\tremaining: 42.2s\n",
            "257:\tlearn: 0.1409000\ttotal: 14.6s\tremaining: 42.1s\n",
            "258:\tlearn: 0.1407481\ttotal: 14.7s\tremaining: 42s\n",
            "259:\tlearn: 0.1405878\ttotal: 14.7s\tremaining: 41.9s\n",
            "260:\tlearn: 0.1401771\ttotal: 14.8s\tremaining: 41.8s\n",
            "261:\tlearn: 0.1399269\ttotal: 14.8s\tremaining: 41.8s\n",
            "262:\tlearn: 0.1397945\ttotal: 14.9s\tremaining: 41.7s\n",
            "263:\tlearn: 0.1396019\ttotal: 14.9s\tremaining: 41.6s\n",
            "264:\tlearn: 0.1392363\ttotal: 15s\tremaining: 41.5s\n",
            "265:\tlearn: 0.1388573\ttotal: 15s\tremaining: 41.5s\n",
            "266:\tlearn: 0.1385755\ttotal: 15.1s\tremaining: 41.4s\n",
            "267:\tlearn: 0.1383459\ttotal: 15.1s\tremaining: 41.3s\n",
            "268:\tlearn: 0.1380152\ttotal: 15.2s\tremaining: 41.3s\n",
            "269:\tlearn: 0.1375868\ttotal: 15.2s\tremaining: 41.2s\n",
            "270:\tlearn: 0.1373382\ttotal: 15.3s\tremaining: 41.1s\n",
            "271:\tlearn: 0.1369473\ttotal: 15.3s\tremaining: 41s\n",
            "272:\tlearn: 0.1366293\ttotal: 15.4s\tremaining: 41s\n",
            "273:\tlearn: 0.1363178\ttotal: 15.4s\tremaining: 40.9s\n",
            "274:\tlearn: 0.1358869\ttotal: 15.5s\tremaining: 40.8s\n",
            "275:\tlearn: 0.1356318\ttotal: 15.5s\tremaining: 40.7s\n",
            "276:\tlearn: 0.1355242\ttotal: 15.6s\tremaining: 40.7s\n",
            "277:\tlearn: 0.1353241\ttotal: 15.6s\tremaining: 40.6s\n",
            "278:\tlearn: 0.1351645\ttotal: 15.7s\tremaining: 40.5s\n",
            "279:\tlearn: 0.1350518\ttotal: 15.7s\tremaining: 40.5s\n",
            "280:\tlearn: 0.1347543\ttotal: 15.8s\tremaining: 40.4s\n",
            "281:\tlearn: 0.1344789\ttotal: 15.8s\tremaining: 40.3s\n",
            "282:\tlearn: 0.1344754\ttotal: 15.9s\tremaining: 40.2s\n",
            "283:\tlearn: 0.1341446\ttotal: 15.9s\tremaining: 40.2s\n",
            "284:\tlearn: 0.1340104\ttotal: 16s\tremaining: 40.1s\n",
            "285:\tlearn: 0.1339409\ttotal: 16s\tremaining: 40s\n",
            "286:\tlearn: 0.1337667\ttotal: 16.1s\tremaining: 39.9s\n",
            "287:\tlearn: 0.1334911\ttotal: 16.1s\tremaining: 39.9s\n",
            "288:\tlearn: 0.1333376\ttotal: 16.2s\tremaining: 39.8s\n",
            "289:\tlearn: 0.1331398\ttotal: 16.2s\tremaining: 39.7s\n",
            "290:\tlearn: 0.1327763\ttotal: 16.3s\tremaining: 39.7s\n",
            "291:\tlearn: 0.1325709\ttotal: 16.3s\tremaining: 39.6s\n",
            "292:\tlearn: 0.1323649\ttotal: 16.4s\tremaining: 39.5s\n",
            "293:\tlearn: 0.1320980\ttotal: 16.4s\tremaining: 39.4s\n",
            "294:\tlearn: 0.1318832\ttotal: 16.5s\tremaining: 39.3s\n",
            "295:\tlearn: 0.1316753\ttotal: 16.5s\tremaining: 39.3s\n",
            "296:\tlearn: 0.1313285\ttotal: 16.6s\tremaining: 39.2s\n",
            "297:\tlearn: 0.1309932\ttotal: 16.6s\tremaining: 39.2s\n",
            "298:\tlearn: 0.1307185\ttotal: 16.7s\tremaining: 39.1s\n",
            "299:\tlearn: 0.1304304\ttotal: 16.7s\tremaining: 39s\n",
            "300:\tlearn: 0.1303066\ttotal: 16.8s\tremaining: 39s\n",
            "301:\tlearn: 0.1301491\ttotal: 16.8s\tremaining: 38.9s\n",
            "302:\tlearn: 0.1300944\ttotal: 16.9s\tremaining: 38.8s\n",
            "303:\tlearn: 0.1298348\ttotal: 17s\tremaining: 38.8s\n",
            "304:\tlearn: 0.1297555\ttotal: 17s\tremaining: 38.8s\n",
            "305:\tlearn: 0.1295696\ttotal: 17.1s\tremaining: 38.8s\n",
            "306:\tlearn: 0.1292512\ttotal: 17.2s\tremaining: 38.8s\n",
            "307:\tlearn: 0.1290523\ttotal: 17.3s\tremaining: 38.9s\n",
            "308:\tlearn: 0.1285700\ttotal: 17.4s\tremaining: 38.9s\n",
            "309:\tlearn: 0.1282221\ttotal: 17.4s\tremaining: 38.8s\n",
            "310:\tlearn: 0.1280801\ttotal: 17.5s\tremaining: 38.9s\n",
            "311:\tlearn: 0.1278469\ttotal: 17.6s\tremaining: 38.8s\n",
            "312:\tlearn: 0.1277584\ttotal: 17.7s\tremaining: 38.9s\n",
            "313:\tlearn: 0.1275777\ttotal: 17.8s\tremaining: 38.9s\n",
            "314:\tlearn: 0.1272896\ttotal: 17.9s\tremaining: 38.9s\n",
            "315:\tlearn: 0.1271826\ttotal: 18s\tremaining: 38.9s\n",
            "316:\tlearn: 0.1270352\ttotal: 18.1s\tremaining: 38.9s\n",
            "317:\tlearn: 0.1268230\ttotal: 18.1s\tremaining: 38.9s\n",
            "318:\tlearn: 0.1266778\ttotal: 18.2s\tremaining: 38.9s\n",
            "319:\tlearn: 0.1266330\ttotal: 18.3s\tremaining: 38.9s\n",
            "320:\tlearn: 0.1264967\ttotal: 18.4s\tremaining: 38.9s\n",
            "321:\tlearn: 0.1264426\ttotal: 18.5s\tremaining: 38.9s\n",
            "322:\tlearn: 0.1263560\ttotal: 18.6s\tremaining: 38.9s\n",
            "323:\tlearn: 0.1261802\ttotal: 18.7s\tremaining: 38.9s\n",
            "324:\tlearn: 0.1260333\ttotal: 18.7s\tremaining: 38.9s\n",
            "325:\tlearn: 0.1258724\ttotal: 18.8s\tremaining: 38.9s\n",
            "326:\tlearn: 0.1256722\ttotal: 18.9s\tremaining: 38.9s\n",
            "327:\tlearn: 0.1253951\ttotal: 19s\tremaining: 38.9s\n",
            "328:\tlearn: 0.1251790\ttotal: 19.1s\tremaining: 38.9s\n",
            "329:\tlearn: 0.1249518\ttotal: 19.1s\tremaining: 38.9s\n",
            "330:\tlearn: 0.1247915\ttotal: 19.2s\tremaining: 38.9s\n",
            "331:\tlearn: 0.1246625\ttotal: 19.3s\tremaining: 38.8s\n",
            "332:\tlearn: 0.1245571\ttotal: 19.4s\tremaining: 38.8s\n",
            "333:\tlearn: 0.1243924\ttotal: 19.5s\tremaining: 38.8s\n",
            "334:\tlearn: 0.1243396\ttotal: 19.6s\tremaining: 38.8s\n",
            "335:\tlearn: 0.1241495\ttotal: 19.6s\tremaining: 38.8s\n",
            "336:\tlearn: 0.1240243\ttotal: 19.7s\tremaining: 38.8s\n",
            "337:\tlearn: 0.1239083\ttotal: 19.8s\tremaining: 38.8s\n",
            "338:\tlearn: 0.1236374\ttotal: 19.9s\tremaining: 38.8s\n",
            "339:\tlearn: 0.1234962\ttotal: 20s\tremaining: 38.8s\n",
            "340:\tlearn: 0.1233652\ttotal: 20.1s\tremaining: 38.8s\n",
            "341:\tlearn: 0.1232564\ttotal: 20.2s\tremaining: 38.8s\n",
            "342:\tlearn: 0.1231947\ttotal: 20.2s\tremaining: 38.8s\n",
            "343:\tlearn: 0.1230702\ttotal: 20.3s\tremaining: 38.7s\n",
            "344:\tlearn: 0.1228677\ttotal: 20.4s\tremaining: 38.6s\n",
            "345:\tlearn: 0.1227388\ttotal: 20.4s\tremaining: 38.6s\n",
            "346:\tlearn: 0.1225909\ttotal: 20.5s\tremaining: 38.5s\n",
            "347:\tlearn: 0.1224041\ttotal: 20.5s\tremaining: 38.4s\n",
            "348:\tlearn: 0.1221714\ttotal: 20.6s\tremaining: 38.4s\n",
            "349:\tlearn: 0.1220794\ttotal: 20.6s\tremaining: 38.3s\n",
            "350:\tlearn: 0.1220500\ttotal: 20.7s\tremaining: 38.2s\n",
            "351:\tlearn: 0.1219571\ttotal: 20.7s\tremaining: 38.1s\n",
            "352:\tlearn: 0.1218400\ttotal: 20.8s\tremaining: 38.1s\n",
            "353:\tlearn: 0.1217031\ttotal: 20.8s\tremaining: 38s\n",
            "354:\tlearn: 0.1214804\ttotal: 20.9s\tremaining: 37.9s\n",
            "355:\tlearn: 0.1214638\ttotal: 20.9s\tremaining: 37.8s\n",
            "356:\tlearn: 0.1213492\ttotal: 21s\tremaining: 37.8s\n",
            "357:\tlearn: 0.1212543\ttotal: 21s\tremaining: 37.7s\n",
            "358:\tlearn: 0.1211806\ttotal: 21.1s\tremaining: 37.6s\n",
            "359:\tlearn: 0.1211063\ttotal: 21.1s\tremaining: 37.6s\n",
            "360:\tlearn: 0.1208240\ttotal: 21.2s\tremaining: 37.5s\n",
            "361:\tlearn: 0.1206911\ttotal: 21.2s\tremaining: 37.4s\n",
            "362:\tlearn: 0.1205274\ttotal: 21.3s\tremaining: 37.3s\n",
            "363:\tlearn: 0.1201515\ttotal: 21.3s\tremaining: 37.3s\n",
            "364:\tlearn: 0.1200414\ttotal: 21.4s\tremaining: 37.2s\n",
            "365:\tlearn: 0.1199791\ttotal: 21.4s\tremaining: 37.1s\n",
            "366:\tlearn: 0.1197752\ttotal: 21.5s\tremaining: 37s\n",
            "367:\tlearn: 0.1195964\ttotal: 21.5s\tremaining: 37s\n",
            "368:\tlearn: 0.1194541\ttotal: 21.6s\tremaining: 36.9s\n",
            "369:\tlearn: 0.1194051\ttotal: 21.6s\tremaining: 36.8s\n",
            "370:\tlearn: 0.1193196\ttotal: 21.7s\tremaining: 36.7s\n",
            "371:\tlearn: 0.1192122\ttotal: 21.7s\tremaining: 36.7s\n",
            "372:\tlearn: 0.1191267\ttotal: 21.8s\tremaining: 36.6s\n",
            "373:\tlearn: 0.1190211\ttotal: 21.8s\tremaining: 36.6s\n",
            "374:\tlearn: 0.1188932\ttotal: 21.9s\tremaining: 36.5s\n",
            "375:\tlearn: 0.1187648\ttotal: 21.9s\tremaining: 36.4s\n",
            "376:\tlearn: 0.1184883\ttotal: 22s\tremaining: 36.3s\n",
            "377:\tlearn: 0.1183764\ttotal: 22s\tremaining: 36.3s\n",
            "378:\tlearn: 0.1182451\ttotal: 22.1s\tremaining: 36.2s\n",
            "379:\tlearn: 0.1180136\ttotal: 22.1s\tremaining: 36.1s\n",
            "380:\tlearn: 0.1178202\ttotal: 22.2s\tremaining: 36s\n",
            "381:\tlearn: 0.1176880\ttotal: 22.2s\tremaining: 36s\n",
            "382:\tlearn: 0.1176724\ttotal: 22.3s\tremaining: 35.9s\n",
            "383:\tlearn: 0.1174844\ttotal: 22.3s\tremaining: 35.8s\n",
            "384:\tlearn: 0.1173503\ttotal: 22.4s\tremaining: 35.7s\n",
            "385:\tlearn: 0.1171634\ttotal: 22.4s\tremaining: 35.7s\n",
            "386:\tlearn: 0.1170894\ttotal: 22.5s\tremaining: 35.6s\n",
            "387:\tlearn: 0.1169807\ttotal: 22.5s\tremaining: 35.5s\n",
            "388:\tlearn: 0.1169124\ttotal: 22.6s\tremaining: 35.5s\n",
            "389:\tlearn: 0.1168796\ttotal: 22.6s\tremaining: 35.4s\n",
            "390:\tlearn: 0.1166100\ttotal: 22.7s\tremaining: 35.3s\n",
            "391:\tlearn: 0.1164860\ttotal: 22.7s\tremaining: 35.2s\n",
            "392:\tlearn: 0.1163721\ttotal: 22.8s\tremaining: 35.2s\n",
            "393:\tlearn: 0.1162478\ttotal: 22.8s\tremaining: 35.1s\n",
            "394:\tlearn: 0.1162072\ttotal: 22.9s\tremaining: 35s\n",
            "395:\tlearn: 0.1160908\ttotal: 22.9s\tremaining: 35s\n",
            "396:\tlearn: 0.1159893\ttotal: 23s\tremaining: 34.9s\n",
            "397:\tlearn: 0.1159828\ttotal: 23s\tremaining: 34.8s\n",
            "398:\tlearn: 0.1158466\ttotal: 23s\tremaining: 34.7s\n",
            "399:\tlearn: 0.1157371\ttotal: 23.1s\tremaining: 34.6s\n",
            "400:\tlearn: 0.1157081\ttotal: 23.1s\tremaining: 34.6s\n",
            "401:\tlearn: 0.1155756\ttotal: 23.2s\tremaining: 34.5s\n",
            "402:\tlearn: 0.1154833\ttotal: 23.2s\tremaining: 34.4s\n",
            "403:\tlearn: 0.1154455\ttotal: 23.3s\tremaining: 34.4s\n",
            "404:\tlearn: 0.1154455\ttotal: 23.3s\tremaining: 34.3s\n",
            "405:\tlearn: 0.1154016\ttotal: 23.4s\tremaining: 34.2s\n",
            "406:\tlearn: 0.1153468\ttotal: 23.4s\tremaining: 34.1s\n",
            "407:\tlearn: 0.1151319\ttotal: 23.5s\tremaining: 34.1s\n",
            "408:\tlearn: 0.1150514\ttotal: 23.5s\tremaining: 34s\n",
            "409:\tlearn: 0.1149890\ttotal: 23.6s\tremaining: 33.9s\n",
            "410:\tlearn: 0.1148424\ttotal: 23.6s\tremaining: 33.8s\n",
            "411:\tlearn: 0.1148044\ttotal: 23.7s\tremaining: 33.8s\n",
            "412:\tlearn: 0.1147238\ttotal: 23.7s\tremaining: 33.7s\n",
            "413:\tlearn: 0.1147098\ttotal: 23.8s\tremaining: 33.6s\n",
            "414:\tlearn: 0.1145852\ttotal: 23.8s\tremaining: 33.6s\n",
            "415:\tlearn: 0.1145457\ttotal: 23.9s\tremaining: 33.5s\n",
            "416:\tlearn: 0.1144077\ttotal: 23.9s\tremaining: 33.5s\n",
            "417:\tlearn: 0.1143827\ttotal: 24s\tremaining: 33.4s\n",
            "418:\tlearn: 0.1143251\ttotal: 24s\tremaining: 33.3s\n",
            "419:\tlearn: 0.1141611\ttotal: 24.1s\tremaining: 33.2s\n",
            "420:\tlearn: 0.1139745\ttotal: 24.1s\tremaining: 33.2s\n",
            "421:\tlearn: 0.1138779\ttotal: 24.2s\tremaining: 33.1s\n",
            "422:\tlearn: 0.1137968\ttotal: 24.2s\tremaining: 33s\n",
            "423:\tlearn: 0.1137277\ttotal: 24.3s\tremaining: 33s\n",
            "424:\tlearn: 0.1135590\ttotal: 24.3s\tremaining: 32.9s\n",
            "425:\tlearn: 0.1134004\ttotal: 24.4s\tremaining: 32.8s\n",
            "426:\tlearn: 0.1133118\ttotal: 24.4s\tremaining: 32.8s\n",
            "427:\tlearn: 0.1132100\ttotal: 24.5s\tremaining: 32.7s\n",
            "428:\tlearn: 0.1130798\ttotal: 24.5s\tremaining: 32.6s\n",
            "429:\tlearn: 0.1129799\ttotal: 24.6s\tremaining: 32.6s\n",
            "430:\tlearn: 0.1128468\ttotal: 24.6s\tremaining: 32.5s\n",
            "431:\tlearn: 0.1127968\ttotal: 24.7s\tremaining: 32.4s\n",
            "432:\tlearn: 0.1127256\ttotal: 24.7s\tremaining: 32.4s\n",
            "433:\tlearn: 0.1126367\ttotal: 24.8s\tremaining: 32.3s\n",
            "434:\tlearn: 0.1125103\ttotal: 24.8s\tremaining: 32.2s\n",
            "435:\tlearn: 0.1124430\ttotal: 24.9s\tremaining: 32.2s\n",
            "436:\tlearn: 0.1123614\ttotal: 24.9s\tremaining: 32.1s\n",
            "437:\tlearn: 0.1123318\ttotal: 25s\tremaining: 32.1s\n",
            "438:\tlearn: 0.1122306\ttotal: 25s\tremaining: 32s\n",
            "439:\tlearn: 0.1121831\ttotal: 25.1s\tremaining: 31.9s\n",
            "440:\tlearn: 0.1121772\ttotal: 25.1s\tremaining: 31.9s\n",
            "441:\tlearn: 0.1120324\ttotal: 25.2s\tremaining: 31.8s\n",
            "442:\tlearn: 0.1119442\ttotal: 25.2s\tremaining: 31.7s\n",
            "443:\tlearn: 0.1118131\ttotal: 25.3s\tremaining: 31.7s\n",
            "444:\tlearn: 0.1117407\ttotal: 25.4s\tremaining: 31.6s\n",
            "445:\tlearn: 0.1117011\ttotal: 25.4s\tremaining: 31.6s\n",
            "446:\tlearn: 0.1116720\ttotal: 25.4s\tremaining: 31.5s\n",
            "447:\tlearn: 0.1115813\ttotal: 25.5s\tremaining: 31.4s\n",
            "448:\tlearn: 0.1114961\ttotal: 25.6s\tremaining: 31.4s\n",
            "449:\tlearn: 0.1113339\ttotal: 25.6s\tremaining: 31.3s\n",
            "450:\tlearn: 0.1112743\ttotal: 25.6s\tremaining: 31.2s\n",
            "451:\tlearn: 0.1112463\ttotal: 25.7s\tremaining: 31.2s\n",
            "452:\tlearn: 0.1111770\ttotal: 25.8s\tremaining: 31.1s\n",
            "453:\tlearn: 0.1109740\ttotal: 25.8s\tremaining: 31s\n",
            "454:\tlearn: 0.1108537\ttotal: 25.8s\tremaining: 31s\n",
            "455:\tlearn: 0.1107547\ttotal: 25.9s\tremaining: 30.9s\n",
            "456:\tlearn: 0.1107140\ttotal: 26s\tremaining: 30.8s\n",
            "457:\tlearn: 0.1106887\ttotal: 26s\tremaining: 30.8s\n",
            "458:\tlearn: 0.1106351\ttotal: 26.1s\tremaining: 30.7s\n",
            "459:\tlearn: 0.1105678\ttotal: 26.1s\tremaining: 30.6s\n",
            "460:\tlearn: 0.1104987\ttotal: 26.2s\tremaining: 30.6s\n",
            "461:\tlearn: 0.1104213\ttotal: 26.2s\tremaining: 30.5s\n",
            "462:\tlearn: 0.1102796\ttotal: 26.3s\tremaining: 30.5s\n",
            "463:\tlearn: 0.1102119\ttotal: 26.3s\tremaining: 30.4s\n",
            "464:\tlearn: 0.1101717\ttotal: 26.4s\tremaining: 30.3s\n",
            "465:\tlearn: 0.1100685\ttotal: 26.4s\tremaining: 30.3s\n",
            "466:\tlearn: 0.1100417\ttotal: 26.5s\tremaining: 30.2s\n",
            "467:\tlearn: 0.1099717\ttotal: 26.5s\tremaining: 30.1s\n",
            "468:\tlearn: 0.1098771\ttotal: 26.5s\tremaining: 30.1s\n",
            "469:\tlearn: 0.1097659\ttotal: 26.6s\tremaining: 30s\n",
            "470:\tlearn: 0.1096803\ttotal: 26.7s\tremaining: 29.9s\n",
            "471:\tlearn: 0.1095708\ttotal: 26.7s\tremaining: 29.9s\n",
            "472:\tlearn: 0.1094931\ttotal: 26.8s\tremaining: 29.8s\n",
            "473:\tlearn: 0.1094307\ttotal: 26.8s\tremaining: 29.7s\n",
            "474:\tlearn: 0.1093342\ttotal: 26.9s\tremaining: 29.7s\n",
            "475:\tlearn: 0.1091712\ttotal: 26.9s\tremaining: 29.6s\n",
            "476:\tlearn: 0.1090800\ttotal: 27s\tremaining: 29.6s\n",
            "477:\tlearn: 0.1089960\ttotal: 27s\tremaining: 29.5s\n",
            "478:\tlearn: 0.1089691\ttotal: 27.1s\tremaining: 29.4s\n",
            "479:\tlearn: 0.1089051\ttotal: 27.1s\tremaining: 29.4s\n",
            "480:\tlearn: 0.1088455\ttotal: 27.2s\tremaining: 29.3s\n",
            "481:\tlearn: 0.1087750\ttotal: 27.2s\tremaining: 29.3s\n",
            "482:\tlearn: 0.1087183\ttotal: 27.3s\tremaining: 29.2s\n",
            "483:\tlearn: 0.1086483\ttotal: 27.3s\tremaining: 29.1s\n",
            "484:\tlearn: 0.1086309\ttotal: 27.4s\tremaining: 29.1s\n",
            "485:\tlearn: 0.1085819\ttotal: 27.4s\tremaining: 29s\n",
            "486:\tlearn: 0.1085644\ttotal: 27.5s\tremaining: 28.9s\n",
            "487:\tlearn: 0.1084890\ttotal: 27.5s\tremaining: 28.9s\n",
            "488:\tlearn: 0.1083748\ttotal: 27.6s\tremaining: 28.8s\n",
            "489:\tlearn: 0.1082972\ttotal: 27.6s\tremaining: 28.7s\n",
            "490:\tlearn: 0.1082130\ttotal: 27.7s\tremaining: 28.7s\n",
            "491:\tlearn: 0.1080904\ttotal: 27.7s\tremaining: 28.6s\n",
            "492:\tlearn: 0.1080077\ttotal: 27.8s\tremaining: 28.5s\n",
            "493:\tlearn: 0.1079388\ttotal: 27.8s\tremaining: 28.5s\n",
            "494:\tlearn: 0.1078777\ttotal: 27.9s\tremaining: 28.4s\n",
            "495:\tlearn: 0.1077716\ttotal: 27.9s\tremaining: 28.4s\n",
            "496:\tlearn: 0.1076894\ttotal: 28s\tremaining: 28.3s\n",
            "497:\tlearn: 0.1076817\ttotal: 28s\tremaining: 28.2s\n",
            "498:\tlearn: 0.1076082\ttotal: 28.1s\tremaining: 28.2s\n",
            "499:\tlearn: 0.1074866\ttotal: 28.1s\tremaining: 28.1s\n",
            "500:\tlearn: 0.1073957\ttotal: 28.2s\tremaining: 28.1s\n",
            "501:\tlearn: 0.1073233\ttotal: 28.2s\tremaining: 28s\n",
            "502:\tlearn: 0.1072642\ttotal: 28.3s\tremaining: 27.9s\n",
            "503:\tlearn: 0.1072192\ttotal: 28.3s\tremaining: 27.9s\n",
            "504:\tlearn: 0.1071800\ttotal: 28.4s\tremaining: 27.8s\n",
            "505:\tlearn: 0.1070876\ttotal: 28.4s\tremaining: 27.7s\n",
            "506:\tlearn: 0.1070133\ttotal: 28.5s\tremaining: 27.7s\n",
            "507:\tlearn: 0.1069464\ttotal: 28.5s\tremaining: 27.6s\n",
            "508:\tlearn: 0.1068858\ttotal: 28.6s\tremaining: 27.6s\n",
            "509:\tlearn: 0.1068274\ttotal: 28.6s\tremaining: 27.5s\n",
            "510:\tlearn: 0.1067190\ttotal: 28.7s\tremaining: 27.4s\n",
            "511:\tlearn: 0.1066714\ttotal: 28.7s\tremaining: 27.4s\n",
            "512:\tlearn: 0.1066043\ttotal: 28.8s\tremaining: 27.3s\n",
            "513:\tlearn: 0.1065405\ttotal: 28.8s\tremaining: 27.3s\n",
            "514:\tlearn: 0.1064918\ttotal: 28.9s\tremaining: 27.2s\n",
            "515:\tlearn: 0.1064676\ttotal: 28.9s\tremaining: 27.1s\n",
            "516:\tlearn: 0.1064504\ttotal: 29s\tremaining: 27.1s\n",
            "517:\tlearn: 0.1063871\ttotal: 29s\tremaining: 27s\n",
            "518:\tlearn: 0.1063629\ttotal: 29.1s\tremaining: 27s\n",
            "519:\tlearn: 0.1063446\ttotal: 29.1s\tremaining: 26.9s\n",
            "520:\tlearn: 0.1063027\ttotal: 29.2s\tremaining: 26.8s\n",
            "521:\tlearn: 0.1062262\ttotal: 29.2s\tremaining: 26.8s\n",
            "522:\tlearn: 0.1061722\ttotal: 29.3s\tremaining: 26.7s\n",
            "523:\tlearn: 0.1061529\ttotal: 29.3s\tremaining: 26.7s\n",
            "524:\tlearn: 0.1061020\ttotal: 29.4s\tremaining: 26.6s\n",
            "525:\tlearn: 0.1059807\ttotal: 29.4s\tremaining: 26.5s\n",
            "526:\tlearn: 0.1059335\ttotal: 29.5s\tremaining: 26.5s\n",
            "527:\tlearn: 0.1058569\ttotal: 29.5s\tremaining: 26.4s\n",
            "528:\tlearn: 0.1058146\ttotal: 29.6s\tremaining: 26.3s\n",
            "529:\tlearn: 0.1057437\ttotal: 29.6s\tremaining: 26.3s\n",
            "530:\tlearn: 0.1056694\ttotal: 29.7s\tremaining: 26.2s\n",
            "531:\tlearn: 0.1055817\ttotal: 29.7s\tremaining: 26.2s\n",
            "532:\tlearn: 0.1054766\ttotal: 29.8s\tremaining: 26.1s\n",
            "533:\tlearn: 0.1053788\ttotal: 29.8s\tremaining: 26s\n",
            "534:\tlearn: 0.1053494\ttotal: 29.9s\tremaining: 26s\n",
            "535:\tlearn: 0.1053483\ttotal: 29.9s\tremaining: 25.9s\n",
            "536:\tlearn: 0.1053077\ttotal: 30s\tremaining: 25.9s\n",
            "537:\tlearn: 0.1052288\ttotal: 30s\tremaining: 25.8s\n",
            "538:\tlearn: 0.1052016\ttotal: 30.1s\tremaining: 25.7s\n",
            "539:\tlearn: 0.1051218\ttotal: 30.2s\tremaining: 25.7s\n",
            "540:\tlearn: 0.1050636\ttotal: 30.2s\tremaining: 25.7s\n",
            "541:\tlearn: 0.1050451\ttotal: 30.3s\tremaining: 25.6s\n",
            "542:\tlearn: 0.1050023\ttotal: 30.4s\tremaining: 25.6s\n",
            "543:\tlearn: 0.1049372\ttotal: 30.5s\tremaining: 25.6s\n",
            "544:\tlearn: 0.1048316\ttotal: 30.6s\tremaining: 25.5s\n",
            "545:\tlearn: 0.1047461\ttotal: 30.7s\tremaining: 25.5s\n",
            "546:\tlearn: 0.1046682\ttotal: 30.8s\tremaining: 25.5s\n",
            "547:\tlearn: 0.1046545\ttotal: 30.8s\tremaining: 25.4s\n",
            "548:\tlearn: 0.1045553\ttotal: 30.9s\tremaining: 25.4s\n",
            "549:\tlearn: 0.1045290\ttotal: 31s\tremaining: 25.4s\n",
            "550:\tlearn: 0.1044932\ttotal: 31.1s\tremaining: 25.4s\n",
            "551:\tlearn: 0.1044591\ttotal: 31.2s\tremaining: 25.3s\n",
            "552:\tlearn: 0.1044279\ttotal: 31.3s\tremaining: 25.3s\n",
            "553:\tlearn: 0.1043901\ttotal: 31.4s\tremaining: 25.3s\n",
            "554:\tlearn: 0.1043715\ttotal: 31.5s\tremaining: 25.2s\n",
            "555:\tlearn: 0.1043052\ttotal: 31.5s\tremaining: 25.2s\n",
            "556:\tlearn: 0.1042500\ttotal: 31.6s\tremaining: 25.2s\n",
            "557:\tlearn: 0.1042251\ttotal: 31.7s\tremaining: 25.1s\n",
            "558:\tlearn: 0.1041573\ttotal: 31.8s\tremaining: 25.1s\n",
            "559:\tlearn: 0.1041433\ttotal: 31.9s\tremaining: 25.1s\n",
            "560:\tlearn: 0.1041106\ttotal: 32s\tremaining: 25s\n",
            "561:\tlearn: 0.1040523\ttotal: 32.1s\tremaining: 25s\n",
            "562:\tlearn: 0.1040127\ttotal: 32.1s\tremaining: 24.9s\n",
            "563:\tlearn: 0.1039793\ttotal: 32.2s\tremaining: 24.9s\n",
            "564:\tlearn: 0.1039636\ttotal: 32.3s\tremaining: 24.8s\n",
            "565:\tlearn: 0.1039581\ttotal: 32.3s\tremaining: 24.8s\n",
            "566:\tlearn: 0.1039082\ttotal: 32.4s\tremaining: 24.8s\n",
            "567:\tlearn: 0.1038505\ttotal: 32.5s\tremaining: 24.7s\n",
            "568:\tlearn: 0.1037907\ttotal: 32.6s\tremaining: 24.7s\n",
            "569:\tlearn: 0.1037614\ttotal: 32.7s\tremaining: 24.7s\n",
            "570:\tlearn: 0.1037612\ttotal: 32.7s\tremaining: 24.6s\n",
            "571:\tlearn: 0.1037604\ttotal: 32.8s\tremaining: 24.6s\n",
            "572:\tlearn: 0.1037086\ttotal: 32.9s\tremaining: 24.5s\n",
            "573:\tlearn: 0.1036752\ttotal: 33s\tremaining: 24.5s\n",
            "574:\tlearn: 0.1036408\ttotal: 33.1s\tremaining: 24.5s\n",
            "575:\tlearn: 0.1035830\ttotal: 33.2s\tremaining: 24.4s\n",
            "576:\tlearn: 0.1035547\ttotal: 33.3s\tremaining: 24.4s\n",
            "577:\tlearn: 0.1035546\ttotal: 33.4s\tremaining: 24.4s\n",
            "578:\tlearn: 0.1035201\ttotal: 33.5s\tremaining: 24.3s\n",
            "579:\tlearn: 0.1034324\ttotal: 33.5s\tremaining: 24.3s\n",
            "580:\tlearn: 0.1033661\ttotal: 33.6s\tremaining: 24.3s\n",
            "581:\tlearn: 0.1033435\ttotal: 33.7s\tremaining: 24.2s\n",
            "582:\tlearn: 0.1032309\ttotal: 33.8s\tremaining: 24.1s\n",
            "583:\tlearn: 0.1032016\ttotal: 33.8s\tremaining: 24.1s\n",
            "584:\tlearn: 0.1031319\ttotal: 33.9s\tremaining: 24s\n",
            "585:\tlearn: 0.1031004\ttotal: 33.9s\tremaining: 24s\n",
            "586:\tlearn: 0.1030728\ttotal: 34s\tremaining: 23.9s\n",
            "587:\tlearn: 0.1030001\ttotal: 34s\tremaining: 23.8s\n",
            "588:\tlearn: 0.1029716\ttotal: 34.1s\tremaining: 23.8s\n",
            "589:\tlearn: 0.1029240\ttotal: 34.1s\tremaining: 23.7s\n",
            "590:\tlearn: 0.1028462\ttotal: 34.2s\tremaining: 23.6s\n",
            "591:\tlearn: 0.1028018\ttotal: 34.2s\tremaining: 23.6s\n",
            "592:\tlearn: 0.1027214\ttotal: 34.3s\tremaining: 23.5s\n",
            "593:\tlearn: 0.1027067\ttotal: 34.3s\tremaining: 23.5s\n",
            "594:\tlearn: 0.1026732\ttotal: 34.4s\tremaining: 23.4s\n",
            "595:\tlearn: 0.1025922\ttotal: 34.4s\tremaining: 23.3s\n",
            "596:\tlearn: 0.1025206\ttotal: 34.5s\tremaining: 23.3s\n",
            "597:\tlearn: 0.1024701\ttotal: 34.5s\tremaining: 23.2s\n",
            "598:\tlearn: 0.1024504\ttotal: 34.6s\tremaining: 23.1s\n",
            "599:\tlearn: 0.1023841\ttotal: 34.6s\tremaining: 23.1s\n",
            "600:\tlearn: 0.1023567\ttotal: 34.7s\tremaining: 23s\n",
            "601:\tlearn: 0.1022703\ttotal: 34.7s\tremaining: 22.9s\n",
            "602:\tlearn: 0.1022501\ttotal: 34.7s\tremaining: 22.9s\n",
            "603:\tlearn: 0.1022381\ttotal: 34.8s\tremaining: 22.8s\n",
            "604:\tlearn: 0.1021764\ttotal: 34.8s\tremaining: 22.8s\n",
            "605:\tlearn: 0.1021060\ttotal: 34.9s\tremaining: 22.7s\n",
            "606:\tlearn: 0.1020400\ttotal: 34.9s\tremaining: 22.6s\n",
            "607:\tlearn: 0.1020061\ttotal: 35s\tremaining: 22.6s\n",
            "608:\tlearn: 0.1019632\ttotal: 35s\tremaining: 22.5s\n",
            "609:\tlearn: 0.1019099\ttotal: 35.1s\tremaining: 22.4s\n",
            "610:\tlearn: 0.1018401\ttotal: 35.1s\tremaining: 22.4s\n",
            "611:\tlearn: 0.1018107\ttotal: 35.2s\tremaining: 22.3s\n",
            "612:\tlearn: 0.1017754\ttotal: 35.2s\tremaining: 22.3s\n",
            "613:\tlearn: 0.1017413\ttotal: 35.3s\tremaining: 22.2s\n",
            "614:\tlearn: 0.1017228\ttotal: 35.3s\tremaining: 22.1s\n",
            "615:\tlearn: 0.1016699\ttotal: 35.4s\tremaining: 22.1s\n",
            "616:\tlearn: 0.1016417\ttotal: 35.4s\tremaining: 22s\n",
            "617:\tlearn: 0.1016003\ttotal: 35.5s\tremaining: 21.9s\n",
            "618:\tlearn: 0.1015630\ttotal: 35.5s\tremaining: 21.9s\n",
            "619:\tlearn: 0.1014818\ttotal: 35.6s\tremaining: 21.8s\n",
            "620:\tlearn: 0.1014157\ttotal: 35.6s\tremaining: 21.7s\n",
            "621:\tlearn: 0.1013748\ttotal: 35.7s\tremaining: 21.7s\n",
            "622:\tlearn: 0.1013497\ttotal: 35.7s\tremaining: 21.6s\n",
            "623:\tlearn: 0.1013085\ttotal: 35.8s\tremaining: 21.6s\n",
            "624:\tlearn: 0.1012566\ttotal: 35.8s\tremaining: 21.5s\n",
            "625:\tlearn: 0.1012096\ttotal: 35.9s\tremaining: 21.4s\n",
            "626:\tlearn: 0.1011647\ttotal: 35.9s\tremaining: 21.4s\n",
            "627:\tlearn: 0.1011142\ttotal: 36s\tremaining: 21.3s\n",
            "628:\tlearn: 0.1010661\ttotal: 36s\tremaining: 21.2s\n",
            "629:\tlearn: 0.1010170\ttotal: 36.1s\tremaining: 21.2s\n",
            "630:\tlearn: 0.1009984\ttotal: 36.1s\tremaining: 21.1s\n",
            "631:\tlearn: 0.1009389\ttotal: 36.2s\tremaining: 21.1s\n",
            "632:\tlearn: 0.1008684\ttotal: 36.2s\tremaining: 21s\n",
            "633:\tlearn: 0.1008114\ttotal: 36.3s\tremaining: 20.9s\n",
            "634:\tlearn: 0.1007552\ttotal: 36.3s\tremaining: 20.9s\n",
            "635:\tlearn: 0.1007202\ttotal: 36.4s\tremaining: 20.8s\n",
            "636:\tlearn: 0.1006651\ttotal: 36.4s\tremaining: 20.8s\n",
            "637:\tlearn: 0.1006513\ttotal: 36.5s\tremaining: 20.7s\n",
            "638:\tlearn: 0.1005940\ttotal: 36.5s\tremaining: 20.6s\n",
            "639:\tlearn: 0.1005797\ttotal: 36.6s\tremaining: 20.6s\n",
            "640:\tlearn: 0.1005641\ttotal: 36.6s\tremaining: 20.5s\n",
            "641:\tlearn: 0.1005302\ttotal: 36.7s\tremaining: 20.4s\n",
            "642:\tlearn: 0.1004805\ttotal: 36.7s\tremaining: 20.4s\n",
            "643:\tlearn: 0.1004522\ttotal: 36.8s\tremaining: 20.3s\n",
            "644:\tlearn: 0.1004126\ttotal: 36.8s\tremaining: 20.3s\n",
            "645:\tlearn: 0.1003672\ttotal: 36.9s\tremaining: 20.2s\n",
            "646:\tlearn: 0.1003670\ttotal: 36.9s\tremaining: 20.1s\n",
            "647:\tlearn: 0.1003656\ttotal: 37s\tremaining: 20.1s\n",
            "648:\tlearn: 0.1003465\ttotal: 37s\tremaining: 20s\n",
            "649:\tlearn: 0.1003445\ttotal: 37.1s\tremaining: 20s\n",
            "650:\tlearn: 0.1003128\ttotal: 37.1s\tremaining: 19.9s\n",
            "651:\tlearn: 0.1002708\ttotal: 37.1s\tremaining: 19.8s\n",
            "652:\tlearn: 0.1002108\ttotal: 37.2s\tremaining: 19.8s\n",
            "653:\tlearn: 0.1001648\ttotal: 37.3s\tremaining: 19.7s\n",
            "654:\tlearn: 0.1001215\ttotal: 37.3s\tremaining: 19.7s\n",
            "655:\tlearn: 0.1000879\ttotal: 37.4s\tremaining: 19.6s\n",
            "656:\tlearn: 0.1000729\ttotal: 37.4s\tremaining: 19.5s\n",
            "657:\tlearn: 0.1000235\ttotal: 37.5s\tremaining: 19.5s\n",
            "658:\tlearn: 0.0999522\ttotal: 37.5s\tremaining: 19.4s\n",
            "659:\tlearn: 0.0999190\ttotal: 37.6s\tremaining: 19.3s\n",
            "660:\tlearn: 0.0998968\ttotal: 37.6s\tremaining: 19.3s\n",
            "661:\tlearn: 0.0998508\ttotal: 37.7s\tremaining: 19.2s\n",
            "662:\tlearn: 0.0998189\ttotal: 37.7s\tremaining: 19.2s\n",
            "663:\tlearn: 0.0997589\ttotal: 37.7s\tremaining: 19.1s\n",
            "664:\tlearn: 0.0997203\ttotal: 37.8s\tremaining: 19s\n",
            "665:\tlearn: 0.0997157\ttotal: 37.8s\tremaining: 19s\n",
            "666:\tlearn: 0.0996990\ttotal: 37.9s\tremaining: 18.9s\n",
            "667:\tlearn: 0.0996747\ttotal: 37.9s\tremaining: 18.9s\n",
            "668:\tlearn: 0.0996330\ttotal: 38s\tremaining: 18.8s\n",
            "669:\tlearn: 0.0996236\ttotal: 38s\tremaining: 18.7s\n",
            "670:\tlearn: 0.0996028\ttotal: 38.1s\tremaining: 18.7s\n",
            "671:\tlearn: 0.0995838\ttotal: 38.1s\tremaining: 18.6s\n",
            "672:\tlearn: 0.0995603\ttotal: 38.2s\tremaining: 18.6s\n",
            "673:\tlearn: 0.0995585\ttotal: 38.2s\tremaining: 18.5s\n",
            "674:\tlearn: 0.0995443\ttotal: 38.3s\tremaining: 18.4s\n",
            "675:\tlearn: 0.0995283\ttotal: 38.3s\tremaining: 18.4s\n",
            "676:\tlearn: 0.0994724\ttotal: 38.4s\tremaining: 18.3s\n",
            "677:\tlearn: 0.0994573\ttotal: 38.4s\tremaining: 18.3s\n",
            "678:\tlearn: 0.0994164\ttotal: 38.5s\tremaining: 18.2s\n",
            "679:\tlearn: 0.0993703\ttotal: 38.5s\tremaining: 18.1s\n",
            "680:\tlearn: 0.0993237\ttotal: 38.6s\tremaining: 18.1s\n",
            "681:\tlearn: 0.0992892\ttotal: 38.6s\tremaining: 18s\n",
            "682:\tlearn: 0.0992526\ttotal: 38.7s\tremaining: 18s\n",
            "683:\tlearn: 0.0992198\ttotal: 38.7s\tremaining: 17.9s\n",
            "684:\tlearn: 0.0991846\ttotal: 38.8s\tremaining: 17.8s\n",
            "685:\tlearn: 0.0991320\ttotal: 38.8s\tremaining: 17.8s\n",
            "686:\tlearn: 0.0990822\ttotal: 38.9s\tremaining: 17.7s\n",
            "687:\tlearn: 0.0990209\ttotal: 38.9s\tremaining: 17.7s\n",
            "688:\tlearn: 0.0989759\ttotal: 39s\tremaining: 17.6s\n",
            "689:\tlearn: 0.0989073\ttotal: 39s\tremaining: 17.5s\n",
            "690:\tlearn: 0.0988540\ttotal: 39.1s\tremaining: 17.5s\n",
            "691:\tlearn: 0.0988423\ttotal: 39.1s\tremaining: 17.4s\n",
            "692:\tlearn: 0.0987718\ttotal: 39.2s\tremaining: 17.4s\n",
            "693:\tlearn: 0.0987423\ttotal: 39.2s\tremaining: 17.3s\n",
            "694:\tlearn: 0.0987422\ttotal: 39.3s\tremaining: 17.2s\n",
            "695:\tlearn: 0.0987273\ttotal: 39.4s\tremaining: 17.2s\n",
            "696:\tlearn: 0.0987138\ttotal: 39.4s\tremaining: 17.1s\n",
            "697:\tlearn: 0.0986963\ttotal: 39.4s\tremaining: 17.1s\n",
            "698:\tlearn: 0.0986813\ttotal: 39.5s\tremaining: 17s\n",
            "699:\tlearn: 0.0986609\ttotal: 39.5s\tremaining: 16.9s\n",
            "700:\tlearn: 0.0986235\ttotal: 39.6s\tremaining: 16.9s\n",
            "701:\tlearn: 0.0986009\ttotal: 39.6s\tremaining: 16.8s\n",
            "702:\tlearn: 0.0985644\ttotal: 39.7s\tremaining: 16.8s\n",
            "703:\tlearn: 0.0985325\ttotal: 39.7s\tremaining: 16.7s\n",
            "704:\tlearn: 0.0985136\ttotal: 39.8s\tremaining: 16.6s\n",
            "705:\tlearn: 0.0984492\ttotal: 39.8s\tremaining: 16.6s\n",
            "706:\tlearn: 0.0983975\ttotal: 39.9s\tremaining: 16.5s\n",
            "707:\tlearn: 0.0983883\ttotal: 39.9s\tremaining: 16.5s\n",
            "708:\tlearn: 0.0983587\ttotal: 40s\tremaining: 16.4s\n",
            "709:\tlearn: 0.0983486\ttotal: 40s\tremaining: 16.4s\n",
            "710:\tlearn: 0.0983204\ttotal: 40.1s\tremaining: 16.3s\n",
            "711:\tlearn: 0.0983131\ttotal: 40.1s\tremaining: 16.2s\n",
            "712:\tlearn: 0.0983108\ttotal: 40.2s\tremaining: 16.2s\n",
            "713:\tlearn: 0.0982683\ttotal: 40.2s\tremaining: 16.1s\n",
            "714:\tlearn: 0.0982162\ttotal: 40.3s\tremaining: 16.1s\n",
            "715:\tlearn: 0.0981719\ttotal: 40.4s\tremaining: 16s\n",
            "716:\tlearn: 0.0981129\ttotal: 40.4s\tremaining: 15.9s\n",
            "717:\tlearn: 0.0981128\ttotal: 40.4s\tremaining: 15.9s\n",
            "718:\tlearn: 0.0980902\ttotal: 40.5s\tremaining: 15.8s\n",
            "719:\tlearn: 0.0980752\ttotal: 40.5s\tremaining: 15.8s\n",
            "720:\tlearn: 0.0980503\ttotal: 40.6s\tremaining: 15.7s\n",
            "721:\tlearn: 0.0980456\ttotal: 40.6s\tremaining: 15.6s\n",
            "722:\tlearn: 0.0979905\ttotal: 40.7s\tremaining: 15.6s\n",
            "723:\tlearn: 0.0979645\ttotal: 40.7s\tremaining: 15.5s\n",
            "724:\tlearn: 0.0979557\ttotal: 40.8s\tremaining: 15.5s\n",
            "725:\tlearn: 0.0979082\ttotal: 40.8s\tremaining: 15.4s\n",
            "726:\tlearn: 0.0978790\ttotal: 40.9s\tremaining: 15.4s\n",
            "727:\tlearn: 0.0978674\ttotal: 40.9s\tremaining: 15.3s\n",
            "728:\tlearn: 0.0978305\ttotal: 41s\tremaining: 15.2s\n",
            "729:\tlearn: 0.0978048\ttotal: 41s\tremaining: 15.2s\n",
            "730:\tlearn: 0.0977597\ttotal: 41.1s\tremaining: 15.1s\n",
            "731:\tlearn: 0.0977246\ttotal: 41.1s\tremaining: 15.1s\n",
            "732:\tlearn: 0.0976962\ttotal: 41.2s\tremaining: 15s\n",
            "733:\tlearn: 0.0976826\ttotal: 41.2s\tremaining: 14.9s\n",
            "734:\tlearn: 0.0976560\ttotal: 41.3s\tremaining: 14.9s\n",
            "735:\tlearn: 0.0976392\ttotal: 41.3s\tremaining: 14.8s\n",
            "736:\tlearn: 0.0976038\ttotal: 41.4s\tremaining: 14.8s\n",
            "737:\tlearn: 0.0975880\ttotal: 41.4s\tremaining: 14.7s\n",
            "738:\tlearn: 0.0975716\ttotal: 41.5s\tremaining: 14.6s\n",
            "739:\tlearn: 0.0975537\ttotal: 41.5s\tremaining: 14.6s\n",
            "740:\tlearn: 0.0975268\ttotal: 41.6s\tremaining: 14.5s\n",
            "741:\tlearn: 0.0974983\ttotal: 41.6s\tremaining: 14.5s\n",
            "742:\tlearn: 0.0974511\ttotal: 41.7s\tremaining: 14.4s\n",
            "743:\tlearn: 0.0974274\ttotal: 41.7s\tremaining: 14.4s\n",
            "744:\tlearn: 0.0973854\ttotal: 41.8s\tremaining: 14.3s\n",
            "745:\tlearn: 0.0973639\ttotal: 41.8s\tremaining: 14.2s\n",
            "746:\tlearn: 0.0973469\ttotal: 41.9s\tremaining: 14.2s\n",
            "747:\tlearn: 0.0973305\ttotal: 41.9s\tremaining: 14.1s\n",
            "748:\tlearn: 0.0973058\ttotal: 42s\tremaining: 14.1s\n",
            "749:\tlearn: 0.0972894\ttotal: 42s\tremaining: 14s\n",
            "750:\tlearn: 0.0972756\ttotal: 42.1s\tremaining: 13.9s\n",
            "751:\tlearn: 0.0972645\ttotal: 42.1s\tremaining: 13.9s\n",
            "752:\tlearn: 0.0972391\ttotal: 42.2s\tremaining: 13.8s\n",
            "753:\tlearn: 0.0972312\ttotal: 42.2s\tremaining: 13.8s\n",
            "754:\tlearn: 0.0972304\ttotal: 42.3s\tremaining: 13.7s\n",
            "755:\tlearn: 0.0972284\ttotal: 42.3s\tremaining: 13.7s\n",
            "756:\tlearn: 0.0972273\ttotal: 42.4s\tremaining: 13.6s\n",
            "757:\tlearn: 0.0972149\ttotal: 42.4s\tremaining: 13.5s\n",
            "758:\tlearn: 0.0971694\ttotal: 42.5s\tremaining: 13.5s\n",
            "759:\tlearn: 0.0971550\ttotal: 42.5s\tremaining: 13.4s\n",
            "760:\tlearn: 0.0971550\ttotal: 42.6s\tremaining: 13.4s\n",
            "761:\tlearn: 0.0971338\ttotal: 42.6s\tremaining: 13.3s\n",
            "762:\tlearn: 0.0971272\ttotal: 42.7s\tremaining: 13.3s\n",
            "763:\tlearn: 0.0970798\ttotal: 42.7s\tremaining: 13.2s\n",
            "764:\tlearn: 0.0970523\ttotal: 42.8s\tremaining: 13.1s\n",
            "765:\tlearn: 0.0970340\ttotal: 42.8s\tremaining: 13.1s\n",
            "766:\tlearn: 0.0970236\ttotal: 42.9s\tremaining: 13s\n",
            "767:\tlearn: 0.0969558\ttotal: 42.9s\tremaining: 13s\n",
            "768:\tlearn: 0.0969134\ttotal: 43s\tremaining: 12.9s\n",
            "769:\tlearn: 0.0968805\ttotal: 43s\tremaining: 12.8s\n",
            "770:\tlearn: 0.0968539\ttotal: 43.1s\tremaining: 12.8s\n",
            "771:\tlearn: 0.0968209\ttotal: 43.1s\tremaining: 12.7s\n",
            "772:\tlearn: 0.0967811\ttotal: 43.1s\tremaining: 12.7s\n",
            "773:\tlearn: 0.0967518\ttotal: 43.2s\tremaining: 12.6s\n",
            "774:\tlearn: 0.0967229\ttotal: 43.2s\tremaining: 12.6s\n",
            "775:\tlearn: 0.0967024\ttotal: 43.3s\tremaining: 12.5s\n",
            "776:\tlearn: 0.0966634\ttotal: 43.3s\tremaining: 12.4s\n",
            "777:\tlearn: 0.0966180\ttotal: 43.4s\tremaining: 12.4s\n",
            "778:\tlearn: 0.0965760\ttotal: 43.5s\tremaining: 12.3s\n",
            "779:\tlearn: 0.0965715\ttotal: 43.5s\tremaining: 12.3s\n",
            "780:\tlearn: 0.0965607\ttotal: 43.5s\tremaining: 12.2s\n",
            "781:\tlearn: 0.0965370\ttotal: 43.6s\tremaining: 12.2s\n",
            "782:\tlearn: 0.0965331\ttotal: 43.7s\tremaining: 12.1s\n",
            "783:\tlearn: 0.0965177\ttotal: 43.7s\tremaining: 12.1s\n",
            "784:\tlearn: 0.0964952\ttotal: 43.8s\tremaining: 12s\n",
            "785:\tlearn: 0.0964734\ttotal: 43.9s\tremaining: 12s\n",
            "786:\tlearn: 0.0964446\ttotal: 44s\tremaining: 11.9s\n",
            "787:\tlearn: 0.0964322\ttotal: 44.1s\tremaining: 11.9s\n",
            "788:\tlearn: 0.0964313\ttotal: 44.2s\tremaining: 11.8s\n",
            "789:\tlearn: 0.0964312\ttotal: 44.3s\tremaining: 11.8s\n",
            "790:\tlearn: 0.0964208\ttotal: 44.4s\tremaining: 11.7s\n",
            "791:\tlearn: 0.0964208\ttotal: 44.5s\tremaining: 11.7s\n",
            "792:\tlearn: 0.0964141\ttotal: 44.5s\tremaining: 11.6s\n",
            "793:\tlearn: 0.0963887\ttotal: 44.6s\tremaining: 11.6s\n",
            "794:\tlearn: 0.0963467\ttotal: 44.7s\tremaining: 11.5s\n",
            "795:\tlearn: 0.0962990\ttotal: 44.8s\tremaining: 11.5s\n",
            "796:\tlearn: 0.0962756\ttotal: 44.9s\tremaining: 11.4s\n",
            "797:\tlearn: 0.0962749\ttotal: 45s\tremaining: 11.4s\n",
            "798:\tlearn: 0.0962485\ttotal: 45.1s\tremaining: 11.3s\n",
            "799:\tlearn: 0.0962234\ttotal: 45.1s\tremaining: 11.3s\n",
            "800:\tlearn: 0.0961991\ttotal: 45.2s\tremaining: 11.2s\n",
            "801:\tlearn: 0.0961824\ttotal: 45.3s\tremaining: 11.2s\n",
            "802:\tlearn: 0.0961622\ttotal: 45.4s\tremaining: 11.1s\n",
            "803:\tlearn: 0.0961317\ttotal: 45.5s\tremaining: 11.1s\n",
            "804:\tlearn: 0.0961100\ttotal: 45.6s\tremaining: 11s\n",
            "805:\tlearn: 0.0960878\ttotal: 45.7s\tremaining: 11s\n",
            "806:\tlearn: 0.0960446\ttotal: 45.7s\tremaining: 10.9s\n",
            "807:\tlearn: 0.0960131\ttotal: 45.8s\tremaining: 10.9s\n",
            "808:\tlearn: 0.0960131\ttotal: 45.9s\tremaining: 10.8s\n",
            "809:\tlearn: 0.0959980\ttotal: 46s\tremaining: 10.8s\n",
            "810:\tlearn: 0.0959650\ttotal: 46.1s\tremaining: 10.7s\n",
            "811:\tlearn: 0.0959416\ttotal: 46.1s\tremaining: 10.7s\n",
            "812:\tlearn: 0.0959192\ttotal: 46.2s\tremaining: 10.6s\n",
            "813:\tlearn: 0.0958990\ttotal: 46.3s\tremaining: 10.6s\n",
            "814:\tlearn: 0.0958783\ttotal: 46.4s\tremaining: 10.5s\n",
            "815:\tlearn: 0.0958633\ttotal: 46.5s\tremaining: 10.5s\n",
            "816:\tlearn: 0.0958273\ttotal: 46.6s\tremaining: 10.4s\n",
            "817:\tlearn: 0.0958075\ttotal: 46.7s\tremaining: 10.4s\n",
            "818:\tlearn: 0.0957732\ttotal: 46.8s\tremaining: 10.3s\n",
            "819:\tlearn: 0.0957731\ttotal: 46.9s\tremaining: 10.3s\n",
            "820:\tlearn: 0.0957561\ttotal: 47s\tremaining: 10.2s\n",
            "821:\tlearn: 0.0957415\ttotal: 47.1s\tremaining: 10.2s\n",
            "822:\tlearn: 0.0957362\ttotal: 47.1s\tremaining: 10.1s\n",
            "823:\tlearn: 0.0957092\ttotal: 47.2s\tremaining: 10.1s\n",
            "824:\tlearn: 0.0956889\ttotal: 47.3s\tremaining: 10s\n",
            "825:\tlearn: 0.0956572\ttotal: 47.3s\tremaining: 9.97s\n",
            "826:\tlearn: 0.0956313\ttotal: 47.4s\tremaining: 9.91s\n",
            "827:\tlearn: 0.0955945\ttotal: 47.4s\tremaining: 9.85s\n",
            "828:\tlearn: 0.0955395\ttotal: 47.5s\tremaining: 9.79s\n",
            "829:\tlearn: 0.0955170\ttotal: 47.5s\tremaining: 9.74s\n",
            "830:\tlearn: 0.0954894\ttotal: 47.6s\tremaining: 9.68s\n",
            "831:\tlearn: 0.0954725\ttotal: 47.6s\tremaining: 9.62s\n",
            "832:\tlearn: 0.0954533\ttotal: 47.7s\tremaining: 9.56s\n",
            "833:\tlearn: 0.0954532\ttotal: 47.7s\tremaining: 9.5s\n",
            "834:\tlearn: 0.0954531\ttotal: 47.8s\tremaining: 9.44s\n",
            "835:\tlearn: 0.0954448\ttotal: 47.8s\tremaining: 9.38s\n",
            "836:\tlearn: 0.0954339\ttotal: 47.9s\tremaining: 9.32s\n",
            "837:\tlearn: 0.0954276\ttotal: 47.9s\tremaining: 9.26s\n",
            "838:\tlearn: 0.0954253\ttotal: 48s\tremaining: 9.2s\n",
            "839:\tlearn: 0.0954236\ttotal: 48s\tremaining: 9.14s\n",
            "840:\tlearn: 0.0953954\ttotal: 48.1s\tremaining: 9.09s\n",
            "841:\tlearn: 0.0953785\ttotal: 48.1s\tremaining: 9.03s\n",
            "842:\tlearn: 0.0953785\ttotal: 48.1s\tremaining: 8.97s\n",
            "843:\tlearn: 0.0953633\ttotal: 48.2s\tremaining: 8.91s\n",
            "844:\tlearn: 0.0953633\ttotal: 48.2s\tremaining: 8.85s\n",
            "845:\tlearn: 0.0953537\ttotal: 48.3s\tremaining: 8.79s\n",
            "846:\tlearn: 0.0953480\ttotal: 48.3s\tremaining: 8.73s\n",
            "847:\tlearn: 0.0953284\ttotal: 48.4s\tremaining: 8.67s\n",
            "848:\tlearn: 0.0953283\ttotal: 48.4s\tremaining: 8.62s\n",
            "849:\tlearn: 0.0953140\ttotal: 48.5s\tremaining: 8.56s\n",
            "850:\tlearn: 0.0952967\ttotal: 48.5s\tremaining: 8.5s\n",
            "851:\tlearn: 0.0952761\ttotal: 48.6s\tremaining: 8.44s\n",
            "852:\tlearn: 0.0952684\ttotal: 48.6s\tremaining: 8.38s\n",
            "853:\tlearn: 0.0952684\ttotal: 48.7s\tremaining: 8.32s\n",
            "854:\tlearn: 0.0952586\ttotal: 48.7s\tremaining: 8.27s\n",
            "855:\tlearn: 0.0952252\ttotal: 48.8s\tremaining: 8.21s\n",
            "856:\tlearn: 0.0952167\ttotal: 48.8s\tremaining: 8.15s\n",
            "857:\tlearn: 0.0952167\ttotal: 48.9s\tremaining: 8.09s\n",
            "858:\tlearn: 0.0952040\ttotal: 48.9s\tremaining: 8.03s\n",
            "859:\tlearn: 0.0951813\ttotal: 49s\tremaining: 7.97s\n",
            "860:\tlearn: 0.0951773\ttotal: 49s\tremaining: 7.92s\n",
            "861:\tlearn: 0.0951709\ttotal: 49.1s\tremaining: 7.86s\n",
            "862:\tlearn: 0.0951495\ttotal: 49.1s\tremaining: 7.8s\n",
            "863:\tlearn: 0.0951351\ttotal: 49.2s\tremaining: 7.74s\n",
            "864:\tlearn: 0.0950978\ttotal: 49.2s\tremaining: 7.68s\n",
            "865:\tlearn: 0.0950827\ttotal: 49.3s\tremaining: 7.63s\n",
            "866:\tlearn: 0.0950734\ttotal: 49.3s\tremaining: 7.57s\n",
            "867:\tlearn: 0.0950733\ttotal: 49.4s\tremaining: 7.51s\n",
            "868:\tlearn: 0.0950639\ttotal: 49.4s\tremaining: 7.45s\n",
            "869:\tlearn: 0.0950565\ttotal: 49.5s\tremaining: 7.39s\n",
            "870:\tlearn: 0.0950487\ttotal: 49.5s\tremaining: 7.34s\n",
            "871:\tlearn: 0.0950231\ttotal: 49.6s\tremaining: 7.28s\n",
            "872:\tlearn: 0.0950103\ttotal: 49.6s\tremaining: 7.22s\n",
            "873:\tlearn: 0.0950103\ttotal: 49.7s\tremaining: 7.16s\n",
            "874:\tlearn: 0.0949965\ttotal: 49.7s\tremaining: 7.1s\n",
            "875:\tlearn: 0.0949587\ttotal: 49.8s\tremaining: 7.05s\n",
            "876:\tlearn: 0.0949444\ttotal: 49.8s\tremaining: 6.99s\n",
            "877:\tlearn: 0.0949110\ttotal: 49.9s\tremaining: 6.93s\n",
            "878:\tlearn: 0.0948811\ttotal: 49.9s\tremaining: 6.87s\n",
            "879:\tlearn: 0.0948730\ttotal: 50s\tremaining: 6.82s\n",
            "880:\tlearn: 0.0948676\ttotal: 50s\tremaining: 6.76s\n",
            "881:\tlearn: 0.0948395\ttotal: 50.1s\tremaining: 6.7s\n",
            "882:\tlearn: 0.0948154\ttotal: 50.1s\tremaining: 6.64s\n",
            "883:\tlearn: 0.0947999\ttotal: 50.2s\tremaining: 6.58s\n",
            "884:\tlearn: 0.0947897\ttotal: 50.2s\tremaining: 6.53s\n",
            "885:\tlearn: 0.0947716\ttotal: 50.3s\tremaining: 6.47s\n",
            "886:\tlearn: 0.0947469\ttotal: 50.3s\tremaining: 6.41s\n",
            "887:\tlearn: 0.0947293\ttotal: 50.4s\tremaining: 6.35s\n",
            "888:\tlearn: 0.0947291\ttotal: 50.4s\tremaining: 6.3s\n",
            "889:\tlearn: 0.0947292\ttotal: 50.5s\tremaining: 6.24s\n",
            "890:\tlearn: 0.0947048\ttotal: 50.5s\tremaining: 6.18s\n",
            "891:\tlearn: 0.0946692\ttotal: 50.6s\tremaining: 6.12s\n",
            "892:\tlearn: 0.0946466\ttotal: 50.6s\tremaining: 6.07s\n",
            "893:\tlearn: 0.0946385\ttotal: 50.7s\tremaining: 6.01s\n",
            "894:\tlearn: 0.0946125\ttotal: 50.7s\tremaining: 5.95s\n",
            "895:\tlearn: 0.0945937\ttotal: 50.8s\tremaining: 5.89s\n",
            "896:\tlearn: 0.0945623\ttotal: 50.8s\tremaining: 5.84s\n",
            "897:\tlearn: 0.0945389\ttotal: 50.9s\tremaining: 5.78s\n",
            "898:\tlearn: 0.0945352\ttotal: 50.9s\tremaining: 5.72s\n",
            "899:\tlearn: 0.0945274\ttotal: 51s\tremaining: 5.66s\n",
            "900:\tlearn: 0.0944959\ttotal: 51s\tremaining: 5.61s\n",
            "901:\tlearn: 0.0944846\ttotal: 51.1s\tremaining: 5.55s\n",
            "902:\tlearn: 0.0944742\ttotal: 51.1s\tremaining: 5.49s\n",
            "903:\tlearn: 0.0944628\ttotal: 51.2s\tremaining: 5.43s\n",
            "904:\tlearn: 0.0944389\ttotal: 51.2s\tremaining: 5.38s\n",
            "905:\tlearn: 0.0944123\ttotal: 51.3s\tremaining: 5.32s\n",
            "906:\tlearn: 0.0943992\ttotal: 51.3s\tremaining: 5.26s\n",
            "907:\tlearn: 0.0943838\ttotal: 51.4s\tremaining: 5.2s\n",
            "908:\tlearn: 0.0943616\ttotal: 51.4s\tremaining: 5.15s\n",
            "909:\tlearn: 0.0943446\ttotal: 51.5s\tremaining: 5.09s\n",
            "910:\tlearn: 0.0943283\ttotal: 51.5s\tremaining: 5.03s\n",
            "911:\tlearn: 0.0943281\ttotal: 51.6s\tremaining: 4.97s\n",
            "912:\tlearn: 0.0943171\ttotal: 51.6s\tremaining: 4.92s\n",
            "913:\tlearn: 0.0943070\ttotal: 51.7s\tremaining: 4.86s\n",
            "914:\tlearn: 0.0942932\ttotal: 51.7s\tremaining: 4.8s\n",
            "915:\tlearn: 0.0942857\ttotal: 51.8s\tremaining: 4.75s\n",
            "916:\tlearn: 0.0942783\ttotal: 51.8s\tremaining: 4.69s\n",
            "917:\tlearn: 0.0942731\ttotal: 51.9s\tremaining: 4.63s\n",
            "918:\tlearn: 0.0942599\ttotal: 51.9s\tremaining: 4.58s\n",
            "919:\tlearn: 0.0942287\ttotal: 52s\tremaining: 4.52s\n",
            "920:\tlearn: 0.0942070\ttotal: 52s\tremaining: 4.46s\n",
            "921:\tlearn: 0.0942025\ttotal: 52.1s\tremaining: 4.41s\n",
            "922:\tlearn: 0.0941936\ttotal: 52.1s\tremaining: 4.35s\n",
            "923:\tlearn: 0.0941742\ttotal: 52.2s\tremaining: 4.29s\n",
            "924:\tlearn: 0.0941677\ttotal: 52.2s\tremaining: 4.23s\n",
            "925:\tlearn: 0.0941677\ttotal: 52.3s\tremaining: 4.18s\n",
            "926:\tlearn: 0.0941541\ttotal: 52.3s\tremaining: 4.12s\n",
            "927:\tlearn: 0.0941293\ttotal: 52.4s\tremaining: 4.06s\n",
            "928:\tlearn: 0.0941248\ttotal: 52.4s\tremaining: 4.01s\n",
            "929:\tlearn: 0.0941114\ttotal: 52.5s\tremaining: 3.95s\n",
            "930:\tlearn: 0.0941113\ttotal: 52.5s\tremaining: 3.89s\n",
            "931:\tlearn: 0.0940918\ttotal: 52.6s\tremaining: 3.83s\n",
            "932:\tlearn: 0.0940850\ttotal: 52.6s\tremaining: 3.78s\n",
            "933:\tlearn: 0.0940850\ttotal: 52.6s\tremaining: 3.72s\n",
            "934:\tlearn: 0.0940849\ttotal: 52.7s\tremaining: 3.66s\n",
            "935:\tlearn: 0.0940644\ttotal: 52.8s\tremaining: 3.61s\n",
            "936:\tlearn: 0.0940552\ttotal: 52.8s\tremaining: 3.55s\n",
            "937:\tlearn: 0.0940346\ttotal: 52.8s\tremaining: 3.49s\n",
            "938:\tlearn: 0.0940126\ttotal: 52.9s\tremaining: 3.44s\n",
            "939:\tlearn: 0.0939923\ttotal: 53s\tremaining: 3.38s\n",
            "940:\tlearn: 0.0939785\ttotal: 53s\tremaining: 3.32s\n",
            "941:\tlearn: 0.0939620\ttotal: 53s\tremaining: 3.27s\n",
            "942:\tlearn: 0.0939620\ttotal: 53.1s\tremaining: 3.21s\n",
            "943:\tlearn: 0.0939512\ttotal: 53.2s\tremaining: 3.15s\n",
            "944:\tlearn: 0.0939333\ttotal: 53.2s\tremaining: 3.1s\n",
            "945:\tlearn: 0.0939059\ttotal: 53.2s\tremaining: 3.04s\n",
            "946:\tlearn: 0.0938798\ttotal: 53.3s\tremaining: 2.98s\n",
            "947:\tlearn: 0.0938798\ttotal: 53.3s\tremaining: 2.92s\n",
            "948:\tlearn: 0.0938797\ttotal: 53.3s\tremaining: 2.87s\n",
            "949:\tlearn: 0.0938473\ttotal: 53.4s\tremaining: 2.81s\n",
            "950:\tlearn: 0.0938139\ttotal: 53.5s\tremaining: 2.75s\n",
            "951:\tlearn: 0.0938139\ttotal: 53.5s\tremaining: 2.7s\n",
            "952:\tlearn: 0.0938058\ttotal: 53.5s\tremaining: 2.64s\n",
            "953:\tlearn: 0.0937837\ttotal: 53.6s\tremaining: 2.58s\n",
            "954:\tlearn: 0.0937742\ttotal: 53.6s\tremaining: 2.53s\n",
            "955:\tlearn: 0.0937591\ttotal: 53.7s\tremaining: 2.47s\n",
            "956:\tlearn: 0.0937364\ttotal: 53.8s\tremaining: 2.42s\n",
            "957:\tlearn: 0.0937240\ttotal: 53.8s\tremaining: 2.36s\n",
            "958:\tlearn: 0.0936985\ttotal: 53.8s\tremaining: 2.3s\n",
            "959:\tlearn: 0.0936587\ttotal: 53.9s\tremaining: 2.25s\n",
            "960:\tlearn: 0.0936431\ttotal: 54s\tremaining: 2.19s\n",
            "961:\tlearn: 0.0936290\ttotal: 54s\tremaining: 2.13s\n",
            "962:\tlearn: 0.0936153\ttotal: 54s\tremaining: 2.08s\n",
            "963:\tlearn: 0.0936016\ttotal: 54.1s\tremaining: 2.02s\n",
            "964:\tlearn: 0.0935850\ttotal: 54.1s\tremaining: 1.96s\n",
            "965:\tlearn: 0.0935671\ttotal: 54.2s\tremaining: 1.91s\n",
            "966:\tlearn: 0.0935468\ttotal: 54.2s\tremaining: 1.85s\n",
            "967:\tlearn: 0.0935416\ttotal: 54.3s\tremaining: 1.79s\n",
            "968:\tlearn: 0.0935275\ttotal: 54.3s\tremaining: 1.74s\n",
            "969:\tlearn: 0.0935275\ttotal: 54.4s\tremaining: 1.68s\n",
            "970:\tlearn: 0.0935030\ttotal: 54.4s\tremaining: 1.63s\n",
            "971:\tlearn: 0.0934819\ttotal: 54.5s\tremaining: 1.57s\n",
            "972:\tlearn: 0.0934705\ttotal: 54.5s\tremaining: 1.51s\n",
            "973:\tlearn: 0.0934664\ttotal: 54.6s\tremaining: 1.46s\n",
            "974:\tlearn: 0.0934544\ttotal: 54.6s\tremaining: 1.4s\n",
            "975:\tlearn: 0.0934327\ttotal: 54.7s\tremaining: 1.34s\n",
            "976:\tlearn: 0.0934227\ttotal: 54.7s\tremaining: 1.29s\n",
            "977:\tlearn: 0.0934149\ttotal: 54.8s\tremaining: 1.23s\n",
            "978:\tlearn: 0.0934149\ttotal: 54.8s\tremaining: 1.18s\n",
            "979:\tlearn: 0.0934041\ttotal: 54.9s\tremaining: 1.12s\n",
            "980:\tlearn: 0.0933933\ttotal: 54.9s\tremaining: 1.06s\n",
            "981:\tlearn: 0.0933933\ttotal: 55s\tremaining: 1.01s\n",
            "982:\tlearn: 0.0933896\ttotal: 55s\tremaining: 952ms\n",
            "983:\tlearn: 0.0933878\ttotal: 55.1s\tremaining: 896ms\n",
            "984:\tlearn: 0.0933859\ttotal: 55.1s\tremaining: 840ms\n",
            "985:\tlearn: 0.0933745\ttotal: 55.2s\tremaining: 784ms\n",
            "986:\tlearn: 0.0933554\ttotal: 55.2s\tremaining: 727ms\n",
            "987:\tlearn: 0.0933379\ttotal: 55.3s\tremaining: 671ms\n",
            "988:\tlearn: 0.0933379\ttotal: 55.3s\tremaining: 615ms\n",
            "989:\tlearn: 0.0933253\ttotal: 55.4s\tremaining: 559ms\n",
            "990:\tlearn: 0.0933141\ttotal: 55.4s\tremaining: 503ms\n",
            "991:\tlearn: 0.0933096\ttotal: 55.5s\tremaining: 447ms\n",
            "992:\tlearn: 0.0933094\ttotal: 55.5s\tremaining: 391ms\n",
            "993:\tlearn: 0.0933094\ttotal: 55.6s\tremaining: 335ms\n",
            "994:\tlearn: 0.0933094\ttotal: 55.6s\tremaining: 279ms\n",
            "995:\tlearn: 0.0933035\ttotal: 55.7s\tremaining: 224ms\n",
            "996:\tlearn: 0.0933034\ttotal: 55.7s\tremaining: 168ms\n",
            "997:\tlearn: 0.0932788\ttotal: 55.8s\tremaining: 112ms\n",
            "998:\tlearn: 0.0932622\ttotal: 55.8s\tremaining: 55.9ms\n",
            "999:\tlearn: 0.0932393\ttotal: 55.9s\tremaining: 0us\n",
            "0:\tlearn: 0.6008456\ttotal: 46.2ms\tremaining: 46.2s\n",
            "1:\tlearn: 0.5376482\ttotal: 94.6ms\tremaining: 47.2s\n",
            "2:\tlearn: 0.4947607\ttotal: 140ms\tremaining: 46.5s\n",
            "3:\tlearn: 0.4715739\ttotal: 185ms\tremaining: 46.1s\n",
            "4:\tlearn: 0.4544150\ttotal: 231ms\tremaining: 45.9s\n",
            "5:\tlearn: 0.4417140\ttotal: 286ms\tremaining: 47.5s\n",
            "6:\tlearn: 0.4274622\ttotal: 334ms\tremaining: 47.4s\n",
            "7:\tlearn: 0.4200360\ttotal: 380ms\tremaining: 47.2s\n",
            "8:\tlearn: 0.4161655\ttotal: 426ms\tremaining: 46.9s\n",
            "9:\tlearn: 0.4096153\ttotal: 477ms\tremaining: 47.2s\n",
            "10:\tlearn: 0.4038209\ttotal: 534ms\tremaining: 48s\n",
            "11:\tlearn: 0.4001119\ttotal: 579ms\tremaining: 47.7s\n",
            "12:\tlearn: 0.3947895\ttotal: 626ms\tremaining: 47.5s\n",
            "13:\tlearn: 0.3920251\ttotal: 672ms\tremaining: 47.3s\n",
            "14:\tlearn: 0.3862036\ttotal: 719ms\tremaining: 47.2s\n",
            "15:\tlearn: 0.3833492\ttotal: 792ms\tremaining: 48.7s\n",
            "16:\tlearn: 0.3820723\ttotal: 841ms\tremaining: 48.6s\n",
            "17:\tlearn: 0.3737875\ttotal: 886ms\tremaining: 48.4s\n",
            "18:\tlearn: 0.3716386\ttotal: 937ms\tremaining: 48.4s\n",
            "19:\tlearn: 0.3703716\ttotal: 989ms\tremaining: 48.4s\n",
            "20:\tlearn: 0.3663817\ttotal: 1.04s\tremaining: 48.7s\n",
            "21:\tlearn: 0.3631719\ttotal: 1.09s\tremaining: 48.5s\n",
            "22:\tlearn: 0.3599099\ttotal: 1.14s\tremaining: 48.3s\n",
            "23:\tlearn: 0.3573214\ttotal: 1.18s\tremaining: 48.1s\n",
            "24:\tlearn: 0.3550546\ttotal: 1.26s\tremaining: 49s\n",
            "25:\tlearn: 0.3529148\ttotal: 1.33s\tremaining: 49.7s\n",
            "26:\tlearn: 0.3506510\ttotal: 1.41s\tremaining: 50.7s\n",
            "27:\tlearn: 0.3494534\ttotal: 1.49s\tremaining: 51.9s\n",
            "28:\tlearn: 0.3488278\ttotal: 1.57s\tremaining: 52.7s\n",
            "29:\tlearn: 0.3459181\ttotal: 1.66s\tremaining: 53.6s\n",
            "30:\tlearn: 0.3409736\ttotal: 1.74s\tremaining: 54.4s\n",
            "31:\tlearn: 0.3388334\ttotal: 1.83s\tremaining: 55.3s\n",
            "32:\tlearn: 0.3369502\ttotal: 1.91s\tremaining: 56s\n",
            "33:\tlearn: 0.3341784\ttotal: 1.99s\tremaining: 56.5s\n",
            "34:\tlearn: 0.3312592\ttotal: 2.07s\tremaining: 57.2s\n",
            "35:\tlearn: 0.3292881\ttotal: 2.16s\tremaining: 57.8s\n",
            "36:\tlearn: 0.3271427\ttotal: 2.25s\tremaining: 58.6s\n",
            "37:\tlearn: 0.3252323\ttotal: 2.33s\tremaining: 59s\n",
            "38:\tlearn: 0.3237810\ttotal: 2.42s\tremaining: 59.6s\n",
            "39:\tlearn: 0.3201947\ttotal: 2.51s\tremaining: 1m\n",
            "40:\tlearn: 0.3193593\ttotal: 2.6s\tremaining: 1m\n",
            "41:\tlearn: 0.3169565\ttotal: 2.68s\tremaining: 1m 1s\n",
            "42:\tlearn: 0.3157757\ttotal: 2.77s\tremaining: 1m 1s\n",
            "43:\tlearn: 0.3149374\ttotal: 2.87s\tremaining: 1m 2s\n",
            "44:\tlearn: 0.3131325\ttotal: 2.96s\tremaining: 1m 2s\n",
            "45:\tlearn: 0.3124973\ttotal: 3.04s\tremaining: 1m 2s\n",
            "46:\tlearn: 0.3102194\ttotal: 3.13s\tremaining: 1m 3s\n",
            "47:\tlearn: 0.3087931\ttotal: 3.2s\tremaining: 1m 3s\n",
            "48:\tlearn: 0.3064976\ttotal: 3.29s\tremaining: 1m 3s\n",
            "49:\tlearn: 0.3019896\ttotal: 3.38s\tremaining: 1m 4s\n",
            "50:\tlearn: 0.2984470\ttotal: 3.48s\tremaining: 1m 4s\n",
            "51:\tlearn: 0.2961914\ttotal: 3.58s\tremaining: 1m 5s\n",
            "52:\tlearn: 0.2949913\ttotal: 3.68s\tremaining: 1m 5s\n",
            "53:\tlearn: 0.2929323\ttotal: 3.79s\tremaining: 1m 6s\n",
            "54:\tlearn: 0.2923522\ttotal: 3.88s\tremaining: 1m 6s\n",
            "55:\tlearn: 0.2908353\ttotal: 3.98s\tremaining: 1m 7s\n",
            "56:\tlearn: 0.2905332\ttotal: 4.07s\tremaining: 1m 7s\n",
            "57:\tlearn: 0.2903434\ttotal: 4.16s\tremaining: 1m 7s\n",
            "58:\tlearn: 0.2899137\ttotal: 4.26s\tremaining: 1m 7s\n",
            "59:\tlearn: 0.2886175\ttotal: 4.35s\tremaining: 1m 8s\n",
            "60:\tlearn: 0.2859893\ttotal: 4.44s\tremaining: 1m 8s\n",
            "61:\tlearn: 0.2819997\ttotal: 4.53s\tremaining: 1m 8s\n",
            "62:\tlearn: 0.2795706\ttotal: 4.63s\tremaining: 1m 8s\n",
            "63:\tlearn: 0.2769644\ttotal: 4.71s\tremaining: 1m 8s\n",
            "64:\tlearn: 0.2753416\ttotal: 4.81s\tremaining: 1m 9s\n",
            "65:\tlearn: 0.2743286\ttotal: 4.91s\tremaining: 1m 9s\n",
            "66:\tlearn: 0.2721665\ttotal: 5s\tremaining: 1m 9s\n",
            "67:\tlearn: 0.2693002\ttotal: 5.09s\tremaining: 1m 9s\n",
            "68:\tlearn: 0.2692207\ttotal: 5.19s\tremaining: 1m 10s\n",
            "69:\tlearn: 0.2659703\ttotal: 5.29s\tremaining: 1m 10s\n",
            "70:\tlearn: 0.2645220\ttotal: 5.37s\tremaining: 1m 10s\n",
            "71:\tlearn: 0.2630048\ttotal: 5.46s\tremaining: 1m 10s\n",
            "72:\tlearn: 0.2604318\ttotal: 5.55s\tremaining: 1m 10s\n",
            "73:\tlearn: 0.2600241\ttotal: 5.65s\tremaining: 1m 10s\n",
            "74:\tlearn: 0.2581384\ttotal: 5.75s\tremaining: 1m 10s\n",
            "75:\tlearn: 0.2547524\ttotal: 5.85s\tremaining: 1m 11s\n",
            "76:\tlearn: 0.2535304\ttotal: 5.94s\tremaining: 1m 11s\n",
            "77:\tlearn: 0.2522866\ttotal: 6.04s\tremaining: 1m 11s\n",
            "78:\tlearn: 0.2501661\ttotal: 6.14s\tremaining: 1m 11s\n",
            "79:\tlearn: 0.2485923\ttotal: 6.24s\tremaining: 1m 11s\n",
            "80:\tlearn: 0.2479153\ttotal: 6.34s\tremaining: 1m 11s\n",
            "81:\tlearn: 0.2465705\ttotal: 6.43s\tremaining: 1m 11s\n",
            "82:\tlearn: 0.2450870\ttotal: 6.52s\tremaining: 1m 12s\n",
            "83:\tlearn: 0.2427070\ttotal: 6.6s\tremaining: 1m 11s\n",
            "84:\tlearn: 0.2417581\ttotal: 6.67s\tremaining: 1m 11s\n",
            "85:\tlearn: 0.2407494\ttotal: 6.77s\tremaining: 1m 11s\n",
            "86:\tlearn: 0.2397871\ttotal: 6.85s\tremaining: 1m 11s\n",
            "87:\tlearn: 0.2379265\ttotal: 6.92s\tremaining: 1m 11s\n",
            "88:\tlearn: 0.2363406\ttotal: 7.01s\tremaining: 1m 11s\n",
            "89:\tlearn: 0.2360029\ttotal: 7.11s\tremaining: 1m 11s\n",
            "90:\tlearn: 0.2358504\ttotal: 7.18s\tremaining: 1m 11s\n",
            "91:\tlearn: 0.2344386\ttotal: 7.28s\tremaining: 1m 11s\n",
            "92:\tlearn: 0.2330056\ttotal: 7.38s\tremaining: 1m 11s\n",
            "93:\tlearn: 0.2299776\ttotal: 7.46s\tremaining: 1m 11s\n",
            "94:\tlearn: 0.2286276\ttotal: 7.55s\tremaining: 1m 11s\n",
            "95:\tlearn: 0.2278019\ttotal: 7.64s\tremaining: 1m 11s\n",
            "96:\tlearn: 0.2262366\ttotal: 7.74s\tremaining: 1m 12s\n",
            "97:\tlearn: 0.2246202\ttotal: 7.83s\tremaining: 1m 12s\n",
            "98:\tlearn: 0.2241487\ttotal: 7.92s\tremaining: 1m 12s\n",
            "99:\tlearn: 0.2230538\ttotal: 8.01s\tremaining: 1m 12s\n",
            "100:\tlearn: 0.2223479\ttotal: 8.1s\tremaining: 1m 12s\n",
            "101:\tlearn: 0.2216459\ttotal: 8.2s\tremaining: 1m 12s\n",
            "102:\tlearn: 0.2204644\ttotal: 8.29s\tremaining: 1m 12s\n",
            "103:\tlearn: 0.2197144\ttotal: 8.34s\tremaining: 1m 11s\n",
            "104:\tlearn: 0.2179407\ttotal: 8.39s\tremaining: 1m 11s\n",
            "105:\tlearn: 0.2167121\ttotal: 8.45s\tremaining: 1m 11s\n",
            "106:\tlearn: 0.2155337\ttotal: 8.49s\tremaining: 1m 10s\n",
            "107:\tlearn: 0.2137831\ttotal: 8.54s\tremaining: 1m 10s\n",
            "108:\tlearn: 0.2126131\ttotal: 8.59s\tremaining: 1m 10s\n",
            "109:\tlearn: 0.2123220\ttotal: 8.63s\tremaining: 1m 9s\n",
            "110:\tlearn: 0.2107895\ttotal: 8.69s\tremaining: 1m 9s\n",
            "111:\tlearn: 0.2102243\ttotal: 8.74s\tremaining: 1m 9s\n",
            "112:\tlearn: 0.2094849\ttotal: 8.78s\tremaining: 1m 8s\n",
            "113:\tlearn: 0.2086293\ttotal: 8.82s\tremaining: 1m 8s\n",
            "114:\tlearn: 0.2084109\ttotal: 8.87s\tremaining: 1m 8s\n",
            "115:\tlearn: 0.2067628\ttotal: 8.93s\tremaining: 1m 8s\n",
            "116:\tlearn: 0.2056284\ttotal: 8.97s\tremaining: 1m 7s\n",
            "117:\tlearn: 0.2048820\ttotal: 9.02s\tremaining: 1m 7s\n",
            "118:\tlearn: 0.2041960\ttotal: 9.07s\tremaining: 1m 7s\n",
            "119:\tlearn: 0.2029221\ttotal: 9.11s\tremaining: 1m 6s\n",
            "120:\tlearn: 0.2029043\ttotal: 9.17s\tremaining: 1m 6s\n",
            "121:\tlearn: 0.2018372\ttotal: 9.23s\tremaining: 1m 6s\n",
            "122:\tlearn: 0.2010585\ttotal: 9.28s\tremaining: 1m 6s\n",
            "123:\tlearn: 0.1997560\ttotal: 9.32s\tremaining: 1m 5s\n",
            "124:\tlearn: 0.1990900\ttotal: 9.37s\tremaining: 1m 5s\n",
            "125:\tlearn: 0.1978269\ttotal: 9.42s\tremaining: 1m 5s\n",
            "126:\tlearn: 0.1970301\ttotal: 9.47s\tremaining: 1m 5s\n",
            "127:\tlearn: 0.1964434\ttotal: 9.52s\tremaining: 1m 4s\n",
            "128:\tlearn: 0.1956551\ttotal: 9.58s\tremaining: 1m 4s\n",
            "129:\tlearn: 0.1950466\ttotal: 9.63s\tremaining: 1m 4s\n",
            "130:\tlearn: 0.1947899\ttotal: 9.68s\tremaining: 1m 4s\n",
            "131:\tlearn: 0.1945135\ttotal: 9.73s\tremaining: 1m 3s\n",
            "132:\tlearn: 0.1939193\ttotal: 9.78s\tremaining: 1m 3s\n",
            "133:\tlearn: 0.1926341\ttotal: 9.83s\tremaining: 1m 3s\n",
            "134:\tlearn: 0.1920521\ttotal: 9.88s\tremaining: 1m 3s\n",
            "135:\tlearn: 0.1917340\ttotal: 9.93s\tremaining: 1m 3s\n",
            "136:\tlearn: 0.1911571\ttotal: 9.98s\tremaining: 1m 2s\n",
            "137:\tlearn: 0.1904539\ttotal: 10s\tremaining: 1m 2s\n",
            "138:\tlearn: 0.1893822\ttotal: 10.1s\tremaining: 1m 2s\n",
            "139:\tlearn: 0.1883028\ttotal: 10.1s\tremaining: 1m 2s\n",
            "140:\tlearn: 0.1869414\ttotal: 10.2s\tremaining: 1m 2s\n",
            "141:\tlearn: 0.1862985\ttotal: 10.2s\tremaining: 1m 1s\n",
            "142:\tlearn: 0.1854452\ttotal: 10.3s\tremaining: 1m 1s\n",
            "143:\tlearn: 0.1849356\ttotal: 10.4s\tremaining: 1m 1s\n",
            "144:\tlearn: 0.1841422\ttotal: 10.4s\tremaining: 1m 1s\n",
            "145:\tlearn: 0.1835695\ttotal: 10.4s\tremaining: 1m 1s\n",
            "146:\tlearn: 0.1832468\ttotal: 10.5s\tremaining: 1m\n",
            "147:\tlearn: 0.1822903\ttotal: 10.5s\tremaining: 1m\n",
            "148:\tlearn: 0.1811054\ttotal: 10.6s\tremaining: 1m\n",
            "149:\tlearn: 0.1805412\ttotal: 10.6s\tremaining: 1m\n",
            "150:\tlearn: 0.1798957\ttotal: 10.7s\tremaining: 1m\n",
            "151:\tlearn: 0.1794593\ttotal: 10.7s\tremaining: 59.9s\n",
            "152:\tlearn: 0.1787641\ttotal: 10.8s\tremaining: 59.8s\n",
            "153:\tlearn: 0.1779125\ttotal: 10.8s\tremaining: 59.6s\n",
            "154:\tlearn: 0.1769334\ttotal: 10.9s\tremaining: 59.4s\n",
            "155:\tlearn: 0.1762522\ttotal: 10.9s\tremaining: 59.2s\n",
            "156:\tlearn: 0.1755360\ttotal: 11s\tremaining: 59s\n",
            "157:\tlearn: 0.1751211\ttotal: 11s\tremaining: 58.9s\n",
            "158:\tlearn: 0.1746369\ttotal: 11.1s\tremaining: 58.7s\n",
            "159:\tlearn: 0.1739673\ttotal: 11.1s\tremaining: 58.5s\n",
            "160:\tlearn: 0.1735473\ttotal: 11.2s\tremaining: 58.3s\n",
            "161:\tlearn: 0.1731306\ttotal: 11.2s\tremaining: 58.2s\n",
            "162:\tlearn: 0.1731026\ttotal: 11.3s\tremaining: 58.1s\n",
            "163:\tlearn: 0.1724594\ttotal: 11.4s\tremaining: 57.9s\n",
            "164:\tlearn: 0.1716434\ttotal: 11.4s\tremaining: 57.7s\n",
            "165:\tlearn: 0.1711871\ttotal: 11.5s\tremaining: 57.6s\n",
            "166:\tlearn: 0.1706904\ttotal: 11.5s\tremaining: 57.4s\n",
            "167:\tlearn: 0.1704011\ttotal: 11.6s\tremaining: 57.3s\n",
            "168:\tlearn: 0.1698379\ttotal: 11.6s\tremaining: 57.1s\n",
            "169:\tlearn: 0.1693214\ttotal: 11.7s\tremaining: 56.9s\n",
            "170:\tlearn: 0.1684932\ttotal: 11.7s\tremaining: 56.7s\n",
            "171:\tlearn: 0.1678488\ttotal: 11.8s\tremaining: 56.6s\n",
            "172:\tlearn: 0.1672880\ttotal: 11.8s\tremaining: 56.4s\n",
            "173:\tlearn: 0.1672703\ttotal: 11.9s\tremaining: 56.3s\n",
            "174:\tlearn: 0.1662985\ttotal: 11.9s\tremaining: 56.1s\n",
            "175:\tlearn: 0.1656548\ttotal: 12s\tremaining: 56s\n",
            "176:\tlearn: 0.1653845\ttotal: 12s\tremaining: 55.8s\n",
            "177:\tlearn: 0.1648530\ttotal: 12.1s\tremaining: 55.7s\n",
            "178:\tlearn: 0.1646361\ttotal: 12.1s\tremaining: 55.5s\n",
            "179:\tlearn: 0.1646223\ttotal: 12.2s\tremaining: 55.4s\n",
            "180:\tlearn: 0.1646163\ttotal: 12.2s\tremaining: 55.1s\n",
            "181:\tlearn: 0.1639545\ttotal: 12.2s\tremaining: 54.9s\n",
            "182:\tlearn: 0.1636136\ttotal: 12.3s\tremaining: 54.9s\n",
            "183:\tlearn: 0.1635836\ttotal: 12.3s\tremaining: 54.7s\n",
            "184:\tlearn: 0.1630567\ttotal: 12.4s\tremaining: 54.6s\n",
            "185:\tlearn: 0.1626909\ttotal: 12.4s\tremaining: 54.4s\n",
            "186:\tlearn: 0.1622260\ttotal: 12.5s\tremaining: 54.3s\n",
            "187:\tlearn: 0.1619956\ttotal: 12.5s\tremaining: 54.1s\n",
            "188:\tlearn: 0.1618976\ttotal: 12.6s\tremaining: 54s\n",
            "189:\tlearn: 0.1612968\ttotal: 12.6s\tremaining: 53.9s\n",
            "190:\tlearn: 0.1609440\ttotal: 12.7s\tremaining: 53.7s\n",
            "191:\tlearn: 0.1605779\ttotal: 12.7s\tremaining: 53.6s\n",
            "192:\tlearn: 0.1600670\ttotal: 12.8s\tremaining: 53.4s\n",
            "193:\tlearn: 0.1594319\ttotal: 12.8s\tremaining: 53.3s\n",
            "194:\tlearn: 0.1591713\ttotal: 12.9s\tremaining: 53.2s\n",
            "195:\tlearn: 0.1586319\ttotal: 12.9s\tremaining: 53s\n",
            "196:\tlearn: 0.1583851\ttotal: 13s\tremaining: 52.9s\n",
            "197:\tlearn: 0.1577301\ttotal: 13s\tremaining: 52.8s\n",
            "198:\tlearn: 0.1572325\ttotal: 13.1s\tremaining: 52.6s\n",
            "199:\tlearn: 0.1565746\ttotal: 13.1s\tremaining: 52.5s\n",
            "200:\tlearn: 0.1562333\ttotal: 13.2s\tremaining: 52.3s\n",
            "201:\tlearn: 0.1554473\ttotal: 13.2s\tremaining: 52.2s\n",
            "202:\tlearn: 0.1553157\ttotal: 13.3s\tremaining: 52.1s\n",
            "203:\tlearn: 0.1552545\ttotal: 13.3s\tremaining: 52s\n",
            "204:\tlearn: 0.1549404\ttotal: 13.4s\tremaining: 51.9s\n",
            "205:\tlearn: 0.1543264\ttotal: 13.4s\tremaining: 51.7s\n",
            "206:\tlearn: 0.1538661\ttotal: 13.5s\tremaining: 51.6s\n",
            "207:\tlearn: 0.1534977\ttotal: 13.5s\tremaining: 51.5s\n",
            "208:\tlearn: 0.1530669\ttotal: 13.6s\tremaining: 51.4s\n",
            "209:\tlearn: 0.1528653\ttotal: 13.6s\tremaining: 51.2s\n",
            "210:\tlearn: 0.1523321\ttotal: 13.7s\tremaining: 51.1s\n",
            "211:\tlearn: 0.1520492\ttotal: 13.7s\tremaining: 51s\n",
            "212:\tlearn: 0.1518376\ttotal: 13.8s\tremaining: 50.9s\n",
            "213:\tlearn: 0.1514687\ttotal: 13.8s\tremaining: 50.7s\n",
            "214:\tlearn: 0.1511550\ttotal: 13.9s\tremaining: 50.6s\n",
            "215:\tlearn: 0.1507161\ttotal: 13.9s\tremaining: 50.5s\n",
            "216:\tlearn: 0.1501382\ttotal: 14s\tremaining: 50.4s\n",
            "217:\tlearn: 0.1499718\ttotal: 14s\tremaining: 50.3s\n",
            "218:\tlearn: 0.1494357\ttotal: 14.1s\tremaining: 50.1s\n",
            "219:\tlearn: 0.1491390\ttotal: 14.1s\tremaining: 50s\n",
            "220:\tlearn: 0.1486009\ttotal: 14.2s\tremaining: 49.9s\n",
            "221:\tlearn: 0.1481091\ttotal: 14.2s\tremaining: 49.8s\n",
            "222:\tlearn: 0.1476683\ttotal: 14.3s\tremaining: 49.7s\n",
            "223:\tlearn: 0.1472218\ttotal: 14.3s\tremaining: 49.6s\n",
            "224:\tlearn: 0.1469587\ttotal: 14.4s\tremaining: 49.5s\n",
            "225:\tlearn: 0.1468837\ttotal: 14.4s\tremaining: 49.4s\n",
            "226:\tlearn: 0.1466780\ttotal: 14.5s\tremaining: 49.3s\n",
            "227:\tlearn: 0.1464551\ttotal: 14.5s\tremaining: 49.2s\n",
            "228:\tlearn: 0.1462157\ttotal: 14.6s\tremaining: 49s\n",
            "229:\tlearn: 0.1460339\ttotal: 14.6s\tremaining: 48.9s\n",
            "230:\tlearn: 0.1456581\ttotal: 14.7s\tremaining: 48.8s\n",
            "231:\tlearn: 0.1453460\ttotal: 14.7s\tremaining: 48.7s\n",
            "232:\tlearn: 0.1451439\ttotal: 14.8s\tremaining: 48.6s\n",
            "233:\tlearn: 0.1444804\ttotal: 14.8s\tremaining: 48.5s\n",
            "234:\tlearn: 0.1442317\ttotal: 14.9s\tremaining: 48.4s\n",
            "235:\tlearn: 0.1440058\ttotal: 14.9s\tremaining: 48.2s\n",
            "236:\tlearn: 0.1434387\ttotal: 15s\tremaining: 48.2s\n",
            "237:\tlearn: 0.1430319\ttotal: 15s\tremaining: 48s\n",
            "238:\tlearn: 0.1428297\ttotal: 15.1s\tremaining: 47.9s\n",
            "239:\tlearn: 0.1425671\ttotal: 15.1s\tremaining: 47.8s\n",
            "240:\tlearn: 0.1423094\ttotal: 15.1s\tremaining: 47.7s\n",
            "241:\tlearn: 0.1420739\ttotal: 15.2s\tremaining: 47.6s\n",
            "242:\tlearn: 0.1418218\ttotal: 15.2s\tremaining: 47.5s\n",
            "243:\tlearn: 0.1413208\ttotal: 15.3s\tremaining: 47.4s\n",
            "244:\tlearn: 0.1410688\ttotal: 15.4s\tremaining: 47.3s\n",
            "245:\tlearn: 0.1410379\ttotal: 15.4s\tremaining: 47.2s\n",
            "246:\tlearn: 0.1407446\ttotal: 15.4s\tremaining: 47.1s\n",
            "247:\tlearn: 0.1405544\ttotal: 15.5s\tremaining: 47s\n",
            "248:\tlearn: 0.1403709\ttotal: 15.5s\tremaining: 46.9s\n",
            "249:\tlearn: 0.1399738\ttotal: 15.6s\tremaining: 46.7s\n",
            "250:\tlearn: 0.1396288\ttotal: 15.6s\tremaining: 46.6s\n",
            "251:\tlearn: 0.1392036\ttotal: 15.7s\tremaining: 46.6s\n",
            "252:\tlearn: 0.1388348\ttotal: 15.7s\tremaining: 46.5s\n",
            "253:\tlearn: 0.1385323\ttotal: 15.8s\tremaining: 46.3s\n",
            "254:\tlearn: 0.1382774\ttotal: 15.8s\tremaining: 46.3s\n",
            "255:\tlearn: 0.1382588\ttotal: 15.9s\tremaining: 46.1s\n",
            "256:\tlearn: 0.1379093\ttotal: 15.9s\tremaining: 46.1s\n",
            "257:\tlearn: 0.1375904\ttotal: 16s\tremaining: 46s\n",
            "258:\tlearn: 0.1370260\ttotal: 16s\tremaining: 45.9s\n",
            "259:\tlearn: 0.1367146\ttotal: 16.1s\tremaining: 45.8s\n",
            "260:\tlearn: 0.1363810\ttotal: 16.1s\tremaining: 45.7s\n",
            "261:\tlearn: 0.1358415\ttotal: 16.2s\tremaining: 45.6s\n",
            "262:\tlearn: 0.1356039\ttotal: 16.2s\tremaining: 45.5s\n",
            "263:\tlearn: 0.1355168\ttotal: 16.3s\tremaining: 45.4s\n",
            "264:\tlearn: 0.1353074\ttotal: 16.4s\tremaining: 45.4s\n",
            "265:\tlearn: 0.1349245\ttotal: 16.4s\tremaining: 45.4s\n",
            "266:\tlearn: 0.1347581\ttotal: 16.5s\tremaining: 45.3s\n",
            "267:\tlearn: 0.1345557\ttotal: 16.6s\tremaining: 45.3s\n",
            "268:\tlearn: 0.1343738\ttotal: 16.7s\tremaining: 45.3s\n",
            "269:\tlearn: 0.1340659\ttotal: 16.7s\tremaining: 45.2s\n",
            "270:\tlearn: 0.1340427\ttotal: 16.8s\tremaining: 45.3s\n",
            "271:\tlearn: 0.1336793\ttotal: 16.9s\tremaining: 45.2s\n",
            "272:\tlearn: 0.1334809\ttotal: 17s\tremaining: 45.2s\n",
            "273:\tlearn: 0.1334422\ttotal: 17.1s\tremaining: 45.2s\n",
            "274:\tlearn: 0.1332802\ttotal: 17.2s\tremaining: 45.2s\n",
            "275:\tlearn: 0.1330318\ttotal: 17.2s\tremaining: 45.2s\n",
            "276:\tlearn: 0.1328231\ttotal: 17.3s\tremaining: 45.2s\n",
            "277:\tlearn: 0.1326370\ttotal: 17.4s\tremaining: 45.2s\n",
            "278:\tlearn: 0.1324555\ttotal: 17.5s\tremaining: 45.2s\n",
            "279:\tlearn: 0.1323045\ttotal: 17.6s\tremaining: 45.2s\n",
            "280:\tlearn: 0.1322018\ttotal: 17.7s\tremaining: 45.3s\n",
            "281:\tlearn: 0.1320679\ttotal: 17.8s\tremaining: 45.3s\n",
            "282:\tlearn: 0.1316720\ttotal: 17.9s\tremaining: 45.3s\n",
            "283:\tlearn: 0.1314599\ttotal: 18s\tremaining: 45.3s\n",
            "284:\tlearn: 0.1313628\ttotal: 18.1s\tremaining: 45.3s\n",
            "285:\tlearn: 0.1311730\ttotal: 18.1s\tremaining: 45.3s\n",
            "286:\tlearn: 0.1309882\ttotal: 18.2s\tremaining: 45.3s\n",
            "287:\tlearn: 0.1308239\ttotal: 18.3s\tremaining: 45.2s\n",
            "288:\tlearn: 0.1305786\ttotal: 18.4s\tremaining: 45.2s\n",
            "289:\tlearn: 0.1302707\ttotal: 18.5s\tremaining: 45.2s\n",
            "290:\tlearn: 0.1299941\ttotal: 18.6s\tremaining: 45.3s\n",
            "291:\tlearn: 0.1296034\ttotal: 18.7s\tremaining: 45.2s\n",
            "292:\tlearn: 0.1293928\ttotal: 18.7s\tremaining: 45.2s\n",
            "293:\tlearn: 0.1291317\ttotal: 18.8s\tremaining: 45.2s\n",
            "294:\tlearn: 0.1288529\ttotal: 18.9s\tremaining: 45.2s\n",
            "295:\tlearn: 0.1286936\ttotal: 19s\tremaining: 45.2s\n",
            "296:\tlearn: 0.1285564\ttotal: 19.1s\tremaining: 45.2s\n",
            "297:\tlearn: 0.1283731\ttotal: 19.2s\tremaining: 45.2s\n",
            "298:\tlearn: 0.1283064\ttotal: 19.3s\tremaining: 45.2s\n",
            "299:\tlearn: 0.1280737\ttotal: 19.4s\tremaining: 45.2s\n",
            "300:\tlearn: 0.1280162\ttotal: 19.4s\tremaining: 45.2s\n",
            "301:\tlearn: 0.1277955\ttotal: 19.5s\tremaining: 45.1s\n",
            "302:\tlearn: 0.1275583\ttotal: 19.6s\tremaining: 45.1s\n",
            "303:\tlearn: 0.1273912\ttotal: 19.7s\tremaining: 45.1s\n",
            "304:\tlearn: 0.1272707\ttotal: 19.8s\tremaining: 45.1s\n",
            "305:\tlearn: 0.1270555\ttotal: 19.9s\tremaining: 45s\n",
            "306:\tlearn: 0.1267809\ttotal: 19.9s\tremaining: 44.9s\n",
            "307:\tlearn: 0.1265444\ttotal: 20s\tremaining: 44.8s\n",
            "308:\tlearn: 0.1263589\ttotal: 20s\tremaining: 44.8s\n",
            "309:\tlearn: 0.1261674\ttotal: 20.1s\tremaining: 44.7s\n",
            "310:\tlearn: 0.1259166\ttotal: 20.1s\tremaining: 44.5s\n",
            "311:\tlearn: 0.1257814\ttotal: 20.2s\tremaining: 44.4s\n",
            "312:\tlearn: 0.1257683\ttotal: 20.2s\tremaining: 44.3s\n",
            "313:\tlearn: 0.1256196\ttotal: 20.3s\tremaining: 44.3s\n",
            "314:\tlearn: 0.1255009\ttotal: 20.3s\tremaining: 44.2s\n",
            "315:\tlearn: 0.1253714\ttotal: 20.4s\tremaining: 44.1s\n",
            "316:\tlearn: 0.1250909\ttotal: 20.4s\tremaining: 44s\n",
            "317:\tlearn: 0.1249398\ttotal: 20.4s\tremaining: 43.8s\n",
            "318:\tlearn: 0.1247128\ttotal: 20.5s\tremaining: 43.8s\n",
            "319:\tlearn: 0.1244757\ttotal: 20.6s\tremaining: 43.7s\n",
            "320:\tlearn: 0.1241936\ttotal: 20.6s\tremaining: 43.6s\n",
            "321:\tlearn: 0.1240746\ttotal: 20.7s\tremaining: 43.5s\n",
            "322:\tlearn: 0.1240302\ttotal: 20.7s\tremaining: 43.4s\n",
            "323:\tlearn: 0.1239342\ttotal: 20.8s\tremaining: 43.3s\n",
            "324:\tlearn: 0.1239088\ttotal: 20.8s\tremaining: 43.2s\n",
            "325:\tlearn: 0.1236344\ttotal: 20.9s\tremaining: 43.1s\n",
            "326:\tlearn: 0.1234708\ttotal: 20.9s\tremaining: 43s\n",
            "327:\tlearn: 0.1233316\ttotal: 21s\tremaining: 43s\n",
            "328:\tlearn: 0.1233232\ttotal: 21s\tremaining: 42.8s\n",
            "329:\tlearn: 0.1231149\ttotal: 21s\tremaining: 42.7s\n",
            "330:\tlearn: 0.1229551\ttotal: 21.1s\tremaining: 42.6s\n",
            "331:\tlearn: 0.1227495\ttotal: 21.1s\tremaining: 42.5s\n",
            "332:\tlearn: 0.1225605\ttotal: 21.2s\tremaining: 42.4s\n",
            "333:\tlearn: 0.1224132\ttotal: 21.2s\tremaining: 42.3s\n",
            "334:\tlearn: 0.1223086\ttotal: 21.3s\tremaining: 42.2s\n",
            "335:\tlearn: 0.1221750\ttotal: 21.3s\tremaining: 42.1s\n",
            "336:\tlearn: 0.1220732\ttotal: 21.4s\tremaining: 42s\n",
            "337:\tlearn: 0.1219650\ttotal: 21.4s\tremaining: 42s\n",
            "338:\tlearn: 0.1218128\ttotal: 21.5s\tremaining: 41.9s\n",
            "339:\tlearn: 0.1217857\ttotal: 21.5s\tremaining: 41.8s\n",
            "340:\tlearn: 0.1216526\ttotal: 21.6s\tremaining: 41.7s\n",
            "341:\tlearn: 0.1216188\ttotal: 21.6s\tremaining: 41.6s\n",
            "342:\tlearn: 0.1214796\ttotal: 21.7s\tremaining: 41.5s\n",
            "343:\tlearn: 0.1213319\ttotal: 21.7s\tremaining: 41.4s\n",
            "344:\tlearn: 0.1212702\ttotal: 21.8s\tremaining: 41.3s\n",
            "345:\tlearn: 0.1210479\ttotal: 21.8s\tremaining: 41.2s\n",
            "346:\tlearn: 0.1208974\ttotal: 21.9s\tremaining: 41.2s\n",
            "347:\tlearn: 0.1207043\ttotal: 21.9s\tremaining: 41.1s\n",
            "348:\tlearn: 0.1205598\ttotal: 22s\tremaining: 41s\n",
            "349:\tlearn: 0.1203621\ttotal: 22s\tremaining: 40.9s\n",
            "350:\tlearn: 0.1202276\ttotal: 22.1s\tremaining: 40.8s\n",
            "351:\tlearn: 0.1200692\ttotal: 22.1s\tremaining: 40.7s\n",
            "352:\tlearn: 0.1199130\ttotal: 22.2s\tremaining: 40.6s\n",
            "353:\tlearn: 0.1198398\ttotal: 22.2s\tremaining: 40.5s\n",
            "354:\tlearn: 0.1197745\ttotal: 22.3s\tremaining: 40.5s\n",
            "355:\tlearn: 0.1197260\ttotal: 22.3s\tremaining: 40.4s\n",
            "356:\tlearn: 0.1196288\ttotal: 22.4s\tremaining: 40.3s\n",
            "357:\tlearn: 0.1195577\ttotal: 22.4s\tremaining: 40.2s\n",
            "358:\tlearn: 0.1194075\ttotal: 22.5s\tremaining: 40.1s\n",
            "359:\tlearn: 0.1192610\ttotal: 22.5s\tremaining: 40s\n",
            "360:\tlearn: 0.1190894\ttotal: 22.6s\tremaining: 39.9s\n",
            "361:\tlearn: 0.1189696\ttotal: 22.6s\tremaining: 39.9s\n",
            "362:\tlearn: 0.1188333\ttotal: 22.7s\tremaining: 39.8s\n",
            "363:\tlearn: 0.1188298\ttotal: 22.7s\tremaining: 39.7s\n",
            "364:\tlearn: 0.1188182\ttotal: 22.8s\tremaining: 39.6s\n",
            "365:\tlearn: 0.1186887\ttotal: 22.8s\tremaining: 39.5s\n",
            "366:\tlearn: 0.1184514\ttotal: 22.9s\tremaining: 39.4s\n",
            "367:\tlearn: 0.1183592\ttotal: 22.9s\tremaining: 39.4s\n",
            "368:\tlearn: 0.1182960\ttotal: 23s\tremaining: 39.3s\n",
            "369:\tlearn: 0.1182161\ttotal: 23s\tremaining: 39.2s\n",
            "370:\tlearn: 0.1181462\ttotal: 23.1s\tremaining: 39.1s\n",
            "371:\tlearn: 0.1179074\ttotal: 23.1s\tremaining: 39s\n",
            "372:\tlearn: 0.1177919\ttotal: 23.2s\tremaining: 38.9s\n",
            "373:\tlearn: 0.1177080\ttotal: 23.2s\tremaining: 38.9s\n",
            "374:\tlearn: 0.1176538\ttotal: 23.3s\tremaining: 38.8s\n",
            "375:\tlearn: 0.1175356\ttotal: 23.3s\tremaining: 38.7s\n",
            "376:\tlearn: 0.1174957\ttotal: 23.4s\tremaining: 38.6s\n",
            "377:\tlearn: 0.1173552\ttotal: 23.4s\tremaining: 38.5s\n",
            "378:\tlearn: 0.1173193\ttotal: 23.5s\tremaining: 38.4s\n",
            "379:\tlearn: 0.1171744\ttotal: 23.5s\tremaining: 38.4s\n",
            "380:\tlearn: 0.1170279\ttotal: 23.6s\tremaining: 38.3s\n",
            "381:\tlearn: 0.1168737\ttotal: 23.6s\tremaining: 38.2s\n",
            "382:\tlearn: 0.1167488\ttotal: 23.7s\tremaining: 38.1s\n",
            "383:\tlearn: 0.1166575\ttotal: 23.7s\tremaining: 38.1s\n",
            "384:\tlearn: 0.1165461\ttotal: 23.8s\tremaining: 38s\n",
            "385:\tlearn: 0.1164056\ttotal: 23.8s\tremaining: 37.9s\n",
            "386:\tlearn: 0.1163116\ttotal: 23.9s\tremaining: 37.8s\n",
            "387:\tlearn: 0.1162558\ttotal: 23.9s\tremaining: 37.7s\n",
            "388:\tlearn: 0.1162217\ttotal: 24s\tremaining: 37.7s\n",
            "389:\tlearn: 0.1161550\ttotal: 24s\tremaining: 37.6s\n",
            "390:\tlearn: 0.1159714\ttotal: 24.1s\tremaining: 37.5s\n",
            "391:\tlearn: 0.1158610\ttotal: 24.1s\tremaining: 37.4s\n",
            "392:\tlearn: 0.1157245\ttotal: 24.2s\tremaining: 37.3s\n",
            "393:\tlearn: 0.1156364\ttotal: 24.2s\tremaining: 37.3s\n",
            "394:\tlearn: 0.1155458\ttotal: 24.3s\tremaining: 37.2s\n",
            "395:\tlearn: 0.1154001\ttotal: 24.3s\tremaining: 37.1s\n",
            "396:\tlearn: 0.1153425\ttotal: 24.4s\tremaining: 37s\n",
            "397:\tlearn: 0.1152160\ttotal: 24.4s\tremaining: 36.9s\n",
            "398:\tlearn: 0.1150900\ttotal: 24.5s\tremaining: 36.9s\n",
            "399:\tlearn: 0.1150430\ttotal: 24.5s\tremaining: 36.8s\n",
            "400:\tlearn: 0.1149745\ttotal: 24.6s\tremaining: 36.7s\n",
            "401:\tlearn: 0.1148520\ttotal: 24.6s\tremaining: 36.6s\n",
            "402:\tlearn: 0.1147136\ttotal: 24.7s\tremaining: 36.6s\n",
            "403:\tlearn: 0.1145535\ttotal: 24.7s\tremaining: 36.5s\n",
            "404:\tlearn: 0.1144519\ttotal: 24.8s\tremaining: 36.4s\n",
            "405:\tlearn: 0.1143709\ttotal: 24.8s\tremaining: 36.3s\n",
            "406:\tlearn: 0.1142271\ttotal: 24.9s\tremaining: 36.2s\n",
            "407:\tlearn: 0.1140672\ttotal: 24.9s\tremaining: 36.2s\n",
            "408:\tlearn: 0.1139558\ttotal: 25s\tremaining: 36.1s\n",
            "409:\tlearn: 0.1138994\ttotal: 25s\tremaining: 36s\n",
            "410:\tlearn: 0.1137906\ttotal: 25.1s\tremaining: 35.9s\n",
            "411:\tlearn: 0.1137029\ttotal: 25.1s\tremaining: 35.8s\n",
            "412:\tlearn: 0.1136214\ttotal: 25.2s\tremaining: 35.8s\n",
            "413:\tlearn: 0.1135422\ttotal: 25.2s\tremaining: 35.7s\n",
            "414:\tlearn: 0.1135229\ttotal: 25.3s\tremaining: 35.6s\n",
            "415:\tlearn: 0.1134775\ttotal: 25.3s\tremaining: 35.5s\n",
            "416:\tlearn: 0.1133795\ttotal: 25.4s\tremaining: 35.5s\n",
            "417:\tlearn: 0.1133294\ttotal: 25.4s\tremaining: 35.4s\n",
            "418:\tlearn: 0.1133082\ttotal: 25.5s\tremaining: 35.3s\n",
            "419:\tlearn: 0.1132106\ttotal: 25.5s\tremaining: 35.2s\n",
            "420:\tlearn: 0.1130854\ttotal: 25.6s\tremaining: 35.1s\n",
            "421:\tlearn: 0.1129969\ttotal: 25.6s\tremaining: 35.1s\n",
            "422:\tlearn: 0.1129192\ttotal: 25.7s\tremaining: 35s\n",
            "423:\tlearn: 0.1128184\ttotal: 25.7s\tremaining: 34.9s\n",
            "424:\tlearn: 0.1127581\ttotal: 25.8s\tremaining: 34.9s\n",
            "425:\tlearn: 0.1127397\ttotal: 25.8s\tremaining: 34.8s\n",
            "426:\tlearn: 0.1127237\ttotal: 25.9s\tremaining: 34.7s\n",
            "427:\tlearn: 0.1127170\ttotal: 25.9s\tremaining: 34.6s\n",
            "428:\tlearn: 0.1127037\ttotal: 26s\tremaining: 34.6s\n",
            "429:\tlearn: 0.1125861\ttotal: 26s\tremaining: 34.5s\n",
            "430:\tlearn: 0.1125457\ttotal: 26.1s\tremaining: 34.4s\n",
            "431:\tlearn: 0.1125068\ttotal: 26.1s\tremaining: 34.3s\n",
            "432:\tlearn: 0.1124458\ttotal: 26.2s\tremaining: 34.3s\n",
            "433:\tlearn: 0.1124251\ttotal: 26.2s\tremaining: 34.2s\n",
            "434:\tlearn: 0.1123018\ttotal: 26.3s\tremaining: 34.1s\n",
            "435:\tlearn: 0.1122037\ttotal: 26.3s\tremaining: 34s\n",
            "436:\tlearn: 0.1121340\ttotal: 26.4s\tremaining: 34s\n",
            "437:\tlearn: 0.1120281\ttotal: 26.4s\tremaining: 33.9s\n",
            "438:\tlearn: 0.1118704\ttotal: 26.5s\tremaining: 33.8s\n",
            "439:\tlearn: 0.1118152\ttotal: 26.5s\tremaining: 33.7s\n",
            "440:\tlearn: 0.1117205\ttotal: 26.6s\tremaining: 33.7s\n",
            "441:\tlearn: 0.1115853\ttotal: 26.6s\tremaining: 33.6s\n",
            "442:\tlearn: 0.1115080\ttotal: 26.7s\tremaining: 33.5s\n",
            "443:\tlearn: 0.1114024\ttotal: 26.7s\tremaining: 33.4s\n",
            "444:\tlearn: 0.1113770\ttotal: 26.8s\tremaining: 33.4s\n",
            "445:\tlearn: 0.1112926\ttotal: 26.8s\tremaining: 33.3s\n",
            "446:\tlearn: 0.1111904\ttotal: 26.9s\tremaining: 33.2s\n",
            "447:\tlearn: 0.1111047\ttotal: 26.9s\tremaining: 33.2s\n",
            "448:\tlearn: 0.1109985\ttotal: 27s\tremaining: 33.1s\n",
            "449:\tlearn: 0.1108705\ttotal: 27s\tremaining: 33s\n",
            "450:\tlearn: 0.1108117\ttotal: 27s\tremaining: 32.9s\n",
            "451:\tlearn: 0.1107078\ttotal: 27.1s\tremaining: 32.9s\n",
            "452:\tlearn: 0.1106102\ttotal: 27.2s\tremaining: 32.8s\n",
            "453:\tlearn: 0.1105429\ttotal: 27.2s\tremaining: 32.7s\n",
            "454:\tlearn: 0.1104521\ttotal: 27.2s\tremaining: 32.6s\n",
            "455:\tlearn: 0.1103915\ttotal: 27.3s\tremaining: 32.6s\n",
            "456:\tlearn: 0.1103434\ttotal: 27.3s\tremaining: 32.5s\n",
            "457:\tlearn: 0.1103416\ttotal: 27.4s\tremaining: 32.4s\n",
            "458:\tlearn: 0.1103043\ttotal: 27.4s\tremaining: 32.3s\n",
            "459:\tlearn: 0.1101972\ttotal: 27.5s\tremaining: 32.3s\n",
            "460:\tlearn: 0.1100752\ttotal: 27.5s\tremaining: 32.2s\n",
            "461:\tlearn: 0.1100464\ttotal: 27.6s\tremaining: 32.1s\n",
            "462:\tlearn: 0.1100381\ttotal: 27.6s\tremaining: 32s\n",
            "463:\tlearn: 0.1099609\ttotal: 27.7s\tremaining: 32s\n",
            "464:\tlearn: 0.1099109\ttotal: 27.7s\tremaining: 31.9s\n",
            "465:\tlearn: 0.1098338\ttotal: 27.8s\tremaining: 31.8s\n",
            "466:\tlearn: 0.1098108\ttotal: 27.8s\tremaining: 31.8s\n",
            "467:\tlearn: 0.1097519\ttotal: 27.9s\tremaining: 31.7s\n",
            "468:\tlearn: 0.1096509\ttotal: 27.9s\tremaining: 31.6s\n",
            "469:\tlearn: 0.1095816\ttotal: 28s\tremaining: 31.6s\n",
            "470:\tlearn: 0.1095732\ttotal: 28s\tremaining: 31.5s\n",
            "471:\tlearn: 0.1094261\ttotal: 28.1s\tremaining: 31.4s\n",
            "472:\tlearn: 0.1093580\ttotal: 28.1s\tremaining: 31.3s\n",
            "473:\tlearn: 0.1092200\ttotal: 28.2s\tremaining: 31.3s\n",
            "474:\tlearn: 0.1091381\ttotal: 28.2s\tremaining: 31.2s\n",
            "475:\tlearn: 0.1090799\ttotal: 28.3s\tremaining: 31.1s\n",
            "476:\tlearn: 0.1090678\ttotal: 28.3s\tremaining: 31.1s\n",
            "477:\tlearn: 0.1090059\ttotal: 28.4s\tremaining: 31s\n",
            "478:\tlearn: 0.1089025\ttotal: 28.4s\tremaining: 30.9s\n",
            "479:\tlearn: 0.1087460\ttotal: 28.5s\tremaining: 30.8s\n",
            "480:\tlearn: 0.1086647\ttotal: 28.5s\tremaining: 30.8s\n",
            "481:\tlearn: 0.1086216\ttotal: 28.6s\tremaining: 30.7s\n",
            "482:\tlearn: 0.1085135\ttotal: 28.6s\tremaining: 30.6s\n",
            "483:\tlearn: 0.1084545\ttotal: 28.7s\tremaining: 30.6s\n",
            "484:\tlearn: 0.1083914\ttotal: 28.7s\tremaining: 30.5s\n",
            "485:\tlearn: 0.1082480\ttotal: 28.8s\tremaining: 30.4s\n",
            "486:\tlearn: 0.1081960\ttotal: 28.8s\tremaining: 30.4s\n",
            "487:\tlearn: 0.1081047\ttotal: 28.9s\tremaining: 30.3s\n",
            "488:\tlearn: 0.1080330\ttotal: 28.9s\tremaining: 30.2s\n",
            "489:\tlearn: 0.1079517\ttotal: 29s\tremaining: 30.2s\n",
            "490:\tlearn: 0.1079446\ttotal: 29s\tremaining: 30.1s\n",
            "491:\tlearn: 0.1078761\ttotal: 29.1s\tremaining: 30s\n",
            "492:\tlearn: 0.1078745\ttotal: 29.1s\tremaining: 29.9s\n",
            "493:\tlearn: 0.1078295\ttotal: 29.2s\tremaining: 29.9s\n",
            "494:\tlearn: 0.1077857\ttotal: 29.2s\tremaining: 29.8s\n",
            "495:\tlearn: 0.1077208\ttotal: 29.3s\tremaining: 29.7s\n",
            "496:\tlearn: 0.1076889\ttotal: 29.3s\tremaining: 29.7s\n",
            "497:\tlearn: 0.1076178\ttotal: 29.4s\tremaining: 29.6s\n",
            "498:\tlearn: 0.1075330\ttotal: 29.4s\tremaining: 29.5s\n",
            "499:\tlearn: 0.1074686\ttotal: 29.4s\tremaining: 29.4s\n",
            "500:\tlearn: 0.1074096\ttotal: 29.5s\tremaining: 29.4s\n",
            "501:\tlearn: 0.1074061\ttotal: 29.6s\tremaining: 29.3s\n",
            "502:\tlearn: 0.1073493\ttotal: 29.6s\tremaining: 29.2s\n",
            "503:\tlearn: 0.1072542\ttotal: 29.6s\tremaining: 29.2s\n",
            "504:\tlearn: 0.1072196\ttotal: 29.7s\tremaining: 29.1s\n",
            "505:\tlearn: 0.1071147\ttotal: 29.7s\tremaining: 29s\n",
            "506:\tlearn: 0.1070646\ttotal: 29.8s\tremaining: 29s\n",
            "507:\tlearn: 0.1070224\ttotal: 29.9s\tremaining: 28.9s\n",
            "508:\tlearn: 0.1069365\ttotal: 30s\tremaining: 28.9s\n",
            "509:\tlearn: 0.1068678\ttotal: 30s\tremaining: 28.9s\n",
            "510:\tlearn: 0.1068644\ttotal: 30.1s\tremaining: 28.8s\n",
            "511:\tlearn: 0.1067759\ttotal: 30.2s\tremaining: 28.8s\n",
            "512:\tlearn: 0.1067423\ttotal: 30.3s\tremaining: 28.7s\n",
            "513:\tlearn: 0.1067176\ttotal: 30.3s\tremaining: 28.7s\n",
            "514:\tlearn: 0.1066658\ttotal: 30.4s\tremaining: 28.6s\n",
            "515:\tlearn: 0.1066008\ttotal: 30.5s\tremaining: 28.6s\n",
            "516:\tlearn: 0.1065341\ttotal: 30.6s\tremaining: 28.6s\n",
            "517:\tlearn: 0.1064735\ttotal: 30.7s\tremaining: 28.5s\n",
            "518:\tlearn: 0.1064071\ttotal: 30.8s\tremaining: 28.5s\n",
            "519:\tlearn: 0.1063383\ttotal: 30.8s\tremaining: 28.5s\n",
            "520:\tlearn: 0.1062409\ttotal: 30.9s\tremaining: 28.4s\n",
            "521:\tlearn: 0.1061780\ttotal: 31s\tremaining: 28.4s\n",
            "522:\tlearn: 0.1061764\ttotal: 31.1s\tremaining: 28.4s\n",
            "523:\tlearn: 0.1060927\ttotal: 31.2s\tremaining: 28.3s\n",
            "524:\tlearn: 0.1060591\ttotal: 31.3s\tremaining: 28.3s\n",
            "525:\tlearn: 0.1059857\ttotal: 31.4s\tremaining: 28.3s\n",
            "526:\tlearn: 0.1059279\ttotal: 31.4s\tremaining: 28.2s\n",
            "527:\tlearn: 0.1058678\ttotal: 31.5s\tremaining: 28.2s\n",
            "528:\tlearn: 0.1057920\ttotal: 31.6s\tremaining: 28.1s\n",
            "529:\tlearn: 0.1057008\ttotal: 31.7s\tremaining: 28.1s\n",
            "530:\tlearn: 0.1056412\ttotal: 31.8s\tremaining: 28.1s\n",
            "531:\tlearn: 0.1055784\ttotal: 31.9s\tremaining: 28s\n",
            "532:\tlearn: 0.1055096\ttotal: 32s\tremaining: 28s\n",
            "533:\tlearn: 0.1054545\ttotal: 32.1s\tremaining: 28s\n",
            "534:\tlearn: 0.1053764\ttotal: 32.2s\tremaining: 27.9s\n",
            "535:\tlearn: 0.1053198\ttotal: 32.2s\tremaining: 27.9s\n",
            "536:\tlearn: 0.1052301\ttotal: 32.3s\tremaining: 27.9s\n",
            "537:\tlearn: 0.1051788\ttotal: 32.4s\tremaining: 27.8s\n",
            "538:\tlearn: 0.1051315\ttotal: 32.5s\tremaining: 27.8s\n",
            "539:\tlearn: 0.1050512\ttotal: 32.6s\tremaining: 27.8s\n",
            "540:\tlearn: 0.1049878\ttotal: 32.7s\tremaining: 27.7s\n",
            "541:\tlearn: 0.1049714\ttotal: 32.8s\tremaining: 27.7s\n",
            "542:\tlearn: 0.1049067\ttotal: 32.9s\tremaining: 27.7s\n",
            "543:\tlearn: 0.1048198\ttotal: 33s\tremaining: 27.6s\n",
            "544:\tlearn: 0.1047636\ttotal: 33.1s\tremaining: 27.6s\n",
            "545:\tlearn: 0.1047242\ttotal: 33.1s\tremaining: 27.6s\n",
            "546:\tlearn: 0.1047000\ttotal: 33.2s\tremaining: 27.5s\n",
            "547:\tlearn: 0.1046590\ttotal: 33.3s\tremaining: 27.5s\n",
            "548:\tlearn: 0.1045505\ttotal: 33.4s\tremaining: 27.5s\n",
            "549:\tlearn: 0.1044681\ttotal: 33.5s\tremaining: 27.4s\n",
            "550:\tlearn: 0.1043642\ttotal: 33.5s\tremaining: 27.3s\n",
            "551:\tlearn: 0.1042516\ttotal: 33.6s\tremaining: 27.3s\n",
            "552:\tlearn: 0.1042066\ttotal: 33.6s\tremaining: 27.2s\n",
            "553:\tlearn: 0.1041672\ttotal: 33.7s\tremaining: 27.1s\n",
            "554:\tlearn: 0.1041321\ttotal: 33.7s\tremaining: 27s\n",
            "555:\tlearn: 0.1041201\ttotal: 33.8s\tremaining: 27s\n",
            "556:\tlearn: 0.1040813\ttotal: 33.8s\tremaining: 26.9s\n",
            "557:\tlearn: 0.1040377\ttotal: 33.9s\tremaining: 26.8s\n",
            "558:\tlearn: 0.1040000\ttotal: 33.9s\tremaining: 26.8s\n",
            "559:\tlearn: 0.1039864\ttotal: 34s\tremaining: 26.7s\n",
            "560:\tlearn: 0.1039257\ttotal: 34s\tremaining: 26.6s\n",
            "561:\tlearn: 0.1038741\ttotal: 34.1s\tremaining: 26.6s\n",
            "562:\tlearn: 0.1038404\ttotal: 34.1s\tremaining: 26.5s\n",
            "563:\tlearn: 0.1037705\ttotal: 34.2s\tremaining: 26.4s\n",
            "564:\tlearn: 0.1037617\ttotal: 34.2s\tremaining: 26.4s\n",
            "565:\tlearn: 0.1037038\ttotal: 34.3s\tremaining: 26.3s\n",
            "566:\tlearn: 0.1036635\ttotal: 34.3s\tremaining: 26.2s\n",
            "567:\tlearn: 0.1036096\ttotal: 34.4s\tremaining: 26.2s\n",
            "568:\tlearn: 0.1036096\ttotal: 34.4s\tremaining: 26.1s\n",
            "569:\tlearn: 0.1036018\ttotal: 34.5s\tremaining: 26s\n",
            "570:\tlearn: 0.1035845\ttotal: 34.5s\tremaining: 25.9s\n",
            "571:\tlearn: 0.1035236\ttotal: 34.6s\tremaining: 25.9s\n",
            "572:\tlearn: 0.1034667\ttotal: 34.6s\tremaining: 25.8s\n",
            "573:\tlearn: 0.1033989\ttotal: 34.7s\tremaining: 25.7s\n",
            "574:\tlearn: 0.1033403\ttotal: 34.7s\tremaining: 25.7s\n",
            "575:\tlearn: 0.1033368\ttotal: 34.8s\tremaining: 25.6s\n",
            "576:\tlearn: 0.1033142\ttotal: 34.8s\tremaining: 25.5s\n",
            "577:\tlearn: 0.1032509\ttotal: 34.9s\tremaining: 25.4s\n",
            "578:\tlearn: 0.1032386\ttotal: 34.9s\tremaining: 25.4s\n",
            "579:\tlearn: 0.1031933\ttotal: 35s\tremaining: 25.3s\n",
            "580:\tlearn: 0.1031933\ttotal: 35s\tremaining: 25.2s\n",
            "581:\tlearn: 0.1031570\ttotal: 35s\tremaining: 25.2s\n",
            "582:\tlearn: 0.1031137\ttotal: 35.1s\tremaining: 25.1s\n",
            "583:\tlearn: 0.1030601\ttotal: 35.1s\tremaining: 25s\n",
            "584:\tlearn: 0.1029683\ttotal: 35.2s\tremaining: 25s\n",
            "585:\tlearn: 0.1029333\ttotal: 35.2s\tremaining: 24.9s\n",
            "586:\tlearn: 0.1029008\ttotal: 35.3s\tremaining: 24.8s\n",
            "587:\tlearn: 0.1028955\ttotal: 35.3s\tremaining: 24.8s\n",
            "588:\tlearn: 0.1028763\ttotal: 35.4s\tremaining: 24.7s\n",
            "589:\tlearn: 0.1028325\ttotal: 35.4s\tremaining: 24.6s\n",
            "590:\tlearn: 0.1027753\ttotal: 35.5s\tremaining: 24.6s\n",
            "591:\tlearn: 0.1027532\ttotal: 35.5s\tremaining: 24.5s\n",
            "592:\tlearn: 0.1027365\ttotal: 35.6s\tremaining: 24.4s\n",
            "593:\tlearn: 0.1027318\ttotal: 35.6s\tremaining: 24.3s\n",
            "594:\tlearn: 0.1027169\ttotal: 35.7s\tremaining: 24.3s\n",
            "595:\tlearn: 0.1026927\ttotal: 35.7s\tremaining: 24.2s\n",
            "596:\tlearn: 0.1026775\ttotal: 35.8s\tremaining: 24.1s\n",
            "597:\tlearn: 0.1026198\ttotal: 35.8s\tremaining: 24.1s\n",
            "598:\tlearn: 0.1025740\ttotal: 35.9s\tremaining: 24s\n",
            "599:\tlearn: 0.1025313\ttotal: 35.9s\tremaining: 23.9s\n",
            "600:\tlearn: 0.1024955\ttotal: 36s\tremaining: 23.9s\n",
            "601:\tlearn: 0.1024518\ttotal: 36s\tremaining: 23.8s\n",
            "602:\tlearn: 0.1024319\ttotal: 36.1s\tremaining: 23.7s\n",
            "603:\tlearn: 0.1024317\ttotal: 36.1s\tremaining: 23.7s\n",
            "604:\tlearn: 0.1023967\ttotal: 36.2s\tremaining: 23.6s\n",
            "605:\tlearn: 0.1023620\ttotal: 36.2s\tremaining: 23.5s\n",
            "606:\tlearn: 0.1023591\ttotal: 36.3s\tremaining: 23.5s\n",
            "607:\tlearn: 0.1023372\ttotal: 36.3s\tremaining: 23.4s\n",
            "608:\tlearn: 0.1023103\ttotal: 36.4s\tremaining: 23.3s\n",
            "609:\tlearn: 0.1022988\ttotal: 36.4s\tremaining: 23.3s\n",
            "610:\tlearn: 0.1022678\ttotal: 36.5s\tremaining: 23.2s\n",
            "611:\tlearn: 0.1022532\ttotal: 36.5s\tremaining: 23.1s\n",
            "612:\tlearn: 0.1022053\ttotal: 36.6s\tremaining: 23.1s\n",
            "613:\tlearn: 0.1022012\ttotal: 36.6s\tremaining: 23s\n",
            "614:\tlearn: 0.1021737\ttotal: 36.7s\tremaining: 22.9s\n",
            "615:\tlearn: 0.1021430\ttotal: 36.7s\tremaining: 22.9s\n",
            "616:\tlearn: 0.1020991\ttotal: 36.7s\tremaining: 22.8s\n",
            "617:\tlearn: 0.1020873\ttotal: 36.8s\tremaining: 22.7s\n",
            "618:\tlearn: 0.1020207\ttotal: 36.9s\tremaining: 22.7s\n",
            "619:\tlearn: 0.1019875\ttotal: 36.9s\tremaining: 22.6s\n",
            "620:\tlearn: 0.1019085\ttotal: 36.9s\tremaining: 22.5s\n",
            "621:\tlearn: 0.1018813\ttotal: 37s\tremaining: 22.5s\n",
            "622:\tlearn: 0.1018453\ttotal: 37s\tremaining: 22.4s\n",
            "623:\tlearn: 0.1017910\ttotal: 37.1s\tremaining: 22.4s\n",
            "624:\tlearn: 0.1017681\ttotal: 37.2s\tremaining: 22.3s\n",
            "625:\tlearn: 0.1017670\ttotal: 37.2s\tremaining: 22.2s\n",
            "626:\tlearn: 0.1017553\ttotal: 37.2s\tremaining: 22.2s\n",
            "627:\tlearn: 0.1016914\ttotal: 37.3s\tremaining: 22.1s\n",
            "628:\tlearn: 0.1016382\ttotal: 37.3s\tremaining: 22s\n",
            "629:\tlearn: 0.1015962\ttotal: 37.4s\tremaining: 22s\n",
            "630:\tlearn: 0.1015777\ttotal: 37.4s\tremaining: 21.9s\n",
            "631:\tlearn: 0.1015535\ttotal: 37.5s\tremaining: 21.8s\n",
            "632:\tlearn: 0.1014658\ttotal: 37.5s\tremaining: 21.8s\n",
            "633:\tlearn: 0.1014431\ttotal: 37.6s\tremaining: 21.7s\n",
            "634:\tlearn: 0.1014282\ttotal: 37.6s\tremaining: 21.6s\n",
            "635:\tlearn: 0.1014088\ttotal: 37.7s\tremaining: 21.6s\n",
            "636:\tlearn: 0.1013513\ttotal: 37.7s\tremaining: 21.5s\n",
            "637:\tlearn: 0.1012971\ttotal: 37.8s\tremaining: 21.4s\n",
            "638:\tlearn: 0.1012483\ttotal: 37.8s\tremaining: 21.4s\n",
            "639:\tlearn: 0.1012269\ttotal: 37.9s\tremaining: 21.3s\n",
            "640:\tlearn: 0.1011917\ttotal: 37.9s\tremaining: 21.2s\n",
            "641:\tlearn: 0.1011462\ttotal: 38s\tremaining: 21.2s\n",
            "642:\tlearn: 0.1011406\ttotal: 38s\tremaining: 21.1s\n",
            "643:\tlearn: 0.1010999\ttotal: 38.1s\tremaining: 21.1s\n",
            "644:\tlearn: 0.1010768\ttotal: 38.1s\tremaining: 21s\n",
            "645:\tlearn: 0.1010354\ttotal: 38.2s\tremaining: 20.9s\n",
            "646:\tlearn: 0.1010011\ttotal: 38.2s\tremaining: 20.9s\n",
            "647:\tlearn: 0.1009418\ttotal: 38.3s\tremaining: 20.8s\n",
            "648:\tlearn: 0.1008934\ttotal: 38.3s\tremaining: 20.7s\n",
            "649:\tlearn: 0.1008499\ttotal: 38.4s\tremaining: 20.7s\n",
            "650:\tlearn: 0.1008262\ttotal: 38.4s\tremaining: 20.6s\n",
            "651:\tlearn: 0.1008253\ttotal: 38.5s\tremaining: 20.5s\n",
            "652:\tlearn: 0.1008059\ttotal: 38.5s\tremaining: 20.5s\n",
            "653:\tlearn: 0.1007679\ttotal: 38.6s\tremaining: 20.4s\n",
            "654:\tlearn: 0.1007550\ttotal: 38.6s\tremaining: 20.3s\n",
            "655:\tlearn: 0.1007062\ttotal: 38.7s\tremaining: 20.3s\n",
            "656:\tlearn: 0.1006770\ttotal: 38.7s\tremaining: 20.2s\n",
            "657:\tlearn: 0.1006362\ttotal: 38.8s\tremaining: 20.2s\n",
            "658:\tlearn: 0.1005811\ttotal: 38.8s\tremaining: 20.1s\n",
            "659:\tlearn: 0.1005384\ttotal: 38.9s\tremaining: 20s\n",
            "660:\tlearn: 0.1004979\ttotal: 38.9s\tremaining: 20s\n",
            "661:\tlearn: 0.1004644\ttotal: 39s\tremaining: 19.9s\n",
            "662:\tlearn: 0.1004408\ttotal: 39s\tremaining: 19.8s\n",
            "663:\tlearn: 0.1003987\ttotal: 39.1s\tremaining: 19.8s\n",
            "664:\tlearn: 0.1003818\ttotal: 39.1s\tremaining: 19.7s\n",
            "665:\tlearn: 0.1003653\ttotal: 39.2s\tremaining: 19.6s\n",
            "666:\tlearn: 0.1003615\ttotal: 39.2s\tremaining: 19.6s\n",
            "667:\tlearn: 0.1003523\ttotal: 39.2s\tremaining: 19.5s\n",
            "668:\tlearn: 0.1003080\ttotal: 39.3s\tremaining: 19.4s\n",
            "669:\tlearn: 0.1002997\ttotal: 39.3s\tremaining: 19.4s\n",
            "670:\tlearn: 0.1002397\ttotal: 39.4s\tremaining: 19.3s\n",
            "671:\tlearn: 0.1001923\ttotal: 39.4s\tremaining: 19.2s\n",
            "672:\tlearn: 0.1001293\ttotal: 39.5s\tremaining: 19.2s\n",
            "673:\tlearn: 0.1000916\ttotal: 39.5s\tremaining: 19.1s\n",
            "674:\tlearn: 0.1000687\ttotal: 39.6s\tremaining: 19.1s\n",
            "675:\tlearn: 0.1000686\ttotal: 39.6s\tremaining: 19s\n",
            "676:\tlearn: 0.1000466\ttotal: 39.7s\tremaining: 18.9s\n",
            "677:\tlearn: 0.1000354\ttotal: 39.7s\tremaining: 18.9s\n",
            "678:\tlearn: 0.1000067\ttotal: 39.8s\tremaining: 18.8s\n",
            "679:\tlearn: 0.0999669\ttotal: 39.8s\tremaining: 18.7s\n",
            "680:\tlearn: 0.0999668\ttotal: 39.9s\tremaining: 18.7s\n",
            "681:\tlearn: 0.0999353\ttotal: 39.9s\tremaining: 18.6s\n",
            "682:\tlearn: 0.0998976\ttotal: 40s\tremaining: 18.6s\n",
            "683:\tlearn: 0.0998680\ttotal: 40s\tremaining: 18.5s\n",
            "684:\tlearn: 0.0998540\ttotal: 40.1s\tremaining: 18.4s\n",
            "685:\tlearn: 0.0998220\ttotal: 40.1s\tremaining: 18.4s\n",
            "686:\tlearn: 0.0997799\ttotal: 40.2s\tremaining: 18.3s\n",
            "687:\tlearn: 0.0997637\ttotal: 40.2s\tremaining: 18.2s\n",
            "688:\tlearn: 0.0997272\ttotal: 40.3s\tremaining: 18.2s\n",
            "689:\tlearn: 0.0996899\ttotal: 40.3s\tremaining: 18.1s\n",
            "690:\tlearn: 0.0996385\ttotal: 40.4s\tremaining: 18.1s\n",
            "691:\tlearn: 0.0996057\ttotal: 40.4s\tremaining: 18s\n",
            "692:\tlearn: 0.0995841\ttotal: 40.5s\tremaining: 17.9s\n",
            "693:\tlearn: 0.0995369\ttotal: 40.5s\tremaining: 17.9s\n",
            "694:\tlearn: 0.0994943\ttotal: 40.6s\tremaining: 17.8s\n",
            "695:\tlearn: 0.0994942\ttotal: 40.6s\tremaining: 17.7s\n",
            "696:\tlearn: 0.0994941\ttotal: 40.6s\tremaining: 17.7s\n",
            "697:\tlearn: 0.0994941\ttotal: 40.7s\tremaining: 17.6s\n",
            "698:\tlearn: 0.0994564\ttotal: 40.7s\tremaining: 17.5s\n",
            "699:\tlearn: 0.0994318\ttotal: 40.8s\tremaining: 17.5s\n",
            "700:\tlearn: 0.0993852\ttotal: 40.8s\tremaining: 17.4s\n",
            "701:\tlearn: 0.0993307\ttotal: 40.9s\tremaining: 17.3s\n",
            "702:\tlearn: 0.0993249\ttotal: 40.9s\tremaining: 17.3s\n",
            "703:\tlearn: 0.0992751\ttotal: 41s\tremaining: 17.2s\n",
            "704:\tlearn: 0.0992326\ttotal: 41s\tremaining: 17.2s\n",
            "705:\tlearn: 0.0992050\ttotal: 41.1s\tremaining: 17.1s\n",
            "706:\tlearn: 0.0991664\ttotal: 41.1s\tremaining: 17s\n",
            "707:\tlearn: 0.0991482\ttotal: 41.2s\tremaining: 17s\n",
            "708:\tlearn: 0.0991460\ttotal: 41.2s\tremaining: 16.9s\n",
            "709:\tlearn: 0.0991214\ttotal: 41.3s\tremaining: 16.9s\n",
            "710:\tlearn: 0.0991012\ttotal: 41.3s\tremaining: 16.8s\n",
            "711:\tlearn: 0.0990331\ttotal: 41.4s\tremaining: 16.7s\n",
            "712:\tlearn: 0.0990147\ttotal: 41.4s\tremaining: 16.7s\n",
            "713:\tlearn: 0.0990001\ttotal: 41.5s\tremaining: 16.6s\n",
            "714:\tlearn: 0.0989719\ttotal: 41.5s\tremaining: 16.5s\n",
            "715:\tlearn: 0.0989611\ttotal: 41.6s\tremaining: 16.5s\n",
            "716:\tlearn: 0.0989410\ttotal: 41.6s\tremaining: 16.4s\n",
            "717:\tlearn: 0.0989191\ttotal: 41.7s\tremaining: 16.4s\n",
            "718:\tlearn: 0.0988790\ttotal: 41.7s\tremaining: 16.3s\n",
            "719:\tlearn: 0.0988509\ttotal: 41.8s\tremaining: 16.2s\n",
            "720:\tlearn: 0.0988080\ttotal: 41.8s\tremaining: 16.2s\n",
            "721:\tlearn: 0.0987750\ttotal: 41.8s\tremaining: 16.1s\n",
            "722:\tlearn: 0.0987559\ttotal: 41.9s\tremaining: 16.1s\n",
            "723:\tlearn: 0.0987470\ttotal: 41.9s\tremaining: 16s\n",
            "724:\tlearn: 0.0987199\ttotal: 42s\tremaining: 15.9s\n",
            "725:\tlearn: 0.0986826\ttotal: 42s\tremaining: 15.9s\n",
            "726:\tlearn: 0.0986511\ttotal: 42.1s\tremaining: 15.8s\n",
            "727:\tlearn: 0.0985985\ttotal: 42.1s\tremaining: 15.7s\n",
            "728:\tlearn: 0.0985625\ttotal: 42.2s\tremaining: 15.7s\n",
            "729:\tlearn: 0.0985189\ttotal: 42.2s\tremaining: 15.6s\n",
            "730:\tlearn: 0.0984916\ttotal: 42.3s\tremaining: 15.6s\n",
            "731:\tlearn: 0.0984732\ttotal: 42.3s\tremaining: 15.5s\n",
            "732:\tlearn: 0.0984574\ttotal: 42.4s\tremaining: 15.4s\n",
            "733:\tlearn: 0.0984394\ttotal: 42.4s\tremaining: 15.4s\n",
            "734:\tlearn: 0.0984275\ttotal: 42.5s\tremaining: 15.3s\n",
            "735:\tlearn: 0.0984189\ttotal: 42.5s\tremaining: 15.3s\n",
            "736:\tlearn: 0.0984040\ttotal: 42.6s\tremaining: 15.2s\n",
            "737:\tlearn: 0.0983965\ttotal: 42.6s\tremaining: 15.1s\n",
            "738:\tlearn: 0.0983757\ttotal: 42.7s\tremaining: 15.1s\n",
            "739:\tlearn: 0.0983593\ttotal: 42.7s\tremaining: 15s\n",
            "740:\tlearn: 0.0983266\ttotal: 42.8s\tremaining: 15s\n",
            "741:\tlearn: 0.0983121\ttotal: 42.8s\tremaining: 14.9s\n",
            "742:\tlearn: 0.0982952\ttotal: 42.9s\tremaining: 14.8s\n",
            "743:\tlearn: 0.0982786\ttotal: 42.9s\tremaining: 14.8s\n",
            "744:\tlearn: 0.0982489\ttotal: 43s\tremaining: 14.7s\n",
            "745:\tlearn: 0.0982373\ttotal: 43s\tremaining: 14.7s\n",
            "746:\tlearn: 0.0982022\ttotal: 43.1s\tremaining: 14.6s\n",
            "747:\tlearn: 0.0981710\ttotal: 43.1s\tremaining: 14.5s\n",
            "748:\tlearn: 0.0981306\ttotal: 43.2s\tremaining: 14.5s\n",
            "749:\tlearn: 0.0981115\ttotal: 43.2s\tremaining: 14.4s\n",
            "750:\tlearn: 0.0981116\ttotal: 43.3s\tremaining: 14.4s\n",
            "751:\tlearn: 0.0981114\ttotal: 43.3s\tremaining: 14.3s\n",
            "752:\tlearn: 0.0981107\ttotal: 43.4s\tremaining: 14.2s\n",
            "753:\tlearn: 0.0980572\ttotal: 43.5s\tremaining: 14.2s\n",
            "754:\tlearn: 0.0980447\ttotal: 43.6s\tremaining: 14.1s\n",
            "755:\tlearn: 0.0980341\ttotal: 43.6s\tremaining: 14.1s\n",
            "756:\tlearn: 0.0980182\ttotal: 43.7s\tremaining: 14s\n",
            "757:\tlearn: 0.0979736\ttotal: 43.8s\tremaining: 14s\n",
            "758:\tlearn: 0.0979537\ttotal: 43.9s\tremaining: 13.9s\n",
            "759:\tlearn: 0.0979307\ttotal: 44s\tremaining: 13.9s\n",
            "760:\tlearn: 0.0978821\ttotal: 44.1s\tremaining: 13.9s\n",
            "761:\tlearn: 0.0978670\ttotal: 44.2s\tremaining: 13.8s\n",
            "762:\tlearn: 0.0978418\ttotal: 44.3s\tremaining: 13.8s\n",
            "763:\tlearn: 0.0978282\ttotal: 44.4s\tremaining: 13.7s\n",
            "764:\tlearn: 0.0978214\ttotal: 44.4s\tremaining: 13.7s\n",
            "765:\tlearn: 0.0978145\ttotal: 44.5s\tremaining: 13.6s\n",
            "766:\tlearn: 0.0977987\ttotal: 44.6s\tremaining: 13.6s\n",
            "767:\tlearn: 0.0977950\ttotal: 44.7s\tremaining: 13.5s\n",
            "768:\tlearn: 0.0977552\ttotal: 44.8s\tremaining: 13.5s\n",
            "769:\tlearn: 0.0977413\ttotal: 44.9s\tremaining: 13.4s\n",
            "770:\tlearn: 0.0977074\ttotal: 45s\tremaining: 13.4s\n",
            "771:\tlearn: 0.0976858\ttotal: 45.1s\tremaining: 13.3s\n",
            "772:\tlearn: 0.0976593\ttotal: 45.1s\tremaining: 13.3s\n",
            "773:\tlearn: 0.0976467\ttotal: 45.2s\tremaining: 13.2s\n",
            "774:\tlearn: 0.0976198\ttotal: 45.3s\tremaining: 13.2s\n",
            "775:\tlearn: 0.0975953\ttotal: 45.4s\tremaining: 13.1s\n",
            "776:\tlearn: 0.0975777\ttotal: 45.5s\tremaining: 13.1s\n",
            "777:\tlearn: 0.0975479\ttotal: 45.6s\tremaining: 13s\n",
            "778:\tlearn: 0.0975164\ttotal: 45.7s\tremaining: 13s\n",
            "779:\tlearn: 0.0974792\ttotal: 45.8s\tremaining: 12.9s\n",
            "780:\tlearn: 0.0974422\ttotal: 45.8s\tremaining: 12.9s\n",
            "781:\tlearn: 0.0974304\ttotal: 45.9s\tremaining: 12.8s\n",
            "782:\tlearn: 0.0973962\ttotal: 46s\tremaining: 12.8s\n",
            "783:\tlearn: 0.0973673\ttotal: 46.1s\tremaining: 12.7s\n",
            "784:\tlearn: 0.0973244\ttotal: 46.2s\tremaining: 12.7s\n",
            "785:\tlearn: 0.0973119\ttotal: 46.3s\tremaining: 12.6s\n",
            "786:\tlearn: 0.0973097\ttotal: 46.4s\tremaining: 12.6s\n",
            "787:\tlearn: 0.0972892\ttotal: 46.5s\tremaining: 12.5s\n",
            "788:\tlearn: 0.0972797\ttotal: 46.6s\tremaining: 12.5s\n",
            "789:\tlearn: 0.0972620\ttotal: 46.7s\tremaining: 12.4s\n",
            "790:\tlearn: 0.0972513\ttotal: 46.8s\tremaining: 12.4s\n",
            "791:\tlearn: 0.0972212\ttotal: 46.9s\tremaining: 12.3s\n",
            "792:\tlearn: 0.0972006\ttotal: 46.9s\tremaining: 12.3s\n",
            "793:\tlearn: 0.0971857\ttotal: 47s\tremaining: 12.2s\n",
            "794:\tlearn: 0.0971554\ttotal: 47s\tremaining: 12.1s\n",
            "795:\tlearn: 0.0971310\ttotal: 47.1s\tremaining: 12.1s\n",
            "796:\tlearn: 0.0971058\ttotal: 47.1s\tremaining: 12s\n",
            "797:\tlearn: 0.0971057\ttotal: 47.2s\tremaining: 11.9s\n",
            "798:\tlearn: 0.0970801\ttotal: 47.2s\tremaining: 11.9s\n",
            "799:\tlearn: 0.0970522\ttotal: 47.3s\tremaining: 11.8s\n",
            "800:\tlearn: 0.0970522\ttotal: 47.3s\tremaining: 11.8s\n",
            "801:\tlearn: 0.0970522\ttotal: 47.4s\tremaining: 11.7s\n",
            "802:\tlearn: 0.0970357\ttotal: 47.4s\tremaining: 11.6s\n",
            "803:\tlearn: 0.0970095\ttotal: 47.5s\tremaining: 11.6s\n",
            "804:\tlearn: 0.0969852\ttotal: 47.5s\tremaining: 11.5s\n",
            "805:\tlearn: 0.0969497\ttotal: 47.6s\tremaining: 11.5s\n",
            "806:\tlearn: 0.0969497\ttotal: 47.6s\tremaining: 11.4s\n",
            "807:\tlearn: 0.0969497\ttotal: 47.7s\tremaining: 11.3s\n",
            "808:\tlearn: 0.0969475\ttotal: 47.7s\tremaining: 11.3s\n",
            "809:\tlearn: 0.0969086\ttotal: 47.8s\tremaining: 11.2s\n",
            "810:\tlearn: 0.0968861\ttotal: 47.8s\tremaining: 11.1s\n",
            "811:\tlearn: 0.0968575\ttotal: 47.9s\tremaining: 11.1s\n",
            "812:\tlearn: 0.0968383\ttotal: 47.9s\tremaining: 11s\n",
            "813:\tlearn: 0.0967923\ttotal: 48s\tremaining: 11s\n",
            "814:\tlearn: 0.0967748\ttotal: 48s\tremaining: 10.9s\n",
            "815:\tlearn: 0.0967746\ttotal: 48.1s\tremaining: 10.8s\n",
            "816:\tlearn: 0.0967692\ttotal: 48.1s\tremaining: 10.8s\n",
            "817:\tlearn: 0.0967587\ttotal: 48.1s\tremaining: 10.7s\n",
            "818:\tlearn: 0.0967492\ttotal: 48.2s\tremaining: 10.7s\n",
            "819:\tlearn: 0.0967393\ttotal: 48.2s\tremaining: 10.6s\n",
            "820:\tlearn: 0.0967344\ttotal: 48.3s\tremaining: 10.5s\n",
            "821:\tlearn: 0.0967134\ttotal: 48.3s\tremaining: 10.5s\n",
            "822:\tlearn: 0.0966881\ttotal: 48.4s\tremaining: 10.4s\n",
            "823:\tlearn: 0.0966811\ttotal: 48.4s\tremaining: 10.3s\n",
            "824:\tlearn: 0.0966812\ttotal: 48.5s\tremaining: 10.3s\n",
            "825:\tlearn: 0.0966636\ttotal: 48.5s\tremaining: 10.2s\n",
            "826:\tlearn: 0.0966634\ttotal: 48.6s\tremaining: 10.2s\n",
            "827:\tlearn: 0.0966632\ttotal: 48.6s\tremaining: 10.1s\n",
            "828:\tlearn: 0.0966403\ttotal: 48.7s\tremaining: 10s\n",
            "829:\tlearn: 0.0966104\ttotal: 48.7s\tremaining: 9.98s\n",
            "830:\tlearn: 0.0965928\ttotal: 48.8s\tremaining: 9.92s\n",
            "831:\tlearn: 0.0965750\ttotal: 48.8s\tremaining: 9.86s\n",
            "832:\tlearn: 0.0965616\ttotal: 48.9s\tremaining: 9.8s\n",
            "833:\tlearn: 0.0965485\ttotal: 48.9s\tremaining: 9.74s\n",
            "834:\tlearn: 0.0965473\ttotal: 49s\tremaining: 9.68s\n",
            "835:\tlearn: 0.0965360\ttotal: 49s\tremaining: 9.62s\n",
            "836:\tlearn: 0.0965202\ttotal: 49.1s\tremaining: 9.56s\n",
            "837:\tlearn: 0.0965013\ttotal: 49.1s\tremaining: 9.5s\n",
            "838:\tlearn: 0.0964892\ttotal: 49.2s\tremaining: 9.43s\n",
            "839:\tlearn: 0.0964892\ttotal: 49.2s\tremaining: 9.37s\n",
            "840:\tlearn: 0.0964783\ttotal: 49.3s\tremaining: 9.31s\n",
            "841:\tlearn: 0.0964555\ttotal: 49.3s\tremaining: 9.26s\n",
            "842:\tlearn: 0.0964555\ttotal: 49.4s\tremaining: 9.2s\n",
            "843:\tlearn: 0.0964555\ttotal: 49.4s\tremaining: 9.13s\n",
            "844:\tlearn: 0.0964298\ttotal: 49.5s\tremaining: 9.07s\n",
            "845:\tlearn: 0.0964160\ttotal: 49.5s\tremaining: 9.02s\n",
            "846:\tlearn: 0.0963867\ttotal: 49.6s\tremaining: 8.96s\n",
            "847:\tlearn: 0.0963470\ttotal: 49.6s\tremaining: 8.9s\n",
            "848:\tlearn: 0.0963315\ttotal: 49.7s\tremaining: 8.84s\n",
            "849:\tlearn: 0.0963132\ttotal: 49.7s\tremaining: 8.77s\n",
            "850:\tlearn: 0.0962916\ttotal: 49.8s\tremaining: 8.71s\n",
            "851:\tlearn: 0.0962690\ttotal: 49.8s\tremaining: 8.65s\n",
            "852:\tlearn: 0.0962447\ttotal: 49.9s\tremaining: 8.59s\n",
            "853:\tlearn: 0.0962178\ttotal: 49.9s\tremaining: 8.54s\n",
            "854:\tlearn: 0.0961954\ttotal: 50s\tremaining: 8.47s\n",
            "855:\tlearn: 0.0961840\ttotal: 50s\tremaining: 8.41s\n",
            "856:\tlearn: 0.0961739\ttotal: 50.1s\tremaining: 8.36s\n",
            "857:\tlearn: 0.0961420\ttotal: 50.1s\tremaining: 8.29s\n",
            "858:\tlearn: 0.0961216\ttotal: 50.2s\tremaining: 8.23s\n",
            "859:\tlearn: 0.0960796\ttotal: 50.2s\tremaining: 8.17s\n",
            "860:\tlearn: 0.0960743\ttotal: 50.3s\tremaining: 8.11s\n",
            "861:\tlearn: 0.0960620\ttotal: 50.3s\tremaining: 8.05s\n",
            "862:\tlearn: 0.0960340\ttotal: 50.4s\tremaining: 7.99s\n",
            "863:\tlearn: 0.0960209\ttotal: 50.4s\tremaining: 7.93s\n",
            "864:\tlearn: 0.0959989\ttotal: 50.5s\tremaining: 7.87s\n",
            "865:\tlearn: 0.0959877\ttotal: 50.5s\tremaining: 7.82s\n",
            "866:\tlearn: 0.0959772\ttotal: 50.6s\tremaining: 7.76s\n",
            "867:\tlearn: 0.0959408\ttotal: 50.6s\tremaining: 7.7s\n",
            "868:\tlearn: 0.0959153\ttotal: 50.7s\tremaining: 7.64s\n",
            "869:\tlearn: 0.0959005\ttotal: 50.7s\tremaining: 7.58s\n",
            "870:\tlearn: 0.0958778\ttotal: 50.8s\tremaining: 7.52s\n",
            "871:\tlearn: 0.0958649\ttotal: 50.8s\tremaining: 7.46s\n",
            "872:\tlearn: 0.0958431\ttotal: 50.9s\tremaining: 7.4s\n",
            "873:\tlearn: 0.0958182\ttotal: 50.9s\tremaining: 7.34s\n",
            "874:\tlearn: 0.0957978\ttotal: 51s\tremaining: 7.28s\n",
            "875:\tlearn: 0.0957734\ttotal: 51s\tremaining: 7.22s\n",
            "876:\tlearn: 0.0957509\ttotal: 51.1s\tremaining: 7.16s\n",
            "877:\tlearn: 0.0957509\ttotal: 51.1s\tremaining: 7.1s\n",
            "878:\tlearn: 0.0957433\ttotal: 51.1s\tremaining: 7.04s\n",
            "879:\tlearn: 0.0957270\ttotal: 51.2s\tremaining: 6.98s\n",
            "880:\tlearn: 0.0957161\ttotal: 51.3s\tremaining: 6.92s\n",
            "881:\tlearn: 0.0956931\ttotal: 51.3s\tremaining: 6.86s\n",
            "882:\tlearn: 0.0956796\ttotal: 51.3s\tremaining: 6.8s\n",
            "883:\tlearn: 0.0956652\ttotal: 51.4s\tremaining: 6.74s\n",
            "884:\tlearn: 0.0956449\ttotal: 51.4s\tremaining: 6.68s\n",
            "885:\tlearn: 0.0956362\ttotal: 51.5s\tremaining: 6.63s\n",
            "886:\tlearn: 0.0956049\ttotal: 51.6s\tremaining: 6.57s\n",
            "887:\tlearn: 0.0955902\ttotal: 51.6s\tremaining: 6.51s\n",
            "888:\tlearn: 0.0955759\ttotal: 51.7s\tremaining: 6.45s\n",
            "889:\tlearn: 0.0955747\ttotal: 51.7s\tremaining: 6.39s\n",
            "890:\tlearn: 0.0955434\ttotal: 51.8s\tremaining: 6.33s\n",
            "891:\tlearn: 0.0955211\ttotal: 51.8s\tremaining: 6.27s\n",
            "892:\tlearn: 0.0955166\ttotal: 51.8s\tremaining: 6.21s\n",
            "893:\tlearn: 0.0955048\ttotal: 51.9s\tremaining: 6.15s\n",
            "894:\tlearn: 0.0955016\ttotal: 52s\tremaining: 6.09s\n",
            "895:\tlearn: 0.0955014\ttotal: 52s\tremaining: 6.04s\n",
            "896:\tlearn: 0.0955013\ttotal: 52s\tremaining: 5.97s\n",
            "897:\tlearn: 0.0955013\ttotal: 52.1s\tremaining: 5.92s\n",
            "898:\tlearn: 0.0954943\ttotal: 52.1s\tremaining: 5.86s\n",
            "899:\tlearn: 0.0954943\ttotal: 52.2s\tremaining: 5.8s\n",
            "900:\tlearn: 0.0954713\ttotal: 52.2s\tremaining: 5.74s\n",
            "901:\tlearn: 0.0954541\ttotal: 52.3s\tremaining: 5.68s\n",
            "902:\tlearn: 0.0954426\ttotal: 52.3s\tremaining: 5.62s\n",
            "903:\tlearn: 0.0954287\ttotal: 52.4s\tremaining: 5.56s\n",
            "904:\tlearn: 0.0954228\ttotal: 52.4s\tremaining: 5.5s\n",
            "905:\tlearn: 0.0954156\ttotal: 52.5s\tremaining: 5.44s\n",
            "906:\tlearn: 0.0953826\ttotal: 52.5s\tremaining: 5.38s\n",
            "907:\tlearn: 0.0953751\ttotal: 52.6s\tremaining: 5.33s\n",
            "908:\tlearn: 0.0953589\ttotal: 52.6s\tremaining: 5.27s\n",
            "909:\tlearn: 0.0953483\ttotal: 52.7s\tremaining: 5.21s\n",
            "910:\tlearn: 0.0953304\ttotal: 52.7s\tremaining: 5.15s\n",
            "911:\tlearn: 0.0953063\ttotal: 52.8s\tremaining: 5.09s\n",
            "912:\tlearn: 0.0952806\ttotal: 52.8s\tremaining: 5.03s\n",
            "913:\tlearn: 0.0952525\ttotal: 52.9s\tremaining: 4.97s\n",
            "914:\tlearn: 0.0952411\ttotal: 52.9s\tremaining: 4.92s\n",
            "915:\tlearn: 0.0952369\ttotal: 53s\tremaining: 4.86s\n",
            "916:\tlearn: 0.0952278\ttotal: 53s\tremaining: 4.8s\n",
            "917:\tlearn: 0.0952188\ttotal: 53.1s\tremaining: 4.74s\n",
            "918:\tlearn: 0.0952187\ttotal: 53.1s\tremaining: 4.68s\n",
            "919:\tlearn: 0.0952186\ttotal: 53.2s\tremaining: 4.62s\n",
            "920:\tlearn: 0.0952103\ttotal: 53.2s\tremaining: 4.56s\n",
            "921:\tlearn: 0.0951939\ttotal: 53.3s\tremaining: 4.5s\n",
            "922:\tlearn: 0.0951870\ttotal: 53.3s\tremaining: 4.45s\n",
            "923:\tlearn: 0.0951632\ttotal: 53.3s\tremaining: 4.39s\n",
            "924:\tlearn: 0.0951438\ttotal: 53.4s\tremaining: 4.33s\n",
            "925:\tlearn: 0.0951119\ttotal: 53.5s\tremaining: 4.27s\n",
            "926:\tlearn: 0.0951024\ttotal: 53.5s\tremaining: 4.21s\n",
            "927:\tlearn: 0.0950921\ttotal: 53.6s\tremaining: 4.16s\n",
            "928:\tlearn: 0.0950782\ttotal: 53.6s\tremaining: 4.1s\n",
            "929:\tlearn: 0.0950679\ttotal: 53.7s\tremaining: 4.04s\n",
            "930:\tlearn: 0.0950595\ttotal: 53.7s\tremaining: 3.98s\n",
            "931:\tlearn: 0.0950564\ttotal: 53.8s\tremaining: 3.92s\n",
            "932:\tlearn: 0.0950534\ttotal: 53.8s\tremaining: 3.86s\n",
            "933:\tlearn: 0.0950360\ttotal: 53.9s\tremaining: 3.81s\n",
            "934:\tlearn: 0.0950197\ttotal: 53.9s\tremaining: 3.75s\n",
            "935:\tlearn: 0.0949879\ttotal: 54s\tremaining: 3.69s\n",
            "936:\tlearn: 0.0949578\ttotal: 54s\tremaining: 3.63s\n",
            "937:\tlearn: 0.0949304\ttotal: 54s\tremaining: 3.57s\n",
            "938:\tlearn: 0.0949134\ttotal: 54.1s\tremaining: 3.52s\n",
            "939:\tlearn: 0.0948977\ttotal: 54.2s\tremaining: 3.46s\n",
            "940:\tlearn: 0.0948964\ttotal: 54.2s\tremaining: 3.4s\n",
            "941:\tlearn: 0.0948964\ttotal: 54.2s\tremaining: 3.34s\n",
            "942:\tlearn: 0.0948822\ttotal: 54.3s\tremaining: 3.28s\n",
            "943:\tlearn: 0.0948657\ttotal: 54.3s\tremaining: 3.22s\n",
            "944:\tlearn: 0.0948584\ttotal: 54.4s\tremaining: 3.17s\n",
            "945:\tlearn: 0.0948584\ttotal: 54.4s\tremaining: 3.11s\n",
            "946:\tlearn: 0.0948584\ttotal: 54.5s\tremaining: 3.05s\n",
            "947:\tlearn: 0.0948440\ttotal: 54.5s\tremaining: 2.99s\n",
            "948:\tlearn: 0.0948343\ttotal: 54.6s\tremaining: 2.93s\n",
            "949:\tlearn: 0.0948108\ttotal: 54.6s\tremaining: 2.88s\n",
            "950:\tlearn: 0.0947869\ttotal: 54.7s\tremaining: 2.82s\n",
            "951:\tlearn: 0.0947606\ttotal: 54.7s\tremaining: 2.76s\n",
            "952:\tlearn: 0.0947406\ttotal: 54.8s\tremaining: 2.7s\n",
            "953:\tlearn: 0.0947274\ttotal: 54.8s\tremaining: 2.64s\n",
            "954:\tlearn: 0.0947182\ttotal: 54.9s\tremaining: 2.59s\n",
            "955:\tlearn: 0.0947071\ttotal: 54.9s\tremaining: 2.53s\n",
            "956:\tlearn: 0.0946977\ttotal: 55s\tremaining: 2.47s\n",
            "957:\tlearn: 0.0946714\ttotal: 55s\tremaining: 2.41s\n",
            "958:\tlearn: 0.0946424\ttotal: 55.1s\tremaining: 2.35s\n",
            "959:\tlearn: 0.0946394\ttotal: 55.1s\tremaining: 2.3s\n",
            "960:\tlearn: 0.0946295\ttotal: 55.2s\tremaining: 2.24s\n",
            "961:\tlearn: 0.0946190\ttotal: 55.2s\tremaining: 2.18s\n",
            "962:\tlearn: 0.0946079\ttotal: 55.3s\tremaining: 2.12s\n",
            "963:\tlearn: 0.0946013\ttotal: 55.3s\tremaining: 2.07s\n",
            "964:\tlearn: 0.0945842\ttotal: 55.4s\tremaining: 2.01s\n",
            "965:\tlearn: 0.0945683\ttotal: 55.4s\tremaining: 1.95s\n",
            "966:\tlearn: 0.0945587\ttotal: 55.5s\tremaining: 1.89s\n",
            "967:\tlearn: 0.0945501\ttotal: 55.5s\tremaining: 1.83s\n",
            "968:\tlearn: 0.0945435\ttotal: 55.6s\tremaining: 1.78s\n",
            "969:\tlearn: 0.0945435\ttotal: 55.6s\tremaining: 1.72s\n",
            "970:\tlearn: 0.0945435\ttotal: 55.7s\tremaining: 1.66s\n",
            "971:\tlearn: 0.0945435\ttotal: 55.7s\tremaining: 1.6s\n",
            "972:\tlearn: 0.0945198\ttotal: 55.8s\tremaining: 1.55s\n",
            "973:\tlearn: 0.0945132\ttotal: 55.8s\tremaining: 1.49s\n",
            "974:\tlearn: 0.0945133\ttotal: 55.9s\tremaining: 1.43s\n",
            "975:\tlearn: 0.0945132\ttotal: 55.9s\tremaining: 1.37s\n",
            "976:\tlearn: 0.0945131\ttotal: 56s\tremaining: 1.32s\n",
            "977:\tlearn: 0.0945060\ttotal: 56s\tremaining: 1.26s\n",
            "978:\tlearn: 0.0944914\ttotal: 56.1s\tremaining: 1.2s\n",
            "979:\tlearn: 0.0944811\ttotal: 56.1s\tremaining: 1.14s\n",
            "980:\tlearn: 0.0944600\ttotal: 56.1s\tremaining: 1.09s\n",
            "981:\tlearn: 0.0944481\ttotal: 56.2s\tremaining: 1.03s\n",
            "982:\tlearn: 0.0944482\ttotal: 56.2s\tremaining: 973ms\n",
            "983:\tlearn: 0.0944392\ttotal: 56.3s\tremaining: 915ms\n",
            "984:\tlearn: 0.0944102\ttotal: 56.3s\tremaining: 858ms\n",
            "985:\tlearn: 0.0943889\ttotal: 56.4s\tremaining: 801ms\n",
            "986:\tlearn: 0.0943689\ttotal: 56.4s\tremaining: 743ms\n",
            "987:\tlearn: 0.0943599\ttotal: 56.5s\tremaining: 686ms\n",
            "988:\tlearn: 0.0943542\ttotal: 56.5s\tremaining: 629ms\n",
            "989:\tlearn: 0.0943389\ttotal: 56.6s\tremaining: 572ms\n",
            "990:\tlearn: 0.0943288\ttotal: 56.7s\tremaining: 514ms\n",
            "991:\tlearn: 0.0943164\ttotal: 56.7s\tremaining: 457ms\n",
            "992:\tlearn: 0.0943010\ttotal: 56.7s\tremaining: 400ms\n",
            "993:\tlearn: 0.0942827\ttotal: 56.8s\tremaining: 343ms\n",
            "994:\tlearn: 0.0942707\ttotal: 56.8s\tremaining: 286ms\n",
            "995:\tlearn: 0.0942528\ttotal: 56.9s\tremaining: 229ms\n",
            "996:\tlearn: 0.0942516\ttotal: 57s\tremaining: 171ms\n",
            "997:\tlearn: 0.0942372\ttotal: 57.1s\tremaining: 114ms\n",
            "998:\tlearn: 0.0942296\ttotal: 57.2s\tremaining: 57.2ms\n",
            "999:\tlearn: 0.0942060\ttotal: 57.2s\tremaining: 0us\n",
            "0:\tlearn: 0.5918118\ttotal: 84.2ms\tremaining: 1m 24s\n",
            "1:\tlearn: 0.5305300\ttotal: 167ms\tremaining: 1m 23s\n",
            "2:\tlearn: 0.4907701\ttotal: 258ms\tremaining: 1m 25s\n",
            "3:\tlearn: 0.4727743\ttotal: 349ms\tremaining: 1m 26s\n",
            "4:\tlearn: 0.4561594\ttotal: 439ms\tremaining: 1m 27s\n",
            "5:\tlearn: 0.4430688\ttotal: 515ms\tremaining: 1m 25s\n",
            "6:\tlearn: 0.4327609\ttotal: 605ms\tremaining: 1m 25s\n",
            "7:\tlearn: 0.4275352\ttotal: 691ms\tremaining: 1m 25s\n",
            "8:\tlearn: 0.4243967\ttotal: 771ms\tremaining: 1m 24s\n",
            "9:\tlearn: 0.4175702\ttotal: 870ms\tremaining: 1m 26s\n",
            "10:\tlearn: 0.4113322\ttotal: 943ms\tremaining: 1m 24s\n",
            "11:\tlearn: 0.4084154\ttotal: 1.03s\tremaining: 1m 24s\n",
            "12:\tlearn: 0.4026823\ttotal: 1.13s\tremaining: 1m 25s\n",
            "13:\tlearn: 0.3973875\ttotal: 1.21s\tremaining: 1m 25s\n",
            "14:\tlearn: 0.3957043\ttotal: 1.3s\tremaining: 1m 25s\n",
            "15:\tlearn: 0.3933344\ttotal: 1.38s\tremaining: 1m 24s\n",
            "16:\tlearn: 0.3912584\ttotal: 1.46s\tremaining: 1m 24s\n",
            "17:\tlearn: 0.3868980\ttotal: 1.52s\tremaining: 1m 23s\n",
            "18:\tlearn: 0.3818971\ttotal: 1.61s\tremaining: 1m 23s\n",
            "19:\tlearn: 0.3758067\ttotal: 1.67s\tremaining: 1m 21s\n",
            "20:\tlearn: 0.3714055\ttotal: 1.75s\tremaining: 1m 21s\n",
            "21:\tlearn: 0.3684605\ttotal: 1.84s\tremaining: 1m 22s\n",
            "22:\tlearn: 0.3673099\ttotal: 1.93s\tremaining: 1m 21s\n",
            "23:\tlearn: 0.3640670\ttotal: 2.01s\tremaining: 1m 21s\n",
            "24:\tlearn: 0.3627110\ttotal: 2.12s\tremaining: 1m 22s\n",
            "25:\tlearn: 0.3602101\ttotal: 2.2s\tremaining: 1m 22s\n",
            "26:\tlearn: 0.3579279\ttotal: 2.29s\tremaining: 1m 22s\n",
            "27:\tlearn: 0.3547989\ttotal: 2.39s\tremaining: 1m 22s\n",
            "28:\tlearn: 0.3535222\ttotal: 2.48s\tremaining: 1m 22s\n",
            "29:\tlearn: 0.3523883\ttotal: 2.56s\tremaining: 1m 22s\n",
            "30:\tlearn: 0.3507875\ttotal: 2.66s\tremaining: 1m 23s\n",
            "31:\tlearn: 0.3474148\ttotal: 2.75s\tremaining: 1m 23s\n",
            "32:\tlearn: 0.3461074\ttotal: 2.84s\tremaining: 1m 23s\n",
            "33:\tlearn: 0.3447532\ttotal: 2.92s\tremaining: 1m 22s\n",
            "34:\tlearn: 0.3415320\ttotal: 3.01s\tremaining: 1m 23s\n",
            "35:\tlearn: 0.3367222\ttotal: 3.1s\tremaining: 1m 23s\n",
            "36:\tlearn: 0.3349569\ttotal: 3.17s\tremaining: 1m 22s\n",
            "37:\tlearn: 0.3327490\ttotal: 3.22s\tremaining: 1m 21s\n",
            "38:\tlearn: 0.3283289\ttotal: 3.27s\tremaining: 1m 20s\n",
            "39:\tlearn: 0.3263357\ttotal: 3.33s\tremaining: 1m 19s\n",
            "40:\tlearn: 0.3226691\ttotal: 3.38s\tremaining: 1m 19s\n",
            "41:\tlearn: 0.3207632\ttotal: 3.43s\tremaining: 1m 18s\n",
            "42:\tlearn: 0.3171494\ttotal: 3.49s\tremaining: 1m 17s\n",
            "43:\tlearn: 0.3147557\ttotal: 3.53s\tremaining: 1m 16s\n",
            "44:\tlearn: 0.3109487\ttotal: 3.58s\tremaining: 1m 15s\n",
            "45:\tlearn: 0.3101264\ttotal: 3.63s\tremaining: 1m 15s\n",
            "46:\tlearn: 0.3079280\ttotal: 3.68s\tremaining: 1m 14s\n",
            "47:\tlearn: 0.3062697\ttotal: 3.73s\tremaining: 1m 13s\n",
            "48:\tlearn: 0.3024348\ttotal: 3.78s\tremaining: 1m 13s\n",
            "49:\tlearn: 0.3011664\ttotal: 3.83s\tremaining: 1m 12s\n",
            "50:\tlearn: 0.2971925\ttotal: 3.88s\tremaining: 1m 12s\n",
            "51:\tlearn: 0.2957525\ttotal: 3.93s\tremaining: 1m 11s\n",
            "52:\tlearn: 0.2930169\ttotal: 3.97s\tremaining: 1m 10s\n",
            "53:\tlearn: 0.2901613\ttotal: 4.02s\tremaining: 1m 10s\n",
            "54:\tlearn: 0.2866059\ttotal: 4.07s\tremaining: 1m 9s\n",
            "55:\tlearn: 0.2852168\ttotal: 4.12s\tremaining: 1m 9s\n",
            "56:\tlearn: 0.2831304\ttotal: 4.18s\tremaining: 1m 9s\n",
            "57:\tlearn: 0.2806215\ttotal: 4.22s\tremaining: 1m 8s\n",
            "58:\tlearn: 0.2792236\ttotal: 4.28s\tremaining: 1m 8s\n",
            "59:\tlearn: 0.2778851\ttotal: 4.34s\tremaining: 1m 7s\n",
            "60:\tlearn: 0.2766040\ttotal: 4.4s\tremaining: 1m 7s\n",
            "61:\tlearn: 0.2747612\ttotal: 4.45s\tremaining: 1m 7s\n",
            "62:\tlearn: 0.2713689\ttotal: 4.5s\tremaining: 1m 6s\n",
            "63:\tlearn: 0.2705194\ttotal: 4.55s\tremaining: 1m 6s\n",
            "64:\tlearn: 0.2682286\ttotal: 4.6s\tremaining: 1m 6s\n",
            "65:\tlearn: 0.2661847\ttotal: 4.64s\tremaining: 1m 5s\n",
            "66:\tlearn: 0.2638528\ttotal: 4.69s\tremaining: 1m 5s\n",
            "67:\tlearn: 0.2609014\ttotal: 4.74s\tremaining: 1m 4s\n",
            "68:\tlearn: 0.2603396\ttotal: 4.79s\tremaining: 1m 4s\n",
            "69:\tlearn: 0.2589312\ttotal: 4.84s\tremaining: 1m 4s\n",
            "70:\tlearn: 0.2573976\ttotal: 4.89s\tremaining: 1m 3s\n",
            "71:\tlearn: 0.2571551\ttotal: 4.93s\tremaining: 1m 3s\n",
            "72:\tlearn: 0.2562394\ttotal: 4.98s\tremaining: 1m 3s\n",
            "73:\tlearn: 0.2554432\ttotal: 5.05s\tremaining: 1m 3s\n",
            "74:\tlearn: 0.2550837\ttotal: 5.09s\tremaining: 1m 2s\n",
            "75:\tlearn: 0.2543454\ttotal: 5.14s\tremaining: 1m 2s\n",
            "76:\tlearn: 0.2517724\ttotal: 5.19s\tremaining: 1m 2s\n",
            "77:\tlearn: 0.2501842\ttotal: 5.23s\tremaining: 1m 1s\n",
            "78:\tlearn: 0.2482197\ttotal: 5.29s\tremaining: 1m 1s\n",
            "79:\tlearn: 0.2470787\ttotal: 5.33s\tremaining: 1m 1s\n",
            "80:\tlearn: 0.2440505\ttotal: 5.4s\tremaining: 1m 1s\n",
            "81:\tlearn: 0.2430830\ttotal: 5.44s\tremaining: 1m\n",
            "82:\tlearn: 0.2408916\ttotal: 5.5s\tremaining: 1m\n",
            "83:\tlearn: 0.2399978\ttotal: 5.55s\tremaining: 1m\n",
            "84:\tlearn: 0.2385082\ttotal: 5.6s\tremaining: 1m\n",
            "85:\tlearn: 0.2369149\ttotal: 5.65s\tremaining: 1m\n",
            "86:\tlearn: 0.2346505\ttotal: 5.7s\tremaining: 59.8s\n",
            "87:\tlearn: 0.2331333\ttotal: 5.75s\tremaining: 59.6s\n",
            "88:\tlearn: 0.2309857\ttotal: 5.8s\tremaining: 59.4s\n",
            "89:\tlearn: 0.2304390\ttotal: 5.85s\tremaining: 59.2s\n",
            "90:\tlearn: 0.2290864\ttotal: 5.9s\tremaining: 58.9s\n",
            "91:\tlearn: 0.2269245\ttotal: 5.96s\tremaining: 58.8s\n",
            "92:\tlearn: 0.2256793\ttotal: 6s\tremaining: 58.6s\n",
            "93:\tlearn: 0.2240852\ttotal: 6.06s\tremaining: 58.4s\n",
            "94:\tlearn: 0.2223735\ttotal: 6.1s\tremaining: 58.1s\n",
            "95:\tlearn: 0.2218984\ttotal: 6.15s\tremaining: 57.9s\n",
            "96:\tlearn: 0.2217207\ttotal: 6.21s\tremaining: 57.8s\n",
            "97:\tlearn: 0.2187490\ttotal: 6.25s\tremaining: 57.6s\n",
            "98:\tlearn: 0.2183266\ttotal: 6.3s\tremaining: 57.4s\n",
            "99:\tlearn: 0.2173423\ttotal: 6.35s\tremaining: 57.1s\n",
            "100:\tlearn: 0.2157564\ttotal: 6.4s\tremaining: 57s\n",
            "101:\tlearn: 0.2145707\ttotal: 6.46s\tremaining: 56.9s\n",
            "102:\tlearn: 0.2139198\ttotal: 6.51s\tremaining: 56.7s\n",
            "103:\tlearn: 0.2131867\ttotal: 6.55s\tremaining: 56.5s\n",
            "104:\tlearn: 0.2125019\ttotal: 6.6s\tremaining: 56.3s\n",
            "105:\tlearn: 0.2120477\ttotal: 6.65s\tremaining: 56.1s\n",
            "106:\tlearn: 0.2110280\ttotal: 6.7s\tremaining: 55.9s\n",
            "107:\tlearn: 0.2096418\ttotal: 6.75s\tremaining: 55.8s\n",
            "108:\tlearn: 0.2088101\ttotal: 6.8s\tremaining: 55.6s\n",
            "109:\tlearn: 0.2076996\ttotal: 6.84s\tremaining: 55.4s\n",
            "110:\tlearn: 0.2071821\ttotal: 6.89s\tremaining: 55.2s\n",
            "111:\tlearn: 0.2064506\ttotal: 6.95s\tremaining: 55.1s\n",
            "112:\tlearn: 0.2048628\ttotal: 7s\tremaining: 54.9s\n",
            "113:\tlearn: 0.2041503\ttotal: 7.04s\tremaining: 54.7s\n",
            "114:\tlearn: 0.2029765\ttotal: 7.09s\tremaining: 54.6s\n",
            "115:\tlearn: 0.2024429\ttotal: 7.14s\tremaining: 54.4s\n",
            "116:\tlearn: 0.2013413\ttotal: 7.19s\tremaining: 54.3s\n",
            "117:\tlearn: 0.2006270\ttotal: 7.24s\tremaining: 54.1s\n",
            "118:\tlearn: 0.2002483\ttotal: 7.29s\tremaining: 54s\n",
            "119:\tlearn: 0.1988826\ttotal: 7.34s\tremaining: 53.8s\n",
            "120:\tlearn: 0.1982127\ttotal: 7.38s\tremaining: 53.7s\n",
            "121:\tlearn: 0.1974327\ttotal: 7.45s\tremaining: 53.6s\n",
            "122:\tlearn: 0.1971062\ttotal: 7.5s\tremaining: 53.4s\n",
            "123:\tlearn: 0.1966673\ttotal: 7.54s\tremaining: 53.3s\n",
            "124:\tlearn: 0.1962834\ttotal: 7.59s\tremaining: 53.1s\n",
            "125:\tlearn: 0.1956186\ttotal: 7.63s\tremaining: 53s\n",
            "126:\tlearn: 0.1950943\ttotal: 7.69s\tremaining: 52.9s\n",
            "127:\tlearn: 0.1945444\ttotal: 7.74s\tremaining: 52.7s\n",
            "128:\tlearn: 0.1941900\ttotal: 7.78s\tremaining: 52.6s\n",
            "129:\tlearn: 0.1925958\ttotal: 7.83s\tremaining: 52.4s\n",
            "130:\tlearn: 0.1921078\ttotal: 7.88s\tremaining: 52.3s\n",
            "131:\tlearn: 0.1918681\ttotal: 7.94s\tremaining: 52.2s\n",
            "132:\tlearn: 0.1902384\ttotal: 7.98s\tremaining: 52s\n",
            "133:\tlearn: 0.1894559\ttotal: 8.03s\tremaining: 51.9s\n",
            "134:\tlearn: 0.1881806\ttotal: 8.08s\tremaining: 51.8s\n",
            "135:\tlearn: 0.1878328\ttotal: 8.12s\tremaining: 51.6s\n",
            "136:\tlearn: 0.1869571\ttotal: 8.18s\tremaining: 51.5s\n",
            "137:\tlearn: 0.1860368\ttotal: 8.22s\tremaining: 51.4s\n",
            "138:\tlearn: 0.1853149\ttotal: 8.27s\tremaining: 51.2s\n",
            "139:\tlearn: 0.1845884\ttotal: 8.32s\tremaining: 51.1s\n",
            "140:\tlearn: 0.1837191\ttotal: 8.36s\tremaining: 50.9s\n",
            "141:\tlearn: 0.1831285\ttotal: 8.42s\tremaining: 50.9s\n",
            "142:\tlearn: 0.1824309\ttotal: 8.48s\tremaining: 50.8s\n",
            "143:\tlearn: 0.1815725\ttotal: 8.52s\tremaining: 50.7s\n",
            "144:\tlearn: 0.1809612\ttotal: 8.57s\tremaining: 50.5s\n",
            "145:\tlearn: 0.1799799\ttotal: 8.62s\tremaining: 50.4s\n",
            "146:\tlearn: 0.1792965\ttotal: 8.67s\tremaining: 50.3s\n",
            "147:\tlearn: 0.1789633\ttotal: 8.72s\tremaining: 50.2s\n",
            "148:\tlearn: 0.1787832\ttotal: 8.77s\tremaining: 50.1s\n",
            "149:\tlearn: 0.1786977\ttotal: 8.81s\tremaining: 49.9s\n",
            "150:\tlearn: 0.1783313\ttotal: 8.86s\tremaining: 49.8s\n",
            "151:\tlearn: 0.1779595\ttotal: 8.92s\tremaining: 49.8s\n",
            "152:\tlearn: 0.1769930\ttotal: 8.97s\tremaining: 49.6s\n",
            "153:\tlearn: 0.1766454\ttotal: 9.01s\tremaining: 49.5s\n",
            "154:\tlearn: 0.1762161\ttotal: 9.06s\tremaining: 49.4s\n",
            "155:\tlearn: 0.1759055\ttotal: 9.11s\tremaining: 49.3s\n",
            "156:\tlearn: 0.1750180\ttotal: 9.16s\tremaining: 49.2s\n",
            "157:\tlearn: 0.1740606\ttotal: 9.21s\tremaining: 49.1s\n",
            "158:\tlearn: 0.1735536\ttotal: 9.26s\tremaining: 49s\n",
            "159:\tlearn: 0.1729607\ttotal: 9.3s\tremaining: 48.9s\n",
            "160:\tlearn: 0.1723642\ttotal: 9.35s\tremaining: 48.7s\n",
            "161:\tlearn: 0.1715691\ttotal: 9.41s\tremaining: 48.7s\n",
            "162:\tlearn: 0.1713401\ttotal: 9.46s\tremaining: 48.6s\n",
            "163:\tlearn: 0.1711551\ttotal: 9.51s\tremaining: 48.5s\n",
            "164:\tlearn: 0.1704998\ttotal: 9.56s\tremaining: 48.4s\n",
            "165:\tlearn: 0.1699512\ttotal: 9.6s\tremaining: 48.3s\n",
            "166:\tlearn: 0.1696509\ttotal: 9.66s\tremaining: 48.2s\n",
            "167:\tlearn: 0.1689404\ttotal: 9.7s\tremaining: 48.1s\n",
            "168:\tlearn: 0.1680911\ttotal: 9.75s\tremaining: 48s\n",
            "169:\tlearn: 0.1677427\ttotal: 9.8s\tremaining: 47.9s\n",
            "170:\tlearn: 0.1671715\ttotal: 9.86s\tremaining: 47.8s\n",
            "171:\tlearn: 0.1663864\ttotal: 9.91s\tremaining: 47.7s\n",
            "172:\tlearn: 0.1659917\ttotal: 9.96s\tremaining: 47.6s\n",
            "173:\tlearn: 0.1658944\ttotal: 10s\tremaining: 47.5s\n",
            "174:\tlearn: 0.1656323\ttotal: 10.1s\tremaining: 47.4s\n",
            "175:\tlearn: 0.1652644\ttotal: 10.1s\tremaining: 47.3s\n",
            "176:\tlearn: 0.1647706\ttotal: 10.2s\tremaining: 47.2s\n",
            "177:\tlearn: 0.1639848\ttotal: 10.2s\tremaining: 47.1s\n",
            "178:\tlearn: 0.1635608\ttotal: 10.2s\tremaining: 47s\n",
            "179:\tlearn: 0.1633466\ttotal: 10.3s\tremaining: 46.9s\n",
            "180:\tlearn: 0.1625575\ttotal: 10.3s\tremaining: 46.8s\n",
            "181:\tlearn: 0.1619313\ttotal: 10.4s\tremaining: 46.7s\n",
            "182:\tlearn: 0.1613130\ttotal: 10.4s\tremaining: 46.6s\n",
            "183:\tlearn: 0.1610092\ttotal: 10.5s\tremaining: 46.6s\n",
            "184:\tlearn: 0.1607226\ttotal: 10.6s\tremaining: 46.5s\n",
            "185:\tlearn: 0.1607008\ttotal: 10.6s\tremaining: 46.2s\n",
            "186:\tlearn: 0.1602487\ttotal: 10.6s\tremaining: 46.2s\n",
            "187:\tlearn: 0.1591473\ttotal: 10.7s\tremaining: 46.1s\n",
            "188:\tlearn: 0.1588051\ttotal: 10.7s\tremaining: 46s\n",
            "189:\tlearn: 0.1585585\ttotal: 10.8s\tremaining: 45.9s\n",
            "190:\tlearn: 0.1581164\ttotal: 10.8s\tremaining: 45.8s\n",
            "191:\tlearn: 0.1574582\ttotal: 10.9s\tremaining: 45.7s\n",
            "192:\tlearn: 0.1570305\ttotal: 10.9s\tremaining: 45.6s\n",
            "193:\tlearn: 0.1568857\ttotal: 11s\tremaining: 45.5s\n",
            "194:\tlearn: 0.1563324\ttotal: 11s\tremaining: 45.5s\n",
            "195:\tlearn: 0.1562078\ttotal: 11.1s\tremaining: 45.4s\n",
            "196:\tlearn: 0.1559717\ttotal: 11.1s\tremaining: 45.3s\n",
            "197:\tlearn: 0.1554303\ttotal: 11.2s\tremaining: 45.2s\n",
            "198:\tlearn: 0.1550296\ttotal: 11.2s\tremaining: 45.1s\n",
            "199:\tlearn: 0.1545543\ttotal: 11.3s\tremaining: 45s\n",
            "200:\tlearn: 0.1543206\ttotal: 11.3s\tremaining: 44.9s\n",
            "201:\tlearn: 0.1542384\ttotal: 11.3s\tremaining: 44.8s\n",
            "202:\tlearn: 0.1536176\ttotal: 11.4s\tremaining: 44.7s\n",
            "203:\tlearn: 0.1531461\ttotal: 11.4s\tremaining: 44.6s\n",
            "204:\tlearn: 0.1529340\ttotal: 11.5s\tremaining: 44.6s\n",
            "205:\tlearn: 0.1525664\ttotal: 11.6s\tremaining: 44.6s\n",
            "206:\tlearn: 0.1522507\ttotal: 11.6s\tremaining: 44.5s\n",
            "207:\tlearn: 0.1518798\ttotal: 11.7s\tremaining: 44.4s\n",
            "208:\tlearn: 0.1514117\ttotal: 11.7s\tremaining: 44.3s\n",
            "209:\tlearn: 0.1512811\ttotal: 11.8s\tremaining: 44.3s\n",
            "210:\tlearn: 0.1509462\ttotal: 11.8s\tremaining: 44.2s\n",
            "211:\tlearn: 0.1508628\ttotal: 11.9s\tremaining: 44.1s\n",
            "212:\tlearn: 0.1506456\ttotal: 11.9s\tremaining: 44s\n",
            "213:\tlearn: 0.1500900\ttotal: 12s\tremaining: 43.9s\n",
            "214:\tlearn: 0.1495498\ttotal: 12s\tremaining: 43.9s\n",
            "215:\tlearn: 0.1491377\ttotal: 12.1s\tremaining: 43.8s\n",
            "216:\tlearn: 0.1484390\ttotal: 12.1s\tremaining: 43.7s\n",
            "217:\tlearn: 0.1479389\ttotal: 12.2s\tremaining: 43.6s\n",
            "218:\tlearn: 0.1474531\ttotal: 12.2s\tremaining: 43.5s\n",
            "219:\tlearn: 0.1474074\ttotal: 12.3s\tremaining: 43.4s\n",
            "220:\tlearn: 0.1471134\ttotal: 12.3s\tremaining: 43.4s\n",
            "221:\tlearn: 0.1468388\ttotal: 12.3s\tremaining: 43.3s\n",
            "222:\tlearn: 0.1464124\ttotal: 12.4s\tremaining: 43.2s\n",
            "223:\tlearn: 0.1461920\ttotal: 12.4s\tremaining: 43.1s\n",
            "224:\tlearn: 0.1458104\ttotal: 12.5s\tremaining: 43.1s\n",
            "225:\tlearn: 0.1451248\ttotal: 12.6s\tremaining: 43s\n",
            "226:\tlearn: 0.1448482\ttotal: 12.6s\tremaining: 42.9s\n",
            "227:\tlearn: 0.1444965\ttotal: 12.7s\tremaining: 42.9s\n",
            "228:\tlearn: 0.1441162\ttotal: 12.7s\tremaining: 42.8s\n",
            "229:\tlearn: 0.1436362\ttotal: 12.8s\tremaining: 42.7s\n",
            "230:\tlearn: 0.1431545\ttotal: 12.8s\tremaining: 42.6s\n",
            "231:\tlearn: 0.1427575\ttotal: 12.9s\tremaining: 42.6s\n",
            "232:\tlearn: 0.1423052\ttotal: 12.9s\tremaining: 42.5s\n",
            "233:\tlearn: 0.1420086\ttotal: 13s\tremaining: 42.4s\n",
            "234:\tlearn: 0.1416146\ttotal: 13s\tremaining: 42.3s\n",
            "235:\tlearn: 0.1410227\ttotal: 13.1s\tremaining: 42.3s\n",
            "236:\tlearn: 0.1406601\ttotal: 13.1s\tremaining: 42.3s\n",
            "237:\tlearn: 0.1401071\ttotal: 13.2s\tremaining: 42.3s\n",
            "238:\tlearn: 0.1398558\ttotal: 13.3s\tremaining: 42.3s\n",
            "239:\tlearn: 0.1392978\ttotal: 13.4s\tremaining: 42.3s\n",
            "240:\tlearn: 0.1389977\ttotal: 13.4s\tremaining: 42.4s\n",
            "241:\tlearn: 0.1387251\ttotal: 13.5s\tremaining: 42.4s\n",
            "242:\tlearn: 0.1383597\ttotal: 13.6s\tremaining: 42.5s\n",
            "243:\tlearn: 0.1380692\ttotal: 13.7s\tremaining: 42.5s\n",
            "244:\tlearn: 0.1379768\ttotal: 13.8s\tremaining: 42.5s\n",
            "245:\tlearn: 0.1377026\ttotal: 13.9s\tremaining: 42.6s\n",
            "246:\tlearn: 0.1376140\ttotal: 14s\tremaining: 42.7s\n",
            "247:\tlearn: 0.1373763\ttotal: 14.1s\tremaining: 42.7s\n",
            "248:\tlearn: 0.1370438\ttotal: 14.2s\tremaining: 42.7s\n",
            "249:\tlearn: 0.1368083\ttotal: 14.2s\tremaining: 42.7s\n",
            "250:\tlearn: 0.1365752\ttotal: 14.3s\tremaining: 42.7s\n",
            "251:\tlearn: 0.1362337\ttotal: 14.4s\tremaining: 42.8s\n",
            "252:\tlearn: 0.1359342\ttotal: 14.5s\tremaining: 42.8s\n",
            "253:\tlearn: 0.1355340\ttotal: 14.6s\tremaining: 42.9s\n",
            "254:\tlearn: 0.1353822\ttotal: 14.7s\tremaining: 42.9s\n",
            "255:\tlearn: 0.1353632\ttotal: 14.8s\tremaining: 42.9s\n",
            "256:\tlearn: 0.1352324\ttotal: 14.9s\tremaining: 43s\n",
            "257:\tlearn: 0.1350995\ttotal: 14.9s\tremaining: 43s\n",
            "258:\tlearn: 0.1348305\ttotal: 15s\tremaining: 43s\n",
            "259:\tlearn: 0.1344324\ttotal: 15.1s\tremaining: 43s\n",
            "260:\tlearn: 0.1340322\ttotal: 15.2s\tremaining: 43s\n",
            "261:\tlearn: 0.1338438\ttotal: 15.3s\tremaining: 43s\n",
            "262:\tlearn: 0.1336030\ttotal: 15.4s\tremaining: 43s\n",
            "263:\tlearn: 0.1333437\ttotal: 15.4s\tremaining: 43.1s\n",
            "264:\tlearn: 0.1332402\ttotal: 15.5s\tremaining: 43.1s\n",
            "265:\tlearn: 0.1328647\ttotal: 15.6s\tremaining: 43.1s\n",
            "266:\tlearn: 0.1325838\ttotal: 15.7s\tremaining: 43.2s\n",
            "267:\tlearn: 0.1324913\ttotal: 15.8s\tremaining: 43.1s\n",
            "268:\tlearn: 0.1323330\ttotal: 15.9s\tremaining: 43.1s\n",
            "269:\tlearn: 0.1319862\ttotal: 16s\tremaining: 43.2s\n",
            "270:\tlearn: 0.1314918\ttotal: 16.1s\tremaining: 43.2s\n",
            "271:\tlearn: 0.1311572\ttotal: 16.1s\tremaining: 43.2s\n",
            "272:\tlearn: 0.1307635\ttotal: 16.2s\tremaining: 43.2s\n",
            "273:\tlearn: 0.1304503\ttotal: 16.3s\tremaining: 43.3s\n",
            "274:\tlearn: 0.1302707\ttotal: 16.4s\tremaining: 43.3s\n",
            "275:\tlearn: 0.1299866\ttotal: 16.5s\tremaining: 43.3s\n",
            "276:\tlearn: 0.1297453\ttotal: 16.6s\tremaining: 43.3s\n",
            "277:\tlearn: 0.1297018\ttotal: 16.7s\tremaining: 43.4s\n",
            "278:\tlearn: 0.1295369\ttotal: 16.8s\tremaining: 43.4s\n",
            "279:\tlearn: 0.1292771\ttotal: 16.8s\tremaining: 43.3s\n",
            "280:\tlearn: 0.1290194\ttotal: 16.9s\tremaining: 43.2s\n",
            "281:\tlearn: 0.1289578\ttotal: 16.9s\tremaining: 43.1s\n",
            "282:\tlearn: 0.1288712\ttotal: 17s\tremaining: 43s\n",
            "283:\tlearn: 0.1287880\ttotal: 17s\tremaining: 43s\n",
            "284:\tlearn: 0.1285569\ttotal: 17.1s\tremaining: 42.9s\n",
            "285:\tlearn: 0.1283460\ttotal: 17.1s\tremaining: 42.8s\n",
            "286:\tlearn: 0.1281575\ttotal: 17.2s\tremaining: 42.7s\n",
            "287:\tlearn: 0.1279478\ttotal: 17.2s\tremaining: 42.6s\n",
            "288:\tlearn: 0.1274548\ttotal: 17.3s\tremaining: 42.5s\n",
            "289:\tlearn: 0.1272318\ttotal: 17.3s\tremaining: 42.4s\n",
            "290:\tlearn: 0.1271268\ttotal: 17.4s\tremaining: 42.3s\n",
            "291:\tlearn: 0.1268990\ttotal: 17.4s\tremaining: 42.3s\n",
            "292:\tlearn: 0.1267047\ttotal: 17.5s\tremaining: 42.2s\n",
            "293:\tlearn: 0.1264047\ttotal: 17.5s\tremaining: 42.1s\n",
            "294:\tlearn: 0.1262316\ttotal: 17.6s\tremaining: 42s\n",
            "295:\tlearn: 0.1259375\ttotal: 17.6s\tremaining: 41.9s\n",
            "296:\tlearn: 0.1257855\ttotal: 17.7s\tremaining: 41.8s\n",
            "297:\tlearn: 0.1256063\ttotal: 17.7s\tremaining: 41.8s\n",
            "298:\tlearn: 0.1254625\ttotal: 17.8s\tremaining: 41.7s\n",
            "299:\tlearn: 0.1253350\ttotal: 17.8s\tremaining: 41.6s\n",
            "300:\tlearn: 0.1251138\ttotal: 17.9s\tremaining: 41.5s\n",
            "301:\tlearn: 0.1249406\ttotal: 17.9s\tremaining: 41.5s\n",
            "302:\tlearn: 0.1248190\ttotal: 18s\tremaining: 41.4s\n",
            "303:\tlearn: 0.1246537\ttotal: 18s\tremaining: 41.3s\n",
            "304:\tlearn: 0.1245011\ttotal: 18.1s\tremaining: 41.2s\n",
            "305:\tlearn: 0.1242865\ttotal: 18.1s\tremaining: 41.1s\n",
            "306:\tlearn: 0.1241825\ttotal: 18.2s\tremaining: 41s\n",
            "307:\tlearn: 0.1240387\ttotal: 18.2s\tremaining: 40.9s\n",
            "308:\tlearn: 0.1238913\ttotal: 18.3s\tremaining: 40.9s\n",
            "309:\tlearn: 0.1238846\ttotal: 18.3s\tremaining: 40.8s\n",
            "310:\tlearn: 0.1237344\ttotal: 18.4s\tremaining: 40.7s\n",
            "311:\tlearn: 0.1236211\ttotal: 18.4s\tremaining: 40.6s\n",
            "312:\tlearn: 0.1234657\ttotal: 18.5s\tremaining: 40.5s\n",
            "313:\tlearn: 0.1232885\ttotal: 18.5s\tremaining: 40.4s\n",
            "314:\tlearn: 0.1230385\ttotal: 18.6s\tremaining: 40.4s\n",
            "315:\tlearn: 0.1228207\ttotal: 18.6s\tremaining: 40.3s\n",
            "316:\tlearn: 0.1228159\ttotal: 18.6s\tremaining: 40.2s\n",
            "317:\tlearn: 0.1226773\ttotal: 18.7s\tremaining: 40.1s\n",
            "318:\tlearn: 0.1224663\ttotal: 18.7s\tremaining: 40s\n",
            "319:\tlearn: 0.1223272\ttotal: 18.8s\tremaining: 39.9s\n",
            "320:\tlearn: 0.1220556\ttotal: 18.8s\tremaining: 39.9s\n",
            "321:\tlearn: 0.1219133\ttotal: 18.9s\tremaining: 39.8s\n",
            "322:\tlearn: 0.1216543\ttotal: 18.9s\tremaining: 39.7s\n",
            "323:\tlearn: 0.1215770\ttotal: 19s\tremaining: 39.6s\n",
            "324:\tlearn: 0.1214645\ttotal: 19s\tremaining: 39.6s\n",
            "325:\tlearn: 0.1212894\ttotal: 19.1s\tremaining: 39.5s\n",
            "326:\tlearn: 0.1210885\ttotal: 19.1s\tremaining: 39.4s\n",
            "327:\tlearn: 0.1210505\ttotal: 19.2s\tremaining: 39.3s\n",
            "328:\tlearn: 0.1209337\ttotal: 19.2s\tremaining: 39.2s\n",
            "329:\tlearn: 0.1206944\ttotal: 19.3s\tremaining: 39.2s\n",
            "330:\tlearn: 0.1205093\ttotal: 19.3s\tremaining: 39.1s\n",
            "331:\tlearn: 0.1203193\ttotal: 19.4s\tremaining: 39s\n",
            "332:\tlearn: 0.1201714\ttotal: 19.4s\tremaining: 38.9s\n",
            "333:\tlearn: 0.1200727\ttotal: 19.5s\tremaining: 38.8s\n",
            "334:\tlearn: 0.1197989\ttotal: 19.5s\tremaining: 38.8s\n",
            "335:\tlearn: 0.1197174\ttotal: 19.6s\tremaining: 38.7s\n",
            "336:\tlearn: 0.1196210\ttotal: 19.6s\tremaining: 38.6s\n",
            "337:\tlearn: 0.1194797\ttotal: 19.7s\tremaining: 38.5s\n",
            "338:\tlearn: 0.1193623\ttotal: 19.7s\tremaining: 38.4s\n",
            "339:\tlearn: 0.1192213\ttotal: 19.8s\tremaining: 38.4s\n",
            "340:\tlearn: 0.1190735\ttotal: 19.8s\tremaining: 38.3s\n",
            "341:\tlearn: 0.1189382\ttotal: 19.9s\tremaining: 38.3s\n",
            "342:\tlearn: 0.1187768\ttotal: 19.9s\tremaining: 38.2s\n",
            "343:\tlearn: 0.1186637\ttotal: 20s\tremaining: 38.1s\n",
            "344:\tlearn: 0.1184513\ttotal: 20s\tremaining: 38s\n",
            "345:\tlearn: 0.1182834\ttotal: 20.1s\tremaining: 38s\n",
            "346:\tlearn: 0.1181311\ttotal: 20.1s\tremaining: 37.9s\n",
            "347:\tlearn: 0.1179602\ttotal: 20.2s\tremaining: 37.8s\n",
            "348:\tlearn: 0.1177818\ttotal: 20.2s\tremaining: 37.7s\n",
            "349:\tlearn: 0.1176726\ttotal: 20.3s\tremaining: 37.7s\n",
            "350:\tlearn: 0.1173946\ttotal: 20.3s\tremaining: 37.6s\n",
            "351:\tlearn: 0.1172924\ttotal: 20.4s\tremaining: 37.5s\n",
            "352:\tlearn: 0.1172138\ttotal: 20.4s\tremaining: 37.4s\n",
            "353:\tlearn: 0.1170913\ttotal: 20.5s\tremaining: 37.4s\n",
            "354:\tlearn: 0.1170280\ttotal: 20.5s\tremaining: 37.3s\n",
            "355:\tlearn: 0.1169336\ttotal: 20.6s\tremaining: 37.2s\n",
            "356:\tlearn: 0.1167975\ttotal: 20.6s\tremaining: 37.2s\n",
            "357:\tlearn: 0.1165773\ttotal: 20.7s\tremaining: 37.1s\n",
            "358:\tlearn: 0.1165595\ttotal: 20.7s\tremaining: 37s\n",
            "359:\tlearn: 0.1163303\ttotal: 20.8s\tremaining: 36.9s\n",
            "360:\tlearn: 0.1162104\ttotal: 20.8s\tremaining: 36.9s\n",
            "361:\tlearn: 0.1161733\ttotal: 20.9s\tremaining: 36.8s\n",
            "362:\tlearn: 0.1161099\ttotal: 20.9s\tremaining: 36.7s\n",
            "363:\tlearn: 0.1158471\ttotal: 21s\tremaining: 36.7s\n",
            "364:\tlearn: 0.1157284\ttotal: 21s\tremaining: 36.6s\n",
            "365:\tlearn: 0.1156488\ttotal: 21.1s\tremaining: 36.5s\n",
            "366:\tlearn: 0.1155320\ttotal: 21.1s\tremaining: 36.4s\n",
            "367:\tlearn: 0.1154814\ttotal: 21.2s\tremaining: 36.4s\n",
            "368:\tlearn: 0.1153775\ttotal: 21.2s\tremaining: 36.3s\n",
            "369:\tlearn: 0.1152811\ttotal: 21.3s\tremaining: 36.2s\n",
            "370:\tlearn: 0.1151983\ttotal: 21.3s\tremaining: 36.1s\n",
            "371:\tlearn: 0.1150578\ttotal: 21.4s\tremaining: 36.1s\n",
            "372:\tlearn: 0.1149290\ttotal: 21.4s\tremaining: 36s\n",
            "373:\tlearn: 0.1148476\ttotal: 21.5s\tremaining: 35.9s\n",
            "374:\tlearn: 0.1148104\ttotal: 21.5s\tremaining: 35.9s\n",
            "375:\tlearn: 0.1146628\ttotal: 21.6s\tremaining: 35.8s\n",
            "376:\tlearn: 0.1146005\ttotal: 21.6s\tremaining: 35.7s\n",
            "377:\tlearn: 0.1145627\ttotal: 21.7s\tremaining: 35.6s\n",
            "378:\tlearn: 0.1144542\ttotal: 21.7s\tremaining: 35.6s\n",
            "379:\tlearn: 0.1143030\ttotal: 21.8s\tremaining: 35.5s\n",
            "380:\tlearn: 0.1141471\ttotal: 21.8s\tremaining: 35.5s\n",
            "381:\tlearn: 0.1140147\ttotal: 21.9s\tremaining: 35.4s\n",
            "382:\tlearn: 0.1138927\ttotal: 21.9s\tremaining: 35.3s\n",
            "383:\tlearn: 0.1138118\ttotal: 22s\tremaining: 35.3s\n",
            "384:\tlearn: 0.1137212\ttotal: 22s\tremaining: 35.2s\n",
            "385:\tlearn: 0.1136497\ttotal: 22.1s\tremaining: 35.1s\n",
            "386:\tlearn: 0.1135146\ttotal: 22.1s\tremaining: 35s\n",
            "387:\tlearn: 0.1134289\ttotal: 22.2s\tremaining: 35s\n",
            "388:\tlearn: 0.1132631\ttotal: 22.2s\tremaining: 34.9s\n",
            "389:\tlearn: 0.1131286\ttotal: 22.3s\tremaining: 34.8s\n",
            "390:\tlearn: 0.1130197\ttotal: 22.3s\tremaining: 34.8s\n",
            "391:\tlearn: 0.1129728\ttotal: 22.4s\tremaining: 34.7s\n",
            "392:\tlearn: 0.1128802\ttotal: 22.4s\tremaining: 34.6s\n",
            "393:\tlearn: 0.1126987\ttotal: 22.5s\tremaining: 34.6s\n",
            "394:\tlearn: 0.1124523\ttotal: 22.5s\tremaining: 34.5s\n",
            "395:\tlearn: 0.1124171\ttotal: 22.6s\tremaining: 34.4s\n",
            "396:\tlearn: 0.1123225\ttotal: 22.6s\tremaining: 34.3s\n",
            "397:\tlearn: 0.1122405\ttotal: 22.7s\tremaining: 34.3s\n",
            "398:\tlearn: 0.1121086\ttotal: 22.7s\tremaining: 34.2s\n",
            "399:\tlearn: 0.1119938\ttotal: 22.8s\tremaining: 34.1s\n",
            "400:\tlearn: 0.1119083\ttotal: 22.8s\tremaining: 34.1s\n",
            "401:\tlearn: 0.1117636\ttotal: 22.9s\tremaining: 34s\n",
            "402:\tlearn: 0.1116714\ttotal: 22.9s\tremaining: 34s\n",
            "403:\tlearn: 0.1115785\ttotal: 23s\tremaining: 33.9s\n",
            "404:\tlearn: 0.1115054\ttotal: 23s\tremaining: 33.8s\n",
            "405:\tlearn: 0.1114271\ttotal: 23.1s\tremaining: 33.7s\n",
            "406:\tlearn: 0.1113647\ttotal: 23.1s\tremaining: 33.7s\n",
            "407:\tlearn: 0.1113198\ttotal: 23.2s\tremaining: 33.6s\n",
            "408:\tlearn: 0.1112932\ttotal: 23.2s\tremaining: 33.5s\n",
            "409:\tlearn: 0.1112239\ttotal: 23.3s\tremaining: 33.5s\n",
            "410:\tlearn: 0.1111463\ttotal: 23.3s\tremaining: 33.4s\n",
            "411:\tlearn: 0.1110818\ttotal: 23.4s\tremaining: 33.4s\n",
            "412:\tlearn: 0.1109925\ttotal: 23.4s\tremaining: 33.3s\n",
            "413:\tlearn: 0.1109186\ttotal: 23.5s\tremaining: 33.2s\n",
            "414:\tlearn: 0.1108106\ttotal: 23.5s\tremaining: 33.1s\n",
            "415:\tlearn: 0.1106740\ttotal: 23.6s\tremaining: 33.1s\n",
            "416:\tlearn: 0.1105773\ttotal: 23.6s\tremaining: 33s\n",
            "417:\tlearn: 0.1104415\ttotal: 23.7s\tremaining: 32.9s\n",
            "418:\tlearn: 0.1103562\ttotal: 23.7s\tremaining: 32.9s\n",
            "419:\tlearn: 0.1101981\ttotal: 23.8s\tremaining: 32.8s\n",
            "420:\tlearn: 0.1101010\ttotal: 23.8s\tremaining: 32.7s\n",
            "421:\tlearn: 0.1098981\ttotal: 23.9s\tremaining: 32.7s\n",
            "422:\tlearn: 0.1098451\ttotal: 23.9s\tremaining: 32.6s\n",
            "423:\tlearn: 0.1097054\ttotal: 24s\tremaining: 32.6s\n",
            "424:\tlearn: 0.1095993\ttotal: 24s\tremaining: 32.5s\n",
            "425:\tlearn: 0.1095275\ttotal: 24.1s\tremaining: 32.4s\n",
            "426:\tlearn: 0.1094435\ttotal: 24.1s\tremaining: 32.4s\n",
            "427:\tlearn: 0.1094119\ttotal: 24.2s\tremaining: 32.3s\n",
            "428:\tlearn: 0.1092894\ttotal: 24.2s\tremaining: 32.2s\n",
            "429:\tlearn: 0.1092586\ttotal: 24.2s\tremaining: 32.1s\n",
            "430:\tlearn: 0.1092048\ttotal: 24.3s\tremaining: 32.1s\n",
            "431:\tlearn: 0.1090962\ttotal: 24.3s\tremaining: 32s\n",
            "432:\tlearn: 0.1090096\ttotal: 24.4s\tremaining: 31.9s\n",
            "433:\tlearn: 0.1089680\ttotal: 24.4s\tremaining: 31.9s\n",
            "434:\tlearn: 0.1088931\ttotal: 24.5s\tremaining: 31.8s\n",
            "435:\tlearn: 0.1087742\ttotal: 24.5s\tremaining: 31.7s\n",
            "436:\tlearn: 0.1086894\ttotal: 24.6s\tremaining: 31.7s\n",
            "437:\tlearn: 0.1085978\ttotal: 24.6s\tremaining: 31.6s\n",
            "438:\tlearn: 0.1085185\ttotal: 24.7s\tremaining: 31.5s\n",
            "439:\tlearn: 0.1084018\ttotal: 24.7s\tremaining: 31.5s\n",
            "440:\tlearn: 0.1082762\ttotal: 24.8s\tremaining: 31.4s\n",
            "441:\tlearn: 0.1081702\ttotal: 24.8s\tremaining: 31.4s\n",
            "442:\tlearn: 0.1080484\ttotal: 24.9s\tremaining: 31.3s\n",
            "443:\tlearn: 0.1080142\ttotal: 24.9s\tremaining: 31.2s\n",
            "444:\tlearn: 0.1079067\ttotal: 25s\tremaining: 31.2s\n",
            "445:\tlearn: 0.1078430\ttotal: 25s\tremaining: 31.1s\n",
            "446:\tlearn: 0.1077986\ttotal: 25.1s\tremaining: 31s\n",
            "447:\tlearn: 0.1077088\ttotal: 25.1s\tremaining: 31s\n",
            "448:\tlearn: 0.1076280\ttotal: 25.2s\tremaining: 30.9s\n",
            "449:\tlearn: 0.1075533\ttotal: 25.2s\tremaining: 30.8s\n",
            "450:\tlearn: 0.1074425\ttotal: 25.3s\tremaining: 30.8s\n",
            "451:\tlearn: 0.1073546\ttotal: 25.3s\tremaining: 30.7s\n",
            "452:\tlearn: 0.1073393\ttotal: 25.4s\tremaining: 30.6s\n",
            "453:\tlearn: 0.1073324\ttotal: 25.4s\tremaining: 30.6s\n",
            "454:\tlearn: 0.1072521\ttotal: 25.5s\tremaining: 30.5s\n",
            "455:\tlearn: 0.1072446\ttotal: 25.5s\tremaining: 30.5s\n",
            "456:\tlearn: 0.1072051\ttotal: 25.6s\tremaining: 30.4s\n",
            "457:\tlearn: 0.1070505\ttotal: 25.6s\tremaining: 30.3s\n",
            "458:\tlearn: 0.1068898\ttotal: 25.7s\tremaining: 30.3s\n",
            "459:\tlearn: 0.1067545\ttotal: 25.7s\tremaining: 30.2s\n",
            "460:\tlearn: 0.1066604\ttotal: 25.8s\tremaining: 30.1s\n",
            "461:\tlearn: 0.1065959\ttotal: 25.8s\tremaining: 30.1s\n",
            "462:\tlearn: 0.1065029\ttotal: 25.9s\tremaining: 30s\n",
            "463:\tlearn: 0.1064818\ttotal: 25.9s\tremaining: 30s\n",
            "464:\tlearn: 0.1064011\ttotal: 26s\tremaining: 29.9s\n",
            "465:\tlearn: 0.1063398\ttotal: 26s\tremaining: 29.8s\n",
            "466:\tlearn: 0.1062346\ttotal: 26.1s\tremaining: 29.8s\n",
            "467:\tlearn: 0.1061617\ttotal: 26.1s\tremaining: 29.7s\n",
            "468:\tlearn: 0.1061416\ttotal: 26.2s\tremaining: 29.6s\n",
            "469:\tlearn: 0.1061066\ttotal: 26.2s\tremaining: 29.6s\n",
            "470:\tlearn: 0.1061040\ttotal: 26.2s\tremaining: 29.5s\n",
            "471:\tlearn: 0.1060385\ttotal: 26.3s\tremaining: 29.4s\n",
            "472:\tlearn: 0.1059300\ttotal: 26.3s\tremaining: 29.3s\n",
            "473:\tlearn: 0.1058636\ttotal: 26.4s\tremaining: 29.3s\n",
            "474:\tlearn: 0.1057895\ttotal: 26.4s\tremaining: 29.2s\n",
            "475:\tlearn: 0.1057276\ttotal: 26.5s\tremaining: 29.2s\n",
            "476:\tlearn: 0.1057012\ttotal: 26.5s\tremaining: 29.1s\n",
            "477:\tlearn: 0.1056824\ttotal: 26.6s\tremaining: 29s\n",
            "478:\tlearn: 0.1056341\ttotal: 26.6s\tremaining: 29s\n",
            "479:\tlearn: 0.1055571\ttotal: 26.7s\tremaining: 28.9s\n",
            "480:\tlearn: 0.1055000\ttotal: 26.8s\tremaining: 28.9s\n",
            "481:\tlearn: 0.1054972\ttotal: 26.8s\tremaining: 28.8s\n",
            "482:\tlearn: 0.1054969\ttotal: 26.9s\tremaining: 28.8s\n",
            "483:\tlearn: 0.1054023\ttotal: 27s\tremaining: 28.8s\n",
            "484:\tlearn: 0.1053345\ttotal: 27.1s\tremaining: 28.8s\n",
            "485:\tlearn: 0.1052961\ttotal: 27.2s\tremaining: 28.7s\n",
            "486:\tlearn: 0.1052169\ttotal: 27.3s\tremaining: 28.7s\n",
            "487:\tlearn: 0.1051758\ttotal: 27.3s\tremaining: 28.7s\n",
            "488:\tlearn: 0.1051366\ttotal: 27.4s\tremaining: 28.7s\n",
            "489:\tlearn: 0.1050124\ttotal: 27.5s\tremaining: 28.6s\n",
            "490:\tlearn: 0.1049208\ttotal: 27.6s\tremaining: 28.6s\n",
            "491:\tlearn: 0.1048772\ttotal: 27.7s\tremaining: 28.6s\n",
            "492:\tlearn: 0.1048479\ttotal: 27.8s\tremaining: 28.6s\n",
            "493:\tlearn: 0.1047888\ttotal: 27.9s\tremaining: 28.5s\n",
            "494:\tlearn: 0.1047777\ttotal: 28s\tremaining: 28.5s\n",
            "495:\tlearn: 0.1046948\ttotal: 28.1s\tremaining: 28.5s\n",
            "496:\tlearn: 0.1046459\ttotal: 28.1s\tremaining: 28.5s\n",
            "497:\tlearn: 0.1045934\ttotal: 28.2s\tremaining: 28.4s\n",
            "498:\tlearn: 0.1045005\ttotal: 28.3s\tremaining: 28.4s\n",
            "499:\tlearn: 0.1044369\ttotal: 28.4s\tremaining: 28.4s\n",
            "500:\tlearn: 0.1044046\ttotal: 28.4s\tremaining: 28.3s\n",
            "501:\tlearn: 0.1043496\ttotal: 28.5s\tremaining: 28.3s\n",
            "502:\tlearn: 0.1042634\ttotal: 28.6s\tremaining: 28.3s\n",
            "503:\tlearn: 0.1041402\ttotal: 28.7s\tremaining: 28.2s\n",
            "504:\tlearn: 0.1040401\ttotal: 28.8s\tremaining: 28.2s\n",
            "505:\tlearn: 0.1039883\ttotal: 28.9s\tremaining: 28.2s\n",
            "506:\tlearn: 0.1039822\ttotal: 28.9s\tremaining: 28.1s\n",
            "507:\tlearn: 0.1039608\ttotal: 29s\tremaining: 28.1s\n",
            "508:\tlearn: 0.1038869\ttotal: 29.1s\tremaining: 28s\n",
            "509:\tlearn: 0.1038071\ttotal: 29.1s\tremaining: 28s\n",
            "510:\tlearn: 0.1037323\ttotal: 29.2s\tremaining: 28s\n",
            "511:\tlearn: 0.1036317\ttotal: 29.3s\tremaining: 27.9s\n",
            "512:\tlearn: 0.1035897\ttotal: 29.4s\tremaining: 27.9s\n",
            "513:\tlearn: 0.1035258\ttotal: 29.5s\tremaining: 27.9s\n",
            "514:\tlearn: 0.1034444\ttotal: 29.6s\tremaining: 27.9s\n",
            "515:\tlearn: 0.1033924\ttotal: 29.7s\tremaining: 27.8s\n",
            "516:\tlearn: 0.1033570\ttotal: 29.8s\tremaining: 27.8s\n",
            "517:\tlearn: 0.1033527\ttotal: 29.8s\tremaining: 27.8s\n",
            "518:\tlearn: 0.1033295\ttotal: 29.9s\tremaining: 27.7s\n",
            "519:\tlearn: 0.1032668\ttotal: 30s\tremaining: 27.7s\n",
            "520:\tlearn: 0.1032474\ttotal: 30.1s\tremaining: 27.7s\n",
            "521:\tlearn: 0.1032303\ttotal: 30.2s\tremaining: 27.7s\n",
            "522:\tlearn: 0.1031399\ttotal: 30.3s\tremaining: 27.6s\n",
            "523:\tlearn: 0.1030471\ttotal: 30.4s\tremaining: 27.6s\n",
            "524:\tlearn: 0.1029965\ttotal: 30.5s\tremaining: 27.6s\n",
            "525:\tlearn: 0.1029937\ttotal: 30.5s\tremaining: 27.5s\n",
            "526:\tlearn: 0.1029538\ttotal: 30.6s\tremaining: 27.4s\n",
            "527:\tlearn: 0.1028863\ttotal: 30.6s\tremaining: 27.4s\n",
            "528:\tlearn: 0.1028530\ttotal: 30.7s\tremaining: 27.3s\n",
            "529:\tlearn: 0.1028068\ttotal: 30.7s\tremaining: 27.2s\n",
            "530:\tlearn: 0.1027450\ttotal: 30.8s\tremaining: 27.2s\n",
            "531:\tlearn: 0.1026947\ttotal: 30.8s\tremaining: 27.1s\n",
            "532:\tlearn: 0.1026497\ttotal: 30.9s\tremaining: 27s\n",
            "533:\tlearn: 0.1025684\ttotal: 30.9s\tremaining: 27s\n",
            "534:\tlearn: 0.1024909\ttotal: 31s\tremaining: 26.9s\n",
            "535:\tlearn: 0.1024169\ttotal: 31s\tremaining: 26.8s\n",
            "536:\tlearn: 0.1023537\ttotal: 31.1s\tremaining: 26.8s\n",
            "537:\tlearn: 0.1022532\ttotal: 31.1s\tremaining: 26.7s\n",
            "538:\tlearn: 0.1022135\ttotal: 31.2s\tremaining: 26.7s\n",
            "539:\tlearn: 0.1022118\ttotal: 31.2s\tremaining: 26.6s\n",
            "540:\tlearn: 0.1021763\ttotal: 31.3s\tremaining: 26.5s\n",
            "541:\tlearn: 0.1021002\ttotal: 31.3s\tremaining: 26.5s\n",
            "542:\tlearn: 0.1020773\ttotal: 31.4s\tremaining: 26.4s\n",
            "543:\tlearn: 0.1019837\ttotal: 31.4s\tremaining: 26.3s\n",
            "544:\tlearn: 0.1019539\ttotal: 31.4s\tremaining: 26.3s\n",
            "545:\tlearn: 0.1019163\ttotal: 31.5s\tremaining: 26.2s\n",
            "546:\tlearn: 0.1018814\ttotal: 31.6s\tremaining: 26.1s\n",
            "547:\tlearn: 0.1017840\ttotal: 31.6s\tremaining: 26.1s\n",
            "548:\tlearn: 0.1016930\ttotal: 31.6s\tremaining: 26s\n",
            "549:\tlearn: 0.1016573\ttotal: 31.7s\tremaining: 25.9s\n",
            "550:\tlearn: 0.1016153\ttotal: 31.7s\tremaining: 25.9s\n",
            "551:\tlearn: 0.1015699\ttotal: 31.8s\tremaining: 25.8s\n",
            "552:\tlearn: 0.1015678\ttotal: 31.8s\tremaining: 25.7s\n",
            "553:\tlearn: 0.1014980\ttotal: 31.9s\tremaining: 25.7s\n",
            "554:\tlearn: 0.1013839\ttotal: 31.9s\tremaining: 25.6s\n",
            "555:\tlearn: 0.1012789\ttotal: 32s\tremaining: 25.5s\n",
            "556:\tlearn: 0.1011945\ttotal: 32s\tremaining: 25.5s\n",
            "557:\tlearn: 0.1011544\ttotal: 32.1s\tremaining: 25.4s\n",
            "558:\tlearn: 0.1011195\ttotal: 32.1s\tremaining: 25.4s\n",
            "559:\tlearn: 0.1010860\ttotal: 32.2s\tremaining: 25.3s\n",
            "560:\tlearn: 0.1010162\ttotal: 32.2s\tremaining: 25.2s\n",
            "561:\tlearn: 0.1009542\ttotal: 32.3s\tremaining: 25.2s\n",
            "562:\tlearn: 0.1009507\ttotal: 32.3s\tremaining: 25.1s\n",
            "563:\tlearn: 0.1008920\ttotal: 32.4s\tremaining: 25s\n",
            "564:\tlearn: 0.1008315\ttotal: 32.4s\tremaining: 25s\n",
            "565:\tlearn: 0.1007997\ttotal: 32.5s\tremaining: 24.9s\n",
            "566:\tlearn: 0.1007278\ttotal: 32.5s\tremaining: 24.8s\n",
            "567:\tlearn: 0.1006524\ttotal: 32.6s\tremaining: 24.8s\n",
            "568:\tlearn: 0.1006220\ttotal: 32.6s\tremaining: 24.7s\n",
            "569:\tlearn: 0.1005743\ttotal: 32.7s\tremaining: 24.6s\n",
            "570:\tlearn: 0.1005382\ttotal: 32.7s\tremaining: 24.6s\n",
            "571:\tlearn: 0.1004802\ttotal: 32.8s\tremaining: 24.5s\n",
            "572:\tlearn: 0.1004333\ttotal: 32.8s\tremaining: 24.5s\n",
            "573:\tlearn: 0.1003975\ttotal: 32.9s\tremaining: 24.4s\n",
            "574:\tlearn: 0.1003231\ttotal: 32.9s\tremaining: 24.3s\n",
            "575:\tlearn: 0.1002606\ttotal: 33s\tremaining: 24.3s\n",
            "576:\tlearn: 0.1002093\ttotal: 33s\tremaining: 24.2s\n",
            "577:\tlearn: 0.1001610\ttotal: 33.1s\tremaining: 24.1s\n",
            "578:\tlearn: 0.1001545\ttotal: 33.1s\tremaining: 24.1s\n",
            "579:\tlearn: 0.1001036\ttotal: 33.2s\tremaining: 24s\n",
            "580:\tlearn: 0.1000553\ttotal: 33.2s\tremaining: 24s\n",
            "581:\tlearn: 0.1000035\ttotal: 33.3s\tremaining: 23.9s\n",
            "582:\tlearn: 0.0999430\ttotal: 33.3s\tremaining: 23.8s\n",
            "583:\tlearn: 0.0998966\ttotal: 33.4s\tremaining: 23.8s\n",
            "584:\tlearn: 0.0998389\ttotal: 33.4s\tremaining: 23.7s\n",
            "585:\tlearn: 0.0997943\ttotal: 33.5s\tremaining: 23.6s\n",
            "586:\tlearn: 0.0997550\ttotal: 33.5s\tremaining: 23.6s\n",
            "587:\tlearn: 0.0997047\ttotal: 33.6s\tremaining: 23.5s\n",
            "588:\tlearn: 0.0996449\ttotal: 33.6s\tremaining: 23.5s\n",
            "589:\tlearn: 0.0996105\ttotal: 33.7s\tremaining: 23.4s\n",
            "590:\tlearn: 0.0995505\ttotal: 33.7s\tremaining: 23.3s\n",
            "591:\tlearn: 0.0994950\ttotal: 33.8s\tremaining: 23.3s\n",
            "592:\tlearn: 0.0994766\ttotal: 33.8s\tremaining: 23.2s\n",
            "593:\tlearn: 0.0994063\ttotal: 33.9s\tremaining: 23.1s\n",
            "594:\tlearn: 0.0993410\ttotal: 33.9s\tremaining: 23.1s\n",
            "595:\tlearn: 0.0992837\ttotal: 34s\tremaining: 23s\n",
            "596:\tlearn: 0.0992395\ttotal: 34s\tremaining: 23s\n",
            "597:\tlearn: 0.0992113\ttotal: 34s\tremaining: 22.9s\n",
            "598:\tlearn: 0.0991746\ttotal: 34.1s\tremaining: 22.8s\n",
            "599:\tlearn: 0.0991100\ttotal: 34.1s\tremaining: 22.8s\n",
            "600:\tlearn: 0.0990427\ttotal: 34.2s\tremaining: 22.7s\n",
            "601:\tlearn: 0.0990105\ttotal: 34.3s\tremaining: 22.7s\n",
            "602:\tlearn: 0.0989697\ttotal: 34.3s\tremaining: 22.6s\n",
            "603:\tlearn: 0.0989069\ttotal: 34.4s\tremaining: 22.5s\n",
            "604:\tlearn: 0.0988518\ttotal: 34.4s\tremaining: 22.5s\n",
            "605:\tlearn: 0.0988233\ttotal: 34.5s\tremaining: 22.4s\n",
            "606:\tlearn: 0.0988018\ttotal: 34.5s\tremaining: 22.3s\n",
            "607:\tlearn: 0.0987838\ttotal: 34.6s\tremaining: 22.3s\n",
            "608:\tlearn: 0.0987040\ttotal: 34.6s\tremaining: 22.2s\n",
            "609:\tlearn: 0.0986527\ttotal: 34.7s\tremaining: 22.2s\n",
            "610:\tlearn: 0.0986095\ttotal: 34.7s\tremaining: 22.1s\n",
            "611:\tlearn: 0.0985840\ttotal: 34.8s\tremaining: 22s\n",
            "612:\tlearn: 0.0985767\ttotal: 34.8s\tremaining: 22s\n",
            "613:\tlearn: 0.0985727\ttotal: 34.8s\tremaining: 21.9s\n",
            "614:\tlearn: 0.0985333\ttotal: 34.9s\tremaining: 21.9s\n",
            "615:\tlearn: 0.0984821\ttotal: 35s\tremaining: 21.8s\n",
            "616:\tlearn: 0.0984300\ttotal: 35s\tremaining: 21.7s\n",
            "617:\tlearn: 0.0984084\ttotal: 35s\tremaining: 21.7s\n",
            "618:\tlearn: 0.0983866\ttotal: 35.1s\tremaining: 21.6s\n",
            "619:\tlearn: 0.0983360\ttotal: 35.1s\tremaining: 21.5s\n",
            "620:\tlearn: 0.0982938\ttotal: 35.2s\tremaining: 21.5s\n",
            "621:\tlearn: 0.0982372\ttotal: 35.2s\tremaining: 21.4s\n",
            "622:\tlearn: 0.0981910\ttotal: 35.3s\tremaining: 21.4s\n",
            "623:\tlearn: 0.0981731\ttotal: 35.3s\tremaining: 21.3s\n",
            "624:\tlearn: 0.0981549\ttotal: 35.4s\tremaining: 21.2s\n",
            "625:\tlearn: 0.0981167\ttotal: 35.4s\tremaining: 21.2s\n",
            "626:\tlearn: 0.0980791\ttotal: 35.5s\tremaining: 21.1s\n",
            "627:\tlearn: 0.0979933\ttotal: 35.5s\tremaining: 21.1s\n",
            "628:\tlearn: 0.0979491\ttotal: 35.6s\tremaining: 21s\n",
            "629:\tlearn: 0.0979438\ttotal: 35.6s\tremaining: 20.9s\n",
            "630:\tlearn: 0.0979187\ttotal: 35.7s\tremaining: 20.9s\n",
            "631:\tlearn: 0.0978741\ttotal: 35.7s\tremaining: 20.8s\n",
            "632:\tlearn: 0.0978581\ttotal: 35.8s\tremaining: 20.8s\n",
            "633:\tlearn: 0.0977856\ttotal: 35.8s\tremaining: 20.7s\n",
            "634:\tlearn: 0.0977474\ttotal: 35.9s\tremaining: 20.6s\n",
            "635:\tlearn: 0.0977206\ttotal: 35.9s\tremaining: 20.6s\n",
            "636:\tlearn: 0.0976774\ttotal: 36s\tremaining: 20.5s\n",
            "637:\tlearn: 0.0976757\ttotal: 36s\tremaining: 20.4s\n",
            "638:\tlearn: 0.0976109\ttotal: 36.1s\tremaining: 20.4s\n",
            "639:\tlearn: 0.0976109\ttotal: 36.1s\tremaining: 20.3s\n",
            "640:\tlearn: 0.0975852\ttotal: 36.2s\tremaining: 20.3s\n",
            "641:\tlearn: 0.0975342\ttotal: 36.2s\tremaining: 20.2s\n",
            "642:\tlearn: 0.0974914\ttotal: 36.3s\tremaining: 20.2s\n",
            "643:\tlearn: 0.0974633\ttotal: 36.3s\tremaining: 20.1s\n",
            "644:\tlearn: 0.0974349\ttotal: 36.4s\tremaining: 20s\n",
            "645:\tlearn: 0.0974151\ttotal: 36.5s\tremaining: 20s\n",
            "646:\tlearn: 0.0973948\ttotal: 36.5s\tremaining: 19.9s\n",
            "647:\tlearn: 0.0973618\ttotal: 36.5s\tremaining: 19.9s\n",
            "648:\tlearn: 0.0973379\ttotal: 36.6s\tremaining: 19.8s\n",
            "649:\tlearn: 0.0973047\ttotal: 36.6s\tremaining: 19.7s\n",
            "650:\tlearn: 0.0972541\ttotal: 36.7s\tremaining: 19.7s\n",
            "651:\tlearn: 0.0972195\ttotal: 36.7s\tremaining: 19.6s\n",
            "652:\tlearn: 0.0971667\ttotal: 36.8s\tremaining: 19.5s\n",
            "653:\tlearn: 0.0971209\ttotal: 36.8s\tremaining: 19.5s\n",
            "654:\tlearn: 0.0971209\ttotal: 36.8s\tremaining: 19.4s\n",
            "655:\tlearn: 0.0971195\ttotal: 36.9s\tremaining: 19.3s\n",
            "656:\tlearn: 0.0971193\ttotal: 36.9s\tremaining: 19.3s\n",
            "657:\tlearn: 0.0971090\ttotal: 37s\tremaining: 19.2s\n",
            "658:\tlearn: 0.0970830\ttotal: 37s\tremaining: 19.2s\n",
            "659:\tlearn: 0.0970531\ttotal: 37.1s\tremaining: 19.1s\n",
            "660:\tlearn: 0.0970107\ttotal: 37.1s\tremaining: 19s\n",
            "661:\tlearn: 0.0969691\ttotal: 37.2s\tremaining: 19s\n",
            "662:\tlearn: 0.0969613\ttotal: 37.2s\tremaining: 18.9s\n",
            "663:\tlearn: 0.0969315\ttotal: 37.3s\tremaining: 18.9s\n",
            "664:\tlearn: 0.0968987\ttotal: 37.4s\tremaining: 18.8s\n",
            "665:\tlearn: 0.0968890\ttotal: 37.4s\tremaining: 18.8s\n",
            "666:\tlearn: 0.0968746\ttotal: 37.5s\tremaining: 18.7s\n",
            "667:\tlearn: 0.0968355\ttotal: 37.5s\tremaining: 18.6s\n",
            "668:\tlearn: 0.0968149\ttotal: 37.5s\tremaining: 18.6s\n",
            "669:\tlearn: 0.0968039\ttotal: 37.6s\tremaining: 18.5s\n",
            "670:\tlearn: 0.0967702\ttotal: 37.6s\tremaining: 18.5s\n",
            "671:\tlearn: 0.0967320\ttotal: 37.7s\tremaining: 18.4s\n",
            "672:\tlearn: 0.0967231\ttotal: 37.7s\tremaining: 18.3s\n",
            "673:\tlearn: 0.0967102\ttotal: 37.8s\tremaining: 18.3s\n",
            "674:\tlearn: 0.0966807\ttotal: 37.8s\tremaining: 18.2s\n",
            "675:\tlearn: 0.0966405\ttotal: 37.9s\tremaining: 18.2s\n",
            "676:\tlearn: 0.0965905\ttotal: 37.9s\tremaining: 18.1s\n",
            "677:\tlearn: 0.0965789\ttotal: 38s\tremaining: 18s\n",
            "678:\tlearn: 0.0965769\ttotal: 38s\tremaining: 18s\n",
            "679:\tlearn: 0.0965744\ttotal: 38.1s\tremaining: 17.9s\n",
            "680:\tlearn: 0.0965744\ttotal: 38.1s\tremaining: 17.9s\n",
            "681:\tlearn: 0.0965744\ttotal: 38.2s\tremaining: 17.8s\n",
            "682:\tlearn: 0.0965314\ttotal: 38.2s\tremaining: 17.7s\n",
            "683:\tlearn: 0.0965007\ttotal: 38.3s\tremaining: 17.7s\n",
            "684:\tlearn: 0.0964833\ttotal: 38.4s\tremaining: 17.6s\n",
            "685:\tlearn: 0.0964718\ttotal: 38.4s\tremaining: 17.6s\n",
            "686:\tlearn: 0.0964451\ttotal: 38.4s\tremaining: 17.5s\n",
            "687:\tlearn: 0.0964404\ttotal: 38.5s\tremaining: 17.5s\n",
            "688:\tlearn: 0.0964016\ttotal: 38.5s\tremaining: 17.4s\n",
            "689:\tlearn: 0.0963776\ttotal: 38.6s\tremaining: 17.3s\n",
            "690:\tlearn: 0.0963775\ttotal: 38.6s\tremaining: 17.3s\n",
            "691:\tlearn: 0.0963512\ttotal: 38.6s\tremaining: 17.2s\n",
            "692:\tlearn: 0.0962981\ttotal: 38.7s\tremaining: 17.1s\n",
            "693:\tlearn: 0.0962960\ttotal: 38.7s\tremaining: 17.1s\n",
            "694:\tlearn: 0.0962705\ttotal: 38.8s\tremaining: 17s\n",
            "695:\tlearn: 0.0962390\ttotal: 38.9s\tremaining: 17s\n",
            "696:\tlearn: 0.0962103\ttotal: 38.9s\tremaining: 16.9s\n",
            "697:\tlearn: 0.0962100\ttotal: 38.9s\tremaining: 16.9s\n",
            "698:\tlearn: 0.0961888\ttotal: 39s\tremaining: 16.8s\n",
            "699:\tlearn: 0.0961534\ttotal: 39s\tremaining: 16.7s\n",
            "700:\tlearn: 0.0961266\ttotal: 39.1s\tremaining: 16.7s\n",
            "701:\tlearn: 0.0961079\ttotal: 39.2s\tremaining: 16.6s\n",
            "702:\tlearn: 0.0960852\ttotal: 39.2s\tremaining: 16.6s\n",
            "703:\tlearn: 0.0960764\ttotal: 39.2s\tremaining: 16.5s\n",
            "704:\tlearn: 0.0960391\ttotal: 39.3s\tremaining: 16.4s\n",
            "705:\tlearn: 0.0960095\ttotal: 39.3s\tremaining: 16.4s\n",
            "706:\tlearn: 0.0959783\ttotal: 39.4s\tremaining: 16.3s\n",
            "707:\tlearn: 0.0959510\ttotal: 39.4s\tremaining: 16.3s\n",
            "708:\tlearn: 0.0959158\ttotal: 39.5s\tremaining: 16.2s\n",
            "709:\tlearn: 0.0958865\ttotal: 39.5s\tremaining: 16.1s\n",
            "710:\tlearn: 0.0958352\ttotal: 39.6s\tremaining: 16.1s\n",
            "711:\tlearn: 0.0957990\ttotal: 39.6s\tremaining: 16s\n",
            "712:\tlearn: 0.0957664\ttotal: 39.7s\tremaining: 16s\n",
            "713:\tlearn: 0.0957560\ttotal: 39.7s\tremaining: 15.9s\n",
            "714:\tlearn: 0.0957244\ttotal: 39.8s\tremaining: 15.9s\n",
            "715:\tlearn: 0.0956978\ttotal: 39.8s\tremaining: 15.8s\n",
            "716:\tlearn: 0.0956742\ttotal: 39.9s\tremaining: 15.7s\n",
            "717:\tlearn: 0.0956348\ttotal: 39.9s\tremaining: 15.7s\n",
            "718:\tlearn: 0.0956228\ttotal: 40s\tremaining: 15.6s\n",
            "719:\tlearn: 0.0956149\ttotal: 40s\tremaining: 15.6s\n",
            "720:\tlearn: 0.0955972\ttotal: 40.1s\tremaining: 15.5s\n",
            "721:\tlearn: 0.0955754\ttotal: 40.1s\tremaining: 15.4s\n",
            "722:\tlearn: 0.0955543\ttotal: 40.2s\tremaining: 15.4s\n",
            "723:\tlearn: 0.0955489\ttotal: 40.2s\tremaining: 15.3s\n",
            "724:\tlearn: 0.0955257\ttotal: 40.2s\tremaining: 15.3s\n",
            "725:\tlearn: 0.0954895\ttotal: 40.3s\tremaining: 15.2s\n",
            "726:\tlearn: 0.0954636\ttotal: 40.3s\tremaining: 15.1s\n",
            "727:\tlearn: 0.0954034\ttotal: 40.4s\tremaining: 15.1s\n",
            "728:\tlearn: 0.0953874\ttotal: 40.4s\tremaining: 15s\n",
            "729:\tlearn: 0.0953723\ttotal: 40.5s\tremaining: 15s\n",
            "730:\tlearn: 0.0953537\ttotal: 40.6s\tremaining: 14.9s\n",
            "731:\tlearn: 0.0953125\ttotal: 40.7s\tremaining: 14.9s\n",
            "732:\tlearn: 0.0952894\ttotal: 40.8s\tremaining: 14.9s\n",
            "733:\tlearn: 0.0952685\ttotal: 40.9s\tremaining: 14.8s\n",
            "734:\tlearn: 0.0952624\ttotal: 41s\tremaining: 14.8s\n",
            "735:\tlearn: 0.0952581\ttotal: 41s\tremaining: 14.7s\n",
            "736:\tlearn: 0.0952410\ttotal: 41.1s\tremaining: 14.7s\n",
            "737:\tlearn: 0.0952005\ttotal: 41.2s\tremaining: 14.6s\n",
            "738:\tlearn: 0.0951585\ttotal: 41.3s\tremaining: 14.6s\n",
            "739:\tlearn: 0.0951279\ttotal: 41.4s\tremaining: 14.5s\n",
            "740:\tlearn: 0.0951151\ttotal: 41.5s\tremaining: 14.5s\n",
            "741:\tlearn: 0.0951027\ttotal: 41.6s\tremaining: 14.5s\n",
            "742:\tlearn: 0.0950767\ttotal: 41.6s\tremaining: 14.4s\n",
            "743:\tlearn: 0.0950393\ttotal: 41.7s\tremaining: 14.4s\n",
            "744:\tlearn: 0.0950083\ttotal: 41.8s\tremaining: 14.3s\n",
            "745:\tlearn: 0.0949677\ttotal: 41.9s\tremaining: 14.3s\n",
            "746:\tlearn: 0.0949543\ttotal: 42s\tremaining: 14.2s\n",
            "747:\tlearn: 0.0949019\ttotal: 42.1s\tremaining: 14.2s\n",
            "748:\tlearn: 0.0949018\ttotal: 42.1s\tremaining: 14.1s\n",
            "749:\tlearn: 0.0948819\ttotal: 42.2s\tremaining: 14.1s\n",
            "750:\tlearn: 0.0948629\ttotal: 42.3s\tremaining: 14s\n",
            "751:\tlearn: 0.0948281\ttotal: 42.4s\tremaining: 14s\n",
            "752:\tlearn: 0.0948175\ttotal: 42.5s\tremaining: 13.9s\n",
            "753:\tlearn: 0.0947962\ttotal: 42.6s\tremaining: 13.9s\n",
            "754:\tlearn: 0.0947590\ttotal: 42.7s\tremaining: 13.8s\n",
            "755:\tlearn: 0.0947366\ttotal: 42.8s\tremaining: 13.8s\n",
            "756:\tlearn: 0.0947312\ttotal: 42.8s\tremaining: 13.8s\n",
            "757:\tlearn: 0.0947186\ttotal: 42.9s\tremaining: 13.7s\n",
            "758:\tlearn: 0.0947185\ttotal: 43s\tremaining: 13.7s\n",
            "759:\tlearn: 0.0946887\ttotal: 43.1s\tremaining: 13.6s\n",
            "760:\tlearn: 0.0946870\ttotal: 43.2s\tremaining: 13.6s\n",
            "761:\tlearn: 0.0946548\ttotal: 43.3s\tremaining: 13.5s\n",
            "762:\tlearn: 0.0946332\ttotal: 43.4s\tremaining: 13.5s\n",
            "763:\tlearn: 0.0946143\ttotal: 43.5s\tremaining: 13.4s\n",
            "764:\tlearn: 0.0945696\ttotal: 43.6s\tremaining: 13.4s\n",
            "765:\tlearn: 0.0945551\ttotal: 43.7s\tremaining: 13.3s\n",
            "766:\tlearn: 0.0945209\ttotal: 43.7s\tremaining: 13.3s\n",
            "767:\tlearn: 0.0945208\ttotal: 43.8s\tremaining: 13.2s\n",
            "768:\tlearn: 0.0944731\ttotal: 43.9s\tremaining: 13.2s\n",
            "769:\tlearn: 0.0944653\ttotal: 44s\tremaining: 13.1s\n",
            "770:\tlearn: 0.0944295\ttotal: 44.1s\tremaining: 13.1s\n",
            "771:\tlearn: 0.0943832\ttotal: 44.1s\tremaining: 13s\n",
            "772:\tlearn: 0.0943639\ttotal: 44.2s\tremaining: 13s\n",
            "773:\tlearn: 0.0943407\ttotal: 44.2s\tremaining: 12.9s\n",
            "774:\tlearn: 0.0943019\ttotal: 44.3s\tremaining: 12.9s\n",
            "775:\tlearn: 0.0942767\ttotal: 44.3s\tremaining: 12.8s\n",
            "776:\tlearn: 0.0942715\ttotal: 44.4s\tremaining: 12.7s\n",
            "777:\tlearn: 0.0942454\ttotal: 44.4s\tremaining: 12.7s\n",
            "778:\tlearn: 0.0942182\ttotal: 44.5s\tremaining: 12.6s\n",
            "779:\tlearn: 0.0942043\ttotal: 44.5s\tremaining: 12.6s\n",
            "780:\tlearn: 0.0941665\ttotal: 44.6s\tremaining: 12.5s\n",
            "781:\tlearn: 0.0941499\ttotal: 44.6s\tremaining: 12.4s\n",
            "782:\tlearn: 0.0941162\ttotal: 44.7s\tremaining: 12.4s\n",
            "783:\tlearn: 0.0940970\ttotal: 44.7s\tremaining: 12.3s\n",
            "784:\tlearn: 0.0940784\ttotal: 44.8s\tremaining: 12.3s\n",
            "785:\tlearn: 0.0940730\ttotal: 44.8s\tremaining: 12.2s\n",
            "786:\tlearn: 0.0940623\ttotal: 44.9s\tremaining: 12.1s\n",
            "787:\tlearn: 0.0940441\ttotal: 44.9s\tremaining: 12.1s\n",
            "788:\tlearn: 0.0940400\ttotal: 45s\tremaining: 12s\n",
            "789:\tlearn: 0.0940401\ttotal: 45s\tremaining: 12s\n",
            "790:\tlearn: 0.0940164\ttotal: 45.1s\tremaining: 11.9s\n",
            "791:\tlearn: 0.0939814\ttotal: 45.1s\tremaining: 11.9s\n",
            "792:\tlearn: 0.0939381\ttotal: 45.2s\tremaining: 11.8s\n",
            "793:\tlearn: 0.0938958\ttotal: 45.2s\tremaining: 11.7s\n",
            "794:\tlearn: 0.0938592\ttotal: 45.3s\tremaining: 11.7s\n",
            "795:\tlearn: 0.0938319\ttotal: 45.3s\tremaining: 11.6s\n",
            "796:\tlearn: 0.0937950\ttotal: 45.4s\tremaining: 11.6s\n",
            "797:\tlearn: 0.0937601\ttotal: 45.4s\tremaining: 11.5s\n",
            "798:\tlearn: 0.0937416\ttotal: 45.5s\tremaining: 11.4s\n",
            "799:\tlearn: 0.0937416\ttotal: 45.5s\tremaining: 11.4s\n",
            "800:\tlearn: 0.0937341\ttotal: 45.6s\tremaining: 11.3s\n",
            "801:\tlearn: 0.0937045\ttotal: 45.6s\tremaining: 11.3s\n",
            "802:\tlearn: 0.0936793\ttotal: 45.7s\tremaining: 11.2s\n",
            "803:\tlearn: 0.0936478\ttotal: 45.7s\tremaining: 11.1s\n",
            "804:\tlearn: 0.0936365\ttotal: 45.8s\tremaining: 11.1s\n",
            "805:\tlearn: 0.0936249\ttotal: 45.8s\tremaining: 11s\n",
            "806:\tlearn: 0.0936086\ttotal: 45.9s\tremaining: 11s\n",
            "807:\tlearn: 0.0936015\ttotal: 45.9s\tremaining: 10.9s\n",
            "808:\tlearn: 0.0935720\ttotal: 46s\tremaining: 10.9s\n",
            "809:\tlearn: 0.0935490\ttotal: 46s\tremaining: 10.8s\n",
            "810:\tlearn: 0.0935308\ttotal: 46.1s\tremaining: 10.7s\n",
            "811:\tlearn: 0.0934929\ttotal: 46.1s\tremaining: 10.7s\n",
            "812:\tlearn: 0.0934698\ttotal: 46.2s\tremaining: 10.6s\n",
            "813:\tlearn: 0.0934525\ttotal: 46.2s\tremaining: 10.6s\n",
            "814:\tlearn: 0.0934390\ttotal: 46.3s\tremaining: 10.5s\n",
            "815:\tlearn: 0.0934230\ttotal: 46.3s\tremaining: 10.4s\n",
            "816:\tlearn: 0.0934023\ttotal: 46.4s\tremaining: 10.4s\n",
            "817:\tlearn: 0.0933701\ttotal: 46.4s\tremaining: 10.3s\n",
            "818:\tlearn: 0.0933267\ttotal: 46.5s\tremaining: 10.3s\n",
            "819:\tlearn: 0.0933166\ttotal: 46.5s\tremaining: 10.2s\n",
            "820:\tlearn: 0.0932923\ttotal: 46.6s\tremaining: 10.2s\n",
            "821:\tlearn: 0.0932569\ttotal: 46.6s\tremaining: 10.1s\n",
            "822:\tlearn: 0.0932351\ttotal: 46.7s\tremaining: 10s\n",
            "823:\tlearn: 0.0932136\ttotal: 46.7s\tremaining: 9.98s\n",
            "824:\tlearn: 0.0931834\ttotal: 46.8s\tremaining: 9.93s\n",
            "825:\tlearn: 0.0931621\ttotal: 46.8s\tremaining: 9.87s\n",
            "826:\tlearn: 0.0931423\ttotal: 46.9s\tremaining: 9.81s\n",
            "827:\tlearn: 0.0931180\ttotal: 46.9s\tremaining: 9.75s\n",
            "828:\tlearn: 0.0930919\ttotal: 47s\tremaining: 9.69s\n",
            "829:\tlearn: 0.0930732\ttotal: 47s\tremaining: 9.63s\n",
            "830:\tlearn: 0.0930529\ttotal: 47.1s\tremaining: 9.58s\n",
            "831:\tlearn: 0.0930128\ttotal: 47.1s\tremaining: 9.52s\n",
            "832:\tlearn: 0.0929967\ttotal: 47.2s\tremaining: 9.46s\n",
            "833:\tlearn: 0.0929847\ttotal: 47.2s\tremaining: 9.4s\n",
            "834:\tlearn: 0.0929798\ttotal: 47.3s\tremaining: 9.34s\n",
            "835:\tlearn: 0.0929624\ttotal: 47.3s\tremaining: 9.29s\n",
            "836:\tlearn: 0.0929579\ttotal: 47.4s\tremaining: 9.23s\n",
            "837:\tlearn: 0.0929547\ttotal: 47.4s\tremaining: 9.17s\n",
            "838:\tlearn: 0.0929506\ttotal: 47.5s\tremaining: 9.11s\n",
            "839:\tlearn: 0.0929300\ttotal: 47.5s\tremaining: 9.05s\n",
            "840:\tlearn: 0.0929126\ttotal: 47.6s\tremaining: 8.99s\n",
            "841:\tlearn: 0.0929087\ttotal: 47.6s\tremaining: 8.94s\n",
            "842:\tlearn: 0.0928936\ttotal: 47.7s\tremaining: 8.88s\n",
            "843:\tlearn: 0.0928935\ttotal: 47.7s\tremaining: 8.82s\n",
            "844:\tlearn: 0.0928808\ttotal: 47.8s\tremaining: 8.76s\n",
            "845:\tlearn: 0.0928608\ttotal: 47.8s\tremaining: 8.71s\n",
            "846:\tlearn: 0.0928359\ttotal: 47.9s\tremaining: 8.65s\n",
            "847:\tlearn: 0.0928359\ttotal: 47.9s\tremaining: 8.59s\n",
            "848:\tlearn: 0.0928248\ttotal: 48s\tremaining: 8.53s\n",
            "849:\tlearn: 0.0928004\ttotal: 48s\tremaining: 8.48s\n",
            "850:\tlearn: 0.0928003\ttotal: 48.1s\tremaining: 8.42s\n",
            "851:\tlearn: 0.0927812\ttotal: 48.1s\tremaining: 8.36s\n",
            "852:\tlearn: 0.0927537\ttotal: 48.2s\tremaining: 8.3s\n",
            "853:\tlearn: 0.0927333\ttotal: 48.2s\tremaining: 8.24s\n",
            "854:\tlearn: 0.0927255\ttotal: 48.3s\tremaining: 8.19s\n",
            "855:\tlearn: 0.0927146\ttotal: 48.3s\tremaining: 8.13s\n",
            "856:\tlearn: 0.0927094\ttotal: 48.4s\tremaining: 8.07s\n",
            "857:\tlearn: 0.0926966\ttotal: 48.4s\tremaining: 8.01s\n",
            "858:\tlearn: 0.0926966\ttotal: 48.5s\tremaining: 7.96s\n",
            "859:\tlearn: 0.0926840\ttotal: 48.5s\tremaining: 7.9s\n",
            "860:\tlearn: 0.0926552\ttotal: 48.6s\tremaining: 7.84s\n",
            "861:\tlearn: 0.0926354\ttotal: 48.6s\tremaining: 7.78s\n",
            "862:\tlearn: 0.0926193\ttotal: 48.7s\tremaining: 7.73s\n",
            "863:\tlearn: 0.0925901\ttotal: 48.7s\tremaining: 7.67s\n",
            "864:\tlearn: 0.0925785\ttotal: 48.8s\tremaining: 7.61s\n",
            "865:\tlearn: 0.0925554\ttotal: 48.8s\tremaining: 7.55s\n",
            "866:\tlearn: 0.0925346\ttotal: 48.9s\tremaining: 7.5s\n",
            "867:\tlearn: 0.0925344\ttotal: 48.9s\tremaining: 7.44s\n",
            "868:\tlearn: 0.0925183\ttotal: 49s\tremaining: 7.38s\n",
            "869:\tlearn: 0.0925021\ttotal: 49s\tremaining: 7.33s\n",
            "870:\tlearn: 0.0924543\ttotal: 49.1s\tremaining: 7.27s\n",
            "871:\tlearn: 0.0924408\ttotal: 49.1s\tremaining: 7.21s\n",
            "872:\tlearn: 0.0924267\ttotal: 49.2s\tremaining: 7.15s\n",
            "873:\tlearn: 0.0924038\ttotal: 49.2s\tremaining: 7.09s\n",
            "874:\tlearn: 0.0923789\ttotal: 49.3s\tremaining: 7.04s\n",
            "875:\tlearn: 0.0923686\ttotal: 49.3s\tremaining: 6.98s\n",
            "876:\tlearn: 0.0923400\ttotal: 49.4s\tremaining: 6.92s\n",
            "877:\tlearn: 0.0923165\ttotal: 49.4s\tremaining: 6.87s\n",
            "878:\tlearn: 0.0922934\ttotal: 49.5s\tremaining: 6.81s\n",
            "879:\tlearn: 0.0922709\ttotal: 49.5s\tremaining: 6.75s\n",
            "880:\tlearn: 0.0922597\ttotal: 49.6s\tremaining: 6.7s\n",
            "881:\tlearn: 0.0922407\ttotal: 49.6s\tremaining: 6.64s\n",
            "882:\tlearn: 0.0922360\ttotal: 49.7s\tremaining: 6.58s\n",
            "883:\tlearn: 0.0922118\ttotal: 49.7s\tremaining: 6.53s\n",
            "884:\tlearn: 0.0921904\ttotal: 49.8s\tremaining: 6.47s\n",
            "885:\tlearn: 0.0921776\ttotal: 49.8s\tremaining: 6.41s\n",
            "886:\tlearn: 0.0921764\ttotal: 49.9s\tremaining: 6.36s\n",
            "887:\tlearn: 0.0921551\ttotal: 49.9s\tremaining: 6.3s\n",
            "888:\tlearn: 0.0921182\ttotal: 50s\tremaining: 6.24s\n",
            "889:\tlearn: 0.0920959\ttotal: 50s\tremaining: 6.18s\n",
            "890:\tlearn: 0.0920827\ttotal: 50.1s\tremaining: 6.13s\n",
            "891:\tlearn: 0.0920729\ttotal: 50.1s\tremaining: 6.07s\n",
            "892:\tlearn: 0.0920509\ttotal: 50.2s\tremaining: 6.01s\n",
            "893:\tlearn: 0.0920356\ttotal: 50.2s\tremaining: 5.96s\n",
            "894:\tlearn: 0.0920152\ttotal: 50.3s\tremaining: 5.9s\n",
            "895:\tlearn: 0.0919943\ttotal: 50.3s\tremaining: 5.84s\n",
            "896:\tlearn: 0.0919797\ttotal: 50.4s\tremaining: 5.79s\n",
            "897:\tlearn: 0.0919634\ttotal: 50.4s\tremaining: 5.73s\n",
            "898:\tlearn: 0.0919409\ttotal: 50.5s\tremaining: 5.67s\n",
            "899:\tlearn: 0.0919248\ttotal: 50.5s\tremaining: 5.62s\n",
            "900:\tlearn: 0.0919228\ttotal: 50.6s\tremaining: 5.56s\n",
            "901:\tlearn: 0.0919140\ttotal: 50.6s\tremaining: 5.5s\n",
            "902:\tlearn: 0.0918743\ttotal: 50.7s\tremaining: 5.45s\n",
            "903:\tlearn: 0.0918531\ttotal: 50.8s\tremaining: 5.39s\n",
            "904:\tlearn: 0.0918396\ttotal: 50.8s\tremaining: 5.33s\n",
            "905:\tlearn: 0.0918388\ttotal: 50.9s\tremaining: 5.28s\n",
            "906:\tlearn: 0.0918388\ttotal: 50.9s\tremaining: 5.22s\n",
            "907:\tlearn: 0.0918320\ttotal: 51s\tremaining: 5.16s\n",
            "908:\tlearn: 0.0918203\ttotal: 51s\tremaining: 5.11s\n",
            "909:\tlearn: 0.0918106\ttotal: 51s\tremaining: 5.05s\n",
            "910:\tlearn: 0.0917883\ttotal: 51.1s\tremaining: 4.99s\n",
            "911:\tlearn: 0.0917712\ttotal: 51.1s\tremaining: 4.93s\n",
            "912:\tlearn: 0.0917712\ttotal: 51.2s\tremaining: 4.88s\n",
            "913:\tlearn: 0.0917682\ttotal: 51.2s\tremaining: 4.82s\n",
            "914:\tlearn: 0.0917615\ttotal: 51.3s\tremaining: 4.76s\n",
            "915:\tlearn: 0.0917417\ttotal: 51.3s\tremaining: 4.71s\n",
            "916:\tlearn: 0.0917201\ttotal: 51.4s\tremaining: 4.65s\n",
            "917:\tlearn: 0.0917037\ttotal: 51.4s\tremaining: 4.59s\n",
            "918:\tlearn: 0.0916823\ttotal: 51.5s\tremaining: 4.54s\n",
            "919:\tlearn: 0.0916720\ttotal: 51.5s\tremaining: 4.48s\n",
            "920:\tlearn: 0.0916607\ttotal: 51.6s\tremaining: 4.42s\n",
            "921:\tlearn: 0.0916575\ttotal: 51.6s\tremaining: 4.36s\n",
            "922:\tlearn: 0.0916409\ttotal: 51.6s\tremaining: 4.31s\n",
            "923:\tlearn: 0.0916396\ttotal: 51.7s\tremaining: 4.25s\n",
            "924:\tlearn: 0.0916396\ttotal: 51.8s\tremaining: 4.2s\n",
            "925:\tlearn: 0.0916394\ttotal: 51.8s\tremaining: 4.14s\n",
            "926:\tlearn: 0.0916381\ttotal: 51.9s\tremaining: 4.08s\n",
            "927:\tlearn: 0.0916257\ttotal: 51.9s\tremaining: 4.03s\n",
            "928:\tlearn: 0.0916256\ttotal: 52s\tremaining: 3.97s\n",
            "929:\tlearn: 0.0916255\ttotal: 52s\tremaining: 3.92s\n",
            "930:\tlearn: 0.0916034\ttotal: 52.1s\tremaining: 3.86s\n",
            "931:\tlearn: 0.0915828\ttotal: 52.1s\tremaining: 3.8s\n",
            "932:\tlearn: 0.0915620\ttotal: 52.2s\tremaining: 3.75s\n",
            "933:\tlearn: 0.0915498\ttotal: 52.2s\tremaining: 3.69s\n",
            "934:\tlearn: 0.0915299\ttotal: 52.3s\tremaining: 3.63s\n",
            "935:\tlearn: 0.0915047\ttotal: 52.3s\tremaining: 3.58s\n",
            "936:\tlearn: 0.0914851\ttotal: 52.4s\tremaining: 3.52s\n",
            "937:\tlearn: 0.0914622\ttotal: 52.4s\tremaining: 3.46s\n",
            "938:\tlearn: 0.0914425\ttotal: 52.5s\tremaining: 3.41s\n",
            "939:\tlearn: 0.0914278\ttotal: 52.5s\tremaining: 3.35s\n",
            "940:\tlearn: 0.0914085\ttotal: 52.5s\tremaining: 3.29s\n",
            "941:\tlearn: 0.0913947\ttotal: 52.6s\tremaining: 3.24s\n",
            "942:\tlearn: 0.0913775\ttotal: 52.7s\tremaining: 3.18s\n",
            "943:\tlearn: 0.0913543\ttotal: 52.7s\tremaining: 3.13s\n",
            "944:\tlearn: 0.0913387\ttotal: 52.8s\tremaining: 3.07s\n",
            "945:\tlearn: 0.0913051\ttotal: 52.8s\tremaining: 3.01s\n",
            "946:\tlearn: 0.0912944\ttotal: 52.9s\tremaining: 2.96s\n",
            "947:\tlearn: 0.0912894\ttotal: 52.9s\tremaining: 2.9s\n",
            "948:\tlearn: 0.0912823\ttotal: 53s\tremaining: 2.85s\n",
            "949:\tlearn: 0.0912601\ttotal: 53s\tremaining: 2.79s\n",
            "950:\tlearn: 0.0912442\ttotal: 53s\tremaining: 2.73s\n",
            "951:\tlearn: 0.0912441\ttotal: 53.1s\tremaining: 2.68s\n",
            "952:\tlearn: 0.0912442\ttotal: 53.1s\tremaining: 2.62s\n",
            "953:\tlearn: 0.0912367\ttotal: 53.2s\tremaining: 2.56s\n",
            "954:\tlearn: 0.0912199\ttotal: 53.2s\tremaining: 2.51s\n",
            "955:\tlearn: 0.0912140\ttotal: 53.3s\tremaining: 2.45s\n",
            "956:\tlearn: 0.0911904\ttotal: 53.3s\tremaining: 2.4s\n",
            "957:\tlearn: 0.0911661\ttotal: 53.4s\tremaining: 2.34s\n",
            "958:\tlearn: 0.0911420\ttotal: 53.4s\tremaining: 2.28s\n",
            "959:\tlearn: 0.0911421\ttotal: 53.5s\tremaining: 2.23s\n",
            "960:\tlearn: 0.0911304\ttotal: 53.5s\tremaining: 2.17s\n",
            "961:\tlearn: 0.0911000\ttotal: 53.6s\tremaining: 2.12s\n",
            "962:\tlearn: 0.0910829\ttotal: 53.6s\tremaining: 2.06s\n",
            "963:\tlearn: 0.0910669\ttotal: 53.7s\tremaining: 2s\n",
            "964:\tlearn: 0.0910465\ttotal: 53.7s\tremaining: 1.95s\n",
            "965:\tlearn: 0.0910279\ttotal: 53.8s\tremaining: 1.89s\n",
            "966:\tlearn: 0.0910127\ttotal: 53.8s\tremaining: 1.84s\n",
            "967:\tlearn: 0.0910127\ttotal: 53.9s\tremaining: 1.78s\n",
            "968:\tlearn: 0.0910064\ttotal: 53.9s\tremaining: 1.73s\n",
            "969:\tlearn: 0.0910038\ttotal: 54s\tremaining: 1.67s\n",
            "970:\tlearn: 0.0909586\ttotal: 54.1s\tremaining: 1.61s\n",
            "971:\tlearn: 0.0909367\ttotal: 54.2s\tremaining: 1.56s\n",
            "972:\tlearn: 0.0909314\ttotal: 54.2s\tremaining: 1.5s\n",
            "973:\tlearn: 0.0909151\ttotal: 54.3s\tremaining: 1.45s\n",
            "974:\tlearn: 0.0908767\ttotal: 54.4s\tremaining: 1.4s\n",
            "975:\tlearn: 0.0908562\ttotal: 54.5s\tremaining: 1.34s\n",
            "976:\tlearn: 0.0908496\ttotal: 54.6s\tremaining: 1.28s\n",
            "977:\tlearn: 0.0908312\ttotal: 54.7s\tremaining: 1.23s\n",
            "978:\tlearn: 0.0908209\ttotal: 54.8s\tremaining: 1.18s\n",
            "979:\tlearn: 0.0907979\ttotal: 54.9s\tremaining: 1.12s\n",
            "980:\tlearn: 0.0907674\ttotal: 55s\tremaining: 1.06s\n",
            "981:\tlearn: 0.0907674\ttotal: 55s\tremaining: 1.01s\n",
            "982:\tlearn: 0.0907602\ttotal: 55.1s\tremaining: 953ms\n",
            "983:\tlearn: 0.0907548\ttotal: 55.2s\tremaining: 897ms\n",
            "984:\tlearn: 0.0907351\ttotal: 55.3s\tremaining: 841ms\n",
            "985:\tlearn: 0.0907223\ttotal: 55.3s\tremaining: 786ms\n",
            "986:\tlearn: 0.0907136\ttotal: 55.4s\tremaining: 730ms\n",
            "987:\tlearn: 0.0907028\ttotal: 55.5s\tremaining: 674ms\n",
            "988:\tlearn: 0.0906998\ttotal: 55.6s\tremaining: 619ms\n",
            "989:\tlearn: 0.0906997\ttotal: 55.7s\tremaining: 563ms\n",
            "990:\tlearn: 0.0906890\ttotal: 55.8s\tremaining: 507ms\n",
            "991:\tlearn: 0.0906697\ttotal: 55.9s\tremaining: 451ms\n",
            "992:\tlearn: 0.0906542\ttotal: 55.9s\tremaining: 394ms\n",
            "993:\tlearn: 0.0906428\ttotal: 56s\tremaining: 338ms\n",
            "994:\tlearn: 0.0906339\ttotal: 56.1s\tremaining: 282ms\n",
            "995:\tlearn: 0.0906154\ttotal: 56.2s\tremaining: 226ms\n",
            "996:\tlearn: 0.0906015\ttotal: 56.3s\tremaining: 169ms\n",
            "997:\tlearn: 0.0905921\ttotal: 56.4s\tremaining: 113ms\n",
            "998:\tlearn: 0.0905862\ttotal: 56.4s\tremaining: 56.5ms\n",
            "999:\tlearn: 0.0905761\ttotal: 56.5s\tremaining: 0us\n",
            "0:\tlearn: 0.5986514\ttotal: 88.6ms\tremaining: 1m 28s\n",
            "1:\tlearn: 0.5365014\ttotal: 162ms\tremaining: 1m 20s\n",
            "2:\tlearn: 0.4970315\ttotal: 252ms\tremaining: 1m 23s\n",
            "3:\tlearn: 0.4759626\ttotal: 335ms\tremaining: 1m 23s\n",
            "4:\tlearn: 0.4595820\ttotal: 408ms\tremaining: 1m 21s\n",
            "5:\tlearn: 0.4435956\ttotal: 492ms\tremaining: 1m 21s\n",
            "6:\tlearn: 0.4279881\ttotal: 585ms\tremaining: 1m 23s\n",
            "7:\tlearn: 0.4225642\ttotal: 676ms\tremaining: 1m 23s\n",
            "8:\tlearn: 0.4177377\ttotal: 762ms\tremaining: 1m 23s\n",
            "9:\tlearn: 0.4097303\ttotal: 819ms\tremaining: 1m 21s\n",
            "10:\tlearn: 0.4054017\ttotal: 867ms\tremaining: 1m 17s\n",
            "11:\tlearn: 0.4006861\ttotal: 912ms\tremaining: 1m 15s\n",
            "12:\tlearn: 0.3974557\ttotal: 961ms\tremaining: 1m 12s\n",
            "13:\tlearn: 0.3924215\ttotal: 1.01s\tremaining: 1m 11s\n",
            "14:\tlearn: 0.3900715\ttotal: 1.07s\tremaining: 1m 10s\n",
            "15:\tlearn: 0.3851339\ttotal: 1.11s\tremaining: 1m 8s\n",
            "16:\tlearn: 0.3797025\ttotal: 1.17s\tremaining: 1m 7s\n",
            "17:\tlearn: 0.3762532\ttotal: 1.22s\tremaining: 1m 6s\n",
            "18:\tlearn: 0.3717315\ttotal: 1.27s\tremaining: 1m 5s\n",
            "19:\tlearn: 0.3685818\ttotal: 1.33s\tremaining: 1m 5s\n",
            "20:\tlearn: 0.3641964\ttotal: 1.38s\tremaining: 1m 4s\n",
            "21:\tlearn: 0.3611808\ttotal: 1.43s\tremaining: 1m 3s\n",
            "22:\tlearn: 0.3605615\ttotal: 1.47s\tremaining: 1m 2s\n",
            "23:\tlearn: 0.3576902\ttotal: 1.53s\tremaining: 1m 2s\n",
            "24:\tlearn: 0.3558684\ttotal: 1.57s\tremaining: 1m 1s\n",
            "25:\tlearn: 0.3502279\ttotal: 1.62s\tremaining: 1m\n",
            "26:\tlearn: 0.3489335\ttotal: 1.67s\tremaining: 1m\n",
            "27:\tlearn: 0.3473329\ttotal: 1.72s\tremaining: 59.7s\n",
            "28:\tlearn: 0.3461736\ttotal: 1.78s\tremaining: 59.6s\n",
            "29:\tlearn: 0.3429036\ttotal: 1.83s\tremaining: 59.1s\n",
            "30:\tlearn: 0.3399489\ttotal: 1.87s\tremaining: 58.6s\n",
            "31:\tlearn: 0.3378108\ttotal: 1.92s\tremaining: 58.1s\n",
            "32:\tlearn: 0.3366810\ttotal: 1.97s\tremaining: 57.6s\n",
            "33:\tlearn: 0.3335328\ttotal: 2.02s\tremaining: 57.4s\n",
            "34:\tlearn: 0.3309082\ttotal: 2.07s\tremaining: 57s\n",
            "35:\tlearn: 0.3279422\ttotal: 2.11s\tremaining: 56.6s\n",
            "36:\tlearn: 0.3255310\ttotal: 2.16s\tremaining: 56.3s\n",
            "37:\tlearn: 0.3229573\ttotal: 2.22s\tremaining: 56.1s\n",
            "38:\tlearn: 0.3190113\ttotal: 2.28s\tremaining: 56.2s\n",
            "39:\tlearn: 0.3173588\ttotal: 2.33s\tremaining: 55.8s\n",
            "40:\tlearn: 0.3158645\ttotal: 2.37s\tremaining: 55.5s\n",
            "41:\tlearn: 0.3146070\ttotal: 2.42s\tremaining: 55.2s\n",
            "42:\tlearn: 0.3116167\ttotal: 2.46s\tremaining: 54.8s\n",
            "43:\tlearn: 0.3091771\ttotal: 2.52s\tremaining: 54.7s\n",
            "44:\tlearn: 0.3087060\ttotal: 2.57s\tremaining: 54.5s\n",
            "45:\tlearn: 0.3073826\ttotal: 2.61s\tremaining: 54.2s\n",
            "46:\tlearn: 0.3044292\ttotal: 2.66s\tremaining: 53.9s\n",
            "47:\tlearn: 0.3020493\ttotal: 2.71s\tremaining: 53.8s\n",
            "48:\tlearn: 0.3002697\ttotal: 2.77s\tremaining: 53.7s\n",
            "49:\tlearn: 0.2992149\ttotal: 2.81s\tremaining: 53.5s\n",
            "50:\tlearn: 0.2984954\ttotal: 2.86s\tremaining: 53.2s\n",
            "51:\tlearn: 0.2951534\ttotal: 2.91s\tremaining: 53s\n",
            "52:\tlearn: 0.2934312\ttotal: 2.95s\tremaining: 52.8s\n",
            "53:\tlearn: 0.2916047\ttotal: 3.01s\tremaining: 52.7s\n",
            "54:\tlearn: 0.2896027\ttotal: 3.06s\tremaining: 52.5s\n",
            "55:\tlearn: 0.2883509\ttotal: 3.1s\tremaining: 52.3s\n",
            "56:\tlearn: 0.2861927\ttotal: 3.15s\tremaining: 52.1s\n",
            "57:\tlearn: 0.2856488\ttotal: 3.19s\tremaining: 51.9s\n",
            "58:\tlearn: 0.2839345\ttotal: 3.27s\tremaining: 52.1s\n",
            "59:\tlearn: 0.2828590\ttotal: 3.31s\tremaining: 51.8s\n",
            "60:\tlearn: 0.2816437\ttotal: 3.35s\tremaining: 51.6s\n",
            "61:\tlearn: 0.2799850\ttotal: 3.4s\tremaining: 51.4s\n",
            "62:\tlearn: 0.2774187\ttotal: 3.44s\tremaining: 51.2s\n",
            "63:\tlearn: 0.2773891\ttotal: 3.45s\tremaining: 50.5s\n",
            "64:\tlearn: 0.2735172\ttotal: 3.5s\tremaining: 50.4s\n",
            "65:\tlearn: 0.2723504\ttotal: 3.55s\tremaining: 50.3s\n",
            "66:\tlearn: 0.2700822\ttotal: 3.6s\tremaining: 50.1s\n",
            "67:\tlearn: 0.2695524\ttotal: 3.65s\tremaining: 50s\n",
            "68:\tlearn: 0.2679559\ttotal: 3.69s\tremaining: 49.8s\n",
            "69:\tlearn: 0.2679066\ttotal: 3.75s\tremaining: 49.9s\n",
            "70:\tlearn: 0.2668511\ttotal: 3.81s\tremaining: 49.8s\n",
            "71:\tlearn: 0.2632781\ttotal: 3.85s\tremaining: 49.6s\n",
            "72:\tlearn: 0.2619595\ttotal: 3.9s\tremaining: 49.5s\n",
            "73:\tlearn: 0.2615223\ttotal: 3.95s\tremaining: 49.4s\n",
            "74:\tlearn: 0.2587509\ttotal: 4s\tremaining: 49.4s\n",
            "75:\tlearn: 0.2582016\ttotal: 4.05s\tremaining: 49.2s\n",
            "76:\tlearn: 0.2565415\ttotal: 4.09s\tremaining: 49.1s\n",
            "77:\tlearn: 0.2555446\ttotal: 4.14s\tremaining: 49s\n",
            "78:\tlearn: 0.2521159\ttotal: 4.19s\tremaining: 48.9s\n",
            "79:\tlearn: 0.2508761\ttotal: 4.24s\tremaining: 48.8s\n",
            "80:\tlearn: 0.2490792\ttotal: 4.31s\tremaining: 48.9s\n",
            "81:\tlearn: 0.2478394\ttotal: 4.36s\tremaining: 48.8s\n",
            "82:\tlearn: 0.2464582\ttotal: 4.4s\tremaining: 48.6s\n",
            "83:\tlearn: 0.2451144\ttotal: 4.45s\tremaining: 48.5s\n",
            "84:\tlearn: 0.2431664\ttotal: 4.51s\tremaining: 48.5s\n",
            "85:\tlearn: 0.2422354\ttotal: 4.55s\tremaining: 48.4s\n",
            "86:\tlearn: 0.2411189\ttotal: 4.6s\tremaining: 48.3s\n",
            "87:\tlearn: 0.2397944\ttotal: 4.66s\tremaining: 48.3s\n",
            "88:\tlearn: 0.2395453\ttotal: 4.71s\tremaining: 48.2s\n",
            "89:\tlearn: 0.2393458\ttotal: 4.76s\tremaining: 48.1s\n",
            "90:\tlearn: 0.2384025\ttotal: 4.8s\tremaining: 48s\n",
            "91:\tlearn: 0.2382149\ttotal: 4.85s\tremaining: 47.9s\n",
            "92:\tlearn: 0.2366101\ttotal: 4.91s\tremaining: 47.9s\n",
            "93:\tlearn: 0.2353319\ttotal: 4.96s\tremaining: 47.8s\n",
            "94:\tlearn: 0.2339875\ttotal: 5s\tremaining: 47.7s\n",
            "95:\tlearn: 0.2325284\ttotal: 5.05s\tremaining: 47.6s\n",
            "96:\tlearn: 0.2315002\ttotal: 5.1s\tremaining: 47.4s\n",
            "97:\tlearn: 0.2299772\ttotal: 5.16s\tremaining: 47.5s\n",
            "98:\tlearn: 0.2299406\ttotal: 5.21s\tremaining: 47.4s\n",
            "99:\tlearn: 0.2295903\ttotal: 5.25s\tremaining: 47.3s\n",
            "100:\tlearn: 0.2278554\ttotal: 5.31s\tremaining: 47.3s\n",
            "101:\tlearn: 0.2258651\ttotal: 5.36s\tremaining: 47.2s\n",
            "102:\tlearn: 0.2247330\ttotal: 5.42s\tremaining: 47.2s\n",
            "103:\tlearn: 0.2226399\ttotal: 5.46s\tremaining: 47.1s\n",
            "104:\tlearn: 0.2220089\ttotal: 5.51s\tremaining: 47s\n",
            "105:\tlearn: 0.2205492\ttotal: 5.56s\tremaining: 46.9s\n",
            "106:\tlearn: 0.2196124\ttotal: 5.62s\tremaining: 46.9s\n",
            "107:\tlearn: 0.2186340\ttotal: 5.66s\tremaining: 46.8s\n",
            "108:\tlearn: 0.2180364\ttotal: 5.71s\tremaining: 46.7s\n",
            "109:\tlearn: 0.2168632\ttotal: 5.76s\tremaining: 46.6s\n",
            "110:\tlearn: 0.2164385\ttotal: 5.82s\tremaining: 46.6s\n",
            "111:\tlearn: 0.2164242\ttotal: 5.83s\tremaining: 46.3s\n",
            "112:\tlearn: 0.2154121\ttotal: 5.88s\tremaining: 46.1s\n",
            "113:\tlearn: 0.2152201\ttotal: 5.92s\tremaining: 46s\n",
            "114:\tlearn: 0.2145227\ttotal: 5.97s\tremaining: 46s\n",
            "115:\tlearn: 0.2135768\ttotal: 6.02s\tremaining: 45.9s\n",
            "116:\tlearn: 0.2127658\ttotal: 6.07s\tremaining: 45.8s\n",
            "117:\tlearn: 0.2115043\ttotal: 6.12s\tremaining: 45.7s\n",
            "118:\tlearn: 0.2104655\ttotal: 6.17s\tremaining: 45.7s\n",
            "119:\tlearn: 0.2098032\ttotal: 6.22s\tremaining: 45.6s\n",
            "120:\tlearn: 0.2088022\ttotal: 6.27s\tremaining: 45.6s\n",
            "121:\tlearn: 0.2078999\ttotal: 6.33s\tremaining: 45.6s\n",
            "122:\tlearn: 0.2070889\ttotal: 6.38s\tremaining: 45.5s\n",
            "123:\tlearn: 0.2059460\ttotal: 6.43s\tremaining: 45.4s\n",
            "124:\tlearn: 0.2050697\ttotal: 6.49s\tremaining: 45.4s\n",
            "125:\tlearn: 0.2042638\ttotal: 6.53s\tremaining: 45.3s\n",
            "126:\tlearn: 0.2033228\ttotal: 6.58s\tremaining: 45.2s\n",
            "127:\tlearn: 0.2021235\ttotal: 6.62s\tremaining: 45.1s\n",
            "128:\tlearn: 0.2010086\ttotal: 6.67s\tremaining: 45s\n",
            "129:\tlearn: 0.2003982\ttotal: 6.72s\tremaining: 45s\n",
            "130:\tlearn: 0.1996865\ttotal: 6.77s\tremaining: 44.9s\n",
            "131:\tlearn: 0.1984548\ttotal: 6.82s\tremaining: 44.9s\n",
            "132:\tlearn: 0.1979581\ttotal: 6.87s\tremaining: 44.8s\n",
            "133:\tlearn: 0.1973982\ttotal: 6.91s\tremaining: 44.7s\n",
            "134:\tlearn: 0.1960732\ttotal: 6.97s\tremaining: 44.7s\n",
            "135:\tlearn: 0.1956868\ttotal: 7.02s\tremaining: 44.6s\n",
            "136:\tlearn: 0.1945072\ttotal: 7.06s\tremaining: 44.5s\n",
            "137:\tlearn: 0.1943754\ttotal: 7.11s\tremaining: 44.4s\n",
            "138:\tlearn: 0.1940962\ttotal: 7.15s\tremaining: 44.3s\n",
            "139:\tlearn: 0.1928441\ttotal: 7.21s\tremaining: 44.3s\n",
            "140:\tlearn: 0.1916437\ttotal: 7.26s\tremaining: 44.2s\n",
            "141:\tlearn: 0.1911543\ttotal: 7.3s\tremaining: 44.1s\n",
            "142:\tlearn: 0.1909899\ttotal: 7.36s\tremaining: 44.1s\n",
            "143:\tlearn: 0.1907903\ttotal: 7.41s\tremaining: 44.1s\n",
            "144:\tlearn: 0.1904312\ttotal: 7.47s\tremaining: 44s\n",
            "145:\tlearn: 0.1900329\ttotal: 7.52s\tremaining: 44s\n",
            "146:\tlearn: 0.1898148\ttotal: 7.56s\tremaining: 43.9s\n",
            "147:\tlearn: 0.1887653\ttotal: 7.61s\tremaining: 43.8s\n",
            "148:\tlearn: 0.1875681\ttotal: 7.67s\tremaining: 43.8s\n",
            "149:\tlearn: 0.1871858\ttotal: 7.72s\tremaining: 43.7s\n",
            "150:\tlearn: 0.1865062\ttotal: 7.76s\tremaining: 43.6s\n",
            "151:\tlearn: 0.1856417\ttotal: 7.81s\tremaining: 43.6s\n",
            "152:\tlearn: 0.1846046\ttotal: 7.87s\tremaining: 43.6s\n",
            "153:\tlearn: 0.1843748\ttotal: 7.91s\tremaining: 43.5s\n",
            "154:\tlearn: 0.1839180\ttotal: 7.96s\tremaining: 43.4s\n",
            "155:\tlearn: 0.1833691\ttotal: 8s\tremaining: 43.3s\n",
            "156:\tlearn: 0.1826689\ttotal: 8.05s\tremaining: 43.2s\n",
            "157:\tlearn: 0.1814819\ttotal: 8.11s\tremaining: 43.2s\n",
            "158:\tlearn: 0.1808694\ttotal: 8.15s\tremaining: 43.1s\n",
            "159:\tlearn: 0.1801093\ttotal: 8.2s\tremaining: 43.1s\n",
            "160:\tlearn: 0.1794337\ttotal: 8.25s\tremaining: 43s\n",
            "161:\tlearn: 0.1786625\ttotal: 8.29s\tremaining: 42.9s\n",
            "162:\tlearn: 0.1778477\ttotal: 8.35s\tremaining: 42.9s\n",
            "163:\tlearn: 0.1775438\ttotal: 8.41s\tremaining: 42.9s\n",
            "164:\tlearn: 0.1774229\ttotal: 8.46s\tremaining: 42.8s\n",
            "165:\tlearn: 0.1768912\ttotal: 8.5s\tremaining: 42.7s\n",
            "166:\tlearn: 0.1762643\ttotal: 8.55s\tremaining: 42.7s\n",
            "167:\tlearn: 0.1756471\ttotal: 8.61s\tremaining: 42.6s\n",
            "168:\tlearn: 0.1750110\ttotal: 8.66s\tremaining: 42.6s\n",
            "169:\tlearn: 0.1744433\ttotal: 8.7s\tremaining: 42.5s\n",
            "170:\tlearn: 0.1738082\ttotal: 8.75s\tremaining: 42.4s\n",
            "171:\tlearn: 0.1734227\ttotal: 8.8s\tremaining: 42.4s\n",
            "172:\tlearn: 0.1728592\ttotal: 8.85s\tremaining: 42.3s\n",
            "173:\tlearn: 0.1722849\ttotal: 8.9s\tremaining: 42.3s\n",
            "174:\tlearn: 0.1715558\ttotal: 8.95s\tremaining: 42.2s\n",
            "175:\tlearn: 0.1708929\ttotal: 9s\tremaining: 42.1s\n",
            "176:\tlearn: 0.1707750\ttotal: 9.05s\tremaining: 42.1s\n",
            "177:\tlearn: 0.1701795\ttotal: 9.1s\tremaining: 42s\n",
            "178:\tlearn: 0.1698131\ttotal: 9.15s\tremaining: 42s\n",
            "179:\tlearn: 0.1697975\ttotal: 9.2s\tremaining: 41.9s\n",
            "180:\tlearn: 0.1694882\ttotal: 9.24s\tremaining: 41.8s\n",
            "181:\tlearn: 0.1694815\ttotal: 9.28s\tremaining: 41.7s\n",
            "182:\tlearn: 0.1690217\ttotal: 9.33s\tremaining: 41.6s\n",
            "183:\tlearn: 0.1680433\ttotal: 9.37s\tremaining: 41.6s\n",
            "184:\tlearn: 0.1676274\ttotal: 9.44s\tremaining: 41.6s\n",
            "185:\tlearn: 0.1673160\ttotal: 9.49s\tremaining: 41.5s\n",
            "186:\tlearn: 0.1665314\ttotal: 9.54s\tremaining: 41.5s\n",
            "187:\tlearn: 0.1661863\ttotal: 9.58s\tremaining: 41.4s\n",
            "188:\tlearn: 0.1656858\ttotal: 9.63s\tremaining: 41.3s\n",
            "189:\tlearn: 0.1652339\ttotal: 9.68s\tremaining: 41.3s\n",
            "190:\tlearn: 0.1650463\ttotal: 9.73s\tremaining: 41.2s\n",
            "191:\tlearn: 0.1645825\ttotal: 9.78s\tremaining: 41.2s\n",
            "192:\tlearn: 0.1641290\ttotal: 9.83s\tremaining: 41.1s\n",
            "193:\tlearn: 0.1633161\ttotal: 9.88s\tremaining: 41s\n",
            "194:\tlearn: 0.1628781\ttotal: 9.92s\tremaining: 41s\n",
            "195:\tlearn: 0.1623183\ttotal: 9.98s\tremaining: 40.9s\n",
            "196:\tlearn: 0.1620365\ttotal: 10s\tremaining: 40.9s\n",
            "197:\tlearn: 0.1613489\ttotal: 10.1s\tremaining: 40.8s\n",
            "198:\tlearn: 0.1605454\ttotal: 10.1s\tremaining: 40.8s\n",
            "199:\tlearn: 0.1599235\ttotal: 10.2s\tremaining: 40.7s\n",
            "200:\tlearn: 0.1594269\ttotal: 10.2s\tremaining: 40.7s\n",
            "201:\tlearn: 0.1588861\ttotal: 10.3s\tremaining: 40.6s\n",
            "202:\tlearn: 0.1583257\ttotal: 10.3s\tremaining: 40.6s\n",
            "203:\tlearn: 0.1580157\ttotal: 10.4s\tremaining: 40.5s\n",
            "204:\tlearn: 0.1576364\ttotal: 10.4s\tremaining: 40.4s\n",
            "205:\tlearn: 0.1573429\ttotal: 10.5s\tremaining: 40.4s\n",
            "206:\tlearn: 0.1567008\ttotal: 10.5s\tremaining: 40.4s\n",
            "207:\tlearn: 0.1563892\ttotal: 10.6s\tremaining: 40.3s\n",
            "208:\tlearn: 0.1558349\ttotal: 10.6s\tremaining: 40.3s\n",
            "209:\tlearn: 0.1553602\ttotal: 10.7s\tremaining: 40.3s\n",
            "210:\tlearn: 0.1551116\ttotal: 10.8s\tremaining: 40.3s\n",
            "211:\tlearn: 0.1549385\ttotal: 10.9s\tremaining: 40.4s\n",
            "212:\tlearn: 0.1544475\ttotal: 11s\tremaining: 40.5s\n",
            "213:\tlearn: 0.1538581\ttotal: 11s\tremaining: 40.5s\n",
            "214:\tlearn: 0.1534644\ttotal: 11.1s\tremaining: 40.6s\n",
            "215:\tlearn: 0.1533696\ttotal: 11.2s\tremaining: 40.7s\n",
            "216:\tlearn: 0.1528584\ttotal: 11.3s\tremaining: 40.8s\n",
            "217:\tlearn: 0.1526280\ttotal: 11.4s\tremaining: 40.9s\n",
            "218:\tlearn: 0.1518843\ttotal: 11.5s\tremaining: 41s\n",
            "219:\tlearn: 0.1516053\ttotal: 11.6s\tremaining: 41s\n",
            "220:\tlearn: 0.1510853\ttotal: 11.7s\tremaining: 41.1s\n",
            "221:\tlearn: 0.1508503\ttotal: 11.7s\tremaining: 41.1s\n",
            "222:\tlearn: 0.1504537\ttotal: 11.8s\tremaining: 41.2s\n",
            "223:\tlearn: 0.1503061\ttotal: 11.9s\tremaining: 41.2s\n",
            "224:\tlearn: 0.1502925\ttotal: 12s\tremaining: 41.4s\n",
            "225:\tlearn: 0.1498846\ttotal: 12.1s\tremaining: 41.4s\n",
            "226:\tlearn: 0.1494037\ttotal: 12.2s\tremaining: 41.5s\n",
            "227:\tlearn: 0.1493313\ttotal: 12.3s\tremaining: 41.5s\n",
            "228:\tlearn: 0.1492056\ttotal: 12.3s\tremaining: 41.6s\n",
            "229:\tlearn: 0.1489777\ttotal: 12.4s\tremaining: 41.6s\n",
            "230:\tlearn: 0.1484551\ttotal: 12.5s\tremaining: 41.7s\n",
            "231:\tlearn: 0.1483933\ttotal: 12.6s\tremaining: 41.7s\n",
            "232:\tlearn: 0.1483167\ttotal: 12.7s\tremaining: 41.8s\n",
            "233:\tlearn: 0.1477341\ttotal: 12.8s\tremaining: 41.8s\n",
            "234:\tlearn: 0.1472474\ttotal: 12.9s\tremaining: 41.9s\n",
            "235:\tlearn: 0.1471477\ttotal: 13s\tremaining: 41.9s\n",
            "236:\tlearn: 0.1469441\ttotal: 13s\tremaining: 41.9s\n",
            "237:\tlearn: 0.1465496\ttotal: 13.1s\tremaining: 42s\n",
            "238:\tlearn: 0.1463109\ttotal: 13.2s\tremaining: 42s\n",
            "239:\tlearn: 0.1459430\ttotal: 13.3s\tremaining: 42s\n",
            "240:\tlearn: 0.1459333\ttotal: 13.4s\tremaining: 42.1s\n",
            "241:\tlearn: 0.1458324\ttotal: 13.4s\tremaining: 42.1s\n",
            "242:\tlearn: 0.1457736\ttotal: 13.5s\tremaining: 42.2s\n",
            "243:\tlearn: 0.1456172\ttotal: 13.6s\tremaining: 42.2s\n",
            "244:\tlearn: 0.1452358\ttotal: 13.7s\tremaining: 42.3s\n",
            "245:\tlearn: 0.1449069\ttotal: 13.8s\tremaining: 42.3s\n",
            "246:\tlearn: 0.1445808\ttotal: 13.9s\tremaining: 42.3s\n",
            "247:\tlearn: 0.1443222\ttotal: 14s\tremaining: 42.4s\n",
            "248:\tlearn: 0.1439834\ttotal: 14.1s\tremaining: 42.5s\n",
            "249:\tlearn: 0.1435204\ttotal: 14.1s\tremaining: 42.4s\n",
            "250:\tlearn: 0.1431014\ttotal: 14.2s\tremaining: 42.3s\n",
            "251:\tlearn: 0.1429352\ttotal: 14.2s\tremaining: 42.2s\n",
            "252:\tlearn: 0.1426562\ttotal: 14.3s\tremaining: 42.1s\n",
            "253:\tlearn: 0.1424316\ttotal: 14.3s\tremaining: 42s\n",
            "254:\tlearn: 0.1419469\ttotal: 14.4s\tremaining: 42s\n",
            "255:\tlearn: 0.1414794\ttotal: 14.4s\tremaining: 41.9s\n",
            "256:\tlearn: 0.1412313\ttotal: 14.5s\tremaining: 41.8s\n",
            "257:\tlearn: 0.1409204\ttotal: 14.5s\tremaining: 41.7s\n",
            "258:\tlearn: 0.1405246\ttotal: 14.6s\tremaining: 41.6s\n",
            "259:\tlearn: 0.1401888\ttotal: 14.6s\tremaining: 41.6s\n",
            "260:\tlearn: 0.1399747\ttotal: 14.7s\tremaining: 41.5s\n",
            "261:\tlearn: 0.1396122\ttotal: 14.7s\tremaining: 41.4s\n",
            "262:\tlearn: 0.1391634\ttotal: 14.8s\tremaining: 41.4s\n",
            "263:\tlearn: 0.1386945\ttotal: 14.8s\tremaining: 41.3s\n",
            "264:\tlearn: 0.1383392\ttotal: 14.9s\tremaining: 41.2s\n",
            "265:\tlearn: 0.1381190\ttotal: 14.9s\tremaining: 41.1s\n",
            "266:\tlearn: 0.1381110\ttotal: 14.9s\tremaining: 41s\n",
            "267:\tlearn: 0.1378271\ttotal: 15s\tremaining: 40.9s\n",
            "268:\tlearn: 0.1377225\ttotal: 15s\tremaining: 40.8s\n",
            "269:\tlearn: 0.1375135\ttotal: 15.1s\tremaining: 40.8s\n",
            "270:\tlearn: 0.1373708\ttotal: 15.1s\tremaining: 40.7s\n",
            "271:\tlearn: 0.1370962\ttotal: 15.2s\tremaining: 40.6s\n",
            "272:\tlearn: 0.1369950\ttotal: 15.2s\tremaining: 40.5s\n",
            "273:\tlearn: 0.1369813\ttotal: 15.3s\tremaining: 40.5s\n",
            "274:\tlearn: 0.1364530\ttotal: 15.3s\tremaining: 40.4s\n",
            "275:\tlearn: 0.1359220\ttotal: 15.4s\tremaining: 40.3s\n",
            "276:\tlearn: 0.1357238\ttotal: 15.4s\tremaining: 40.2s\n",
            "277:\tlearn: 0.1354607\ttotal: 15.5s\tremaining: 40.2s\n",
            "278:\tlearn: 0.1353843\ttotal: 15.5s\tremaining: 40.1s\n",
            "279:\tlearn: 0.1352593\ttotal: 15.6s\tremaining: 40s\n",
            "280:\tlearn: 0.1350465\ttotal: 15.6s\tremaining: 40s\n",
            "281:\tlearn: 0.1348582\ttotal: 15.7s\tremaining: 39.9s\n",
            "282:\tlearn: 0.1346547\ttotal: 15.7s\tremaining: 39.8s\n",
            "283:\tlearn: 0.1344354\ttotal: 15.8s\tremaining: 39.8s\n",
            "284:\tlearn: 0.1343433\ttotal: 15.8s\tremaining: 39.7s\n",
            "285:\tlearn: 0.1342833\ttotal: 15.9s\tremaining: 39.6s\n",
            "286:\tlearn: 0.1340073\ttotal: 15.9s\tremaining: 39.5s\n",
            "287:\tlearn: 0.1336999\ttotal: 16s\tremaining: 39.5s\n",
            "288:\tlearn: 0.1334851\ttotal: 16s\tremaining: 39.4s\n",
            "289:\tlearn: 0.1332432\ttotal: 16.1s\tremaining: 39.3s\n",
            "290:\tlearn: 0.1331873\ttotal: 16.1s\tremaining: 39.3s\n",
            "291:\tlearn: 0.1329895\ttotal: 16.2s\tremaining: 39.2s\n",
            "292:\tlearn: 0.1329342\ttotal: 16.2s\tremaining: 39.1s\n",
            "293:\tlearn: 0.1328054\ttotal: 16.3s\tremaining: 39s\n",
            "294:\tlearn: 0.1326865\ttotal: 16.3s\tremaining: 39s\n",
            "295:\tlearn: 0.1325039\ttotal: 16.4s\tremaining: 38.9s\n",
            "296:\tlearn: 0.1325023\ttotal: 16.4s\tremaining: 38.7s\n",
            "297:\tlearn: 0.1324359\ttotal: 16.4s\tremaining: 38.7s\n",
            "298:\tlearn: 0.1322320\ttotal: 16.5s\tremaining: 38.6s\n",
            "299:\tlearn: 0.1320323\ttotal: 16.5s\tremaining: 38.5s\n",
            "300:\tlearn: 0.1318199\ttotal: 16.6s\tremaining: 38.5s\n",
            "301:\tlearn: 0.1316704\ttotal: 16.6s\tremaining: 38.4s\n",
            "302:\tlearn: 0.1313803\ttotal: 16.7s\tremaining: 38.4s\n",
            "303:\tlearn: 0.1312738\ttotal: 16.7s\tremaining: 38.3s\n",
            "304:\tlearn: 0.1312383\ttotal: 16.8s\tremaining: 38.2s\n",
            "305:\tlearn: 0.1312159\ttotal: 16.8s\tremaining: 38.2s\n",
            "306:\tlearn: 0.1308492\ttotal: 16.9s\tremaining: 38.1s\n",
            "307:\tlearn: 0.1306802\ttotal: 16.9s\tremaining: 38s\n",
            "308:\tlearn: 0.1304356\ttotal: 17s\tremaining: 38s\n",
            "309:\tlearn: 0.1302149\ttotal: 17s\tremaining: 37.9s\n",
            "310:\tlearn: 0.1301618\ttotal: 17s\tremaining: 37.7s\n",
            "311:\tlearn: 0.1299618\ttotal: 17.1s\tremaining: 37.7s\n",
            "312:\tlearn: 0.1298947\ttotal: 17.1s\tremaining: 37.6s\n",
            "313:\tlearn: 0.1297695\ttotal: 17.2s\tremaining: 37.5s\n",
            "314:\tlearn: 0.1296709\ttotal: 17.2s\tremaining: 37.4s\n",
            "315:\tlearn: 0.1295595\ttotal: 17.3s\tremaining: 37.4s\n",
            "316:\tlearn: 0.1293529\ttotal: 17.3s\tremaining: 37.3s\n",
            "317:\tlearn: 0.1290026\ttotal: 17.4s\tremaining: 37.3s\n",
            "318:\tlearn: 0.1288585\ttotal: 17.4s\tremaining: 37.2s\n",
            "319:\tlearn: 0.1287033\ttotal: 17.5s\tremaining: 37.1s\n",
            "320:\tlearn: 0.1285294\ttotal: 17.5s\tremaining: 37.1s\n",
            "321:\tlearn: 0.1282744\ttotal: 17.6s\tremaining: 37s\n",
            "322:\tlearn: 0.1281013\ttotal: 17.6s\tremaining: 37s\n",
            "323:\tlearn: 0.1280122\ttotal: 17.7s\tremaining: 36.9s\n",
            "324:\tlearn: 0.1278294\ttotal: 17.7s\tremaining: 36.8s\n",
            "325:\tlearn: 0.1276541\ttotal: 17.8s\tremaining: 36.8s\n",
            "326:\tlearn: 0.1276130\ttotal: 17.8s\tremaining: 36.7s\n",
            "327:\tlearn: 0.1274959\ttotal: 17.9s\tremaining: 36.7s\n",
            "328:\tlearn: 0.1274818\ttotal: 17.9s\tremaining: 36.5s\n",
            "329:\tlearn: 0.1273494\ttotal: 17.9s\tremaining: 36.4s\n",
            "330:\tlearn: 0.1272722\ttotal: 18s\tremaining: 36.4s\n",
            "331:\tlearn: 0.1269742\ttotal: 18s\tremaining: 36.3s\n",
            "332:\tlearn: 0.1268000\ttotal: 18.1s\tremaining: 36.2s\n",
            "333:\tlearn: 0.1266050\ttotal: 18.1s\tremaining: 36.2s\n",
            "334:\tlearn: 0.1265580\ttotal: 18.2s\tremaining: 36.1s\n",
            "335:\tlearn: 0.1263603\ttotal: 18.2s\tremaining: 36s\n",
            "336:\tlearn: 0.1262092\ttotal: 18.3s\tremaining: 36s\n",
            "337:\tlearn: 0.1261214\ttotal: 18.3s\tremaining: 35.9s\n",
            "338:\tlearn: 0.1259484\ttotal: 18.4s\tremaining: 35.9s\n",
            "339:\tlearn: 0.1257670\ttotal: 18.4s\tremaining: 35.8s\n",
            "340:\tlearn: 0.1256121\ttotal: 18.5s\tremaining: 35.7s\n",
            "341:\tlearn: 0.1252928\ttotal: 18.5s\tremaining: 35.7s\n",
            "342:\tlearn: 0.1252376\ttotal: 18.6s\tremaining: 35.6s\n",
            "343:\tlearn: 0.1250279\ttotal: 18.6s\tremaining: 35.5s\n",
            "344:\tlearn: 0.1248157\ttotal: 18.7s\tremaining: 35.5s\n",
            "345:\tlearn: 0.1246651\ttotal: 18.7s\tremaining: 35.4s\n",
            "346:\tlearn: 0.1244958\ttotal: 18.8s\tremaining: 35.4s\n",
            "347:\tlearn: 0.1244251\ttotal: 18.8s\tremaining: 35.3s\n",
            "348:\tlearn: 0.1242093\ttotal: 18.9s\tremaining: 35.2s\n",
            "349:\tlearn: 0.1241340\ttotal: 18.9s\tremaining: 35.2s\n",
            "350:\tlearn: 0.1240315\ttotal: 19s\tremaining: 35.1s\n",
            "351:\tlearn: 0.1239673\ttotal: 19.1s\tremaining: 35.1s\n",
            "352:\tlearn: 0.1237901\ttotal: 19.1s\tremaining: 35s\n",
            "353:\tlearn: 0.1235941\ttotal: 19.1s\tremaining: 34.9s\n",
            "354:\tlearn: 0.1234730\ttotal: 19.2s\tremaining: 34.9s\n",
            "355:\tlearn: 0.1231525\ttotal: 19.3s\tremaining: 34.8s\n",
            "356:\tlearn: 0.1230383\ttotal: 19.3s\tremaining: 34.8s\n",
            "357:\tlearn: 0.1229678\ttotal: 19.3s\tremaining: 34.7s\n",
            "358:\tlearn: 0.1227934\ttotal: 19.4s\tremaining: 34.6s\n",
            "359:\tlearn: 0.1227761\ttotal: 19.4s\tremaining: 34.5s\n",
            "360:\tlearn: 0.1227103\ttotal: 19.5s\tremaining: 34.5s\n",
            "361:\tlearn: 0.1227075\ttotal: 19.5s\tremaining: 34.4s\n",
            "362:\tlearn: 0.1225878\ttotal: 19.6s\tremaining: 34.3s\n",
            "363:\tlearn: 0.1224856\ttotal: 19.6s\tremaining: 34.3s\n",
            "364:\tlearn: 0.1222795\ttotal: 19.7s\tremaining: 34.2s\n",
            "365:\tlearn: 0.1220345\ttotal: 19.7s\tremaining: 34.2s\n",
            "366:\tlearn: 0.1218891\ttotal: 19.8s\tremaining: 34.1s\n",
            "367:\tlearn: 0.1216165\ttotal: 19.8s\tremaining: 34.1s\n",
            "368:\tlearn: 0.1213984\ttotal: 19.9s\tremaining: 34s\n",
            "369:\tlearn: 0.1211236\ttotal: 19.9s\tremaining: 33.9s\n",
            "370:\tlearn: 0.1209988\ttotal: 20s\tremaining: 33.9s\n",
            "371:\tlearn: 0.1208608\ttotal: 20s\tremaining: 33.8s\n",
            "372:\tlearn: 0.1207140\ttotal: 20.1s\tremaining: 33.7s\n",
            "373:\tlearn: 0.1206149\ttotal: 20.1s\tremaining: 33.7s\n",
            "374:\tlearn: 0.1204761\ttotal: 20.2s\tremaining: 33.6s\n",
            "375:\tlearn: 0.1203186\ttotal: 20.2s\tremaining: 33.6s\n",
            "376:\tlearn: 0.1200640\ttotal: 20.3s\tremaining: 33.5s\n",
            "377:\tlearn: 0.1199583\ttotal: 20.3s\tremaining: 33.4s\n",
            "378:\tlearn: 0.1198540\ttotal: 20.4s\tremaining: 33.4s\n",
            "379:\tlearn: 0.1197479\ttotal: 20.4s\tremaining: 33.3s\n",
            "380:\tlearn: 0.1195817\ttotal: 20.5s\tremaining: 33.3s\n",
            "381:\tlearn: 0.1194535\ttotal: 20.5s\tremaining: 33.2s\n",
            "382:\tlearn: 0.1192940\ttotal: 20.6s\tremaining: 33.1s\n",
            "383:\tlearn: 0.1192897\ttotal: 20.6s\tremaining: 33.1s\n",
            "384:\tlearn: 0.1192281\ttotal: 20.7s\tremaining: 33s\n",
            "385:\tlearn: 0.1190937\ttotal: 20.7s\tremaining: 33s\n",
            "386:\tlearn: 0.1187981\ttotal: 20.8s\tremaining: 32.9s\n",
            "387:\tlearn: 0.1187535\ttotal: 20.8s\tremaining: 32.8s\n",
            "388:\tlearn: 0.1185743\ttotal: 20.9s\tremaining: 32.8s\n",
            "389:\tlearn: 0.1184199\ttotal: 20.9s\tremaining: 32.7s\n",
            "390:\tlearn: 0.1183439\ttotal: 21s\tremaining: 32.7s\n",
            "391:\tlearn: 0.1182008\ttotal: 21s\tremaining: 32.6s\n",
            "392:\tlearn: 0.1181158\ttotal: 21.1s\tremaining: 32.5s\n",
            "393:\tlearn: 0.1179908\ttotal: 21.1s\tremaining: 32.5s\n",
            "394:\tlearn: 0.1179865\ttotal: 21.2s\tremaining: 32.4s\n",
            "395:\tlearn: 0.1178008\ttotal: 21.2s\tremaining: 32.4s\n",
            "396:\tlearn: 0.1176508\ttotal: 21.3s\tremaining: 32.3s\n",
            "397:\tlearn: 0.1174830\ttotal: 21.3s\tremaining: 32.2s\n",
            "398:\tlearn: 0.1173428\ttotal: 21.4s\tremaining: 32.2s\n",
            "399:\tlearn: 0.1172105\ttotal: 21.4s\tremaining: 32.1s\n",
            "400:\tlearn: 0.1170119\ttotal: 21.5s\tremaining: 32.1s\n",
            "401:\tlearn: 0.1169151\ttotal: 21.5s\tremaining: 32s\n",
            "402:\tlearn: 0.1168758\ttotal: 21.6s\tremaining: 31.9s\n",
            "403:\tlearn: 0.1168742\ttotal: 21.6s\tremaining: 31.9s\n",
            "404:\tlearn: 0.1167599\ttotal: 21.7s\tremaining: 31.8s\n",
            "405:\tlearn: 0.1166740\ttotal: 21.7s\tremaining: 31.8s\n",
            "406:\tlearn: 0.1165564\ttotal: 21.8s\tremaining: 31.7s\n",
            "407:\tlearn: 0.1164875\ttotal: 21.8s\tremaining: 31.7s\n",
            "408:\tlearn: 0.1164035\ttotal: 21.9s\tremaining: 31.6s\n",
            "409:\tlearn: 0.1162600\ttotal: 21.9s\tremaining: 31.5s\n",
            "410:\tlearn: 0.1161011\ttotal: 22s\tremaining: 31.5s\n",
            "411:\tlearn: 0.1159599\ttotal: 22s\tremaining: 31.4s\n",
            "412:\tlearn: 0.1158791\ttotal: 22.1s\tremaining: 31.4s\n",
            "413:\tlearn: 0.1158537\ttotal: 22.1s\tremaining: 31.3s\n",
            "414:\tlearn: 0.1158035\ttotal: 22.2s\tremaining: 31.3s\n",
            "415:\tlearn: 0.1157635\ttotal: 22.2s\tremaining: 31.2s\n",
            "416:\tlearn: 0.1157405\ttotal: 22.3s\tremaining: 31.1s\n",
            "417:\tlearn: 0.1157063\ttotal: 22.3s\tremaining: 31.1s\n",
            "418:\tlearn: 0.1156086\ttotal: 22.4s\tremaining: 31s\n",
            "419:\tlearn: 0.1154961\ttotal: 22.4s\tremaining: 31s\n",
            "420:\tlearn: 0.1153519\ttotal: 22.5s\tremaining: 30.9s\n",
            "421:\tlearn: 0.1151069\ttotal: 22.5s\tremaining: 30.8s\n",
            "422:\tlearn: 0.1149329\ttotal: 22.6s\tremaining: 30.8s\n",
            "423:\tlearn: 0.1147800\ttotal: 22.6s\tremaining: 30.7s\n",
            "424:\tlearn: 0.1147103\ttotal: 22.7s\tremaining: 30.7s\n",
            "425:\tlearn: 0.1146289\ttotal: 22.7s\tremaining: 30.6s\n",
            "426:\tlearn: 0.1145337\ttotal: 22.8s\tremaining: 30.6s\n",
            "427:\tlearn: 0.1143474\ttotal: 22.8s\tremaining: 30.5s\n",
            "428:\tlearn: 0.1141806\ttotal: 22.9s\tremaining: 30.5s\n",
            "429:\tlearn: 0.1140343\ttotal: 22.9s\tremaining: 30.4s\n",
            "430:\tlearn: 0.1139372\ttotal: 23s\tremaining: 30.3s\n",
            "431:\tlearn: 0.1138120\ttotal: 23s\tremaining: 30.3s\n",
            "432:\tlearn: 0.1137364\ttotal: 23.1s\tremaining: 30.2s\n",
            "433:\tlearn: 0.1136453\ttotal: 23.1s\tremaining: 30.2s\n",
            "434:\tlearn: 0.1135669\ttotal: 23.2s\tremaining: 30.1s\n",
            "435:\tlearn: 0.1134395\ttotal: 23.2s\tremaining: 30.1s\n",
            "436:\tlearn: 0.1133382\ttotal: 23.3s\tremaining: 30s\n",
            "437:\tlearn: 0.1132468\ttotal: 23.3s\tremaining: 29.9s\n",
            "438:\tlearn: 0.1131875\ttotal: 23.4s\tremaining: 29.9s\n",
            "439:\tlearn: 0.1130866\ttotal: 23.4s\tremaining: 29.8s\n",
            "440:\tlearn: 0.1129082\ttotal: 23.5s\tremaining: 29.8s\n",
            "441:\tlearn: 0.1128448\ttotal: 23.5s\tremaining: 29.7s\n",
            "442:\tlearn: 0.1127185\ttotal: 23.6s\tremaining: 29.7s\n",
            "443:\tlearn: 0.1126997\ttotal: 23.6s\tremaining: 29.6s\n",
            "444:\tlearn: 0.1126989\ttotal: 23.7s\tremaining: 29.5s\n",
            "445:\tlearn: 0.1126762\ttotal: 23.7s\tremaining: 29.5s\n",
            "446:\tlearn: 0.1126759\ttotal: 23.8s\tremaining: 29.4s\n",
            "447:\tlearn: 0.1126252\ttotal: 23.8s\tremaining: 29.4s\n",
            "448:\tlearn: 0.1125036\ttotal: 23.9s\tremaining: 29.3s\n",
            "449:\tlearn: 0.1124237\ttotal: 23.9s\tremaining: 29.3s\n",
            "450:\tlearn: 0.1123861\ttotal: 24s\tremaining: 29.2s\n",
            "451:\tlearn: 0.1122637\ttotal: 24.1s\tremaining: 29.2s\n",
            "452:\tlearn: 0.1121375\ttotal: 24.2s\tremaining: 29.2s\n",
            "453:\tlearn: 0.1120450\ttotal: 24.2s\tremaining: 29.2s\n",
            "454:\tlearn: 0.1119385\ttotal: 24.4s\tremaining: 29.2s\n",
            "455:\tlearn: 0.1118856\ttotal: 24.4s\tremaining: 29.1s\n",
            "456:\tlearn: 0.1117913\ttotal: 24.5s\tremaining: 29.1s\n",
            "457:\tlearn: 0.1116294\ttotal: 24.6s\tremaining: 29.1s\n",
            "458:\tlearn: 0.1115668\ttotal: 24.7s\tremaining: 29.1s\n",
            "459:\tlearn: 0.1115407\ttotal: 24.8s\tremaining: 29.1s\n",
            "460:\tlearn: 0.1115397\ttotal: 24.9s\tremaining: 29.1s\n",
            "461:\tlearn: 0.1114647\ttotal: 25s\tremaining: 29.1s\n",
            "462:\tlearn: 0.1113826\ttotal: 25s\tremaining: 29s\n",
            "463:\tlearn: 0.1112872\ttotal: 25.1s\tremaining: 29s\n",
            "464:\tlearn: 0.1111918\ttotal: 25.2s\tremaining: 29s\n",
            "465:\tlearn: 0.1110968\ttotal: 25.3s\tremaining: 29s\n",
            "466:\tlearn: 0.1110351\ttotal: 25.4s\tremaining: 29s\n",
            "467:\tlearn: 0.1109412\ttotal: 25.5s\tremaining: 28.9s\n",
            "468:\tlearn: 0.1108362\ttotal: 25.5s\tremaining: 28.9s\n",
            "469:\tlearn: 0.1107924\ttotal: 25.6s\tremaining: 28.9s\n",
            "470:\tlearn: 0.1107485\ttotal: 25.7s\tremaining: 28.9s\n",
            "471:\tlearn: 0.1106795\ttotal: 25.8s\tremaining: 28.9s\n",
            "472:\tlearn: 0.1106792\ttotal: 25.9s\tremaining: 28.9s\n",
            "473:\tlearn: 0.1105914\ttotal: 26s\tremaining: 28.8s\n",
            "474:\tlearn: 0.1104983\ttotal: 26.1s\tremaining: 28.8s\n",
            "475:\tlearn: 0.1104163\ttotal: 26.1s\tremaining: 28.8s\n",
            "476:\tlearn: 0.1103390\ttotal: 26.2s\tremaining: 28.8s\n",
            "477:\tlearn: 0.1103170\ttotal: 26.3s\tremaining: 28.7s\n",
            "478:\tlearn: 0.1102491\ttotal: 26.4s\tremaining: 28.7s\n",
            "479:\tlearn: 0.1102125\ttotal: 26.5s\tremaining: 28.7s\n",
            "480:\tlearn: 0.1101919\ttotal: 26.6s\tremaining: 28.7s\n",
            "481:\tlearn: 0.1101137\ttotal: 26.6s\tremaining: 28.6s\n",
            "482:\tlearn: 0.1100416\ttotal: 26.7s\tremaining: 28.6s\n",
            "483:\tlearn: 0.1099495\ttotal: 26.8s\tremaining: 28.6s\n",
            "484:\tlearn: 0.1099227\ttotal: 26.9s\tremaining: 28.6s\n",
            "485:\tlearn: 0.1098647\ttotal: 27s\tremaining: 28.6s\n",
            "486:\tlearn: 0.1097867\ttotal: 27.1s\tremaining: 28.5s\n",
            "487:\tlearn: 0.1096671\ttotal: 27.2s\tremaining: 28.5s\n",
            "488:\tlearn: 0.1096452\ttotal: 27.3s\tremaining: 28.5s\n",
            "489:\tlearn: 0.1095603\ttotal: 27.4s\tremaining: 28.5s\n",
            "490:\tlearn: 0.1095440\ttotal: 27.5s\tremaining: 28.5s\n",
            "491:\tlearn: 0.1094623\ttotal: 27.6s\tremaining: 28.5s\n",
            "492:\tlearn: 0.1093687\ttotal: 27.6s\tremaining: 28.4s\n",
            "493:\tlearn: 0.1092879\ttotal: 27.7s\tremaining: 28.4s\n",
            "494:\tlearn: 0.1092281\ttotal: 27.7s\tremaining: 28.3s\n",
            "495:\tlearn: 0.1091532\ttotal: 27.8s\tremaining: 28.2s\n",
            "496:\tlearn: 0.1091409\ttotal: 27.8s\tremaining: 28.2s\n",
            "497:\tlearn: 0.1091410\ttotal: 27.9s\tremaining: 28.1s\n",
            "498:\tlearn: 0.1089748\ttotal: 27.9s\tremaining: 28.1s\n",
            "499:\tlearn: 0.1088765\ttotal: 28s\tremaining: 28s\n",
            "500:\tlearn: 0.1088534\ttotal: 28s\tremaining: 27.9s\n",
            "501:\tlearn: 0.1088161\ttotal: 28.1s\tremaining: 27.9s\n",
            "502:\tlearn: 0.1087805\ttotal: 28.1s\tremaining: 27.8s\n",
            "503:\tlearn: 0.1086701\ttotal: 28.2s\tremaining: 27.7s\n",
            "504:\tlearn: 0.1086072\ttotal: 28.2s\tremaining: 27.7s\n",
            "505:\tlearn: 0.1085211\ttotal: 28.3s\tremaining: 27.6s\n",
            "506:\tlearn: 0.1084907\ttotal: 28.3s\tremaining: 27.6s\n",
            "507:\tlearn: 0.1084234\ttotal: 28.4s\tremaining: 27.5s\n",
            "508:\tlearn: 0.1083861\ttotal: 28.4s\tremaining: 27.4s\n",
            "509:\tlearn: 0.1083233\ttotal: 28.5s\tremaining: 27.4s\n",
            "510:\tlearn: 0.1082375\ttotal: 28.5s\tremaining: 27.3s\n",
            "511:\tlearn: 0.1081825\ttotal: 28.6s\tremaining: 27.2s\n",
            "512:\tlearn: 0.1081479\ttotal: 28.6s\tremaining: 27.2s\n",
            "513:\tlearn: 0.1080523\ttotal: 28.7s\tremaining: 27.1s\n",
            "514:\tlearn: 0.1079709\ttotal: 28.7s\tremaining: 27.1s\n",
            "515:\tlearn: 0.1078693\ttotal: 28.8s\tremaining: 27s\n",
            "516:\tlearn: 0.1078113\ttotal: 28.8s\tremaining: 26.9s\n",
            "517:\tlearn: 0.1077720\ttotal: 28.9s\tremaining: 26.9s\n",
            "518:\tlearn: 0.1076731\ttotal: 28.9s\tremaining: 26.8s\n",
            "519:\tlearn: 0.1076083\ttotal: 29s\tremaining: 26.8s\n",
            "520:\tlearn: 0.1074847\ttotal: 29.1s\tremaining: 26.7s\n",
            "521:\tlearn: 0.1073961\ttotal: 29.1s\tremaining: 26.7s\n",
            "522:\tlearn: 0.1073354\ttotal: 29.2s\tremaining: 26.6s\n",
            "523:\tlearn: 0.1073091\ttotal: 29.2s\tremaining: 26.5s\n",
            "524:\tlearn: 0.1072726\ttotal: 29.3s\tremaining: 26.5s\n",
            "525:\tlearn: 0.1072693\ttotal: 29.3s\tremaining: 26.4s\n",
            "526:\tlearn: 0.1071818\ttotal: 29.4s\tremaining: 26.3s\n",
            "527:\tlearn: 0.1071703\ttotal: 29.4s\tremaining: 26.3s\n",
            "528:\tlearn: 0.1071703\ttotal: 29.5s\tremaining: 26.2s\n",
            "529:\tlearn: 0.1070811\ttotal: 29.5s\tremaining: 26.2s\n",
            "530:\tlearn: 0.1069920\ttotal: 29.6s\tremaining: 26.1s\n",
            "531:\tlearn: 0.1069864\ttotal: 29.6s\tremaining: 26s\n",
            "532:\tlearn: 0.1069692\ttotal: 29.7s\tremaining: 26s\n",
            "533:\tlearn: 0.1069235\ttotal: 29.7s\tremaining: 25.9s\n",
            "534:\tlearn: 0.1068486\ttotal: 29.8s\tremaining: 25.9s\n",
            "535:\tlearn: 0.1067713\ttotal: 29.8s\tremaining: 25.8s\n",
            "536:\tlearn: 0.1067129\ttotal: 29.9s\tremaining: 25.7s\n",
            "537:\tlearn: 0.1066355\ttotal: 29.9s\tremaining: 25.7s\n",
            "538:\tlearn: 0.1065633\ttotal: 30s\tremaining: 25.6s\n",
            "539:\tlearn: 0.1064857\ttotal: 30s\tremaining: 25.6s\n",
            "540:\tlearn: 0.1064199\ttotal: 30.1s\tremaining: 25.5s\n",
            "541:\tlearn: 0.1063412\ttotal: 30.1s\tremaining: 25.5s\n",
            "542:\tlearn: 0.1062839\ttotal: 30.2s\tremaining: 25.4s\n",
            "543:\tlearn: 0.1062810\ttotal: 30.2s\tremaining: 25.3s\n",
            "544:\tlearn: 0.1062082\ttotal: 30.3s\tremaining: 25.3s\n",
            "545:\tlearn: 0.1061438\ttotal: 30.3s\tremaining: 25.2s\n",
            "546:\tlearn: 0.1061063\ttotal: 30.4s\tremaining: 25.2s\n",
            "547:\tlearn: 0.1061061\ttotal: 30.4s\tremaining: 25.1s\n",
            "548:\tlearn: 0.1060297\ttotal: 30.5s\tremaining: 25s\n",
            "549:\tlearn: 0.1059171\ttotal: 30.5s\tremaining: 25s\n",
            "550:\tlearn: 0.1058381\ttotal: 30.6s\tremaining: 24.9s\n",
            "551:\tlearn: 0.1058151\ttotal: 30.6s\tremaining: 24.8s\n",
            "552:\tlearn: 0.1057478\ttotal: 30.7s\tremaining: 24.8s\n",
            "553:\tlearn: 0.1056463\ttotal: 30.7s\tremaining: 24.7s\n",
            "554:\tlearn: 0.1055720\ttotal: 30.8s\tremaining: 24.7s\n",
            "555:\tlearn: 0.1055336\ttotal: 30.8s\tremaining: 24.6s\n",
            "556:\tlearn: 0.1054986\ttotal: 30.9s\tremaining: 24.6s\n",
            "557:\tlearn: 0.1054877\ttotal: 30.9s\tremaining: 24.5s\n",
            "558:\tlearn: 0.1054645\ttotal: 31s\tremaining: 24.4s\n",
            "559:\tlearn: 0.1053950\ttotal: 31s\tremaining: 24.4s\n",
            "560:\tlearn: 0.1053511\ttotal: 31.1s\tremaining: 24.3s\n",
            "561:\tlearn: 0.1052936\ttotal: 31.1s\tremaining: 24.3s\n",
            "562:\tlearn: 0.1052601\ttotal: 31.2s\tremaining: 24.2s\n",
            "563:\tlearn: 0.1052156\ttotal: 31.2s\tremaining: 24.1s\n",
            "564:\tlearn: 0.1051819\ttotal: 31.3s\tremaining: 24.1s\n",
            "565:\tlearn: 0.1051271\ttotal: 31.3s\tremaining: 24s\n",
            "566:\tlearn: 0.1050615\ttotal: 31.4s\tremaining: 24s\n",
            "567:\tlearn: 0.1049923\ttotal: 31.4s\tremaining: 23.9s\n",
            "568:\tlearn: 0.1049238\ttotal: 31.5s\tremaining: 23.8s\n",
            "569:\tlearn: 0.1048278\ttotal: 31.5s\tremaining: 23.8s\n",
            "570:\tlearn: 0.1048197\ttotal: 31.6s\tremaining: 23.7s\n",
            "571:\tlearn: 0.1047757\ttotal: 31.6s\tremaining: 23.7s\n",
            "572:\tlearn: 0.1047323\ttotal: 31.7s\tremaining: 23.6s\n",
            "573:\tlearn: 0.1046210\ttotal: 31.7s\tremaining: 23.5s\n",
            "574:\tlearn: 0.1045451\ttotal: 31.8s\tremaining: 23.5s\n",
            "575:\tlearn: 0.1045404\ttotal: 31.8s\tremaining: 23.4s\n",
            "576:\tlearn: 0.1044760\ttotal: 31.9s\tremaining: 23.4s\n",
            "577:\tlearn: 0.1044498\ttotal: 31.9s\tremaining: 23.3s\n",
            "578:\tlearn: 0.1044176\ttotal: 32s\tremaining: 23.2s\n",
            "579:\tlearn: 0.1043218\ttotal: 32s\tremaining: 23.2s\n",
            "580:\tlearn: 0.1042917\ttotal: 32.1s\tremaining: 23.1s\n",
            "581:\tlearn: 0.1042509\ttotal: 32.1s\tremaining: 23.1s\n",
            "582:\tlearn: 0.1042020\ttotal: 32.2s\tremaining: 23s\n",
            "583:\tlearn: 0.1041422\ttotal: 32.2s\tremaining: 22.9s\n",
            "584:\tlearn: 0.1041105\ttotal: 32.3s\tremaining: 22.9s\n",
            "585:\tlearn: 0.1040775\ttotal: 32.3s\tremaining: 22.8s\n",
            "586:\tlearn: 0.1040679\ttotal: 32.4s\tremaining: 22.8s\n",
            "587:\tlearn: 0.1040298\ttotal: 32.4s\tremaining: 22.7s\n",
            "588:\tlearn: 0.1039782\ttotal: 32.5s\tremaining: 22.7s\n",
            "589:\tlearn: 0.1039682\ttotal: 32.5s\tremaining: 22.6s\n",
            "590:\tlearn: 0.1039572\ttotal: 32.6s\tremaining: 22.5s\n",
            "591:\tlearn: 0.1039525\ttotal: 32.6s\tremaining: 22.5s\n",
            "592:\tlearn: 0.1038933\ttotal: 32.7s\tremaining: 22.4s\n",
            "593:\tlearn: 0.1038318\ttotal: 32.7s\tremaining: 22.4s\n",
            "594:\tlearn: 0.1038120\ttotal: 32.8s\tremaining: 22.3s\n",
            "595:\tlearn: 0.1037463\ttotal: 32.8s\tremaining: 22.2s\n",
            "596:\tlearn: 0.1036823\ttotal: 32.9s\tremaining: 22.2s\n",
            "597:\tlearn: 0.1036464\ttotal: 32.9s\tremaining: 22.1s\n",
            "598:\tlearn: 0.1036006\ttotal: 33s\tremaining: 22.1s\n",
            "599:\tlearn: 0.1035901\ttotal: 33s\tremaining: 22s\n",
            "600:\tlearn: 0.1035304\ttotal: 33s\tremaining: 21.9s\n",
            "601:\tlearn: 0.1034939\ttotal: 33.1s\tremaining: 21.9s\n",
            "602:\tlearn: 0.1034344\ttotal: 33.2s\tremaining: 21.8s\n",
            "603:\tlearn: 0.1034085\ttotal: 33.2s\tremaining: 21.8s\n",
            "604:\tlearn: 0.1033561\ttotal: 33.3s\tremaining: 21.7s\n",
            "605:\tlearn: 0.1033315\ttotal: 33.3s\tremaining: 21.7s\n",
            "606:\tlearn: 0.1033081\ttotal: 33.4s\tremaining: 21.6s\n",
            "607:\tlearn: 0.1032477\ttotal: 33.4s\tremaining: 21.5s\n",
            "608:\tlearn: 0.1032191\ttotal: 33.5s\tremaining: 21.5s\n",
            "609:\tlearn: 0.1032191\ttotal: 33.5s\tremaining: 21.4s\n",
            "610:\tlearn: 0.1031117\ttotal: 33.5s\tremaining: 21.3s\n",
            "611:\tlearn: 0.1030103\ttotal: 33.6s\tremaining: 21.3s\n",
            "612:\tlearn: 0.1029335\ttotal: 33.6s\tremaining: 21.2s\n",
            "613:\tlearn: 0.1028816\ttotal: 33.7s\tremaining: 21.2s\n",
            "614:\tlearn: 0.1028073\ttotal: 33.7s\tremaining: 21.1s\n",
            "615:\tlearn: 0.1027637\ttotal: 33.8s\tremaining: 21s\n",
            "616:\tlearn: 0.1027399\ttotal: 33.8s\tremaining: 21s\n",
            "617:\tlearn: 0.1027069\ttotal: 33.9s\tremaining: 20.9s\n",
            "618:\tlearn: 0.1026366\ttotal: 33.9s\tremaining: 20.9s\n",
            "619:\tlearn: 0.1026240\ttotal: 34s\tremaining: 20.8s\n",
            "620:\tlearn: 0.1025902\ttotal: 34s\tremaining: 20.8s\n",
            "621:\tlearn: 0.1025900\ttotal: 34.1s\tremaining: 20.7s\n",
            "622:\tlearn: 0.1025522\ttotal: 34.1s\tremaining: 20.6s\n",
            "623:\tlearn: 0.1024780\ttotal: 34.2s\tremaining: 20.6s\n",
            "624:\tlearn: 0.1024118\ttotal: 34.2s\tremaining: 20.5s\n",
            "625:\tlearn: 0.1023261\ttotal: 34.3s\tremaining: 20.5s\n",
            "626:\tlearn: 0.1023078\ttotal: 34.3s\tremaining: 20.4s\n",
            "627:\tlearn: 0.1022835\ttotal: 34.4s\tremaining: 20.4s\n",
            "628:\tlearn: 0.1022392\ttotal: 34.4s\tremaining: 20.3s\n",
            "629:\tlearn: 0.1021972\ttotal: 34.5s\tremaining: 20.2s\n",
            "630:\tlearn: 0.1021458\ttotal: 34.5s\tremaining: 20.2s\n",
            "631:\tlearn: 0.1021456\ttotal: 34.6s\tremaining: 20.1s\n",
            "632:\tlearn: 0.1021326\ttotal: 34.6s\tremaining: 20.1s\n",
            "633:\tlearn: 0.1020822\ttotal: 34.7s\tremaining: 20s\n",
            "634:\tlearn: 0.1020820\ttotal: 34.7s\tremaining: 19.9s\n",
            "635:\tlearn: 0.1020821\ttotal: 34.7s\tremaining: 19.9s\n",
            "636:\tlearn: 0.1020310\ttotal: 34.7s\tremaining: 19.8s\n",
            "637:\tlearn: 0.1020308\ttotal: 34.8s\tremaining: 19.7s\n",
            "638:\tlearn: 0.1020306\ttotal: 34.8s\tremaining: 19.7s\n",
            "639:\tlearn: 0.1019617\ttotal: 34.9s\tremaining: 19.6s\n",
            "640:\tlearn: 0.1019045\ttotal: 34.9s\tremaining: 19.6s\n",
            "641:\tlearn: 0.1018534\ttotal: 35s\tremaining: 19.5s\n",
            "642:\tlearn: 0.1018154\ttotal: 35s\tremaining: 19.4s\n",
            "643:\tlearn: 0.1018154\ttotal: 35.1s\tremaining: 19.4s\n",
            "644:\tlearn: 0.1018072\ttotal: 35.1s\tremaining: 19.3s\n",
            "645:\tlearn: 0.1017882\ttotal: 35.2s\tremaining: 19.3s\n",
            "646:\tlearn: 0.1017582\ttotal: 35.2s\tremaining: 19.2s\n",
            "647:\tlearn: 0.1016790\ttotal: 35.3s\tremaining: 19.2s\n",
            "648:\tlearn: 0.1016486\ttotal: 35.3s\tremaining: 19.1s\n",
            "649:\tlearn: 0.1016051\ttotal: 35.4s\tremaining: 19.1s\n",
            "650:\tlearn: 0.1015732\ttotal: 35.4s\tremaining: 19s\n",
            "651:\tlearn: 0.1015172\ttotal: 35.5s\tremaining: 18.9s\n",
            "652:\tlearn: 0.1015173\ttotal: 35.5s\tremaining: 18.9s\n",
            "653:\tlearn: 0.1014444\ttotal: 35.6s\tremaining: 18.8s\n",
            "654:\tlearn: 0.1013990\ttotal: 35.6s\tremaining: 18.8s\n",
            "655:\tlearn: 0.1013506\ttotal: 35.7s\tremaining: 18.7s\n",
            "656:\tlearn: 0.1013149\ttotal: 35.7s\tremaining: 18.6s\n",
            "657:\tlearn: 0.1012647\ttotal: 35.8s\tremaining: 18.6s\n",
            "658:\tlearn: 0.1012526\ttotal: 35.8s\tremaining: 18.5s\n",
            "659:\tlearn: 0.1012335\ttotal: 35.9s\tremaining: 18.5s\n",
            "660:\tlearn: 0.1012014\ttotal: 35.9s\tremaining: 18.4s\n",
            "661:\tlearn: 0.1011735\ttotal: 36s\tremaining: 18.4s\n",
            "662:\tlearn: 0.1011381\ttotal: 36s\tremaining: 18.3s\n",
            "663:\tlearn: 0.1010944\ttotal: 36.1s\tremaining: 18.2s\n",
            "664:\tlearn: 0.1010638\ttotal: 36.1s\tremaining: 18.2s\n",
            "665:\tlearn: 0.1010587\ttotal: 36.2s\tremaining: 18.1s\n",
            "666:\tlearn: 0.1010456\ttotal: 36.2s\tremaining: 18.1s\n",
            "667:\tlearn: 0.1010209\ttotal: 36.3s\tremaining: 18s\n",
            "668:\tlearn: 0.1010152\ttotal: 36.3s\tremaining: 18s\n",
            "669:\tlearn: 0.1009403\ttotal: 36.4s\tremaining: 17.9s\n",
            "670:\tlearn: 0.1008645\ttotal: 36.4s\tremaining: 17.9s\n",
            "671:\tlearn: 0.1007803\ttotal: 36.5s\tremaining: 17.8s\n",
            "672:\tlearn: 0.1007138\ttotal: 36.5s\tremaining: 17.7s\n",
            "673:\tlearn: 0.1006448\ttotal: 36.6s\tremaining: 17.7s\n",
            "674:\tlearn: 0.1006216\ttotal: 36.6s\tremaining: 17.6s\n",
            "675:\tlearn: 0.1005661\ttotal: 36.7s\tremaining: 17.6s\n",
            "676:\tlearn: 0.1005487\ttotal: 36.7s\tremaining: 17.5s\n",
            "677:\tlearn: 0.1005254\ttotal: 36.8s\tremaining: 17.5s\n",
            "678:\tlearn: 0.1005156\ttotal: 36.8s\tremaining: 17.4s\n",
            "679:\tlearn: 0.1004829\ttotal: 36.9s\tremaining: 17.3s\n",
            "680:\tlearn: 0.1004441\ttotal: 36.9s\tremaining: 17.3s\n",
            "681:\tlearn: 0.1003990\ttotal: 37s\tremaining: 17.2s\n",
            "682:\tlearn: 0.1003555\ttotal: 37s\tremaining: 17.2s\n",
            "683:\tlearn: 0.1002989\ttotal: 37.1s\tremaining: 17.1s\n",
            "684:\tlearn: 0.1002700\ttotal: 37.1s\tremaining: 17.1s\n",
            "685:\tlearn: 0.1002626\ttotal: 37.2s\tremaining: 17s\n",
            "686:\tlearn: 0.1002626\ttotal: 37.2s\tremaining: 17s\n",
            "687:\tlearn: 0.1002380\ttotal: 37.3s\tremaining: 16.9s\n",
            "688:\tlearn: 0.1002251\ttotal: 37.3s\tremaining: 16.8s\n",
            "689:\tlearn: 0.1001367\ttotal: 37.4s\tremaining: 16.8s\n",
            "690:\tlearn: 0.1000967\ttotal: 37.4s\tremaining: 16.7s\n",
            "691:\tlearn: 0.1000939\ttotal: 37.4s\tremaining: 16.7s\n",
            "692:\tlearn: 0.1000222\ttotal: 37.5s\tremaining: 16.6s\n",
            "693:\tlearn: 0.0999843\ttotal: 37.6s\tremaining: 16.6s\n",
            "694:\tlearn: 0.0999443\ttotal: 37.7s\tremaining: 16.5s\n",
            "695:\tlearn: 0.0999079\ttotal: 37.7s\tremaining: 16.5s\n",
            "696:\tlearn: 0.0998875\ttotal: 37.8s\tremaining: 16.4s\n",
            "697:\tlearn: 0.0998797\ttotal: 37.9s\tremaining: 16.4s\n",
            "698:\tlearn: 0.0998429\ttotal: 38s\tremaining: 16.4s\n",
            "699:\tlearn: 0.0998429\ttotal: 38.1s\tremaining: 16.3s\n",
            "700:\tlearn: 0.0997675\ttotal: 38.2s\tremaining: 16.3s\n",
            "701:\tlearn: 0.0997260\ttotal: 38.3s\tremaining: 16.2s\n",
            "702:\tlearn: 0.0997080\ttotal: 38.3s\tremaining: 16.2s\n",
            "703:\tlearn: 0.0997001\ttotal: 38.4s\tremaining: 16.2s\n",
            "704:\tlearn: 0.0996534\ttotal: 38.5s\tremaining: 16.1s\n",
            "705:\tlearn: 0.0995959\ttotal: 38.6s\tremaining: 16.1s\n",
            "706:\tlearn: 0.0995647\ttotal: 38.7s\tremaining: 16s\n",
            "707:\tlearn: 0.0995215\ttotal: 38.8s\tremaining: 16s\n",
            "708:\tlearn: 0.0995216\ttotal: 38.8s\tremaining: 15.9s\n",
            "709:\tlearn: 0.0994954\ttotal: 38.9s\tremaining: 15.9s\n",
            "710:\tlearn: 0.0994733\ttotal: 39s\tremaining: 15.8s\n",
            "711:\tlearn: 0.0994440\ttotal: 39s\tremaining: 15.8s\n",
            "712:\tlearn: 0.0994033\ttotal: 39.1s\tremaining: 15.8s\n",
            "713:\tlearn: 0.0994034\ttotal: 39.2s\tremaining: 15.7s\n",
            "714:\tlearn: 0.0993745\ttotal: 39.3s\tremaining: 15.7s\n",
            "715:\tlearn: 0.0993346\ttotal: 39.4s\tremaining: 15.6s\n",
            "716:\tlearn: 0.0993050\ttotal: 39.5s\tremaining: 15.6s\n",
            "717:\tlearn: 0.0992968\ttotal: 39.6s\tremaining: 15.5s\n",
            "718:\tlearn: 0.0992452\ttotal: 39.7s\tremaining: 15.5s\n",
            "719:\tlearn: 0.0991886\ttotal: 39.8s\tremaining: 15.5s\n",
            "720:\tlearn: 0.0991592\ttotal: 39.9s\tremaining: 15.4s\n",
            "721:\tlearn: 0.0990926\ttotal: 39.9s\tremaining: 15.4s\n",
            "722:\tlearn: 0.0990635\ttotal: 40s\tremaining: 15.3s\n",
            "723:\tlearn: 0.0990048\ttotal: 40.1s\tremaining: 15.3s\n",
            "724:\tlearn: 0.0989809\ttotal: 40.2s\tremaining: 15.2s\n",
            "725:\tlearn: 0.0989598\ttotal: 40.3s\tremaining: 15.2s\n",
            "726:\tlearn: 0.0989525\ttotal: 40.4s\tremaining: 15.2s\n",
            "727:\tlearn: 0.0988926\ttotal: 40.5s\tremaining: 15.1s\n",
            "728:\tlearn: 0.0988375\ttotal: 40.6s\tremaining: 15.1s\n",
            "729:\tlearn: 0.0988054\ttotal: 40.6s\tremaining: 15s\n",
            "730:\tlearn: 0.0987826\ttotal: 40.7s\tremaining: 15s\n",
            "731:\tlearn: 0.0987709\ttotal: 40.8s\tremaining: 15s\n",
            "732:\tlearn: 0.0987636\ttotal: 40.9s\tremaining: 14.9s\n",
            "733:\tlearn: 0.0987579\ttotal: 41s\tremaining: 14.9s\n",
            "734:\tlearn: 0.0987259\ttotal: 41.1s\tremaining: 14.8s\n",
            "735:\tlearn: 0.0987066\ttotal: 41.2s\tremaining: 14.8s\n",
            "736:\tlearn: 0.0986846\ttotal: 41.2s\tremaining: 14.7s\n",
            "737:\tlearn: 0.0986393\ttotal: 41.3s\tremaining: 14.7s\n",
            "738:\tlearn: 0.0986074\ttotal: 41.3s\tremaining: 14.6s\n",
            "739:\tlearn: 0.0985962\ttotal: 41.4s\tremaining: 14.5s\n",
            "740:\tlearn: 0.0985484\ttotal: 41.4s\tremaining: 14.5s\n",
            "741:\tlearn: 0.0985311\ttotal: 41.5s\tremaining: 14.4s\n",
            "742:\tlearn: 0.0985311\ttotal: 41.5s\tremaining: 14.4s\n",
            "743:\tlearn: 0.0985312\ttotal: 41.5s\tremaining: 14.3s\n",
            "744:\tlearn: 0.0985221\ttotal: 41.6s\tremaining: 14.2s\n",
            "745:\tlearn: 0.0985221\ttotal: 41.6s\tremaining: 14.2s\n",
            "746:\tlearn: 0.0984988\ttotal: 41.7s\tremaining: 14.1s\n",
            "747:\tlearn: 0.0984881\ttotal: 41.7s\tremaining: 14.1s\n",
            "748:\tlearn: 0.0984681\ttotal: 41.8s\tremaining: 14s\n",
            "749:\tlearn: 0.0984459\ttotal: 41.8s\tremaining: 13.9s\n",
            "750:\tlearn: 0.0984223\ttotal: 41.9s\tremaining: 13.9s\n",
            "751:\tlearn: 0.0983800\ttotal: 41.9s\tremaining: 13.8s\n",
            "752:\tlearn: 0.0983441\ttotal: 42s\tremaining: 13.8s\n",
            "753:\tlearn: 0.0983183\ttotal: 42s\tremaining: 13.7s\n",
            "754:\tlearn: 0.0983183\ttotal: 42.1s\tremaining: 13.6s\n",
            "755:\tlearn: 0.0983183\ttotal: 42.1s\tremaining: 13.6s\n",
            "756:\tlearn: 0.0983182\ttotal: 42.2s\tremaining: 13.5s\n",
            "757:\tlearn: 0.0983151\ttotal: 42.2s\tremaining: 13.5s\n",
            "758:\tlearn: 0.0982932\ttotal: 42.3s\tremaining: 13.4s\n",
            "759:\tlearn: 0.0982930\ttotal: 42.3s\tremaining: 13.4s\n",
            "760:\tlearn: 0.0982813\ttotal: 42.4s\tremaining: 13.3s\n",
            "761:\tlearn: 0.0982619\ttotal: 42.4s\tremaining: 13.2s\n",
            "762:\tlearn: 0.0982297\ttotal: 42.5s\tremaining: 13.2s\n",
            "763:\tlearn: 0.0981918\ttotal: 42.5s\tremaining: 13.1s\n",
            "764:\tlearn: 0.0981470\ttotal: 42.6s\tremaining: 13.1s\n",
            "765:\tlearn: 0.0981103\ttotal: 42.6s\tremaining: 13s\n",
            "766:\tlearn: 0.0980749\ttotal: 42.7s\tremaining: 13s\n",
            "767:\tlearn: 0.0980720\ttotal: 42.7s\tremaining: 12.9s\n",
            "768:\tlearn: 0.0980412\ttotal: 42.8s\tremaining: 12.9s\n",
            "769:\tlearn: 0.0980215\ttotal: 42.8s\tremaining: 12.8s\n",
            "770:\tlearn: 0.0979469\ttotal: 42.9s\tremaining: 12.7s\n",
            "771:\tlearn: 0.0978989\ttotal: 42.9s\tremaining: 12.7s\n",
            "772:\tlearn: 0.0978666\ttotal: 43s\tremaining: 12.6s\n",
            "773:\tlearn: 0.0978359\ttotal: 43s\tremaining: 12.6s\n",
            "774:\tlearn: 0.0977927\ttotal: 43.1s\tremaining: 12.5s\n",
            "775:\tlearn: 0.0977568\ttotal: 43.1s\tremaining: 12.4s\n",
            "776:\tlearn: 0.0977332\ttotal: 43.2s\tremaining: 12.4s\n",
            "777:\tlearn: 0.0977325\ttotal: 43.2s\tremaining: 12.3s\n",
            "778:\tlearn: 0.0976947\ttotal: 43.3s\tremaining: 12.3s\n",
            "779:\tlearn: 0.0976573\ttotal: 43.3s\tremaining: 12.2s\n",
            "780:\tlearn: 0.0976553\ttotal: 43.4s\tremaining: 12.2s\n",
            "781:\tlearn: 0.0976481\ttotal: 43.4s\tremaining: 12.1s\n",
            "782:\tlearn: 0.0976374\ttotal: 43.5s\tremaining: 12s\n",
            "783:\tlearn: 0.0976328\ttotal: 43.5s\tremaining: 12s\n",
            "784:\tlearn: 0.0976012\ttotal: 43.6s\tremaining: 11.9s\n",
            "785:\tlearn: 0.0976012\ttotal: 43.6s\tremaining: 11.9s\n",
            "786:\tlearn: 0.0975648\ttotal: 43.7s\tremaining: 11.8s\n",
            "787:\tlearn: 0.0975197\ttotal: 43.7s\tremaining: 11.8s\n",
            "788:\tlearn: 0.0974851\ttotal: 43.8s\tremaining: 11.7s\n",
            "789:\tlearn: 0.0974595\ttotal: 43.8s\tremaining: 11.6s\n",
            "790:\tlearn: 0.0974103\ttotal: 43.9s\tremaining: 11.6s\n",
            "791:\tlearn: 0.0973863\ttotal: 43.9s\tremaining: 11.5s\n",
            "792:\tlearn: 0.0973481\ttotal: 44s\tremaining: 11.5s\n",
            "793:\tlearn: 0.0973210\ttotal: 44s\tremaining: 11.4s\n",
            "794:\tlearn: 0.0972780\ttotal: 44.1s\tremaining: 11.4s\n",
            "795:\tlearn: 0.0972361\ttotal: 44.1s\tremaining: 11.3s\n",
            "796:\tlearn: 0.0972006\ttotal: 44.2s\tremaining: 11.3s\n",
            "797:\tlearn: 0.0971677\ttotal: 44.2s\tremaining: 11.2s\n",
            "798:\tlearn: 0.0971368\ttotal: 44.3s\tremaining: 11.1s\n",
            "799:\tlearn: 0.0971203\ttotal: 44.3s\tremaining: 11.1s\n",
            "800:\tlearn: 0.0970901\ttotal: 44.4s\tremaining: 11s\n",
            "801:\tlearn: 0.0970653\ttotal: 44.4s\tremaining: 11s\n",
            "802:\tlearn: 0.0970011\ttotal: 44.5s\tremaining: 10.9s\n",
            "803:\tlearn: 0.0969605\ttotal: 44.5s\tremaining: 10.9s\n",
            "804:\tlearn: 0.0969369\ttotal: 44.6s\tremaining: 10.8s\n",
            "805:\tlearn: 0.0969368\ttotal: 44.6s\tremaining: 10.7s\n",
            "806:\tlearn: 0.0969192\ttotal: 44.7s\tremaining: 10.7s\n",
            "807:\tlearn: 0.0969023\ttotal: 44.7s\tremaining: 10.6s\n",
            "808:\tlearn: 0.0969022\ttotal: 44.8s\tremaining: 10.6s\n",
            "809:\tlearn: 0.0968861\ttotal: 44.8s\tremaining: 10.5s\n",
            "810:\tlearn: 0.0968775\ttotal: 44.9s\tremaining: 10.5s\n",
            "811:\tlearn: 0.0968641\ttotal: 44.9s\tremaining: 10.4s\n",
            "812:\tlearn: 0.0968370\ttotal: 45s\tremaining: 10.3s\n",
            "813:\tlearn: 0.0967969\ttotal: 45s\tremaining: 10.3s\n",
            "814:\tlearn: 0.0967708\ttotal: 45.1s\tremaining: 10.2s\n",
            "815:\tlearn: 0.0967373\ttotal: 45.1s\tremaining: 10.2s\n",
            "816:\tlearn: 0.0967094\ttotal: 45.2s\tremaining: 10.1s\n",
            "817:\tlearn: 0.0966874\ttotal: 45.2s\tremaining: 10.1s\n",
            "818:\tlearn: 0.0966545\ttotal: 45.3s\tremaining: 10s\n",
            "819:\tlearn: 0.0966129\ttotal: 45.3s\tremaining: 9.95s\n",
            "820:\tlearn: 0.0965855\ttotal: 45.4s\tremaining: 9.89s\n",
            "821:\tlearn: 0.0965565\ttotal: 45.4s\tremaining: 9.83s\n",
            "822:\tlearn: 0.0965214\ttotal: 45.5s\tremaining: 9.78s\n",
            "823:\tlearn: 0.0964736\ttotal: 45.5s\tremaining: 9.72s\n",
            "824:\tlearn: 0.0964529\ttotal: 45.6s\tremaining: 9.66s\n",
            "825:\tlearn: 0.0964346\ttotal: 45.6s\tremaining: 9.61s\n",
            "826:\tlearn: 0.0964063\ttotal: 45.7s\tremaining: 9.55s\n",
            "827:\tlearn: 0.0964008\ttotal: 45.7s\tremaining: 9.5s\n",
            "828:\tlearn: 0.0963862\ttotal: 45.8s\tremaining: 9.44s\n",
            "829:\tlearn: 0.0963763\ttotal: 45.8s\tremaining: 9.38s\n",
            "830:\tlearn: 0.0963460\ttotal: 45.9s\tremaining: 9.33s\n",
            "831:\tlearn: 0.0963168\ttotal: 45.9s\tremaining: 9.27s\n",
            "832:\tlearn: 0.0963115\ttotal: 46s\tremaining: 9.21s\n",
            "833:\tlearn: 0.0962818\ttotal: 46s\tremaining: 9.16s\n",
            "834:\tlearn: 0.0962754\ttotal: 46.1s\tremaining: 9.1s\n",
            "835:\tlearn: 0.0962615\ttotal: 46.1s\tremaining: 9.04s\n",
            "836:\tlearn: 0.0962548\ttotal: 46.2s\tremaining: 8.99s\n",
            "837:\tlearn: 0.0962349\ttotal: 46.2s\tremaining: 8.93s\n",
            "838:\tlearn: 0.0962243\ttotal: 46.3s\tremaining: 8.88s\n",
            "839:\tlearn: 0.0962115\ttotal: 46.3s\tremaining: 8.82s\n",
            "840:\tlearn: 0.0962115\ttotal: 46.4s\tremaining: 8.76s\n",
            "841:\tlearn: 0.0962114\ttotal: 46.4s\tremaining: 8.71s\n",
            "842:\tlearn: 0.0961884\ttotal: 46.5s\tremaining: 8.65s\n",
            "843:\tlearn: 0.0961884\ttotal: 46.5s\tremaining: 8.6s\n",
            "844:\tlearn: 0.0961729\ttotal: 46.6s\tremaining: 8.54s\n",
            "845:\tlearn: 0.0961712\ttotal: 46.6s\tremaining: 8.48s\n",
            "846:\tlearn: 0.0961610\ttotal: 46.7s\tremaining: 8.43s\n",
            "847:\tlearn: 0.0961382\ttotal: 46.7s\tremaining: 8.37s\n",
            "848:\tlearn: 0.0961072\ttotal: 46.8s\tremaining: 8.32s\n",
            "849:\tlearn: 0.0960939\ttotal: 46.8s\tremaining: 8.26s\n",
            "850:\tlearn: 0.0960939\ttotal: 46.9s\tremaining: 8.2s\n",
            "851:\tlearn: 0.0960765\ttotal: 46.9s\tremaining: 8.15s\n",
            "852:\tlearn: 0.0960604\ttotal: 47s\tremaining: 8.09s\n",
            "853:\tlearn: 0.0960356\ttotal: 47s\tremaining: 8.04s\n",
            "854:\tlearn: 0.0960114\ttotal: 47.1s\tremaining: 7.98s\n",
            "855:\tlearn: 0.0960034\ttotal: 47.1s\tremaining: 7.92s\n",
            "856:\tlearn: 0.0959843\ttotal: 47.2s\tremaining: 7.87s\n",
            "857:\tlearn: 0.0959378\ttotal: 47.2s\tremaining: 7.81s\n",
            "858:\tlearn: 0.0959096\ttotal: 47.3s\tremaining: 7.76s\n",
            "859:\tlearn: 0.0958608\ttotal: 47.3s\tremaining: 7.7s\n",
            "860:\tlearn: 0.0958299\ttotal: 47.3s\tremaining: 7.64s\n",
            "861:\tlearn: 0.0958264\ttotal: 47.4s\tremaining: 7.59s\n",
            "862:\tlearn: 0.0958065\ttotal: 47.5s\tremaining: 7.53s\n",
            "863:\tlearn: 0.0957807\ttotal: 47.5s\tremaining: 7.48s\n",
            "864:\tlearn: 0.0957662\ttotal: 47.6s\tremaining: 7.42s\n",
            "865:\tlearn: 0.0957541\ttotal: 47.6s\tremaining: 7.37s\n",
            "866:\tlearn: 0.0957541\ttotal: 47.7s\tremaining: 7.31s\n",
            "867:\tlearn: 0.0957339\ttotal: 47.7s\tremaining: 7.25s\n",
            "868:\tlearn: 0.0956986\ttotal: 47.7s\tremaining: 7.2s\n",
            "869:\tlearn: 0.0956851\ttotal: 47.8s\tremaining: 7.14s\n",
            "870:\tlearn: 0.0956607\ttotal: 47.8s\tremaining: 7.08s\n",
            "871:\tlearn: 0.0956402\ttotal: 47.9s\tremaining: 7.03s\n",
            "872:\tlearn: 0.0956401\ttotal: 47.9s\tremaining: 6.97s\n",
            "873:\tlearn: 0.0956214\ttotal: 48s\tremaining: 6.92s\n",
            "874:\tlearn: 0.0956178\ttotal: 48s\tremaining: 6.86s\n",
            "875:\tlearn: 0.0956054\ttotal: 48.1s\tremaining: 6.81s\n",
            "876:\tlearn: 0.0955880\ttotal: 48.1s\tremaining: 6.75s\n",
            "877:\tlearn: 0.0955878\ttotal: 48.2s\tremaining: 6.69s\n",
            "878:\tlearn: 0.0955870\ttotal: 48.2s\tremaining: 6.64s\n",
            "879:\tlearn: 0.0955604\ttotal: 48.3s\tremaining: 6.58s\n",
            "880:\tlearn: 0.0955313\ttotal: 48.3s\tremaining: 6.53s\n",
            "881:\tlearn: 0.0954994\ttotal: 48.4s\tremaining: 6.47s\n",
            "882:\tlearn: 0.0954642\ttotal: 48.4s\tremaining: 6.42s\n",
            "883:\tlearn: 0.0954642\ttotal: 48.4s\tremaining: 6.35s\n",
            "884:\tlearn: 0.0954475\ttotal: 48.5s\tremaining: 6.3s\n",
            "885:\tlearn: 0.0954275\ttotal: 48.5s\tremaining: 6.24s\n",
            "886:\tlearn: 0.0954062\ttotal: 48.6s\tremaining: 6.19s\n",
            "887:\tlearn: 0.0953775\ttotal: 48.6s\tremaining: 6.13s\n",
            "888:\tlearn: 0.0953386\ttotal: 48.7s\tremaining: 6.08s\n",
            "889:\tlearn: 0.0953243\ttotal: 48.7s\tremaining: 6.02s\n",
            "890:\tlearn: 0.0952978\ttotal: 48.8s\tremaining: 5.97s\n",
            "891:\tlearn: 0.0952762\ttotal: 48.8s\tremaining: 5.91s\n",
            "892:\tlearn: 0.0952763\ttotal: 48.9s\tremaining: 5.86s\n",
            "893:\tlearn: 0.0952718\ttotal: 48.9s\tremaining: 5.8s\n",
            "894:\tlearn: 0.0952619\ttotal: 49s\tremaining: 5.75s\n",
            "895:\tlearn: 0.0952321\ttotal: 49s\tremaining: 5.69s\n",
            "896:\tlearn: 0.0952033\ttotal: 49.1s\tremaining: 5.64s\n",
            "897:\tlearn: 0.0951790\ttotal: 49.1s\tremaining: 5.58s\n",
            "898:\tlearn: 0.0951650\ttotal: 49.2s\tremaining: 5.53s\n",
            "899:\tlearn: 0.0951649\ttotal: 49.2s\tremaining: 5.47s\n",
            "900:\tlearn: 0.0951643\ttotal: 49.3s\tremaining: 5.41s\n",
            "901:\tlearn: 0.0951583\ttotal: 49.3s\tremaining: 5.36s\n",
            "902:\tlearn: 0.0951436\ttotal: 49.4s\tremaining: 5.3s\n",
            "903:\tlearn: 0.0951311\ttotal: 49.4s\tremaining: 5.25s\n",
            "904:\tlearn: 0.0951246\ttotal: 49.5s\tremaining: 5.19s\n",
            "905:\tlearn: 0.0951007\ttotal: 49.5s\tremaining: 5.14s\n",
            "906:\tlearn: 0.0950876\ttotal: 49.6s\tremaining: 5.08s\n",
            "907:\tlearn: 0.0950796\ttotal: 49.6s\tremaining: 5.03s\n",
            "908:\tlearn: 0.0950795\ttotal: 49.7s\tremaining: 4.97s\n",
            "909:\tlearn: 0.0950795\ttotal: 49.7s\tremaining: 4.92s\n",
            "910:\tlearn: 0.0950503\ttotal: 49.8s\tremaining: 4.86s\n",
            "911:\tlearn: 0.0950502\ttotal: 49.9s\tremaining: 4.81s\n",
            "912:\tlearn: 0.0950417\ttotal: 49.9s\tremaining: 4.75s\n",
            "913:\tlearn: 0.0950213\ttotal: 49.9s\tremaining: 4.7s\n",
            "914:\tlearn: 0.0950045\ttotal: 50s\tremaining: 4.64s\n",
            "915:\tlearn: 0.0949943\ttotal: 50s\tremaining: 4.59s\n",
            "916:\tlearn: 0.0949844\ttotal: 50.1s\tremaining: 4.53s\n",
            "917:\tlearn: 0.0949843\ttotal: 50.1s\tremaining: 4.48s\n",
            "918:\tlearn: 0.0949656\ttotal: 50.2s\tremaining: 4.42s\n",
            "919:\tlearn: 0.0949419\ttotal: 50.2s\tremaining: 4.37s\n",
            "920:\tlearn: 0.0949323\ttotal: 50.3s\tremaining: 4.31s\n",
            "921:\tlearn: 0.0949188\ttotal: 50.3s\tremaining: 4.26s\n",
            "922:\tlearn: 0.0949124\ttotal: 50.4s\tremaining: 4.2s\n",
            "923:\tlearn: 0.0948811\ttotal: 50.4s\tremaining: 4.14s\n",
            "924:\tlearn: 0.0948729\ttotal: 50.4s\tremaining: 4.09s\n",
            "925:\tlearn: 0.0948694\ttotal: 50.5s\tremaining: 4.04s\n",
            "926:\tlearn: 0.0948576\ttotal: 50.5s\tremaining: 3.98s\n",
            "927:\tlearn: 0.0948488\ttotal: 50.6s\tremaining: 3.93s\n",
            "928:\tlearn: 0.0948484\ttotal: 50.7s\tremaining: 3.87s\n",
            "929:\tlearn: 0.0948280\ttotal: 50.7s\tremaining: 3.82s\n",
            "930:\tlearn: 0.0948145\ttotal: 50.8s\tremaining: 3.76s\n",
            "931:\tlearn: 0.0947860\ttotal: 50.8s\tremaining: 3.71s\n",
            "932:\tlearn: 0.0947629\ttotal: 50.9s\tremaining: 3.65s\n",
            "933:\tlearn: 0.0947629\ttotal: 50.9s\tremaining: 3.6s\n",
            "934:\tlearn: 0.0947629\ttotal: 51s\tremaining: 3.54s\n",
            "935:\tlearn: 0.0947432\ttotal: 51s\tremaining: 3.49s\n",
            "936:\tlearn: 0.0947404\ttotal: 51s\tremaining: 3.43s\n",
            "937:\tlearn: 0.0947183\ttotal: 51.1s\tremaining: 3.38s\n",
            "938:\tlearn: 0.0946931\ttotal: 51.2s\tremaining: 3.32s\n",
            "939:\tlearn: 0.0946752\ttotal: 51.3s\tremaining: 3.27s\n",
            "940:\tlearn: 0.0946650\ttotal: 51.3s\tremaining: 3.22s\n",
            "941:\tlearn: 0.0946515\ttotal: 51.4s\tremaining: 3.17s\n",
            "942:\tlearn: 0.0946276\ttotal: 51.5s\tremaining: 3.11s\n",
            "943:\tlearn: 0.0946130\ttotal: 51.6s\tremaining: 3.06s\n",
            "944:\tlearn: 0.0946079\ttotal: 51.6s\tremaining: 3s\n",
            "945:\tlearn: 0.0945890\ttotal: 51.7s\tremaining: 2.95s\n",
            "946:\tlearn: 0.0945644\ttotal: 51.8s\tremaining: 2.9s\n",
            "947:\tlearn: 0.0945496\ttotal: 51.9s\tremaining: 2.85s\n",
            "948:\tlearn: 0.0945493\ttotal: 52s\tremaining: 2.79s\n",
            "949:\tlearn: 0.0945393\ttotal: 52.1s\tremaining: 2.74s\n",
            "950:\tlearn: 0.0945392\ttotal: 52.1s\tremaining: 2.68s\n",
            "951:\tlearn: 0.0945392\ttotal: 52.2s\tremaining: 2.63s\n",
            "952:\tlearn: 0.0945180\ttotal: 52.3s\tremaining: 2.58s\n",
            "953:\tlearn: 0.0945066\ttotal: 52.4s\tremaining: 2.52s\n",
            "954:\tlearn: 0.0944732\ttotal: 52.4s\tremaining: 2.47s\n",
            "955:\tlearn: 0.0944683\ttotal: 52.5s\tremaining: 2.42s\n",
            "956:\tlearn: 0.0944637\ttotal: 52.6s\tremaining: 2.36s\n",
            "957:\tlearn: 0.0944501\ttotal: 52.7s\tremaining: 2.31s\n",
            "958:\tlearn: 0.0944500\ttotal: 52.7s\tremaining: 2.25s\n",
            "959:\tlearn: 0.0944334\ttotal: 52.8s\tremaining: 2.2s\n",
            "960:\tlearn: 0.0944138\ttotal: 52.9s\tremaining: 2.15s\n",
            "961:\tlearn: 0.0944139\ttotal: 53s\tremaining: 2.09s\n",
            "962:\tlearn: 0.0944070\ttotal: 53.1s\tremaining: 2.04s\n",
            "963:\tlearn: 0.0944062\ttotal: 53.1s\tremaining: 1.98s\n",
            "964:\tlearn: 0.0943946\ttotal: 53.2s\tremaining: 1.93s\n",
            "965:\tlearn: 0.0943779\ttotal: 53.3s\tremaining: 1.88s\n",
            "966:\tlearn: 0.0943590\ttotal: 53.4s\tremaining: 1.82s\n",
            "967:\tlearn: 0.0943444\ttotal: 53.5s\tremaining: 1.77s\n",
            "968:\tlearn: 0.0943317\ttotal: 53.6s\tremaining: 1.71s\n",
            "969:\tlearn: 0.0943164\ttotal: 53.6s\tremaining: 1.66s\n",
            "970:\tlearn: 0.0943007\ttotal: 53.7s\tremaining: 1.6s\n",
            "971:\tlearn: 0.0942862\ttotal: 53.8s\tremaining: 1.55s\n",
            "972:\tlearn: 0.0942656\ttotal: 53.9s\tremaining: 1.5s\n",
            "973:\tlearn: 0.0942511\ttotal: 54s\tremaining: 1.44s\n",
            "974:\tlearn: 0.0942336\ttotal: 54.1s\tremaining: 1.39s\n",
            "975:\tlearn: 0.0942335\ttotal: 54.1s\tremaining: 1.33s\n",
            "976:\tlearn: 0.0942141\ttotal: 54.2s\tremaining: 1.27s\n",
            "977:\tlearn: 0.0942088\ttotal: 54.2s\tremaining: 1.22s\n",
            "978:\tlearn: 0.0941909\ttotal: 54.3s\tremaining: 1.17s\n",
            "979:\tlearn: 0.0941907\ttotal: 54.4s\tremaining: 1.11s\n",
            "980:\tlearn: 0.0941719\ttotal: 54.5s\tremaining: 1.05s\n",
            "981:\tlearn: 0.0941522\ttotal: 54.6s\tremaining: 1s\n",
            "982:\tlearn: 0.0941490\ttotal: 54.6s\tremaining: 944ms\n",
            "983:\tlearn: 0.0941490\ttotal: 54.7s\tremaining: 889ms\n",
            "984:\tlearn: 0.0941397\ttotal: 54.7s\tremaining: 833ms\n",
            "985:\tlearn: 0.0941208\ttotal: 54.8s\tremaining: 778ms\n",
            "986:\tlearn: 0.0941105\ttotal: 54.8s\tremaining: 722ms\n",
            "987:\tlearn: 0.0941056\ttotal: 54.9s\tremaining: 666ms\n",
            "988:\tlearn: 0.0940859\ttotal: 54.9s\tremaining: 611ms\n",
            "989:\tlearn: 0.0940859\ttotal: 55s\tremaining: 555ms\n",
            "990:\tlearn: 0.0940804\ttotal: 55s\tremaining: 500ms\n",
            "991:\tlearn: 0.0940557\ttotal: 55.1s\tremaining: 444ms\n",
            "992:\tlearn: 0.0940294\ttotal: 55.1s\tremaining: 389ms\n",
            "993:\tlearn: 0.0940125\ttotal: 55.2s\tremaining: 333ms\n",
            "994:\tlearn: 0.0939948\ttotal: 55.2s\tremaining: 277ms\n",
            "995:\tlearn: 0.0939814\ttotal: 55.3s\tremaining: 222ms\n",
            "996:\tlearn: 0.0939754\ttotal: 55.3s\tremaining: 166ms\n",
            "997:\tlearn: 0.0939554\ttotal: 55.4s\tremaining: 111ms\n",
            "998:\tlearn: 0.0939469\ttotal: 55.4s\tremaining: 55.5ms\n",
            "999:\tlearn: 0.0939376\ttotal: 55.5s\tremaining: 0us\n",
            "0:\tlearn: 0.6430865\ttotal: 6.56ms\tremaining: 1.31s\n",
            "1:\tlearn: 0.6074386\ttotal: 12.7ms\tremaining: 1.25s\n",
            "2:\tlearn: 0.5743759\ttotal: 18.8ms\tremaining: 1.24s\n",
            "3:\tlearn: 0.5488548\ttotal: 25.1ms\tremaining: 1.23s\n",
            "4:\tlearn: 0.5265414\ttotal: 31.5ms\tremaining: 1.23s\n",
            "5:\tlearn: 0.5110811\ttotal: 38ms\tremaining: 1.23s\n",
            "6:\tlearn: 0.4976217\ttotal: 44.3ms\tremaining: 1.22s\n",
            "7:\tlearn: 0.4863579\ttotal: 52.2ms\tremaining: 1.25s\n",
            "8:\tlearn: 0.4791278\ttotal: 61.3ms\tremaining: 1.3s\n",
            "9:\tlearn: 0.4723430\ttotal: 67.5ms\tremaining: 1.28s\n",
            "10:\tlearn: 0.4669539\ttotal: 75.3ms\tremaining: 1.29s\n",
            "11:\tlearn: 0.4621507\ttotal: 81.7ms\tremaining: 1.28s\n",
            "12:\tlearn: 0.4579538\ttotal: 91.6ms\tremaining: 1.32s\n",
            "13:\tlearn: 0.4548092\ttotal: 97.8ms\tremaining: 1.3s\n",
            "14:\tlearn: 0.4517378\ttotal: 104ms\tremaining: 1.28s\n",
            "15:\tlearn: 0.4494240\ttotal: 110ms\tremaining: 1.26s\n",
            "16:\tlearn: 0.4467254\ttotal: 116ms\tremaining: 1.25s\n",
            "17:\tlearn: 0.4446603\ttotal: 122ms\tremaining: 1.24s\n",
            "18:\tlearn: 0.4422837\ttotal: 129ms\tremaining: 1.23s\n",
            "19:\tlearn: 0.4402812\ttotal: 143ms\tremaining: 1.29s\n",
            "20:\tlearn: 0.4388986\ttotal: 155ms\tremaining: 1.32s\n",
            "21:\tlearn: 0.4370813\ttotal: 161ms\tremaining: 1.3s\n",
            "22:\tlearn: 0.4355719\ttotal: 167ms\tremaining: 1.29s\n",
            "23:\tlearn: 0.4348146\ttotal: 173ms\tremaining: 1.27s\n",
            "24:\tlearn: 0.4333086\ttotal: 179ms\tremaining: 1.25s\n",
            "25:\tlearn: 0.4321496\ttotal: 185ms\tremaining: 1.24s\n",
            "26:\tlearn: 0.4312552\ttotal: 192ms\tremaining: 1.23s\n",
            "27:\tlearn: 0.4300666\ttotal: 198ms\tremaining: 1.21s\n",
            "28:\tlearn: 0.4289641\ttotal: 210ms\tremaining: 1.24s\n",
            "29:\tlearn: 0.4274366\ttotal: 224ms\tremaining: 1.27s\n",
            "30:\tlearn: 0.4266988\ttotal: 230ms\tremaining: 1.25s\n",
            "31:\tlearn: 0.4261835\ttotal: 237ms\tremaining: 1.24s\n",
            "32:\tlearn: 0.4254907\ttotal: 243ms\tremaining: 1.23s\n",
            "33:\tlearn: 0.4248198\ttotal: 249ms\tremaining: 1.22s\n",
            "34:\tlearn: 0.4238819\ttotal: 255ms\tremaining: 1.2s\n",
            "35:\tlearn: 0.4228599\ttotal: 261ms\tremaining: 1.19s\n",
            "36:\tlearn: 0.4220290\ttotal: 268ms\tremaining: 1.18s\n",
            "37:\tlearn: 0.4217516\ttotal: 277ms\tremaining: 1.18s\n",
            "38:\tlearn: 0.4207898\ttotal: 283ms\tremaining: 1.17s\n",
            "39:\tlearn: 0.4203614\ttotal: 289ms\tremaining: 1.16s\n",
            "40:\tlearn: 0.4195101\ttotal: 295ms\tremaining: 1.14s\n",
            "41:\tlearn: 0.4192155\ttotal: 301ms\tremaining: 1.13s\n",
            "42:\tlearn: 0.4188181\ttotal: 307ms\tremaining: 1.12s\n",
            "43:\tlearn: 0.4181566\ttotal: 314ms\tremaining: 1.11s\n",
            "44:\tlearn: 0.4178619\ttotal: 320ms\tremaining: 1.1s\n",
            "45:\tlearn: 0.4174004\ttotal: 326ms\tremaining: 1.09s\n",
            "46:\tlearn: 0.4171223\ttotal: 332ms\tremaining: 1.08s\n",
            "47:\tlearn: 0.4166534\ttotal: 338ms\tremaining: 1.07s\n",
            "48:\tlearn: 0.4162940\ttotal: 344ms\tremaining: 1.06s\n",
            "49:\tlearn: 0.4158293\ttotal: 351ms\tremaining: 1.05s\n",
            "50:\tlearn: 0.4152682\ttotal: 357ms\tremaining: 1.04s\n",
            "51:\tlearn: 0.4148432\ttotal: 363ms\tremaining: 1.03s\n",
            "52:\tlearn: 0.4143567\ttotal: 369ms\tremaining: 1.02s\n",
            "53:\tlearn: 0.4137742\ttotal: 376ms\tremaining: 1.01s\n",
            "54:\tlearn: 0.4137188\ttotal: 382ms\tremaining: 1.01s\n",
            "55:\tlearn: 0.4134271\ttotal: 388ms\tremaining: 998ms\n",
            "56:\tlearn: 0.4131039\ttotal: 394ms\tremaining: 989ms\n",
            "57:\tlearn: 0.4126210\ttotal: 401ms\tremaining: 981ms\n",
            "58:\tlearn: 0.4123123\ttotal: 417ms\tremaining: 997ms\n",
            "59:\tlearn: 0.4118804\ttotal: 424ms\tremaining: 989ms\n",
            "60:\tlearn: 0.4112103\ttotal: 430ms\tremaining: 980ms\n",
            "61:\tlearn: 0.4104928\ttotal: 437ms\tremaining: 972ms\n",
            "62:\tlearn: 0.4102154\ttotal: 443ms\tremaining: 963ms\n",
            "63:\tlearn: 0.4097130\ttotal: 449ms\tremaining: 954ms\n",
            "64:\tlearn: 0.4094385\ttotal: 455ms\tremaining: 945ms\n",
            "65:\tlearn: 0.4089040\ttotal: 461ms\tremaining: 936ms\n",
            "66:\tlearn: 0.4083934\ttotal: 467ms\tremaining: 928ms\n",
            "67:\tlearn: 0.4076312\ttotal: 473ms\tremaining: 919ms\n",
            "68:\tlearn: 0.4074097\ttotal: 479ms\tremaining: 910ms\n",
            "69:\tlearn: 0.4068111\ttotal: 485ms\tremaining: 901ms\n",
            "70:\tlearn: 0.4060163\ttotal: 495ms\tremaining: 899ms\n",
            "71:\tlearn: 0.4055552\ttotal: 501ms\tremaining: 891ms\n",
            "72:\tlearn: 0.4052841\ttotal: 507ms\tremaining: 882ms\n",
            "73:\tlearn: 0.4044646\ttotal: 513ms\tremaining: 874ms\n",
            "74:\tlearn: 0.4041960\ttotal: 519ms\tremaining: 866ms\n",
            "75:\tlearn: 0.4037012\ttotal: 526ms\tremaining: 857ms\n",
            "76:\tlearn: 0.4033039\ttotal: 532ms\tremaining: 850ms\n",
            "77:\tlearn: 0.4027439\ttotal: 538ms\tremaining: 842ms\n",
            "78:\tlearn: 0.4019139\ttotal: 545ms\tremaining: 834ms\n",
            "79:\tlearn: 0.4012143\ttotal: 551ms\tremaining: 826ms\n",
            "80:\tlearn: 0.4007033\ttotal: 557ms\tremaining: 818ms\n",
            "81:\tlearn: 0.4003975\ttotal: 563ms\tremaining: 810ms\n",
            "82:\tlearn: 0.3999358\ttotal: 569ms\tremaining: 802ms\n",
            "83:\tlearn: 0.3991616\ttotal: 575ms\tremaining: 795ms\n",
            "84:\tlearn: 0.3987007\ttotal: 582ms\tremaining: 787ms\n",
            "85:\tlearn: 0.3982189\ttotal: 588ms\tremaining: 779ms\n",
            "86:\tlearn: 0.3978042\ttotal: 594ms\tremaining: 771ms\n",
            "87:\tlearn: 0.3973887\ttotal: 600ms\tremaining: 764ms\n",
            "88:\tlearn: 0.3968738\ttotal: 606ms\tremaining: 756ms\n",
            "89:\tlearn: 0.3962600\ttotal: 617ms\tremaining: 754ms\n",
            "90:\tlearn: 0.3956799\ttotal: 631ms\tremaining: 756ms\n",
            "91:\tlearn: 0.3950194\ttotal: 640ms\tremaining: 752ms\n",
            "92:\tlearn: 0.3947872\ttotal: 646ms\tremaining: 743ms\n",
            "93:\tlearn: 0.3942180\ttotal: 652ms\tremaining: 735ms\n",
            "94:\tlearn: 0.3935096\ttotal: 658ms\tremaining: 727ms\n",
            "95:\tlearn: 0.3933547\ttotal: 664ms\tremaining: 719ms\n",
            "96:\tlearn: 0.3930695\ttotal: 670ms\tremaining: 712ms\n",
            "97:\tlearn: 0.3928794\ttotal: 676ms\tremaining: 704ms\n",
            "98:\tlearn: 0.3921673\ttotal: 682ms\tremaining: 696ms\n",
            "99:\tlearn: 0.3920783\ttotal: 691ms\tremaining: 691ms\n",
            "100:\tlearn: 0.3919857\ttotal: 697ms\tremaining: 683ms\n",
            "101:\tlearn: 0.3912530\ttotal: 703ms\tremaining: 676ms\n",
            "102:\tlearn: 0.3907357\ttotal: 709ms\tremaining: 668ms\n",
            "103:\tlearn: 0.3906108\ttotal: 715ms\tremaining: 660ms\n",
            "104:\tlearn: 0.3905267\ttotal: 721ms\tremaining: 652ms\n",
            "105:\tlearn: 0.3898393\ttotal: 727ms\tremaining: 645ms\n",
            "106:\tlearn: 0.3896774\ttotal: 735ms\tremaining: 639ms\n",
            "107:\tlearn: 0.3892852\ttotal: 741ms\tremaining: 631ms\n",
            "108:\tlearn: 0.3889265\ttotal: 747ms\tremaining: 624ms\n",
            "109:\tlearn: 0.3881190\ttotal: 753ms\tremaining: 616ms\n",
            "110:\tlearn: 0.3873158\ttotal: 760ms\tremaining: 609ms\n",
            "111:\tlearn: 0.3871146\ttotal: 765ms\tremaining: 601ms\n",
            "112:\tlearn: 0.3861705\ttotal: 771ms\tremaining: 594ms\n",
            "113:\tlearn: 0.3856679\ttotal: 777ms\tremaining: 586ms\n",
            "114:\tlearn: 0.3850674\ttotal: 783ms\tremaining: 579ms\n",
            "115:\tlearn: 0.3841374\ttotal: 790ms\tremaining: 572ms\n",
            "116:\tlearn: 0.3839882\ttotal: 796ms\tremaining: 565ms\n",
            "117:\tlearn: 0.3834395\ttotal: 802ms\tremaining: 557ms\n",
            "118:\tlearn: 0.3833168\ttotal: 808ms\tremaining: 550ms\n",
            "119:\tlearn: 0.3827788\ttotal: 821ms\tremaining: 547ms\n",
            "120:\tlearn: 0.3821705\ttotal: 829ms\tremaining: 542ms\n",
            "121:\tlearn: 0.3814683\ttotal: 835ms\tremaining: 534ms\n",
            "122:\tlearn: 0.3808344\ttotal: 842ms\tremaining: 527ms\n",
            "123:\tlearn: 0.3807475\ttotal: 848ms\tremaining: 519ms\n",
            "124:\tlearn: 0.3801400\ttotal: 854ms\tremaining: 512ms\n",
            "125:\tlearn: 0.3794068\ttotal: 860ms\tremaining: 505ms\n",
            "126:\tlearn: 0.3788116\ttotal: 866ms\tremaining: 498ms\n",
            "127:\tlearn: 0.3781468\ttotal: 872ms\tremaining: 491ms\n",
            "128:\tlearn: 0.3778211\ttotal: 878ms\tremaining: 484ms\n",
            "129:\tlearn: 0.3773538\ttotal: 884ms\tremaining: 476ms\n",
            "130:\tlearn: 0.3765184\ttotal: 890ms\tremaining: 469ms\n",
            "131:\tlearn: 0.3758686\ttotal: 896ms\tremaining: 462ms\n",
            "132:\tlearn: 0.3750647\ttotal: 903ms\tremaining: 455ms\n",
            "133:\tlearn: 0.3745143\ttotal: 910ms\tremaining: 448ms\n",
            "134:\tlearn: 0.3739143\ttotal: 918ms\tremaining: 442ms\n",
            "135:\tlearn: 0.3735284\ttotal: 924ms\tremaining: 435ms\n",
            "136:\tlearn: 0.3732223\ttotal: 930ms\tremaining: 428ms\n",
            "137:\tlearn: 0.3730318\ttotal: 936ms\tremaining: 420ms\n",
            "138:\tlearn: 0.3728107\ttotal: 942ms\tremaining: 413ms\n",
            "139:\tlearn: 0.3727086\ttotal: 948ms\tremaining: 406ms\n",
            "140:\tlearn: 0.3718069\ttotal: 954ms\tremaining: 399ms\n",
            "141:\tlearn: 0.3709997\ttotal: 960ms\tremaining: 392ms\n",
            "142:\tlearn: 0.3704308\ttotal: 966ms\tremaining: 385ms\n",
            "143:\tlearn: 0.3696448\ttotal: 973ms\tremaining: 378ms\n",
            "144:\tlearn: 0.3690659\ttotal: 979ms\tremaining: 371ms\n",
            "145:\tlearn: 0.3680074\ttotal: 985ms\tremaining: 364ms\n",
            "146:\tlearn: 0.3674449\ttotal: 991ms\tremaining: 357ms\n",
            "147:\tlearn: 0.3668267\ttotal: 998ms\tremaining: 351ms\n",
            "148:\tlearn: 0.3663194\ttotal: 1s\tremaining: 344ms\n",
            "149:\tlearn: 0.3657558\ttotal: 1.01s\tremaining: 337ms\n",
            "150:\tlearn: 0.3652293\ttotal: 1.02s\tremaining: 332ms\n",
            "151:\tlearn: 0.3645606\ttotal: 1.03s\tremaining: 327ms\n",
            "152:\tlearn: 0.3640286\ttotal: 1.05s\tremaining: 322ms\n",
            "153:\tlearn: 0.3634852\ttotal: 1.05s\tremaining: 315ms\n",
            "154:\tlearn: 0.3633794\ttotal: 1.06s\tremaining: 308ms\n",
            "155:\tlearn: 0.3630426\ttotal: 1.07s\tremaining: 301ms\n",
            "156:\tlearn: 0.3625861\ttotal: 1.07s\tremaining: 294ms\n",
            "157:\tlearn: 0.3620992\ttotal: 1.08s\tremaining: 288ms\n",
            "158:\tlearn: 0.3615451\ttotal: 1.09s\tremaining: 281ms\n",
            "159:\tlearn: 0.3608338\ttotal: 1.09s\tremaining: 273ms\n",
            "160:\tlearn: 0.3604730\ttotal: 1.1s\tremaining: 266ms\n",
            "161:\tlearn: 0.3597677\ttotal: 1.11s\tremaining: 260ms\n",
            "162:\tlearn: 0.3588646\ttotal: 1.12s\tremaining: 253ms\n",
            "163:\tlearn: 0.3585054\ttotal: 1.13s\tremaining: 248ms\n",
            "164:\tlearn: 0.3575716\ttotal: 1.14s\tremaining: 242ms\n",
            "165:\tlearn: 0.3572285\ttotal: 1.14s\tremaining: 234ms\n",
            "166:\tlearn: 0.3567036\ttotal: 1.15s\tremaining: 227ms\n",
            "167:\tlearn: 0.3561429\ttotal: 1.16s\tremaining: 220ms\n",
            "168:\tlearn: 0.3555160\ttotal: 1.16s\tremaining: 213ms\n",
            "169:\tlearn: 0.3546698\ttotal: 1.17s\tremaining: 206ms\n",
            "170:\tlearn: 0.3546120\ttotal: 1.18s\tremaining: 199ms\n",
            "171:\tlearn: 0.3542260\ttotal: 1.18s\tremaining: 192ms\n",
            "172:\tlearn: 0.3541523\ttotal: 1.19s\tremaining: 185ms\n",
            "173:\tlearn: 0.3538643\ttotal: 1.19s\tremaining: 178ms\n",
            "174:\tlearn: 0.3533097\ttotal: 1.2s\tremaining: 171ms\n",
            "175:\tlearn: 0.3525152\ttotal: 1.21s\tremaining: 165ms\n",
            "176:\tlearn: 0.3519717\ttotal: 1.22s\tremaining: 158ms\n",
            "177:\tlearn: 0.3513283\ttotal: 1.23s\tremaining: 152ms\n",
            "178:\tlearn: 0.3509318\ttotal: 1.24s\tremaining: 145ms\n",
            "179:\tlearn: 0.3503603\ttotal: 1.24s\tremaining: 138ms\n",
            "180:\tlearn: 0.3499161\ttotal: 1.25s\tremaining: 131ms\n",
            "181:\tlearn: 0.3492537\ttotal: 1.25s\tremaining: 124ms\n",
            "182:\tlearn: 0.3490237\ttotal: 1.26s\tremaining: 117ms\n",
            "183:\tlearn: 0.3485892\ttotal: 1.27s\tremaining: 110ms\n",
            "184:\tlearn: 0.3477830\ttotal: 1.27s\tremaining: 103ms\n",
            "185:\tlearn: 0.3474342\ttotal: 1.28s\tremaining: 96.4ms\n",
            "186:\tlearn: 0.3471108\ttotal: 1.29s\tremaining: 89.5ms\n",
            "187:\tlearn: 0.3466701\ttotal: 1.29s\tremaining: 82.6ms\n",
            "188:\tlearn: 0.3462409\ttotal: 1.3s\tremaining: 75.7ms\n",
            "189:\tlearn: 0.3458924\ttotal: 1.31s\tremaining: 68.8ms\n",
            "190:\tlearn: 0.3456513\ttotal: 1.31s\tremaining: 61.8ms\n",
            "191:\tlearn: 0.3450063\ttotal: 1.32s\tremaining: 55ms\n",
            "192:\tlearn: 0.3441628\ttotal: 1.32s\tremaining: 48.1ms\n",
            "193:\tlearn: 0.3434435\ttotal: 1.33s\tremaining: 41.2ms\n",
            "194:\tlearn: 0.3426736\ttotal: 1.34s\tremaining: 34.3ms\n",
            "195:\tlearn: 0.3421702\ttotal: 1.34s\tremaining: 27.4ms\n",
            "196:\tlearn: 0.3415958\ttotal: 1.35s\tremaining: 20.6ms\n",
            "197:\tlearn: 0.3410487\ttotal: 1.36s\tremaining: 13.7ms\n",
            "198:\tlearn: 0.3403983\ttotal: 1.37s\tremaining: 6.87ms\n",
            "199:\tlearn: 0.3398906\ttotal: 1.37s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=<catboost.core.CatBoostClassifier object at 0x7c316d519ab0>,\n",
              "             param_grid={'depth': [6, 8, 10], 'iterations': [200, 400, 1000],\n",
              "                         'learning_rate': [0.1, 0.08, 0.2],\n",
              "                         'min_data_in_leaf': [12, 40, 80]})"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=&lt;catboost.core.CatBoostClassifier object at 0x7c316d519ab0&gt;,\n",
              "             param_grid={&#x27;depth&#x27;: [6, 8, 10], &#x27;iterations&#x27;: [200, 400, 1000],\n",
              "                         &#x27;learning_rate&#x27;: [0.1, 0.08, 0.2],\n",
              "                         &#x27;min_data_in_leaf&#x27;: [12, 40, 80]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=&lt;catboost.core.CatBoostClassifier object at 0x7c316d519ab0&gt;,\n",
              "             param_grid={&#x27;depth&#x27;: [6, 8, 10], &#x27;iterations&#x27;: [200, 400, 1000],\n",
              "                         &#x27;learning_rate&#x27;: [0.1, 0.08, 0.2],\n",
              "                         &#x27;min_data_in_leaf&#x27;: [12, 40, 80]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x7c316d519ab0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x7c316d519ab0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "params = {\n",
        "    \"depth\": [6,8,10],\n",
        "    \"iterations\": [200, 400, 1000],\n",
        "    \"learning_rate\": [0.1, 0.08, 0.2],\n",
        "    \"min_data_in_leaf\": [12, 40, 80]\n",
        "}\n",
        "model5 = CatBoostClassifier()\n",
        "search = GridSearchCV(model5, params)\n",
        "search.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:32:28.953813Z",
          "end_time": "2024-05-13T21:47:40.961232Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X6avfNyOlLVa",
        "outputId": "94c3e45e-9761-4b9d-9ea8-e4bbfd7ba416"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.435\n",
            "R2: 0.178\n",
            "ROC AUC: 0.79205\n",
            "Score: 0.8109\n",
            "Local Score: 0.8651\n",
            "Best params:  {'cv': None, 'error_score': nan, 'estimator': <catboost.core.CatBoostClassifier object at 0x7c316d519ab0>, 'n_jobs': None, 'param_grid': {'depth': [6, 8, 10], 'iterations': [200, 400, 1000], 'learning_rate': [0.1, 0.08, 0.2], 'min_data_in_leaf': [12, 40, 80]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 0}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(search)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:50:25.963631Z",
          "end_time": "2024-05-13T21:50:25.999454Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl3wkRs5lLVa",
        "outputId": "44b5c299-12a9-4b59-f620-4f7641b31ec0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.435\n",
            "R2: 0.178\n",
            "ROC AUC: 0.79205\n",
            "Score: 0.8109\n",
            "Local Score: 0.8651\n",
            "Best params:  {'iterations': 200, 'learning_rate': 0.1, 'depth': 6, 'min_data_in_leaf': 12}\n"
          ]
        }
      ],
      "source": [
        "model5 = search.best_estimator_\n",
        "estimate_model(model5)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:50:33.451937Z",
          "end_time": "2024-05-13T21:50:33.511260Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc0mB4VTlLVa",
        "outputId": "e43a77af-353b-434c-821e-d2cfbdd88877"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001591 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002126 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001929 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001964 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001563 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000660 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001862 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001507 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002042 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001526 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001885 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001961 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001603 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001394 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001567 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001928 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001936 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002015 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001604 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001600 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001832 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002143 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002071 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001735 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002057 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000769 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000846 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001922 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000781 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002060 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001654 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000857 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002016 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003051 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000587 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000549 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000614 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002110 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000543 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002026 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001844 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002028 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002321 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002154 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002047 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002186 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001977 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000542 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000712 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002189 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001996 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000544 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002155 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002528 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002105 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001566 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000555 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001938 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002051 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002064 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000533 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001977 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002093 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000632 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002128 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000577 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002036 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000594 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000838 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001771 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002125 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004671 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000633 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000769 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3664\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1774, number of negative: 2958\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3642\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374894 -> initscore=-0.511276\n",
            "[LightGBM] [Info] Start training from score -0.511276\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 2959\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3649\n",
            "[LightGBM] [Info] Number of data points in the train set: 4732, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374683 -> initscore=-0.512178\n",
            "[LightGBM] [Info] Start training from score -0.512178\n",
            "[LightGBM] [Info] Number of positive: 2217, number of negative: 3698\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3745\n",
            "[LightGBM] [Info] Number of data points in the train set: 5915, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374810 -> initscore=-0.511637\n",
            "[LightGBM] [Info] Start training from score -0.511637\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=LGBMClassifier(),\n",
              "             param_grid={'boosting_type': ['gbdt', 'dart'],\n",
              "                         'learning_rate': [0.005, 0.01],\n",
              "                         'max_depth': [6, 8, 10, -1],\n",
              "                         'n_estimators': [100, 400, 600],\n",
              "                         'num_leaves': [6, 8, 12, 16],\n",
              "                         'objective': ['binary']},\n",
              "             scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LGBMClassifier(),\n",
              "             param_grid={&#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;],\n",
              "                         &#x27;learning_rate&#x27;: [0.005, 0.01],\n",
              "                         &#x27;max_depth&#x27;: [6, 8, 10, -1],\n",
              "                         &#x27;n_estimators&#x27;: [100, 400, 600],\n",
              "                         &#x27;num_leaves&#x27;: [6, 8, 12, 16],\n",
              "                         &#x27;objective&#x27;: [&#x27;binary&#x27;]},\n",
              "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LGBMClassifier(),\n",
              "             param_grid={&#x27;boosting_type&#x27;: [&#x27;gbdt&#x27;, &#x27;dart&#x27;],\n",
              "                         &#x27;learning_rate&#x27;: [0.005, 0.01],\n",
              "                         &#x27;max_depth&#x27;: [6, 8, 10, -1],\n",
              "                         &#x27;n_estimators&#x27;: [100, 400, 600],\n",
              "                         &#x27;num_leaves&#x27;: [6, 8, 12, 16],\n",
              "                         &#x27;objective&#x27;: [&#x27;binary&#x27;]},\n",
              "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "params = {\n",
        "    'learning_rate': [0.005, 0.01],\n",
        "    'n_estimators': [100, 400, 600],\n",
        "    \"max_depth\": [6, 8, 10, -1],\n",
        "    'num_leaves': [6,8,12,16],\n",
        "    'boosting_type' : ['gbdt', 'dart'],\n",
        "    'objective' : ['binary'],\n",
        "    }\n",
        "model6 = LGBMClassifier()\n",
        "search = GridSearchCV(model6, params, scoring='roc_auc')\n",
        "search.fit(x_train.to_numpy(), y_train.to_numpy())"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:55:35.066689Z",
          "end_time": "2024-05-13T22:10:47.632261Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KLx7q-GRlLVa",
        "outputId": "314fb2fb-ade6-4acf-c6de-22b9cc2335b7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.433\n",
            "R2: 0.186\n",
            "ROC AUC: 0.79207\n",
            "Score: 0.8129\n",
            "Local Score: 0.8051\n",
            "Best params:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.005, 'max_depth': 6, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 600, 'n_jobs': None, 'num_leaves': 8, 'objective': 'binary', 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n"
          ]
        }
      ],
      "source": [
        "model6 = search.best_estimator_\n",
        "estimate_model(model6)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:12:23.053028Z",
          "end_time": "2024-05-13T22:12:23.116255Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WXj3fBmlLVa",
        "outputId": "9c05ab86-ba21-4fdd-84a7-9fe81e8de251"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "XATx0Y60lLVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None, max_b...\n",
              "                                     max_cat_threshold=None,\n",
              "                                     max_cat_to_onehot=None,\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=None, ...),\n",
              "             param_grid={'learning_rate': [0.1, 0.01, 0.05],\n",
              "                         'max_depth': range(2, 10),\n",
              "                         'n_estimators': range(60, 220, 40)},\n",
              "             scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None, max_b...\n",
              "                                     max_cat_threshold=None,\n",
              "                                     max_cat_to_onehot=None,\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=None, ...),\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.05],\n",
              "                         &#x27;max_depth&#x27;: range(2, 10),\n",
              "                         &#x27;n_estimators&#x27;: range(60, 220, 40)},\n",
              "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None, max_b...\n",
              "                                     max_cat_threshold=None,\n",
              "                                     max_cat_to_onehot=None,\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=None, ...),\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.01, 0.05],\n",
              "                         &#x27;max_depth&#x27;: range(2, 10),\n",
              "                         &#x27;n_estimators&#x27;: range(60, 220, 40)},\n",
              "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "params = {\n",
        "    'max_depth': range (2, 10, 1),\n",
        "    'n_estimators': range(60, 220, 40),\n",
        "    'learning_rate': [0.1, 0.01, 0.05]\n",
        "}\n",
        "model7 = XGBClassifier()\n",
        "search = GridSearchCV(model7, params, scoring='roc_auc')\n",
        "search.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:14:01.552751Z",
          "end_time": "2024-05-13T22:15:14.435366Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "3rg1YIHqlLVa",
        "outputId": "7592e718-d623-4cff-f1d2-fc5eb743adda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.434\n",
            "R2: 0.180\n",
            "ROC AUC: 0.78683\n",
            "Score: 0.8114\n",
            "Local Score: 0.8044\n",
            "Best params:  {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.01, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 4, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 180, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
          ]
        }
      ],
      "source": [
        "model7 = search.best_estimator_\n",
        "estimate_model(model7)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:16:15.173026Z",
          "end_time": "2024-05-13T22:16:15.234260Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zskZNL7lLVa",
        "outputId": "7e786f71-7fd2-4fe9-a7e3-1db2f5a92554"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Мы посчитали нужным также провести tuning моделей с помощью optuna вместо GridSearchCV так как это более продвинутым методом, использующим байесовскую оптимизацию для эффективного поиска наилучшего набора гиперпараметров. Таким образом мы избавимся от избыточного перебора и сэкономим время\n",
        "- Лучше всего себя показали модели Catboost и LightGBM. Их мы и используем в финальном решении"
      ],
      "metadata": {
        "collapsed": false,
        "id": "INUxYVHDlLVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Подбор параметров с optuna"
      ],
      "metadata": {
        "collapsed": false,
        "id": "o7UI_9IDlLVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": [
        "import optuna"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:12:06.808127Z",
          "end_time": "2024-05-14T18:12:06.905199Z"
        },
        "id": "z9JmKFt9lLVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1600),\n",
        "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\n",
        "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
        "        ),\n",
        "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 0.01, 18),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
        "        \"used_ram_limit\": \"16gb\",\n",
        "        # \"max_leaves\": trial.suggest_int(\"max_leaves\", 15, 63),\n",
        "    }\n",
        "\n",
        "    if params[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "    optuna_model = CatBoostClassifier(**params)\n",
        "    optuna_model.fit(x_train, y_train, verbose=0)\n",
        "\n",
        "    y_pred = optuna_model.predict(x_test)\n",
        "    accuracy = roc_auc_score(y_test, y_pred)\n",
        "    return accuracy"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:12:33.001284Z",
          "end_time": "2024-05-14T18:12:33.065491Z"
        },
        "id": "cGNMYRE1lLVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-15 21:16:30,689] A new study created in RDB with name: catboost_optuna\n",
            "[I 2024-05-15 21:16:32,621] Trial 0 finished with value: 0.7889087666530629 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.02400232924353648, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 16, 'learning_rate': 0.11444688261280059}. Best is trial 0 with value: 0.7889087666530629.\n",
            "[I 2024-05-15 21:16:42,653] Trial 1 finished with value: 0.7948163760475426 and parameters: {'iterations': 1246, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.04066995390495744, 'depth': 2, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 12, 'learning_rate': 0.07125695214429512, 'subsample': 0.6287745854157244}. Best is trial 1 with value: 0.7948163760475426.\n",
            "[I 2024-05-15 21:16:48,652] Trial 2 finished with value: 0.7956068898815348 and parameters: {'iterations': 643, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.044626942113510586, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 1, 'learning_rate': 0.1071913580380133, 'subsample': 0.6597809786240145}. Best is trial 2 with value: 0.7956068898815348.\n",
            "[I 2024-05-15 21:16:53,384] Trial 3 finished with value: 0.7834601472569618 and parameters: {'iterations': 624, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.013969830663361498, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.02179614187464507, 'bagging_temperature': 3.0510656245234835}. Best is trial 2 with value: 0.7956068898815348.\n",
            "[I 2024-05-15 21:16:59,685] Trial 4 finished with value: 0.7932353483795584 and parameters: {'iterations': 665, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.04906759999589061, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.12229469256362646, 'bagging_temperature': 6.378258242253708}. Best is trial 2 with value: 0.7956068898815348.\n",
            "[I 2024-05-15 21:17:00,239] Trial 5 finished with value: 0.7972074847236277 and parameters: {'iterations': 120, 'objective': 'Logloss', 'colsample_bylevel': 0.01083590259769436, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.14384517423491275, 'subsample': 0.33977459663965826}. Best is trial 5 with value: 0.7972074847236277.\n",
            "[I 2024-05-15 21:17:06,958] Trial 6 finished with value: 0.8007027410815615 and parameters: {'iterations': 817, 'objective': 'Logloss', 'colsample_bylevel': 0.012284068913363411, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.07197182253213998}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:08,615] Trial 7 finished with value: 0.7930687478685756 and parameters: {'iterations': 1227, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.019304970082221894, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03563990980318694, 'bagging_temperature': 7.438634995946973}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:18,463] Trial 8 finished with value: 0.7913636084105304 and parameters: {'iterations': 1103, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.05384140119127515, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 12, 'learning_rate': 0.11649128801863937}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:19,330] Trial 9 finished with value: 0.7902611379150338 and parameters: {'iterations': 347, 'objective': 'Logloss', 'colsample_bylevel': 0.05077519565909471, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.10433476252080227}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:22,433] Trial 10 finished with value: 0.7912590637945781 and parameters: {'iterations': 1491, 'objective': 'Logloss', 'colsample_bylevel': 0.08167868612245525, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.06395494450817953}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:24,072] Trial 11 finished with value: 0.7908017509825517 and parameters: {'iterations': 902, 'objective': 'Logloss', 'colsample_bylevel': 0.07768455634231956, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.14952962264598924, 'subsample': 0.19715102675238688}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:24,727] Trial 12 finished with value: 0.796230803204544 and parameters: {'iterations': 328, 'objective': 'Logloss', 'colsample_bylevel': 0.029982251129344988, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 18, 'learning_rate': 0.04859087633413198, 'subsample': 0.19276061117996443}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:25,132] Trial 13 finished with value: 0.770210934136892 and parameters: {'iterations': 104, 'objective': 'Logloss', 'colsample_bylevel': 0.011947025566728326, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.09011534339253653}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:33,097] Trial 14 finished with value: 0.7905110386815078 and parameters: {'iterations': 892, 'objective': 'Logloss', 'colsample_bylevel': 0.0327693667878777, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 14, 'learning_rate': 0.14392408215199665, 'subsample': 0.9021347362948808}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:36,854] Trial 15 finished with value: 0.7958355462875479 and parameters: {'iterations': 1578, 'objective': 'Logloss', 'colsample_bylevel': 0.06796551866396426, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.08327951708693623}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:41,484] Trial 16 finished with value: 0.7936306052965546 and parameters: {'iterations': 449, 'objective': 'Logloss', 'colsample_bylevel': 0.09584341274385028, 'depth': 5, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 6, 'learning_rate': 0.0538520423355756, 'subsample': 0.4013294877983616}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:44,012] Trial 17 finished with value: 0.7882015530745621 and parameters: {'iterations': 1035, 'objective': 'Logloss', 'colsample_bylevel': 0.03467604383913857, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.13129832302351063, 'subsample': 0.4092904092390562}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:44,908] Trial 18 finished with value: 0.7924856460801359 and parameters: {'iterations': 454, 'objective': 'Logloss', 'colsample_bylevel': 0.010163277548985347, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 18, 'learning_rate': 0.09246601394775175}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:51,932] Trial 19 finished with value: 0.7945877196415294 and parameters: {'iterations': 786, 'objective': 'Logloss', 'colsample_bylevel': 0.06501159461304143, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.07218792745546733, 'bagging_temperature': 0.17254850830128898}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:52,929] Trial 20 finished with value: 0.7870778382186044 and parameters: {'iterations': 107, 'objective': 'Logloss', 'colsample_bylevel': 0.023036390501902947, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.13476718719964453}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:53,532] Trial 21 finished with value: 0.7954190450100911 and parameters: {'iterations': 272, 'objective': 'Logloss', 'colsample_bylevel': 0.026911560027447327, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 18, 'learning_rate': 0.03740093110746371, 'subsample': 0.150443978303648}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:54,857] Trial 22 finished with value: 0.7946497755365599 and parameters: {'iterations': 489, 'objective': 'Logloss', 'colsample_bylevel': 0.03165520987940695, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 16, 'learning_rate': 0.05512859209536934, 'subsample': 0.32758062620335965}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:55,998] Trial 23 finished with value: 0.7857058997825248 and parameters: {'iterations': 342, 'objective': 'Logloss', 'colsample_bylevel': 0.015593036423797516, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 16, 'learning_rate': 0.04695392541286549, 'subsample': 0.104524240345514}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:56,774] Trial 24 finished with value: 0.7732488776827994 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.018870040437510935, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.026059478845652663, 'subsample': 0.29126998992274467}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:57,712] Trial 25 finished with value: 0.7981858434290635 and parameters: {'iterations': 529, 'objective': 'Logloss', 'colsample_bylevel': 0.039217919291352224, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.06506350374687404, 'subsample': 0.5095970405601773}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:17:58,924] Trial 26 finished with value: 0.7951070883485865 and parameters: {'iterations': 736, 'objective': 'Logloss', 'colsample_bylevel': 0.03890021993924622, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.07783307719115101, 'subsample': 0.5054427966036674}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:00,250] Trial 27 finished with value: 0.7915089645610525 and parameters: {'iterations': 561, 'objective': 'Logloss', 'colsample_bylevel': 0.057839550946405825, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.09754575285535613, 'subsample': 0.8125335200290955}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:07,398] Trial 28 finished with value: 0.7964382152500964 and parameters: {'iterations': 966, 'objective': 'Logloss', 'colsample_bylevel': 0.02113836042997327, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.06324476282326223, 'bagging_temperature': 9.297202112542678}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:14,334] Trial 29 finished with value: 0.7937972058075372 and parameters: {'iterations': 789, 'objective': 'Logloss', 'colsample_bylevel': 0.0256258941028512, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.08446943997937643}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:15,586] Trial 30 finished with value: 0.7981237875340328 and parameters: {'iterations': 533, 'objective': 'Logloss', 'colsample_bylevel': 0.03868740644742372, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 14, 'learning_rate': 0.01164878250557412}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:16,737] Trial 31 finished with value: 0.7949617321980645 and parameters: {'iterations': 486, 'objective': 'Logloss', 'colsample_bylevel': 0.038314446462257096, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 14, 'learning_rate': 0.014364630229526849}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:18,004] Trial 32 finished with value: 0.7880349525635792 and parameters: {'iterations': 571, 'objective': 'Logloss', 'colsample_bylevel': 0.04439474206037393, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 12, 'learning_rate': 0.06399800060971528}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:20,015] Trial 33 finished with value: 0.7997668710970477 and parameters: {'iterations': 711, 'objective': 'Logloss', 'colsample_bylevel': 0.060722372571695014, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 15, 'learning_rate': 0.012137338503800707}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:25,163] Trial 34 finished with value: 0.7968547165275535 and parameters: {'iterations': 719, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.06165330556574801, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 15, 'learning_rate': 0.010578988303389854}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:36,502] Trial 35 finished with value: 0.7948163760475426 and parameters: {'iterations': 627, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07397938952910034, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 16, 'learning_rate': 0.026290636034941278}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:42,975] Trial 36 finished with value: 0.7929233917180539 and parameters: {'iterations': 849, 'objective': 'Logloss', 'colsample_bylevel': 0.04680568143281237, 'depth': 2, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 0, 'learning_rate': 0.03469415947792593}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:44,438] Trial 37 finished with value: 0.7971666731890581 and parameters: {'iterations': 691, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.05574176201205262, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 17, 'learning_rate': 0.018667807898343065}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:45,649] Trial 38 finished with value: 0.7957522460320566 and parameters: {'iterations': 538, 'objective': 'Logloss', 'colsample_bylevel': 0.04039918331301211, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 15, 'learning_rate': 0.040107884220598466}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:56,017] Trial 39 finished with value: 0.7970000726780754 and parameters: {'iterations': 1120, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.05108128217466905, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.024276675127959772, 'bagging_temperature': 2.971938290027001}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:57,594] Trial 40 finished with value: 0.7973332737000408 and parameters: {'iterations': 414, 'objective': 'Logloss', 'colsample_bylevel': 0.0886189246300344, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.03182883255484513}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:18:59,326] Trial 41 finished with value: 0.7955235896260434 and parameters: {'iterations': 433, 'objective': 'Logloss', 'colsample_bylevel': 0.08998059903771963, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.010463773162599666}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:01,314] Trial 42 finished with value: 0.7983524439400461 and parameters: {'iterations': 617, 'objective': 'Logloss', 'colsample_bylevel': 0.08567286864398507, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.03139924473627548}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:04,041] Trial 43 finished with value: 0.7961475029490527 and parameters: {'iterations': 631, 'objective': 'Logloss', 'colsample_bylevel': 0.07406741522268846, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.019811439287501537}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:06,366] Trial 44 finished with value: 0.7934019488905413 and parameters: {'iterations': 810, 'objective': 'Logloss', 'colsample_bylevel': 0.09932861807482575, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 12, 'learning_rate': 0.04443906770441709}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:07,826] Trial 45 finished with value: 0.7976452303615456 and parameters: {'iterations': 573, 'objective': 'Logloss', 'colsample_bylevel': 0.08427352984154139, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.03120759831243972}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:10,342] Trial 46 finished with value: 0.7926734909515796 and parameters: {'iterations': 960, 'objective': 'Logloss', 'colsample_bylevel': 0.070158838978915, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.07226851293853359}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:16,364] Trial 47 finished with value: 0.7879108407735184 and parameters: {'iterations': 668, 'objective': 'Logloss', 'colsample_bylevel': 0.04370656038035812, 'depth': 4, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.017097530011504592, 'bagging_temperature': 9.834800056189641}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:17,981] Trial 48 finished with value: 0.7925901906960882 and parameters: {'iterations': 379, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.05842902966061714, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.055023629813027344}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:19,337] Trial 49 finished with value: 0.7961262585885918 and parameters: {'iterations': 737, 'objective': 'Logloss', 'colsample_bylevel': 0.03569518350671196, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 15, 'learning_rate': 0.028266968614936182}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:30,853] Trial 50 finished with value: 0.7967093603770316 and parameters: {'iterations': 1342, 'objective': 'Logloss', 'colsample_bylevel': 0.05199790357089004, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.041357052438022894}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:32,369] Trial 51 finished with value: 0.7937759614470763 and parameters: {'iterations': 572, 'objective': 'Logloss', 'colsample_bylevel': 0.08249825959780703, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.02942007985502696}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:33,643] Trial 52 finished with value: 0.7988930570075642 and parameters: {'iterations': 523, 'objective': 'Logloss', 'colsample_bylevel': 0.08214428252233318, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.021662962385384274}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:34,934] Trial 53 finished with value: 0.7942545186195639 and parameters: {'iterations': 532, 'objective': 'Logloss', 'colsample_bylevel': 0.08772527794723195, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.020345970282367867}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:36,524] Trial 54 finished with value: 0.7992883139245601 and parameters: {'iterations': 641, 'objective': 'Logloss', 'colsample_bylevel': 0.07876150111626268, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.013302659679524115}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:38,591] Trial 55 finished with value: 0.7922782340345835 and parameters: {'iterations': 862, 'objective': 'Logloss', 'colsample_bylevel': 0.07867375622160001, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.10690369660791194}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:39,446] Trial 56 finished with value: 0.7937972058075372 and parameters: {'iterations': 283, 'objective': 'Logloss', 'colsample_bylevel': 0.093655409504118, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.0606916440898857}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:40,755] Trial 57 finished with value: 0.8000788277585523 and parameters: {'iterations': 661, 'objective': 'Logloss', 'colsample_bylevel': 0.07902511742558972, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.016351003948738504, 'bagging_temperature': 0.6102368235187647}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:43,236] Trial 58 finished with value: 0.7987477008570422 and parameters: {'iterations': 758, 'objective': 'Logloss', 'colsample_bylevel': 0.07777869099330027, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.015232194108862692, 'bagging_temperature': 0.09369208748536195}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:51,031] Trial 59 finished with value: 0.7962928590995746 and parameters: {'iterations': 951, 'objective': 'Logloss', 'colsample_bylevel': 0.07733508585325174, 'depth': 2, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.015291479054110272, 'bagging_temperature': 0.11301788252401868}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:52,455] Trial 60 finished with value: 0.79396380631852 and parameters: {'iterations': 761, 'objective': 'Logloss', 'colsample_bylevel': 0.0656274188192516, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.11862506001575032, 'bagging_temperature': 2.2951319304159687}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:53,821] Trial 61 finished with value: 0.7925901906960882 and parameters: {'iterations': 690, 'objective': 'Logloss', 'colsample_bylevel': 0.08422865961979109, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.02305809997872345, 'bagging_temperature': 1.5878246120424349}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:55,365] Trial 62 finished with value: 0.790635150471569 and parameters: {'iterations': 632, 'objective': 'Logloss', 'colsample_bylevel': 0.0734642177300479, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.015402593967492002, 'bagging_temperature': 4.756634958759004}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:57,963] Trial 63 finished with value: 0.7996002705860649 and parameters: {'iterations': 827, 'objective': 'Logloss', 'colsample_bylevel': 0.08035071735298975, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.0221951098065113, 'bagging_temperature': 1.4031894336646615}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:19:59,434] Trial 64 finished with value: 0.7970000726780754 and parameters: {'iterations': 825, 'objective': 'Logloss', 'colsample_bylevel': 0.07894144203170399, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.022280768202772687, 'bagging_temperature': 1.2435986642638426}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:01,135] Trial 65 finished with value: 0.7967714162720619 and parameters: {'iterations': 1036, 'objective': 'Logloss', 'colsample_bylevel': 0.06983537685750552, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 1, 'learning_rate': 0.014745777195634985, 'bagging_temperature': 0.8979938821761945}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:02,890] Trial 66 finished with value: 0.7963141034600354 and parameters: {'iterations': 891, 'objective': 'Logloss', 'colsample_bylevel': 0.0801407878986356, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03643482849467194, 'bagging_temperature': 3.806975586657071}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:04,356] Trial 67 finished with value: 0.7945664752810685 and parameters: {'iterations': 775, 'objective': 'Logloss', 'colsample_bylevel': 0.07628790828979919, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.02043876737523887, 'bagging_temperature': 0.0381299776635733}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:13,094] Trial 68 finished with value: 0.7935260606806023 and parameters: {'iterations': 916, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09297912348230586, 'depth': 1, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.010416016509129326, 'bagging_temperature': 1.8088265766268776}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:14,501] Trial 69 finished with value: 0.7867250700225301 and parameters: {'iterations': 703, 'objective': 'Logloss', 'colsample_bylevel': 0.06368320826040073, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 1, 'learning_rate': 0.12791314226465542, 'bagging_temperature': 1.0419276773768926}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:15,569] Trial 70 finished with value: 0.7999955275030608 and parameters: {'iterations': 491, 'objective': 'Logloss', 'colsample_bylevel': 0.07126329124673074, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.027752729056756388, 'bagging_temperature': 2.472545361879324}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:16,842] Trial 71 finished with value: 0.7937759614470763 and parameters: {'iterations': 603, 'objective': 'Logloss', 'colsample_bylevel': 0.07111371975181914, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.02604455465109197, 'bagging_temperature': 2.4126886277200987}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:17,780] Trial 72 finished with value: 0.7978738867675588 and parameters: {'iterations': 480, 'objective': 'Logloss', 'colsample_bylevel': 0.08161457868666734, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 0, 'learning_rate': 0.017691858773684815, 'bagging_temperature': 0.9995008731981347}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:19,049] Trial 73 finished with value: 0.7904064940655556 and parameters: {'iterations': 667, 'objective': 'Logloss', 'colsample_bylevel': 0.06680507852022205, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.049503491162504576, 'bagging_temperature': 3.8134805423429046}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:20,544] Trial 74 finished with value: 0.7947330757920512 and parameters: {'iterations': 744, 'objective': 'Logloss', 'colsample_bylevel': 0.07538672090081146, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.024219204802246208, 'bagging_temperature': 0.6235249323311655}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:22,558] Trial 75 finished with value: 0.7929233917180539 and parameters: {'iterations': 846, 'objective': 'Logloss', 'colsample_bylevel': 0.061165204144924976, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.0349443455260387, 'bagging_temperature': 2.3341130476834095}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:29,783] Trial 76 finished with value: 0.7972499734445495 and parameters: {'iterations': 794, 'objective': 'Logloss', 'colsample_bylevel': 0.07219376551926292, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.013585574503902366, 'bagging_temperature': 1.825279097264969}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:31,689] Trial 77 finished with value: 0.7968547165275535 and parameters: {'iterations': 999, 'objective': 'Logloss', 'colsample_bylevel': 0.08078868473977956, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.020578099294274534, 'bagging_temperature': 3.160535493806055}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:32,474] Trial 78 finished with value: 0.786058667978599 and parameters: {'iterations': 409, 'objective': 'Logloss', 'colsample_bylevel': 0.06885153094116644, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.027559038918643784, 'bagging_temperature': 5.253648568373405}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:33,731] Trial 79 finished with value: 0.798497800090568 and parameters: {'iterations': 509, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09099805235033476, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.018168530187086654, 'bagging_temperature': 0.5773895364089878}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:39,753] Trial 80 finished with value: 0.7936306052965546 and parameters: {'iterations': 589, 'objective': 'Logloss', 'colsample_bylevel': 0.08654102645307511, 'depth': 2, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 1, 'learning_rate': 0.09974657781146326}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:41,305] Trial 81 finished with value: 0.7947330757920512 and parameters: {'iterations': 672, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08421934278792903, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.01738238160658219, 'bagging_temperature': 0.6075373609665661}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:42,515] Trial 82 finished with value: 0.7952736888595692 and parameters: {'iterations': 500, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08979168439239586, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.02314139496675074, 'bagging_temperature': 1.4211064872754613}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:43,782] Trial 83 finished with value: 0.7983524439400461 and parameters: {'iterations': 464, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09242685355745903, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.012438352900931282, 'bagging_temperature': 0.46696737690460466}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:46,779] Trial 84 finished with value: 0.7951283327090473 and parameters: {'iterations': 726, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09798464643811033, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.01013140469966109, 'bagging_temperature': 1.8369414312543395}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:48,219] Trial 85 finished with value: 0.7953569891150606 and parameters: {'iterations': 653, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08271667166062195, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.01779397447443576, 'bagging_temperature': 2.951776957961883}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:49,247] Trial 86 finished with value: 0.7970000726780754 and parameters: {'iterations': 373, 'objective': 'Logloss', 'colsample_bylevel': 0.07604987679409153, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.030298925080541987}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:51,266] Trial 87 finished with value: 0.7902611379150338 and parameters: {'iterations': 525, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0791074818556511, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.03965710148885972}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:20:53,545] Trial 88 finished with value: 0.7942545186195639 and parameters: {'iterations': 881, 'objective': 'Logloss', 'colsample_bylevel': 0.07217478255404995, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 5, 'learning_rate': 0.014138902544027991, 'subsample': 0.9809927619369401}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:01,497] Trial 89 finished with value: 0.791446908666022 and parameters: {'iterations': 815, 'objective': 'Logloss', 'colsample_bylevel': 0.08973945932782289, 'depth': 7, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03421964051721096, 'bagging_temperature': 0.006281511741987211}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:02,328] Trial 90 finished with value: 0.7975619301060541 and parameters: {'iterations': 287, 'objective': 'Logloss', 'colsample_bylevel': 0.09550451162795132, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.025546950923077655}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:05,292] Trial 91 finished with value: 0.7961475029490527 and parameters: {'iterations': 607, 'objective': 'Logloss', 'colsample_bylevel': 0.08586540256169577, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.03324352205310647}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:07,548] Trial 92 finished with value: 0.7972499734445495 and parameters: {'iterations': 632, 'objective': 'Logloss', 'colsample_bylevel': 0.08509429209064442, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.029005728256096514}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:09,062] Trial 93 finished with value: 0.7999122272475695 and parameters: {'iterations': 553, 'objective': 'Logloss', 'colsample_bylevel': 0.08764881020436978, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.020723094240527033}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:10,385] Trial 94 finished with value: 0.7991429577740383 and parameters: {'iterations': 516, 'objective': 'Logloss', 'colsample_bylevel': 0.0777521853839177, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.021702711175908676}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:11,829] Trial 95 finished with value: 0.7977285306170367 and parameters: {'iterations': 562, 'objective': 'Logloss', 'colsample_bylevel': 0.07663057277089082, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.013511944371345726}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:13,907] Trial 96 finished with value: 0.7955235896260434 and parameters: {'iterations': 707, 'objective': 'Logloss', 'colsample_bylevel': 0.07434585022604802, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.022830077793336644}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:15,473] Trial 97 finished with value: 0.7919042214780485 and parameters: {'iterations': 426, 'objective': 'Logloss', 'colsample_bylevel': 0.0826027585459206, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 17, 'learning_rate': 0.08812030640489991}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:18,618] Trial 98 finished with value: 0.7925281348010578 and parameters: {'iterations': 765, 'objective': 'Logloss', 'colsample_bylevel': 0.07781466750013243, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.0789931184131142}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:26,474] Trial 99 finished with value: 0.7943378188750553 and parameters: {'iterations': 930, 'objective': 'Logloss', 'colsample_bylevel': 0.08748712603954782, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.020399701757324515}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:27,703] Trial 100 finished with value: 0.7979571870230502 and parameters: {'iterations': 447, 'objective': 'Logloss', 'colsample_bylevel': 0.080936716584628, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.01587283822028981}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:28,820] Trial 101 finished with value: 0.7969380167830448 and parameters: {'iterations': 520, 'objective': 'Logloss', 'colsample_bylevel': 0.04889112750066275, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 2, 'learning_rate': 0.01908458412832179}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:30,729] Trial 102 finished with value: 0.7968547165275535 and parameters: {'iterations': 507, 'objective': 'Logloss', 'colsample_bylevel': 0.05510469518872156, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.026087390705549156}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:32,990] Trial 103 finished with value: 0.7973953295950713 and parameters: {'iterations': 587, 'objective': 'Logloss', 'colsample_bylevel': 0.08307588393727065, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.01711018836189462, 'bagging_temperature': 7.682982455119487}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:34,382] Trial 104 finished with value: 0.7949617321980645 and parameters: {'iterations': 550, 'objective': 'Logloss', 'colsample_bylevel': 0.09179813907437108, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.021822516868039633}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:35,273] Trial 105 finished with value: 0.7922782340345835 and parameters: {'iterations': 398, 'objective': 'Logloss', 'colsample_bylevel': 0.07976042361512883, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.06905088012598469, 'bagging_temperature': 0.8244110396354284}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:36,102] Trial 106 finished with value: 0.7725629084647596 and parameters: {'iterations': 482, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.01552539629137082, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 1, 'learning_rate': 0.012922205270087032, 'subsample': 0.7528552120890011}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:37,628] Trial 107 finished with value: 0.7968547165275535 and parameters: {'iterations': 688, 'objective': 'Logloss', 'colsample_bylevel': 0.06778451933990559, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.010129477528677858}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:38,892] Trial 108 finished with value: 0.7962928590995746 and parameters: {'iterations': 634, 'objective': 'Logloss', 'colsample_bylevel': 0.06337523162613826, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.02812564334822117, 'bagging_temperature': 1.3224783066671746}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:46,899] Trial 109 finished with value: 0.7929233917180539 and parameters: {'iterations': 837, 'objective': 'Logloss', 'colsample_bylevel': 0.08795348978804823, 'depth': 2, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.023956651511105508}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:48,519] Trial 110 finished with value: 0.7921328778840618 and parameters: {'iterations': 750, 'objective': 'Logloss', 'colsample_bylevel': 0.07220172894267556, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 2, 'learning_rate': 0.11162222388646609, 'bagging_temperature': 0.5822888079013447}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:50,703] Trial 111 finished with value: 0.7955235896260434 and parameters: {'iterations': 607, 'objective': 'Logloss', 'colsample_bylevel': 0.08566752745957948, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.03122359402143366}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:52,836] Trial 112 finished with value: 0.795440289370552 and parameters: {'iterations': 792, 'objective': 'Logloss', 'colsample_bylevel': 0.09070626802141361, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.01776201197078779}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:54,393] Trial 113 finished with value: 0.7987477008570422 and parameters: {'iterations': 659, 'objective': 'Logloss', 'colsample_bylevel': 0.07775432803947319, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.02051543900623696}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:55,770] Trial 114 finished with value: 0.7965427598660487 and parameters: {'iterations': 558, 'objective': 'Logloss', 'colsample_bylevel': 0.07560778479503584, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.015365619804809666}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:21:58,165] Trial 115 finished with value: 0.7957522460320566 and parameters: {'iterations': 651, 'objective': 'Logloss', 'colsample_bylevel': 0.07830675393758729, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.020928939096970533}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:00,373] Trial 116 finished with value: 0.7895326799760721 and parameters: {'iterations': 724, 'objective': 'Logloss', 'colsample_bylevel': 0.05796565500740045, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.037894317742958115}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:01,432] Trial 117 finished with value: 0.7977072862565759 and parameters: {'iterations': 474, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08107742804623316, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.043925087938048654, 'bagging_temperature': 2.1717103330752003}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:02,330] Trial 118 finished with value: 0.7943378188750553 and parameters: {'iterations': 330, 'objective': 'Logloss', 'colsample_bylevel': 0.07346683966077339, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.025341768908443765}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:02,835] Trial 119 finished with value: 0.7749132056062749 and parameters: {'iterations': 157, 'objective': 'Logloss', 'colsample_bylevel': 0.02771162499074936, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.018974918554914035, 'bagging_temperature': 2.7495167122556103}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:08,342] Trial 120 finished with value: 0.7980404872785416 and parameters: {'iterations': 669, 'objective': 'Logloss', 'colsample_bylevel': 0.07788929873051856, 'depth': 3, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.01225472170202399}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:10,246] Trial 121 finished with value: 0.7978738867675588 and parameters: {'iterations': 613, 'objective': 'Logloss', 'colsample_bylevel': 0.09595031585134176, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.028831408027398513}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:13,056] Trial 122 finished with value: 0.7957522460320566 and parameters: {'iterations': 577, 'objective': 'Logloss', 'colsample_bylevel': 0.08486157396824125, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.021956219139214268}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:14,898] Trial 123 finished with value: 0.7980404872785416 and parameters: {'iterations': 702, 'objective': 'Logloss', 'colsample_bylevel': 0.08351495583081865, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.016621829378396815}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:16,381] Trial 124 finished with value: 0.7949617321980645 and parameters: {'iterations': 539, 'objective': 'Logloss', 'colsample_bylevel': 0.08677623312930613, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.03126454084669337}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:18,707] Trial 125 finished with value: 0.7967093603770316 and parameters: {'iterations': 775, 'objective': 'Logloss', 'colsample_bylevel': 0.0797149533680745, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.05796760469366695, 'bagging_temperature': 6.829540433294687}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:20,045] Trial 126 finished with value: 0.7949617321980645 and parameters: {'iterations': 504, 'objective': 'Logloss', 'colsample_bylevel': 0.08900204129910362, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.024444522310169702}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:21,438] Trial 127 finished with value: 0.7987477008570422 and parameters: {'iterations': 641, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07563948737845486, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 2, 'learning_rate': 0.014289769326519708, 'subsample': 0.7906613073895298}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:24,746] Trial 128 finished with value: 0.7970000726780754 and parameters: {'iterations': 1441, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07146980298589099, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 2, 'learning_rate': 0.013200188313690572, 'subsample': 0.7343788330220616}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:27,395] Trial 129 finished with value: 0.7848320856930413 and parameters: {'iterations': 651, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07654062304968089, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 1, 'learning_rate': 0.13730592039820083, 'subsample': 0.8780037713426772}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:28,912] Trial 130 finished with value: 0.7971666731890581 and parameters: {'iterations': 874, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0743189168175396, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 2, 'learning_rate': 0.019706079339234944, 'subsample': 0.5926794175244339}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:30,318] Trial 131 finished with value: 0.7983524439400461 and parameters: {'iterations': 586, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08067255557764359, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.015601676576410449, 'subsample': 0.9799581075292896}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:31,963] Trial 132 finished with value: 0.7953569891150606 and parameters: {'iterations': 735, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08214138573547729, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 3, 'learning_rate': 0.0187779188624455, 'subsample': 0.6997959335777014}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:33,359] Trial 133 finished with value: 0.7958355462875479 and parameters: {'iterations': 630, 'objective': 'Logloss', 'colsample_bylevel': 0.0691575061463737, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.02212947112041874, 'bagging_temperature': 3.819688188921064}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:35,474] Trial 134 finished with value: 0.7963761593550659 and parameters: {'iterations': 681, 'objective': 'Logloss', 'colsample_bylevel': 0.07719162780174839, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.027653123409303305}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:41,338] Trial 135 finished with value: 0.7951283327090473 and parameters: {'iterations': 437, 'objective': 'Logloss', 'colsample_bylevel': 0.07898545202386764, 'depth': 6, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.010186009656249545, 'bagging_temperature': 0.44797065618409393}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:42,467] Trial 136 finished with value: 0.7945452309206076 and parameters: {'iterations': 611, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.06109222564143166, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 2, 'learning_rate': 0.015550483024141113}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:43,753] Trial 137 finished with value: 0.7983524439400461 and parameters: {'iterations': 518, 'objective': 'Logloss', 'colsample_bylevel': 0.07479616885969549, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.013053575658364477}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:45,310] Trial 138 finished with value: 0.7969167724225839 and parameters: {'iterations': 811, 'objective': 'Logloss', 'colsample_bylevel': 0.08387280858757287, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.02412963801929722, 'bagging_temperature': 1.4272840280190293}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:46,620] Trial 139 finished with value: 0.7935473050410631 and parameters: {'iterations': 556, 'objective': 'Logloss', 'colsample_bylevel': 0.08163739737787946, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 1, 'learning_rate': 0.01792684062338397}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:47,815] Trial 140 finished with value: 0.7882848533300535 and parameters: {'iterations': 708, 'objective': 'Logloss', 'colsample_bylevel': 0.0702014294235243, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 2, 'learning_rate': 0.07554517200313801, 'subsample': 0.8397901699951338}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:49,000] Trial 141 finished with value: 0.7973332737000408 and parameters: {'iterations': 453, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09206873512037346, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.012808718937898292, 'bagging_temperature': 0.35076792505065324}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:50,340] Trial 142 finished with value: 0.7986644006015509 and parameters: {'iterations': 470, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09427541584322881, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.020908459417357653, 'bagging_temperature': 0.9930484936266462}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:52,701] Trial 143 finished with value: 0.7953569891150606 and parameters: {'iterations': 483, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09928105834775343, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.022210014834793054, 'bagging_temperature': 1.1156174453076062}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:53,917] Trial 144 finished with value: 0.7978738867675588 and parameters: {'iterations': 365, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09430522193929411, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.02630484653482957, 'bagging_temperature': 1.8121789581297842}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:55,176] Trial 145 finished with value: 0.7949617321980645 and parameters: {'iterations': 525, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09636715798983539, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.020095968185485813, 'bagging_temperature': 0.8662243068315458}. Best is trial 6 with value: 0.8007027410815615.\n",
            "[I 2024-05-15 21:22:58,530] Trial 146 finished with value: 0.8008693415925444 and parameters: {'iterations': 584, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08624953575572648, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.01654772621755775, 'bagging_temperature': 8.709921196611926}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:00,958] Trial 147 finished with value: 0.7961475029490527 and parameters: {'iterations': 584, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08821174154280823, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.016535419519284944, 'bagging_temperature': 8.189777102249854}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:37,984] Trial 148 finished with value: 0.794087918108581 and parameters: {'iterations': 758, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0903540395616326, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.01023579542823762, 'bagging_temperature': 4.700387341931772}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:39,027] Trial 149 finished with value: 0.7889920669085542 and parameters: {'iterations': 550, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0421592909199842, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.014478125073195978, 'bagging_temperature': 5.739137412142824}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:40,392] Trial 150 finished with value: 0.7939425619580592 and parameters: {'iterations': 668, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07885361509709751, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.01894370435559894, 'bagging_temperature': 9.745893694340216}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:41,853] Trial 151 finished with value: 0.7951070883485865 and parameters: {'iterations': 622, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08604037669075679, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.022881540433354677, 'bagging_temperature': 0.12103116845643382}. Best is trial 146 with value: 0.8008693415925444.\n",
            "[I 2024-05-15 21:23:43,910] Trial 152 finished with value: 0.8010979979985575 and parameters: {'iterations': 411, 'objective': 'Logloss', 'colsample_bylevel': 0.09428136415105004, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.026184968834890562}. Best is trial 152 with value: 0.8010979979985575.\n",
            "[I 2024-05-15 21:23:48,283] Trial 153 finished with value: 0.802512425155559 and parameters: {'iterations': 464, 'objective': 'Logloss', 'colsample_bylevel': 0.09358077754026876, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.020718325226308343}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:23:51,430] Trial 154 finished with value: 0.8006194408260702 and parameters: {'iterations': 403, 'objective': 'Logloss', 'colsample_bylevel': 0.0946182851493781, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.026598953755189017}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:23:54,256] Trial 155 finished with value: 0.7986644006015509 and parameters: {'iterations': 351, 'objective': 'Logloss', 'colsample_bylevel': 0.09650545770608532, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.02648619146420096}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:23:55,329] Trial 156 finished with value: 0.7959188465430393 and parameters: {'iterations': 435, 'objective': 'Logloss', 'colsample_bylevel': 0.05277982213698448, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.03310988998065652}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:23:58,747] Trial 157 finished with value: 0.7891586674195371 and parameters: {'iterations': 406, 'objective': 'Logloss', 'colsample_bylevel': 0.09779151124320218, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.09465141565803936}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:23:59,999] Trial 158 finished with value: 0.7868508589989434 and parameters: {'iterations': 318, 'objective': 'Logloss', 'colsample_bylevel': 0.02003817042254918, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.02976193525270353}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:01,771] Trial 159 finished with value: 0.789699280487055 and parameters: {'iterations': 395, 'objective': 'Logloss', 'colsample_bylevel': 0.0764291603441424, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.052100108109120136}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:24,253] Trial 160 finished with value: 0.7925493791615187 and parameters: {'iterations': 830, 'objective': 'Logloss', 'colsample_bylevel': 0.09373298077161961, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.024539763774317647}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:27,526] Trial 161 finished with value: 0.801181298254049 and parameters: {'iterations': 485, 'objective': 'Logloss', 'colsample_bylevel': 0.09410245395953189, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.021312147233823332}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:29,951] Trial 162 finished with value: 0.7993716141800515 and parameters: {'iterations': 494, 'objective': 'Logloss', 'colsample_bylevel': 0.09179980877319342, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.015701634093653497}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:32,188] Trial 163 finished with value: 0.7958355462875479 and parameters: {'iterations': 457, 'objective': 'Logloss', 'colsample_bylevel': 0.09180545982703629, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.016976966833431743}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:35,486] Trial 164 finished with value: 0.7956068898815348 and parameters: {'iterations': 496, 'objective': 'Logloss', 'colsample_bylevel': 0.099907441571589, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.021105861368445368}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:39,135] Trial 165 finished with value: 0.7964594596105574 and parameters: {'iterations': 431, 'objective': 'Logloss', 'colsample_bylevel': 0.09364947596988892, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.0275166892011994}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:42,538] Trial 166 finished with value: 0.7964594596105574 and parameters: {'iterations': 398, 'objective': 'Logloss', 'colsample_bylevel': 0.09518071868641308, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.023886236295130186}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:43,478] Trial 167 finished with value: 0.7697340541507566 and parameters: {'iterations': 490, 'objective': 'Logloss', 'colsample_bylevel': 0.01205123933500074, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.019630237769083445}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:46,906] Trial 168 finished with value: 0.7977905865120675 and parameters: {'iterations': 538, 'objective': 'Logloss', 'colsample_bylevel': 0.08919366902769138, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.015019588296448075}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:49,597] Trial 169 finished with value: 0.7976452303615456 and parameters: {'iterations': 469, 'objective': 'Logloss', 'colsample_bylevel': 0.08675443264908592, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.01764220224353026}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:52,764] Trial 170 finished with value: 0.7983524439400461 and parameters: {'iterations': 380, 'objective': 'Logloss', 'colsample_bylevel': 0.09760445392926098, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.021893351388786265}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:55,509] Trial 171 finished with value: 0.7909471071330736 and parameters: {'iterations': 570, 'objective': 'Logloss', 'colsample_bylevel': 0.07999209503302938, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.08291692729771777}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:24:58,129] Trial 172 finished with value: 0.795045032453556 and parameters: {'iterations': 507, 'objective': 'Logloss', 'colsample_bylevel': 0.09278211132395603, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.012176684125896977}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:00,427] Trial 173 finished with value: 0.7980404872785416 and parameters: {'iterations': 659, 'objective': 'Logloss', 'colsample_bylevel': 0.07768915950222649, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.014586089873812505}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:02,737] Trial 174 finished with value: 0.7964594596105574 and parameters: {'iterations': 593, 'objective': 'Logloss', 'colsample_bylevel': 0.09054478555680169, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.01613884140194882}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:07,291] Trial 175 finished with value: 0.801576555171045 and parameters: {'iterations': 716, 'objective': 'Logloss', 'colsample_bylevel': 0.08294718594090841, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.013266001182862487}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:10,393] Trial 176 finished with value: 0.7958976021825785 and parameters: {'iterations': 785, 'objective': 'Logloss', 'colsample_bylevel': 0.08378883512142175, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.025407649193311087}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:13,151] Trial 177 finished with value: 0.798497800090568 and parameters: {'iterations': 734, 'objective': 'Logloss', 'colsample_bylevel': 0.08198631930395738, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.019453824920020412}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:17,634] Trial 178 finished with value: 0.7971666731890581 and parameters: {'iterations': 705, 'objective': 'Logloss', 'colsample_bylevel': 0.08768597083140575, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.011931917538166751}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:22,362] Trial 179 finished with value: 0.7929233917180539 and parameters: {'iterations': 436, 'objective': 'Logloss', 'colsample_bylevel': 0.048780038871417464, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.029650540759107225}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:24,240] Trial 180 finished with value: 0.794109162469042 and parameters: {'iterations': 548, 'objective': 'Logloss', 'colsample_bylevel': 0.08488198947782372, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.022242278095823215}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:27,015] Trial 181 finished with value: 0.7980404872785416 and parameters: {'iterations': 646, 'objective': 'Logloss', 'colsample_bylevel': 0.08030981155469159, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.013648802342134126}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:30,254] Trial 182 finished with value: 0.7977285306170367 and parameters: {'iterations': 860, 'objective': 'Logloss', 'colsample_bylevel': 0.07438960121762228, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.016778005792092206}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:33,696] Trial 183 finished with value: 0.7983524439400461 and parameters: {'iterations': 682, 'objective': 'Logloss', 'colsample_bylevel': 0.07302095620198165, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 8, 'learning_rate': 0.010230269883732313, 'subsample': 0.5432956992757418}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:35,286] Trial 184 finished with value: 0.7977905865120675 and parameters: {'iterations': 742, 'objective': 'Logloss', 'colsample_bylevel': 0.07802937076612643, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 7, 'learning_rate': 0.01902295473232737}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:36,904] Trial 185 finished with value: 0.7994549144355431 and parameters: {'iterations': 518, 'objective': 'Logloss', 'colsample_bylevel': 0.0762807167512939, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.014536548600464515}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:38,630] Trial 186 finished with value: 0.7964594596105574 and parameters: {'iterations': 523, 'objective': 'Logloss', 'colsample_bylevel': 0.08245536955008904, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.01666459154344807}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:40,548] Trial 187 finished with value: 0.7972499734445495 and parameters: {'iterations': 458, 'objective': 'Logloss', 'colsample_bylevel': 0.09149165662753826, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 17, 'learning_rate': 0.020920526144021102}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:42,474] Trial 188 finished with value: 0.795440289370552 and parameters: {'iterations': 488, 'objective': 'Logloss', 'colsample_bylevel': 0.07990985610137867, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.024051530036360222}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:43,407] Trial 189 finished with value: 0.7885559984569886 and parameters: {'iterations': 418, 'objective': 'Logloss', 'colsample_bylevel': 0.03408444685195686, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.013360978433920869}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:46,881] Trial 190 finished with value: 0.7903231938100642 and parameters: {'iterations': 800, 'objective': 'Logloss', 'colsample_bylevel': 0.09490995285755921, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.026385221545681133}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:48,198] Trial 191 finished with value: 0.7970833729335668 and parameters: {'iterations': 604, 'objective': 'Logloss', 'colsample_bylevel': 0.07585178202310087, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.014473296315197035}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:49,383] Trial 192 finished with value: 0.7974786298505627 and parameters: {'iterations': 558, 'objective': 'Logloss', 'colsample_bylevel': 0.07612645675551459, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.018018555074280865}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:52,386] Trial 193 finished with value: 0.7968547165275535 and parameters: {'iterations': 710, 'objective': 'Logloss', 'colsample_bylevel': 0.07823405097490732, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.01496318258671624}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:54,275] Trial 194 finished with value: 0.7964594596105574 and parameters: {'iterations': 516, 'objective': 'Logloss', 'colsample_bylevel': 0.08121543349808351, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 7, 'learning_rate': 0.01184808412321172, 'subsample': 0.7648450868637939}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:55,875] Trial 195 finished with value: 0.795045032453556 and parameters: {'iterations': 637, 'objective': 'Logloss', 'colsample_bylevel': 0.07301058188202635, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.01983741136697786}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:25:58,820] Trial 196 finished with value: 0.790635150471569 and parameters: {'iterations': 591, 'objective': 'Logloss', 'colsample_bylevel': 0.08351941212765214, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.022938150710083205}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:05,509] Trial 197 finished with value: 0.7964594596105574 and parameters: {'iterations': 538, 'objective': 'Logloss', 'colsample_bylevel': 0.07723342291729225, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.016148108964122436}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:06,495] Trial 198 finished with value: 0.7943998747700857 and parameters: {'iterations': 474, 'objective': 'Logloss', 'colsample_bylevel': 0.08920144081465997, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 2, 'learning_rate': 0.01020795356459574}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:08,172] Trial 199 finished with value: 0.7963761593550659 and parameters: {'iterations': 774, 'objective': 'Logloss', 'colsample_bylevel': 0.07902083550695084, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.017447106228465056}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:09,513] Trial 200 finished with value: 0.7956689457765652 and parameters: {'iterations': 677, 'objective': 'Logloss', 'colsample_bylevel': 0.06583056378323646, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.02099410838852378}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:11,327] Trial 201 finished with value: 0.7971666731890581 and parameters: {'iterations': 461, 'objective': 'Logloss', 'colsample_bylevel': 0.09396030214473503, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.020477311092716926, 'bagging_temperature': 8.636747440241036}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:15,913] Trial 202 finished with value: 0.7956689457765652 and parameters: {'iterations': 495, 'objective': 'Logloss', 'colsample_bylevel': 0.09693542619427156, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.013584375745438193, 'bagging_temperature': 3.3439630101327236}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:16,975] Trial 203 finished with value: 0.7955856455210739 and parameters: {'iterations': 425, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09271957477633003, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.024299139423593893, 'bagging_temperature': 6.386627799835509}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:17,845] Trial 204 finished with value: 0.7875155838565223 and parameters: {'iterations': 463, 'objective': 'Logloss', 'colsample_bylevel': 0.02302218749989171, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0280920584853847, 'bagging_temperature': 4.3256695542320145}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:19,191] Trial 205 finished with value: 0.7963761593550659 and parameters: {'iterations': 571, 'objective': 'Logloss', 'colsample_bylevel': 0.09402093459885776, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.019554747964324725, 'bagging_temperature': 2.582386874511095}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:20,355] Trial 206 finished with value: 0.7987264564965812 and parameters: {'iterations': 523, 'objective': 'Logloss', 'colsample_bylevel': 0.09594394305474485, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.01554353150517305, 'bagging_temperature': 1.9490123663551926}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:21,415] Trial 207 finished with value: 0.7952736888595692 and parameters: {'iterations': 508, 'objective': 'Logloss', 'colsample_bylevel': 0.09859510857405, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 7, 'learning_rate': 0.015686475675578397, 'subsample': 0.46021979879936675}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:22,775] Trial 208 finished with value: 0.7978738867675588 and parameters: {'iterations': 623, 'objective': 'Logloss', 'colsample_bylevel': 0.07508087348508735, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 3, 'learning_rate': 0.012783028491539651}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:25,928] Trial 209 finished with value: 0.7978738867675588 and parameters: {'iterations': 1138, 'objective': 'Logloss', 'colsample_bylevel': 0.09635759903428565, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.017712559010360507, 'bagging_temperature': 2.162540301282031}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:27,759] Trial 210 finished with value: 0.7976452303615456 and parameters: {'iterations': 530, 'objective': 'Logloss', 'colsample_bylevel': 0.08083719698708727, 'depth': 4, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 4, 'learning_rate': 0.010115714368351642}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:28,885] Trial 211 finished with value: 0.796230803204544 and parameters: {'iterations': 493, 'objective': 'Logloss', 'colsample_bylevel': 0.0953764953349407, 'depth': 3, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 18, 'learning_rate': 0.06653666777398666, 'bagging_temperature': 1.6904933137887408}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:32,539] Trial 212 finished with value: 0.7977285306170367 and parameters: {'iterations': 557, 'objective': 'Logloss', 'colsample_bylevel': 0.090679317042649, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.02280726663839701, 'bagging_temperature': 1.1340740285196054}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:33,870] Trial 213 finished with value: 0.7999122272475695 and parameters: {'iterations': 447, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09493157293601115, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.01897634800681009, 'bagging_temperature': 2.104216704678866}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:35,199] Trial 214 finished with value: 0.8003907844200568 and parameters: {'iterations': 444, 'objective': 'Logloss', 'colsample_bylevel': 0.09730918878072006, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.0151542000989894, 'bagging_temperature': 2.067200189305824}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:36,367] Trial 215 finished with value: 0.7944211191305467 and parameters: {'iterations': 374, 'objective': 'Logloss', 'colsample_bylevel': 0.0977680710404765, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.01833970533744153, 'bagging_temperature': 3.189942369297511}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:37,863] Trial 216 finished with value: 0.7959188465430393 and parameters: {'iterations': 414, 'objective': 'Logloss', 'colsample_bylevel': 0.09228335928212535, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.014693978164294302}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:43,385] Trial 217 finished with value: 0.7970000726780754 and parameters: {'iterations': 447, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08507564614089227, 'depth': 7, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.025623845230839434, 'bagging_temperature': 2.6587682117543174}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:44,229] Trial 218 finished with value: 0.794899676303034 and parameters: {'iterations': 316, 'objective': 'Logloss', 'colsample_bylevel': 0.05983581923763635, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.012910213918502812}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:46,204] Trial 219 finished with value: 0.7886801102470496 and parameters: {'iterations': 826, 'objective': 'Logloss', 'colsample_bylevel': 0.07869668151641371, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 0, 'learning_rate': 0.021857133108189517}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:48,412] Trial 220 finished with value: 0.7889920669085542 and parameters: {'iterations': 717, 'objective': 'Logloss', 'colsample_bylevel': 0.08236530517952027, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.1228607780568832, 'bagging_temperature': 3.541189476989449}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:49,734] Trial 221 finished with value: 0.7987477008570422 and parameters: {'iterations': 446, 'objective': 'Logloss', 'colsample_bylevel': 0.09683422716693676, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.01671378515136324, 'bagging_temperature': 1.9296123220366825}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:51,018] Trial 222 finished with value: 0.7993716141800515 and parameters: {'iterations': 394, 'objective': 'Logloss', 'colsample_bylevel': 0.09508394619102117, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.01854663834952135, 'bagging_temperature': 1.9903770767983973}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:53,096] Trial 223 finished with value: 0.7944211191305467 and parameters: {'iterations': 389, 'objective': 'Logloss', 'colsample_bylevel': 0.0937569018366992, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.019945440857614235, 'bagging_temperature': 1.4006784963657246}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:54,407] Trial 224 finished with value: 0.7978738867675588 and parameters: {'iterations': 339, 'objective': 'Logloss', 'colsample_bylevel': 0.09878536239714869, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.018079853179060974, 'bagging_temperature': 2.395395279701832}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:55,798] Trial 225 finished with value: 0.8000788277585523 and parameters: {'iterations': 423, 'objective': 'Logloss', 'colsample_bylevel': 0.07699241325808254, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.014200962919774298, 'bagging_temperature': 1.5494215572093457}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:57,491] Trial 226 finished with value: 0.7994549144355431 and parameters: {'iterations': 415, 'objective': 'Logloss', 'colsample_bylevel': 0.0953467136511971, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.023106392057500084, 'bagging_temperature': 1.6146095314565645}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:26:59,180] Trial 227 finished with value: 0.7993716141800515 and parameters: {'iterations': 417, 'objective': 'Logloss', 'colsample_bylevel': 0.09577794347803274, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.02339624469562361, 'bagging_temperature': 1.5787600038085483}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:00,663] Trial 228 finished with value: 0.7952116329645387 and parameters: {'iterations': 358, 'objective': 'Logloss', 'colsample_bylevel': 0.09553745790655924, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.026174859342403607, 'bagging_temperature': 2.1201914761802803}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:02,275] Trial 229 finished with value: 0.7985811003460594 and parameters: {'iterations': 409, 'objective': 'Logloss', 'colsample_bylevel': 0.09310248727004367, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.023521412251649707, 'bagging_temperature': 1.4291284316190809}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:03,969] Trial 230 finished with value: 0.7991429577740383 and parameters: {'iterations': 381, 'objective': 'Logloss', 'colsample_bylevel': 0.0947668550829512, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03145280896238583, 'bagging_temperature': 1.664802852682874}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:06,757] Trial 231 finished with value: 0.7999955275030608 and parameters: {'iterations': 379, 'objective': 'Logloss', 'colsample_bylevel': 0.09422897200654888, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.027481730799659737, 'bagging_temperature': 1.7036631447673878}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:08,394] Trial 232 finished with value: 0.8000788277585523 and parameters: {'iterations': 387, 'objective': 'Logloss', 'colsample_bylevel': 0.09506316123329694, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03307889239358011, 'bagging_temperature': 1.744698114387914}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:10,112] Trial 233 finished with value: 0.8001621280140436 and parameters: {'iterations': 408, 'objective': 'Logloss', 'colsample_bylevel': 0.09760913974922553, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03331366581931541, 'bagging_temperature': 1.6191412422465017}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:11,616] Trial 234 finished with value: 0.7982691436845549 and parameters: {'iterations': 358, 'objective': 'Logloss', 'colsample_bylevel': 0.09776231957620295, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03489304814149981, 'bagging_temperature': 1.5762695842323715}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:13,367] Trial 235 finished with value: 0.7996002705860649 and parameters: {'iterations': 405, 'objective': 'Logloss', 'colsample_bylevel': 0.0959669315400745, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030402707670007162, 'bagging_temperature': 2.044469550751977}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:15,054] Trial 236 finished with value: 0.8010359421035271 and parameters: {'iterations': 399, 'objective': 'Logloss', 'colsample_bylevel': 0.09953274897874616, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.032837766719180116, 'bagging_temperature': 2.03385962616653}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:16,746] Trial 237 finished with value: 0.7980404872785416 and parameters: {'iterations': 292, 'objective': 'Logloss', 'colsample_bylevel': 0.09941035692163479, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03837074176455607, 'bagging_temperature': 2.175818223111684}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:19,470] Trial 238 finished with value: 0.8003907844200568 and parameters: {'iterations': 404, 'objective': 'Logloss', 'colsample_bylevel': 0.09764558789940508, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03522206556317899, 'bagging_temperature': 1.953470615988909}. Best is trial 153 with value: 0.802512425155559.\n",
            "[I 2024-05-15 21:27:21,217] Trial 239 finished with value: 0.8025957254110503 and parameters: {'iterations': 348, 'objective': 'Logloss', 'colsample_bylevel': 0.09947393619265593, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03606336046378164, 'bagging_temperature': 2.4737097189561466}. Best is trial 239 with value: 0.8025957254110503.\n",
            "[I 2024-05-15 21:27:22,811] Trial 240 finished with value: 0.7990596575185469 and parameters: {'iterations': 370, 'objective': 'Logloss', 'colsample_bylevel': 0.09909125185327203, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03357822716172121, 'bagging_temperature': 2.5531191550640298}. Best is trial 239 with value: 0.8025957254110503.\n",
            "[I 2024-05-15 21:27:24,231] Trial 241 finished with value: 0.7966881160165706 and parameters: {'iterations': 346, 'objective': 'Logloss', 'colsample_bylevel': 0.09761584792182268, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03617085383366374, 'bagging_temperature': 2.3804287528173167}. Best is trial 239 with value: 0.8025957254110503.\n",
            "[I 2024-05-15 21:27:25,993] Trial 242 finished with value: 0.7963761593550659 and parameters: {'iterations': 418, 'objective': 'Logloss', 'colsample_bylevel': 0.09756969713301643, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03141009432148881, 'bagging_temperature': 2.844006058529231}. Best is trial 239 with value: 0.8025957254110503.\n",
            "[I 2024-05-15 21:27:27,610] Trial 243 finished with value: 0.7967714162720619 and parameters: {'iterations': 379, 'objective': 'Logloss', 'colsample_bylevel': 0.0992536206697605, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03712562458769748, 'bagging_temperature': 1.2562804552012583}. Best is trial 239 with value: 0.8025957254110503.\n",
            "[I 2024-05-15 21:27:28,675] Trial 244 finished with value: 0.8026790256665418 and parameters: {'iterations': 227, 'objective': 'Logloss', 'colsample_bylevel': 0.09666745315589911, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04118873632717417, 'bagging_temperature': 1.7773228599484086}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:29,870] Trial 245 finished with value: 0.7969380167830448 and parameters: {'iterations': 251, 'objective': 'Logloss', 'colsample_bylevel': 0.09987826273884232, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04187260807776328, 'bagging_temperature': 1.8634900131201817}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:36,314] Trial 246 finished with value: 0.7935473050410631 and parameters: {'iterations': 330, 'objective': 'Logloss', 'colsample_bylevel': 0.0963172162790224, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03331787592674617, 'bagging_temperature': 2.0783644489417608}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:37,304] Trial 247 finished with value: 0.7972499734445495 and parameters: {'iterations': 187, 'objective': 'Logloss', 'colsample_bylevel': 0.09751030039310915, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0296391479495028, 'bagging_temperature': 1.7099068008233265}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:39,038] Trial 248 finished with value: 0.7974165739555322 and parameters: {'iterations': 433, 'objective': 'Logloss', 'colsample_bylevel': 0.09460199329787171, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03474962935387361, 'bagging_temperature': 2.3558315023991483}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:40,224] Trial 249 finished with value: 0.796230803204544 and parameters: {'iterations': 266, 'objective': 'Logloss', 'colsample_bylevel': 0.09630684273564313, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.039320180589938726, 'bagging_temperature': 1.4929455329676518}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:41,933] Trial 250 finished with value: 0.8023670690050372 and parameters: {'iterations': 405, 'objective': 'Logloss', 'colsample_bylevel': 0.09778444705476726, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.046504359668240305, 'bagging_temperature': 1.2199204857139103}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:44,473] Trial 251 finished with value: 0.7946497755365599 and parameters: {'iterations': 384, 'objective': 'Logloss', 'colsample_bylevel': 0.09981029918881808, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.044190683860401175, 'bagging_temperature': 0.848790393175977}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:47,004] Trial 252 finished with value: 0.7977905865120675 and parameters: {'iterations': 300, 'objective': 'Logloss', 'colsample_bylevel': 0.09754868573925105, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.035455792998075907, 'bagging_temperature': 1.9127032347938073}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:48,100] Trial 253 finished with value: 0.7951283327090473 and parameters: {'iterations': 213, 'objective': 'Logloss', 'colsample_bylevel': 0.0979199618379821, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04103016186818047, 'bagging_temperature': 1.2733247808378114}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:49,790] Trial 254 finished with value: 0.7952736888595692 and parameters: {'iterations': 433, 'objective': 'Logloss', 'colsample_bylevel': 0.09317268800920443, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03064103060647795, 'bagging_temperature': 1.0271398956360167}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:50,543] Trial 255 finished with value: 0.7978738867675588 and parameters: {'iterations': 125, 'objective': 'Logloss', 'colsample_bylevel': 0.09979189983069155, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04666841560403085, 'bagging_temperature': 2.540159757430477}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:52,536] Trial 256 finished with value: 0.7992883139245601 and parameters: {'iterations': 395, 'objective': 'Logloss', 'colsample_bylevel': 0.09660778902376042, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03186757823028214, 'bagging_temperature': 2.208835283440988}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:27:58,458] Trial 257 finished with value: 0.7929854476130842 and parameters: {'iterations': 451, 'objective': 'Logloss', 'colsample_bylevel': 0.09289240759085894, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03807562179625992, 'bagging_temperature': 1.2168026391044675}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:00,895] Trial 258 finished with value: 0.801576555171045 and parameters: {'iterations': 349, 'objective': 'Logloss', 'colsample_bylevel': 0.09530128603526154, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029436412823493973, 'bagging_temperature': 1.761299145126514}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:02,376] Trial 259 finished with value: 0.7973332737000408 and parameters: {'iterations': 346, 'objective': 'Logloss', 'colsample_bylevel': 0.09489577869578147, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 16, 'learning_rate': 0.02871555631049232, 'bagging_temperature': 1.7835301748339332}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:03,959] Trial 260 finished with value: 0.8021171682385629 and parameters: {'iterations': 361, 'objective': 'Logloss', 'colsample_bylevel': 0.09736485516761872, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03379614441734536, 'bagging_temperature': 2.0086436240470564}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:04,876] Trial 261 finished with value: 0.8003287285250263 and parameters: {'iterations': 327, 'objective': 'Logloss', 'colsample_bylevel': 0.05607892295273043, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.035377934162688474, 'bagging_temperature': 1.4912747960842254}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:06,330] Trial 262 finished with value: 0.7970213170385362 and parameters: {'iterations': 329, 'objective': 'Logloss', 'colsample_bylevel': 0.09749590024542223, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03478248971191581, 'bagging_temperature': 0.743945013891789}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:07,247] Trial 263 finished with value: 0.7974786298505627 and parameters: {'iterations': 358, 'objective': 'Logloss', 'colsample_bylevel': 0.05425208448500679, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.039334892856643364, 'bagging_temperature': 1.7605973755106767}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:08,148] Trial 264 finished with value: 0.7953569891150606 and parameters: {'iterations': 305, 'objective': 'Logloss', 'colsample_bylevel': 0.05743025690257667, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.033650145176605854, 'bagging_temperature': 2.2382633469915243}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:09,217] Trial 265 finished with value: 0.79598090243807 and parameters: {'iterations': 239, 'objective': 'Logloss', 'colsample_bylevel': 0.09120440518106342, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04175370737066544, 'bagging_temperature': 1.518112235071315}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:10,938] Trial 266 finished with value: 0.8010146977430661 and parameters: {'iterations': 363, 'objective': 'Logloss', 'colsample_bylevel': 0.09989272612689264, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03213567574808816, 'bagging_temperature': 1.9450449741041487}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:13,665] Trial 267 finished with value: 0.7992883139245601 and parameters: {'iterations': 358, 'objective': 'Logloss', 'colsample_bylevel': 0.09841382917289625, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03674104614013208, 'bagging_temperature': 1.9001379788372799}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:15,047] Trial 268 finished with value: 0.8004740846755483 and parameters: {'iterations': 322, 'objective': 'Logloss', 'colsample_bylevel': 0.09999246065647216, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03251807994928863, 'bagging_temperature': 2.0526032266159793}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:16,467] Trial 269 finished with value: 0.7981858434290635 and parameters: {'iterations': 319, 'objective': 'Logloss', 'colsample_bylevel': 0.09960933651196391, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.032592629108811194, 'bagging_temperature': 1.3567077208146214}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:17,793] Trial 270 finished with value: 0.795440289370552 and parameters: {'iterations': 294, 'objective': 'Logloss', 'colsample_bylevel': 0.09836585619991242, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03641823840743978, 'bagging_temperature': 1.6708840931965345}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:23,497] Trial 271 finished with value: 0.7968547165275535 and parameters: {'iterations': 342, 'objective': 'Logloss', 'colsample_bylevel': 0.09685618081615675, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.028298391419270208, 'bagging_temperature': 2.4524335929367838}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:27,252] Trial 272 finished with value: 0.7932353483795584 and parameters: {'iterations': 379, 'objective': 'Logloss', 'colsample_bylevel': 0.09994102867504692, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.032429674766929074, 'bagging_temperature': 0.3429051222064663}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:28,084] Trial 273 finished with value: 0.7911970078995477 and parameters: {'iterations': 395, 'objective': 'Logloss', 'colsample_bylevel': 0.029450380744388886, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02916240985770506, 'bagging_temperature': 2.7153267685720337}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:29,473] Trial 274 finished with value: 0.800786041337053 and parameters: {'iterations': 317, 'objective': 'Logloss', 'colsample_bylevel': 0.09713812172359701, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.037388975071049796, 'bagging_temperature': 2.0267887178873885}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:30,255] Trial 275 finished with value: 0.7938592617025677 and parameters: {'iterations': 277, 'objective': 'Logloss', 'colsample_bylevel': 0.04628878227491309, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.038036022208036174, 'bagging_temperature': 1.9981261203219234}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:31,656] Trial 276 finished with value: 0.8007239854420224 and parameters: {'iterations': 313, 'objective': 'Logloss', 'colsample_bylevel': 0.09782433672644451, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.041648479206146005, 'bagging_temperature': 2.278851267379946}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:33,039] Trial 277 finished with value: 0.8009526418480357 and parameters: {'iterations': 318, 'objective': 'Logloss', 'colsample_bylevel': 0.09769076936941741, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04643410156146737, 'bagging_temperature': 2.266480793309709}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:34,248] Trial 278 finished with value: 0.7999122272475695 and parameters: {'iterations': 247, 'objective': 'Logloss', 'colsample_bylevel': 0.09806799098083965, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04600290104407843, 'bagging_temperature': 2.2965363104819803}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:35,646] Trial 279 finished with value: 0.801576555171045 and parameters: {'iterations': 318, 'objective': 'Logloss', 'colsample_bylevel': 0.09980365608629892, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.049450607205971206, 'bagging_temperature': 2.0960834885487913}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:37,622] Trial 280 finished with value: 0.7963141034600354 and parameters: {'iterations': 312, 'objective': 'Logloss', 'colsample_bylevel': 0.09996758656858715, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.051573900885797, 'bagging_temperature': 1.9904055403956427}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:40,299] Trial 281 finished with value: 0.7971046172940276 and parameters: {'iterations': 314, 'objective': 'Logloss', 'colsample_bylevel': 0.09794607564283121, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04362492775480262, 'bagging_temperature': 2.890612314565908}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:41,645] Trial 282 finished with value: 0.7982070877895242 and parameters: {'iterations': 279, 'objective': 'Logloss', 'colsample_bylevel': 0.09673664622649165, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04930036331054287, 'bagging_temperature': 2.3398575385268443}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:45,800] Trial 283 finished with value: 0.7973953295950713 and parameters: {'iterations': 335, 'objective': 'Logloss', 'colsample_bylevel': 0.0977822076849441, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04250498054188677, 'bagging_temperature': 5.576714207781131}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:46,981] Trial 284 finished with value: 0.7994549144355431 and parameters: {'iterations': 234, 'objective': 'Logloss', 'colsample_bylevel': 0.09835364744658388, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04196062841224397, 'bagging_temperature': 7.13596609823683}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:48,591] Trial 285 finished with value: 0.800245428269535 and parameters: {'iterations': 273, 'objective': 'Logloss', 'colsample_bylevel': 0.09988900735007211, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.05776600736831453, 'bagging_temperature': 2.0464938530014076}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:50,139] Trial 286 finished with value: 0.7950662768140169 and parameters: {'iterations': 267, 'objective': 'Logloss', 'colsample_bylevel': 0.09979752037994345, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.05619359697043886, 'bagging_temperature': 2.252563011397681}. Best is trial 244 with value: 0.8026790256665418.\n",
            "[I 2024-05-15 21:28:52,989] Trial 287 finished with value: 0.8027623259220331 and parameters: {'iterations': 312, 'objective': 'Logloss', 'colsample_bylevel': 0.09614388899341661, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.045854133048602905, 'bagging_temperature': 2.079212172184084}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:28:54,402] Trial 288 finished with value: 0.7975619301060541 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.0961361375342329, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.050986608460923344, 'bagging_temperature': 2.094089784272571}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:28:56,067] Trial 289 finished with value: 0.7969380167830448 and parameters: {'iterations': 297, 'objective': 'Logloss', 'colsample_bylevel': 0.09828525751528035, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04458869759234772, 'bagging_temperature': 2.58755980354468}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:28:57,656] Trial 290 finished with value: 0.7924660789060273 and parameters: {'iterations': 272, 'objective': 'Logloss', 'colsample_bylevel': 0.0998146411568452, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.059602349274479174, 'bagging_temperature': 1.8656981919454654}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:28:59,052] Trial 291 finished with value: 0.8003074841645655 and parameters: {'iterations': 324, 'objective': 'Logloss', 'colsample_bylevel': 0.09617698104189472, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04581934838830577, 'bagging_temperature': 2.1011105647729424}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:00,455] Trial 292 finished with value: 0.7970213170385362 and parameters: {'iterations': 326, 'objective': 'Logloss', 'colsample_bylevel': 0.09621715097857085, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04792258979883734, 'bagging_temperature': 2.4896004338179605}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:01,704] Trial 293 finished with value: 0.7951283327090473 and parameters: {'iterations': 296, 'objective': 'Logloss', 'colsample_bylevel': 0.09418974364483958, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.054303199257356057, 'bagging_temperature': 2.104816957172492}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:03,682] Trial 294 finished with value: 0.7974165739555322 and parameters: {'iterations': 353, 'objective': 'Logloss', 'colsample_bylevel': 0.09641518860146558, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04959086243035252, 'bagging_temperature': 1.9673799328555028}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:05,969] Trial 295 finished with value: 0.7953782334755214 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.09994741212744283, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04721831429130553, 'bagging_temperature': 9.332641772669609}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:09,968] Trial 296 finished with value: 0.7995382146910344 and parameters: {'iterations': 323, 'objective': 'Logloss', 'colsample_bylevel': 0.09306224125308259, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.045893065251693115, 'bagging_temperature': 2.2568625125252457}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:11,979] Trial 297 finished with value: 0.8018885118325497 and parameters: {'iterations': 277, 'objective': 'Logloss', 'colsample_bylevel': 0.09613792939409746, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.040322272631632745, 'bagging_temperature': 3.1012398405343418}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:14,705] Trial 298 finished with value: 0.800245428269535 and parameters: {'iterations': 348, 'objective': 'Logloss', 'colsample_bylevel': 0.09500133828019158, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04110136001617062, 'bagging_temperature': 3.1665070842055982}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:17,689] Trial 299 finished with value: 0.7955235896260434 and parameters: {'iterations': 301, 'objective': 'Logloss', 'colsample_bylevel': 0.09664025061071076, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04322085515615812, 'bagging_temperature': 2.7667307447083846}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:21,084] Trial 300 finished with value: 0.8014932549155536 and parameters: {'iterations': 332, 'objective': 'Logloss', 'colsample_bylevel': 0.09425479681015807, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03859563047198614, 'bagging_temperature': 2.9749366430819997}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:23,550] Trial 301 finished with value: 0.7932353483795584 and parameters: {'iterations': 341, 'objective': 'Logloss', 'colsample_bylevel': 0.09247430645131918, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.039736734894740805, 'bagging_temperature': 3.520628442790114}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:26,074] Trial 302 finished with value: 0.799850171352539 and parameters: {'iterations': 361, 'objective': 'Logloss', 'colsample_bylevel': 0.09363637508422458, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.038781321966438624, 'bagging_temperature': 2.6660386361748865}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:26,743] Trial 303 finished with value: 0.7948163760475426 and parameters: {'iterations': 258, 'objective': 'Logloss', 'colsample_bylevel': 0.036132789440347306, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.035914446236455365, 'bagging_temperature': 2.474602127028131}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:27,269] Trial 304 finished with value: 0.7695674536397739 and parameters: {'iterations': 167, 'objective': 'Logloss', 'colsample_bylevel': 0.016940198384075913, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04059376162721307, 'bagging_temperature': 8.082621193728215}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:29,857] Trial 305 finished with value: 0.8022837687495458 and parameters: {'iterations': 293, 'objective': 'Logloss', 'colsample_bylevel': 0.09800347342184366, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.037510258267892956, 'bagging_temperature': 2.8904604666230775}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:33,416] Trial 306 finished with value: 0.7984357441955376 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.0978479062118123, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03826182228240302, 'bagging_temperature': 3.070532617252945}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:35,781] Trial 307 finished with value: 0.8015144992760146 and parameters: {'iterations': 296, 'objective': 'Logloss', 'colsample_bylevel': 0.09461033532710778, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.040493318651678115, 'bagging_temperature': 2.958953082052567}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:52,405] Trial 308 finished with value: 0.7942545186195639 and parameters: {'iterations': 291, 'objective': 'Logloss', 'colsample_bylevel': 0.09473094513443739, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04327455974435668, 'bagging_temperature': 2.901462306782931}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:54,369] Trial 309 finished with value: 0.7980404872785416 and parameters: {'iterations': 267, 'objective': 'Logloss', 'colsample_bylevel': 0.09171895778294897, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0398986585548611, 'bagging_temperature': 3.2473054698866823}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:56,681] Trial 310 finished with value: 0.7960021467985308 and parameters: {'iterations': 307, 'objective': 'Logloss', 'colsample_bylevel': 0.09504727206970577, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.0444331078790668, 'bagging_temperature': 4.305858121686412}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:29:59,934] Trial 311 finished with value: 0.7955856455210739 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09672894334439425, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.048330768868204345, 'bagging_temperature': 3.0745978655020654}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:02,456] Trial 312 finished with value: 0.794504419386038 and parameters: {'iterations': 362, 'objective': 'Logloss', 'colsample_bylevel': 0.09301082550039336, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.08860930163408023, 'bagging_temperature': 2.9015757637693467}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:04,853] Trial 313 finished with value: 0.7979571870230502 and parameters: {'iterations': 294, 'objective': 'Logloss', 'colsample_bylevel': 0.09832879597406717, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03955574708552002, 'bagging_temperature': 3.4245367909175566}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:06,666] Trial 314 finished with value: 0.7937972058075372 and parameters: {'iterations': 331, 'objective': 'Logloss', 'colsample_bylevel': 0.09431391263694398, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04179050908532115, 'bagging_temperature': 3.708924120116957}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:08,549] Trial 315 finished with value: 0.8005573849310398 and parameters: {'iterations': 261, 'objective': 'Logloss', 'colsample_bylevel': 0.09043256253284948, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03679745272030328, 'bagging_temperature': 2.703359407492341}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:09,970] Trial 316 finished with value: 0.7977072862565759 and parameters: {'iterations': 185, 'objective': 'Logloss', 'colsample_bylevel': 0.09004655491375915, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.037651654878493736, 'bagging_temperature': 2.650646743771989}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:13,419] Trial 317 finished with value: 0.8022004684940542 and parameters: {'iterations': 255, 'objective': 'Logloss', 'colsample_bylevel': 0.09151059302713736, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03709078788023773, 'bagging_temperature': 2.7435342560753293}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:15,322] Trial 318 finished with value: 0.7962928590995746 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.08990156973123152, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03716571963371576, 'bagging_temperature': 2.9847315376810903}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:17,269] Trial 319 finished with value: 0.8008693415925444 and parameters: {'iterations': 250, 'objective': 'Logloss', 'colsample_bylevel': 0.09154931700062238, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04181160861752082, 'bagging_temperature': 2.8012416604478467}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:22,074] Trial 320 finished with value: 0.7961262585885918 and parameters: {'iterations': 197, 'objective': 'Logloss', 'colsample_bylevel': 0.09188563178481082, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04507245526059854, 'bagging_temperature': 2.4298639667264252}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:24,440] Trial 321 finished with value: 0.8005573849310398 and parameters: {'iterations': 265, 'objective': 'Logloss', 'colsample_bylevel': 0.09337965241861645, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04180800410453404, 'bagging_temperature': 4.000019208210669}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:27,585] Trial 322 finished with value: 0.7989143013680249 and parameters: {'iterations': 283, 'objective': 'Logloss', 'colsample_bylevel': 0.09214138084237125, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.07093779098411764, 'bagging_temperature': 2.9463206868294276}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:29,233] Trial 323 finished with value: 0.7956689457765652 and parameters: {'iterations': 221, 'objective': 'Logloss', 'colsample_bylevel': 0.09440803498173982, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.04020301881570129, 'bagging_temperature': 6.362428609793345}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:31,362] Trial 324 finished with value: 0.7985811003460594 and parameters: {'iterations': 258, 'objective': 'Logloss', 'colsample_bylevel': 0.09450683111183622, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0431917683271891, 'bagging_temperature': 3.34289234222077}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:33,638] Trial 325 finished with value: 0.795440289370552 and parameters: {'iterations': 360, 'objective': 'Logloss', 'colsample_bylevel': 0.08841653788910221, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04750797831798838, 'bagging_temperature': 2.7651432863400633}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:35,441] Trial 326 finished with value: 0.7967093603770316 and parameters: {'iterations': 300, 'objective': 'Logloss', 'colsample_bylevel': 0.0958679997794536, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.10160799512169455, 'bagging_temperature': 2.394242589028891}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:43,510] Trial 327 finished with value: 0.7909683514935344 and parameters: {'iterations': 910, 'objective': 'Logloss', 'colsample_bylevel': 0.09615601906960423, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.039720458010100615, 'bagging_temperature': 2.5007625558167965}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:45,404] Trial 328 finished with value: 0.7963141034600354 and parameters: {'iterations': 367, 'objective': 'Logloss', 'colsample_bylevel': 0.09142043377723201, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.035530189562690176, 'bagging_temperature': 4.9969672768013575}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:47,277] Trial 329 finished with value: 0.7976452303615456 and parameters: {'iterations': 246, 'objective': 'Logloss', 'colsample_bylevel': 0.09360652353836604, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.0453394296826724, 'bagging_temperature': 3.1445678209362526}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:30:55,921] Trial 330 finished with value: 0.7908638068775822 and parameters: {'iterations': 985, 'objective': 'Logloss', 'colsample_bylevel': 0.09618398898698832, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.050677645161916016, 'bagging_temperature': 2.2987253416012416}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:31:13,093] Trial 331 finished with value: 0.7978738867675588 and parameters: {'iterations': 292, 'objective': 'Logloss', 'colsample_bylevel': 0.0982238933063468, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04255742961085156, 'bagging_temperature': 2.7752537568979876}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:31:14,690] Trial 332 finished with value: 0.7985190444510288 and parameters: {'iterations': 311, 'objective': 'Logloss', 'colsample_bylevel': 0.092688742904161, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03800609978917006, 'bagging_temperature': 1.7897889872142914}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:31:16,330] Trial 333 finished with value: 0.799850171352539 and parameters: {'iterations': 340, 'objective': 'Logloss', 'colsample_bylevel': 0.09561273545483381, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.047562617096122715, 'bagging_temperature': 2.5822708731047177}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:31:20,897] Trial 334 finished with value: 0.794504419386038 and parameters: {'iterations': 368, 'objective': 'Logloss', 'colsample_bylevel': 0.0978511726549762, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03552881248702111, 'bagging_temperature': 2.258902632637498}. Best is trial 287 with value: 0.8027623259220331.\n",
            "[I 2024-05-15 21:31:22,809] Trial 335 finished with value: 0.8028243818170637 and parameters: {'iterations': 267, 'objective': 'Logloss', 'colsample_bylevel': 0.0910990449319523, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03135269072716009, 'bagging_temperature': 1.2096076014056452}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:23,773] Trial 336 finished with value: 0.7960854470540222 and parameters: {'iterations': 208, 'objective': 'Logloss', 'colsample_bylevel': 0.08847361756347517, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.06263148939676633, 'bagging_temperature': 0.9725001424816339}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:25,566] Trial 337 finished with value: 0.796230803204544 and parameters: {'iterations': 254, 'objective': 'Logloss', 'colsample_bylevel': 0.09044507725574702, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03141983947340026, 'bagging_temperature': 1.1768198256810594}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:27,092] Trial 338 finished with value: 0.7924660789060273 and parameters: {'iterations': 279, 'objective': 'Logloss', 'colsample_bylevel': 0.09150889628353015, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0526871791626678, 'bagging_temperature': 1.4114235815906975}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:29,489] Trial 339 finished with value: 0.7958355462875479 and parameters: {'iterations': 314, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09381885493581273, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04088152355062358, 'bagging_temperature': 1.6778600629692795}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:32,942] Trial 340 finished with value: 0.8026790256665418 and parameters: {'iterations': 284, 'objective': 'Logloss', 'colsample_bylevel': 0.09830496947154033, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0340968549195655, 'bagging_temperature': 8.859679459373144}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:35,261] Trial 341 finished with value: 0.7994549144355431 and parameters: {'iterations': 245, 'objective': 'Logloss', 'colsample_bylevel': 0.09831325240476371, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03495845776272766, 'bagging_temperature': 8.848482307421339}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:36,676] Trial 342 finished with value: 0.7974165739555322 and parameters: {'iterations': 154, 'objective': 'Logloss', 'colsample_bylevel': 0.09662977353172555, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03122207729732305, 'bagging_temperature': 8.53385739428981}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:39,231] Trial 343 finished with value: 0.8013478987650319 and parameters: {'iterations': 281, 'objective': 'Logloss', 'colsample_bylevel': 0.09996636218212107, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03762014580838711, 'bagging_temperature': 9.45297469545646}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:53,620] Trial 344 finished with value: 0.7939425619580592 and parameters: {'iterations': 1587, 'objective': 'Logloss', 'colsample_bylevel': 0.09998224786991101, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03437370167320841, 'bagging_temperature': 9.347552314967585}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:55,857] Trial 345 finished with value: 0.7990596575185469 and parameters: {'iterations': 273, 'objective': 'Logloss', 'colsample_bylevel': 0.09556245262340651, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03818941233215838, 'bagging_temperature': 9.968233840159936}. Best is trial 335 with value: 0.8028243818170637.\n",
            "[I 2024-05-15 21:31:58,718] Trial 346 finished with value: 0.8037814961620384 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.0984794842981034, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03299474088806202, 'bagging_temperature': 9.425841902531973}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:01,185] Trial 347 finished with value: 0.7928400914625625 and parameters: {'iterations': 182, 'objective': 'Logloss', 'colsample_bylevel': 0.09998259328985151, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.14606297749317382, 'bagging_temperature': 9.804342667635797}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:03,400] Trial 348 finished with value: 0.8025336695160199 and parameters: {'iterations': 241, 'objective': 'Logloss', 'colsample_bylevel': 0.09874959535668279, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03071822334637294, 'bagging_temperature': 9.423260065256235}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:06,335] Trial 349 finished with value: 0.7996215149465257 and parameters: {'iterations': 212, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09874833609709131, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.030482508745007443, 'bagging_temperature': 9.06942455488828}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:08,061] Trial 350 finished with value: 0.7932353483795584 and parameters: {'iterations': 144, 'objective': 'Logloss', 'colsample_bylevel': 0.09789015818196352, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030874617594532604, 'bagging_temperature': 9.488235200234469}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:09,406] Trial 351 finished with value: 0.7956689457765652 and parameters: {'iterations': 217, 'objective': 'Logloss', 'colsample_bylevel': 0.09638516556500068, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.028091112762872652, 'bagging_temperature': 9.072765823763586}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:11,653] Trial 352 finished with value: 0.7990596575185469 and parameters: {'iterations': 188, 'objective': 'Logloss', 'colsample_bylevel': 0.09999894131086784, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.033214238301734436, 'subsample': 0.6236024073209552}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:14,793] Trial 353 finished with value: 0.799850171352539 and parameters: {'iterations': 271, 'objective': 'Logloss', 'colsample_bylevel': 0.09828938756802322, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03337697618234323, 'bagging_temperature': 8.984368059804153}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:16,635] Trial 354 finished with value: 0.8003907844200568 and parameters: {'iterations': 229, 'objective': 'Logloss', 'colsample_bylevel': 0.09553241277580853, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.028654788922777313, 'bagging_temperature': 9.69305586224281}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:19,527] Trial 355 finished with value: 0.7969380167830448 and parameters: {'iterations': 338, 'objective': 'Logloss', 'colsample_bylevel': 0.09799402279879396, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.032982925951681674, 'bagging_temperature': 9.47115324027238}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:27,971] Trial 356 finished with value: 0.7888467107580324 and parameters: {'iterations': 1413, 'objective': 'Logloss', 'colsample_bylevel': 0.09423163661051691, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.035810040341537674, 'bagging_temperature': 8.745772550936536}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:30,412] Trial 357 finished with value: 0.7994549144355431 and parameters: {'iterations': 285, 'objective': 'Logloss', 'colsample_bylevel': 0.09666766607946498, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029893040998227666, 'bagging_temperature': 8.33474372941549}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:32,493] Trial 358 finished with value: 0.797978431383511 and parameters: {'iterations': 236, 'objective': 'Logloss', 'colsample_bylevel': 0.09823436275004689, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03614734723421109, 'bagging_temperature': 9.481217965465788}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:34,837] Trial 359 finished with value: 0.7958355462875479 and parameters: {'iterations': 285, 'objective': 'Logloss', 'colsample_bylevel': 0.09519377260023418, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03250506187376963, 'bagging_temperature': 9.262991993083181}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:36,846] Trial 360 finished with value: 0.7995382146910344 and parameters: {'iterations': 372, 'objective': 'Logloss', 'colsample_bylevel': 0.09315246992272766, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.027040993290298486, 'bagging_temperature': 9.549316136806592}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:39,757] Trial 361 finished with value: 0.7978951311280196 and parameters: {'iterations': 336, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09838518302900069, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.037609637867623245, 'bagging_temperature': 9.222385946499022}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:41,810] Trial 362 finished with value: 0.8001621280140436 and parameters: {'iterations': 253, 'objective': 'Logloss', 'colsample_bylevel': 0.09616995656505709, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034320313779909475, 'bagging_temperature': 7.7336385336714635}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:43,619] Trial 363 finished with value: 0.8008693415925444 and parameters: {'iterations': 298, 'objective': 'Logloss', 'colsample_bylevel': 0.09989761969644237, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030018299662805163, 'bagging_temperature': 9.75955673610001}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:45,313] Trial 364 finished with value: 0.799850171352539 and parameters: {'iterations': 202, 'objective': 'Logloss', 'colsample_bylevel': 0.09435415695679952, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03878915210949423, 'bagging_temperature': 9.20568714433146}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:48,127] Trial 365 finished with value: 0.8004740846755483 and parameters: {'iterations': 342, 'objective': 'Logloss', 'colsample_bylevel': 0.09699874930387307, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.044299275138411726, 'bagging_temperature': 1.8313314883161496}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:50,733] Trial 366 finished with value: 0.7979571870230502 and parameters: {'iterations': 263, 'objective': 'Logloss', 'colsample_bylevel': 0.09873762962132496, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03243127368590526, 'bagging_temperature': 8.93257805134237}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:54,272] Trial 367 finished with value: 0.7994549144355431 and parameters: {'iterations': 306, 'objective': 'Logloss', 'colsample_bylevel': 0.09659862665607505, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.026271813760429266, 'subsample': 0.6518732085758883}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:56,157] Trial 368 finished with value: 0.7996215149465257 and parameters: {'iterations': 389, 'objective': 'Logloss', 'colsample_bylevel': 0.09997607426955966, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0357563678441366, 'bagging_temperature': 9.60549525833752}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:57,609] Trial 369 finished with value: 0.8021171682385629 and parameters: {'iterations': 174, 'objective': 'Logloss', 'colsample_bylevel': 0.09309549260429825, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03950445055008727, 'bagging_temperature': 9.92538501045707}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:32:59,036] Trial 370 finished with value: 0.7970000726780754 and parameters: {'iterations': 160, 'objective': 'Logloss', 'colsample_bylevel': 0.09388574199731385, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04039561319023581, 'bagging_temperature': 9.97391262462642}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:00,458] Trial 371 finished with value: 0.7997668710970477 and parameters: {'iterations': 234, 'objective': 'Logloss', 'colsample_bylevel': 0.09225347374744325, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.038459182929588207, 'bagging_temperature': 1.263919442134818}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:10,391] Trial 372 finished with value: 0.7882848533300535 and parameters: {'iterations': 1163, 'objective': 'Logloss', 'colsample_bylevel': 0.09536367655333988, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04491203695946338, 'bagging_temperature': 9.892127306799745}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:12,104] Trial 373 finished with value: 0.7933186486350499 and parameters: {'iterations': 170, 'objective': 'Logloss', 'colsample_bylevel': 0.09710330417533966, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0299829265127292, 'bagging_temperature': 2.9917626887786977}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:13,187] Trial 374 finished with value: 0.7981858434290635 and parameters: {'iterations': 113, 'objective': 'Logloss', 'colsample_bylevel': 0.09301000144241503, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03475187022460485, 'bagging_temperature': 9.6890436453517}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:14,094] Trial 375 finished with value: 0.7983524439400461 and parameters: {'iterations': 132, 'objective': 'Logloss', 'colsample_bylevel': 0.09841233374040727, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03854050288646445, 'bagging_temperature': 1.4573570024502442}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:15,209] Trial 376 finished with value: 0.7996002705860649 and parameters: {'iterations': 179, 'objective': 'Logloss', 'colsample_bylevel': 0.09472651143347818, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.040583092390551426, 'bagging_temperature': 3.2349156193305397}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:24,169] Trial 377 finished with value: 0.7920708219890312 and parameters: {'iterations': 1053, 'objective': 'Logloss', 'colsample_bylevel': 0.09663739281784242, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03224656662443712, 'bagging_temperature': 2.1761696310648575}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:25,835] Trial 378 finished with value: 0.7933186486350499 and parameters: {'iterations': 217, 'objective': 'Logloss', 'colsample_bylevel': 0.09071232642376592, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04771713805952942, 'bagging_temperature': 0.6312461620412639}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:27,419] Trial 379 finished with value: 0.7977285306170367 and parameters: {'iterations': 283, 'objective': 'Logloss', 'colsample_bylevel': 0.09850689902655924, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.036521567469434676, 'bagging_temperature': 1.0616843881413423}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:28,544] Trial 380 finished with value: 0.7960642026935613 and parameters: {'iterations': 205, 'objective': 'Logloss', 'colsample_bylevel': 0.09527868150791166, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.044445878502016416, 'bagging_temperature': 2.545902535314051}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:31,802] Trial 381 finished with value: 0.7971666731890581 and parameters: {'iterations': 357, 'objective': 'Logloss', 'colsample_bylevel': 0.09341584613294297, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029480520531854335, 'bagging_temperature': 1.879546946702469}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:34,171] Trial 382 finished with value: 0.8004740846755483 and parameters: {'iterations': 244, 'objective': 'Logloss', 'colsample_bylevel': 0.0999167463591272, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03369323029997396, 'bagging_temperature': 1.6567143522646253}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:36,748] Trial 383 finished with value: 0.7980404872785416 and parameters: {'iterations': 315, 'objective': 'Logloss', 'colsample_bylevel': 0.09670159394042294, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04289350034398554, 'bagging_temperature': 2.380369808206377}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:39,722] Trial 384 finished with value: 0.7973332737000408 and parameters: {'iterations': 340, 'objective': 'Logloss', 'colsample_bylevel': 0.09824694932632475, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03729707212166792, 'bagging_temperature': 3.4865414702664967}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:50,221] Trial 385 finished with value: 0.7934852491460327 and parameters: {'iterations': 1272, 'objective': 'Logloss', 'colsample_bylevel': 0.09486917692904269, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.026827611678746335, 'bagging_temperature': 2.6933676607854284}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:52,051] Trial 386 finished with value: 0.7978118308725282 and parameters: {'iterations': 277, 'objective': 'Logloss', 'colsample_bylevel': 0.09210619252431392, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.04048325070342779, 'subsample': 0.2471654336570192}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:54,668] Trial 387 finished with value: 0.7991429577740383 and parameters: {'iterations': 389, 'objective': 'Logloss', 'colsample_bylevel': 0.08950599302653825, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03392807774684222, 'bagging_temperature': 1.5934723531334622}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:56,094] Trial 388 finished with value: 0.7956901901370262 and parameters: {'iterations': 304, 'objective': 'Logloss', 'colsample_bylevel': 0.0967078567734695, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.05002036597420575, 'bagging_temperature': 2.1674161506162597}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:33:57,763] Trial 389 finished with value: 0.7999122272475695 and parameters: {'iterations': 193, 'objective': 'Logloss', 'colsample_bylevel': 0.0999211955413238, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03160717605832715, 'bagging_temperature': 6.622374076413155}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:00,971] Trial 390 finished with value: 0.7996215149465257 and parameters: {'iterations': 260, 'objective': 'Logloss', 'colsample_bylevel': 0.09795718269222825, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.036636673289246614, 'bagging_temperature': 9.349167483677489}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:01,769] Trial 391 finished with value: 0.7996002705860649 and parameters: {'iterations': 102, 'objective': 'Logloss', 'colsample_bylevel': 0.09547200600970204, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04666591679734251, 'bagging_temperature': 1.8480779251151251}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:03,242] Trial 392 finished with value: 0.8004740846755483 and parameters: {'iterations': 360, 'objective': 'Logloss', 'colsample_bylevel': 0.09284217121640999, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.040587527532536125, 'bagging_temperature': 2.8983482771855806}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:05,838] Trial 393 finished with value: 0.7956068898815348 and parameters: {'iterations': 327, 'objective': 'Logloss', 'colsample_bylevel': 0.0969028984227795, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02839219679450531, 'bagging_temperature': 5.481060278934971}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:07,220] Trial 394 finished with value: 0.7985811003460594 and parameters: {'iterations': 227, 'objective': 'Logloss', 'colsample_bylevel': 0.09396452025884251, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.043148801192722526, 'bagging_temperature': 1.3291380785504414}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:08,520] Trial 395 finished with value: 0.7965427598660487 and parameters: {'iterations': 287, 'objective': 'Logloss', 'colsample_bylevel': 0.09831249249371085, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03901412271211191, 'bagging_temperature': 9.62804058411828}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:12,451] Trial 396 finished with value: 0.7972499734445495 and parameters: {'iterations': 410, 'objective': 'Logloss', 'colsample_bylevel': 0.09092811123957362, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03490181625830411, 'bagging_temperature': 3.0887486980010137}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:14,579] Trial 397 finished with value: 0.7967093603770316 and parameters: {'iterations': 377, 'objective': 'Logloss', 'colsample_bylevel': 0.09533809210897126, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.032310389432518793, 'bagging_temperature': 3.687451254335375}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:27,716] Trial 398 finished with value: 0.7926114350565491 and parameters: {'iterations': 1528, 'objective': 'Logloss', 'colsample_bylevel': 0.09853296995699116, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.02963408730751065, 'bagging_temperature': 2.4473491609988485}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:28,993] Trial 399 finished with value: 0.7975619301060541 and parameters: {'iterations': 254, 'objective': 'Logloss', 'colsample_bylevel': 0.09661660725730983, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02605267175904756, 'bagging_temperature': 1.9955102661002064}. Best is trial 346 with value: 0.8037814961620384.\n",
            "[I 2024-05-15 21:34:31,758] Trial 400 finished with value: 0.8040313969285127 and parameters: {'iterations': 320, 'objective': 'Logloss', 'colsample_bylevel': 0.09991802823024952, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.037785734689699084, 'bagging_temperature': 2.280375294790599}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:34,587] Trial 401 finished with value: 0.7985190444510288 and parameters: {'iterations': 338, 'objective': 'Logloss', 'colsample_bylevel': 0.09813556462850093, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.036776798218524226, 'bagging_temperature': 6.007432150881311}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:37,723] Trial 402 finished with value: 0.7984357441955376 and parameters: {'iterations': 292, 'objective': 'Logloss', 'colsample_bylevel': 0.09947777024140113, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.034224485694662306, 'bagging_temperature': 0.8225476428297258}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:39,890] Trial 403 finished with value: 0.7992883139245601 and parameters: {'iterations': 166, 'objective': 'Logloss', 'colsample_bylevel': 0.0939613678748886, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03826568796113176, 'bagging_temperature': 9.97629255249211}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:43,260] Trial 404 finished with value: 0.7981237875340328 and parameters: {'iterations': 360, 'objective': 'Logloss', 'colsample_bylevel': 0.09976416931984224, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.030983630007682998, 'subsample': 0.405338050857083}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:45,145] Trial 405 finished with value: 0.7975619301060541 and parameters: {'iterations': 228, 'objective': 'Logloss', 'colsample_bylevel': 0.09562861141998219, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.039753346443589216, 'bagging_temperature': 9.102349846371556}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:47,152] Trial 406 finished with value: 0.8026790256665418 and parameters: {'iterations': 417, 'objective': 'Logloss', 'colsample_bylevel': 0.08955804659077847, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03465121503878787, 'bagging_temperature': 2.5585306047309517}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:49,244] Trial 407 finished with value: 0.7992262580295296 and parameters: {'iterations': 429, 'objective': 'Logloss', 'colsample_bylevel': 0.08893846151973654, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.036361953038963085, 'bagging_temperature': 2.807762364967183}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:52,568] Trial 408 finished with value: 0.791820921222557 and parameters: {'iterations': 407, 'objective': 'Logloss', 'colsample_bylevel': 0.0898005163747232, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.1140793056012801, 'bagging_temperature': 2.5499612022549867}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:53,941] Trial 409 finished with value: 0.7978118308725282 and parameters: {'iterations': 273, 'objective': 'Logloss', 'colsample_bylevel': 0.09164860256748235, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03900646487697859, 'bagging_temperature': 3.223471382616987}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:56,228] Trial 410 finished with value: 0.7978738867675588 and parameters: {'iterations': 463, 'objective': 'Logloss', 'colsample_bylevel': 0.09290825932510374, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04215545932790678, 'bagging_temperature': 2.3220918868047575}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:57,898] Trial 411 finished with value: 0.8010359421035271 and parameters: {'iterations': 199, 'objective': 'Logloss', 'colsample_bylevel': 0.08763838000149857, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03526464703996284, 'bagging_temperature': 4.066762471375517}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:34:59,240] Trial 412 finished with value: 0.8022004684940542 and parameters: {'iterations': 307, 'objective': 'Logloss', 'colsample_bylevel': 0.09107032328330186, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03403099735878864, 'bagging_temperature': 2.6376704930171804}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:00,595] Trial 413 finished with value: 0.8007027410815615 and parameters: {'iterations': 317, 'objective': 'Logloss', 'colsample_bylevel': 0.08966835126221744, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0375959072767361, 'bagging_temperature': 9.39979862797258}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:01,707] Trial 414 finished with value: 0.8003074841645655 and parameters: {'iterations': 266, 'objective': 'Logloss', 'colsample_bylevel': 0.08787713616848955, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.040847632337465564, 'bagging_temperature': 3.007987869033052}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:03,081] Trial 415 finished with value: 0.7968547165275535 and parameters: {'iterations': 295, 'objective': 'Logloss', 'colsample_bylevel': 0.09195987487872563, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 13, 'learning_rate': 0.034873506252821845}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:05,072] Trial 416 finished with value: 0.7964594596105574 and parameters: {'iterations': 244, 'objective': 'Logloss', 'colsample_bylevel': 0.090685693430527, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04280898160470902, 'bagging_temperature': 2.7705576374028693}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:06,562] Trial 417 finished with value: 0.7956689457765652 and parameters: {'iterations': 141, 'objective': 'Logloss', 'colsample_bylevel': 0.09290533659096488, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.029967690345128348, 'bagging_temperature': 4.7735485846186165}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:07,963] Trial 418 finished with value: 0.8017219113215669 and parameters: {'iterations': 306, 'objective': 'Logloss', 'colsample_bylevel': 0.09382330814904509, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02614474262344294, 'bagging_temperature': 2.606531842972422}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:10,131] Trial 419 finished with value: 0.7988097567520727 and parameters: {'iterations': 304, 'objective': 'Logloss', 'colsample_bylevel': 0.0908053529948336, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.038003235049789794, 'bagging_temperature': 2.607029997754878}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:11,251] Trial 420 finished with value: 0.7941924627245333 and parameters: {'iterations': 268, 'objective': 'Logloss', 'colsample_bylevel': 0.08664027003873036, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.035159535026367476, 'bagging_temperature': 2.6226697499211276}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:12,520] Trial 421 finished with value: 0.7968547165275535 and parameters: {'iterations': 227, 'objective': 'Logloss', 'colsample_bylevel': 0.09393195888965979, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.027405006889126914, 'bagging_temperature': 2.923597950453174}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:14,858] Trial 422 finished with value: 0.7990596575185469 and parameters: {'iterations': 326, 'objective': 'Logloss', 'colsample_bylevel': 0.09259939680536992, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03296234523232767, 'bagging_temperature': 3.318328150675284}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:17,641] Trial 423 finished with value: 0.7996835708415563 and parameters: {'iterations': 305, 'objective': 'Logloss', 'colsample_bylevel': 0.09506779989739089, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.024567353318997537, 'bagging_temperature': 2.534351310717577}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:20,705] Trial 424 finished with value: 0.7966881160165706 and parameters: {'iterations': 281, 'objective': 'Logloss', 'colsample_bylevel': 0.08995560226883684, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.044323555843622735, 'bagging_temperature': 2.7743301441493573}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:22,015] Trial 425 finished with value: 0.7955023452655825 and parameters: {'iterations': 247, 'objective': 'Logloss', 'colsample_bylevel': 0.09190426734527547, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.037870664012532855, 'bagging_temperature': 2.3015170240574405}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:24,623] Trial 426 finished with value: 0.7983524439400461 and parameters: {'iterations': 329, 'objective': 'Logloss', 'colsample_bylevel': 0.09538715979122926, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04098334566414727, 'bagging_temperature': 3.060213366730343}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:26,154] Trial 427 finished with value: 0.7982691436845549 and parameters: {'iterations': 342, 'objective': 'Logloss', 'colsample_bylevel': 0.0935503244763876, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.030508094823024315, 'bagging_temperature': 2.475549673813052}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:27,563] Trial 428 finished with value: 0.8003907844200568 and parameters: {'iterations': 184, 'objective': 'Logloss', 'colsample_bylevel': 0.08848677418363085, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034166380102930674, 'bagging_temperature': 2.162304239186148}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:29,353] Trial 429 finished with value: 0.795045032453556 and parameters: {'iterations': 286, 'objective': 'Logloss', 'colsample_bylevel': 0.09662183564641548, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.077442682665421, 'bagging_temperature': 2.656408854828673}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:32,377] Trial 430 finished with value: 0.801181298254049 and parameters: {'iterations': 217, 'objective': 'Logloss', 'colsample_bylevel': 0.09457613573294782, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03948865969352891, 'bagging_temperature': 2.3716954956626246}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:34,679] Trial 431 finished with value: 0.798497800090568 and parameters: {'iterations': 253, 'objective': 'Logloss', 'colsample_bylevel': 0.09706641081257834, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03578849448405303, 'bagging_temperature': 1.143076568425203}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:35,997] Trial 432 finished with value: 0.7991429577740383 and parameters: {'iterations': 306, 'objective': 'Logloss', 'colsample_bylevel': 0.09130968425272148, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.042874943746533, 'bagging_temperature': 9.79634211768794}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:38,666] Trial 433 finished with value: 0.7955235896260434 and parameters: {'iterations': 347, 'objective': 'Logloss', 'colsample_bylevel': 0.0957098527906701, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.028657637127631485, 'subsample': 0.91545477970933}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:40,132] Trial 434 finished with value: 0.7967926606325229 and parameters: {'iterations': 275, 'objective': 'Logloss', 'colsample_bylevel': 0.093308255757409, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03221250239016708, 'bagging_temperature': 9.567740167994602}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:41,877] Trial 435 finished with value: 0.7999334716080304 and parameters: {'iterations': 197, 'objective': 'Logloss', 'colsample_bylevel': 0.0970486109783076, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04600004105366989, 'bagging_temperature': 8.542359820523275}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:43,973] Trial 436 finished with value: 0.8025957254110503 and parameters: {'iterations': 311, 'objective': 'Logloss', 'colsample_bylevel': 0.09514957112986874, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03690050683142801, 'bagging_temperature': 2.8306699645796503}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:47,344] Trial 437 finished with value: 0.7988310011125336 and parameters: {'iterations': 321, 'objective': 'Logloss', 'colsample_bylevel': 0.09734027358733846, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.03742409122158384, 'bagging_temperature': 2.889254561845022}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:50,227] Trial 438 finished with value: 0.7902611379150338 and parameters: {'iterations': 294, 'objective': 'Logloss', 'colsample_bylevel': 0.0947785100247473, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.08248838410093287, 'bagging_temperature': 3.063396379886881}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:51,921] Trial 439 finished with value: 0.7960021467985308 and parameters: {'iterations': 258, 'objective': 'Logloss', 'colsample_bylevel': 0.09819224579704619, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.13577279000077636, 'bagging_temperature': 7.224096981965642}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:54,652] Trial 440 finished with value: 0.7951070883485865 and parameters: {'iterations': 360, 'objective': 'Logloss', 'colsample_bylevel': 0.09582503699919542, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.09220472987852704, 'bagging_temperature': 2.6362802234621947}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:56,226] Trial 441 finished with value: 0.7983524439400461 and parameters: {'iterations': 317, 'objective': 'Logloss', 'colsample_bylevel': 0.08931874485359524, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.039468402923881436, 'bagging_temperature': 2.156078597918035}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:35:59,395] Trial 442 finished with value: 0.79662606012154 and parameters: {'iterations': 222, 'objective': 'Logloss', 'colsample_bylevel': 0.09340679811489433, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03499197743206461, 'bagging_temperature': 2.817951749424646}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:01,729] Trial 443 finished with value: 0.7936306052965546 and parameters: {'iterations': 340, 'objective': 'Logloss', 'colsample_bylevel': 0.09129022393011342, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04149910233294368, 'bagging_temperature': 3.2305932401474236}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:03,495] Trial 444 finished with value: 0.7977905865120675 and parameters: {'iterations': 283, 'objective': 'Logloss', 'colsample_bylevel': 0.09834699242723173, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.032658450750886636, 'bagging_temperature': 2.496069684975115}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:05,619] Trial 445 finished with value: 0.7978738867675588 and parameters: {'iterations': 251, 'objective': 'Logloss', 'colsample_bylevel': 0.09642823327599997, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03537319112490077, 'bagging_temperature': 1.5683996981402608}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:25,925] Trial 446 finished with value: 0.8012645985095403 and parameters: {'iterations': 306, 'objective': 'Logloss', 'colsample_bylevel': 0.09994964034210052, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03764767961680932, 'bagging_temperature': 1.817738235958615}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:28,976] Trial 447 finished with value: 0.7921328778840618 and parameters: {'iterations': 357, 'objective': 'Logloss', 'colsample_bylevel': 0.09484691894081378, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03969101869787342, 'bagging_temperature': 3.418162368777647}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:31,685] Trial 448 finished with value: 0.7988930570075642 and parameters: {'iterations': 379, 'objective': 'Logloss', 'colsample_bylevel': 0.09233255380365064, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.030875766626723866, 'bagging_temperature': 2.8223246209857753}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:32,732] Trial 449 finished with value: 0.790489794321047 and parameters: {'iterations': 171, 'objective': 'Logloss', 'colsample_bylevel': 0.0967998664499294, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.10647296241952271, 'bagging_temperature': 9.208678629396552}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:34,284] Trial 450 finished with value: 0.800245428269535 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.0985019860094879, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.049066930528594586, 'bagging_temperature': 2.2774636698675614}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:36,294] Trial 451 finished with value: 0.7934019488905413 and parameters: {'iterations': 282, 'objective': 'Logloss', 'colsample_bylevel': 0.09487930436724716, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04407769922797019, 'bagging_temperature': 2.0273441050650214}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:39,393] Trial 452 finished with value: 0.801971812088041 and parameters: {'iterations': 320, 'objective': 'Logloss', 'colsample_bylevel': 0.0926674737865297, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.03713140429853151, 'bagging_temperature': 3.0903324012759517}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:41,115] Trial 453 finished with value: 0.8004740846755483 and parameters: {'iterations': 341, 'objective': 'Logloss', 'colsample_bylevel': 0.0903058941320245, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.033502923200062144, 'bagging_temperature': 3.192524669116746}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:42,685] Trial 454 finished with value: 0.7978738867675588 and parameters: {'iterations': 320, 'objective': 'Logloss', 'colsample_bylevel': 0.09243375901327916, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.041483503363335866, 'bagging_temperature': 3.6386323078086655}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:44,538] Trial 455 finished with value: 0.8008693415925444 and parameters: {'iterations': 391, 'objective': 'Logloss', 'colsample_bylevel': 0.08864756644706133, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03623975267864485, 'bagging_temperature': 3.0121233641999323}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:46,173] Trial 456 finished with value: 0.7985811003460594 and parameters: {'iterations': 341, 'objective': 'Logloss', 'colsample_bylevel': 0.0868644630050627, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.02835010732764773, 'bagging_temperature': 2.7122048416217575}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:47,956] Trial 457 finished with value: 0.7947330757920512 and parameters: {'iterations': 371, 'objective': 'Logloss', 'colsample_bylevel': 0.09128306261598641, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03162066461007884, 'bagging_temperature': 2.981298765865038}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:49,380] Trial 458 finished with value: 0.8001621280140436 and parameters: {'iterations': 292, 'objective': 'Logloss', 'colsample_bylevel': 0.0928930579760295, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04019407836721975, 'bagging_temperature': 2.4690583304989326}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:51,918] Trial 459 finished with value: 0.7952736888595692 and parameters: {'iterations': 318, 'objective': 'Logloss', 'colsample_bylevel': 0.09373436333094746, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 14, 'learning_rate': 0.04457615997673213, 'subsample': 0.689589567271581}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:53,142] Trial 460 finished with value: 0.8028243818170637 and parameters: {'iterations': 149, 'objective': 'Logloss', 'colsample_bylevel': 0.09528566242640799, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03397375996798897, 'bagging_temperature': 3.3359012266492862}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:54,072] Trial 461 finished with value: 0.7996835708415563 and parameters: {'iterations': 132, 'objective': 'Logloss', 'colsample_bylevel': 0.09579574951374678, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.033584482439501524, 'bagging_temperature': 3.28757463194602}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:55,153] Trial 462 finished with value: 0.8005573849310398 and parameters: {'iterations': 149, 'objective': 'Logloss', 'colsample_bylevel': 0.09660391124129086, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.030479468817144032, 'bagging_temperature': 2.6495728483692274}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:56,257] Trial 463 finished with value: 0.7951283327090473 and parameters: {'iterations': 188, 'objective': 'Logloss', 'colsample_bylevel': 0.09038057981137959, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.027171370430652046, 'bagging_temperature': 3.4410404161755643}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:36:57,253] Trial 464 finished with value: 0.7983524439400461 and parameters: {'iterations': 156, 'objective': 'Logloss', 'colsample_bylevel': 0.09451625429498084, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.053872201360248353, 'bagging_temperature': 0.9409812804170767}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:03,210] Trial 465 finished with value: 0.7991217134135774 and parameters: {'iterations': 209, 'objective': 'Logloss', 'colsample_bylevel': 0.09715487006515629, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.034817057200189815, 'bagging_temperature': 2.3165559868350107}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:05,021] Trial 466 finished with value: 0.8022837687495458 and parameters: {'iterations': 245, 'objective': 'Logloss', 'colsample_bylevel': 0.09199539427272309, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03139691172480074, 'bagging_temperature': 1.442997918595036}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:05,934] Trial 467 finished with value: 0.7970213170385362 and parameters: {'iterations': 117, 'objective': 'Logloss', 'colsample_bylevel': 0.08800084562941883, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.029547277755093707, 'bagging_temperature': 1.4562885153539227}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:06,921] Trial 468 finished with value: 0.7963141034600354 and parameters: {'iterations': 178, 'objective': 'Logloss', 'colsample_bylevel': 0.09119576604801993, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.025200909762095373, 'bagging_temperature': 1.2277254395987032}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:08,055] Trial 469 finished with value: 0.7971666731890581 and parameters: {'iterations': 236, 'objective': 'Logloss', 'colsample_bylevel': 0.09240194215075294, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.031118567378170443, 'bagging_temperature': 1.6116770644722187}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:09,039] Trial 470 finished with value: 0.7948163760475426 and parameters: {'iterations': 209, 'objective': 'Logloss', 'colsample_bylevel': 0.08940680658026703, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.033018524178892436, 'bagging_temperature': 1.7953326161701015}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:10,256] Trial 471 finished with value: 0.7975619301060541 and parameters: {'iterations': 240, 'objective': 'Logloss', 'colsample_bylevel': 0.09299757416179438, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0280122287918701, 'bagging_temperature': 1.2996104763583034}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:11,192] Trial 472 finished with value: 0.8012645985095403 and parameters: {'iterations': 161, 'objective': 'Logloss', 'colsample_bylevel': 0.09580039145848457, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03576913520248017, 'bagging_temperature': 1.7287325362676615}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:12,329] Trial 473 finished with value: 0.800786041337053 and parameters: {'iterations': 259, 'objective': 'Logloss', 'colsample_bylevel': 0.08539095307122956, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0323918751958538, 'bagging_temperature': 1.466725963449571}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:13,623] Trial 474 finished with value: 0.7978738867675588 and parameters: {'iterations': 208, 'objective': 'Logloss', 'colsample_bylevel': 0.09788448597723554, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02539553689697632, 'bagging_temperature': 1.9460453454179882}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:15,109] Trial 475 finished with value: 0.8018052115770582 and parameters: {'iterations': 261, 'objective': 'Logloss', 'colsample_bylevel': 0.08963034503667902, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.028024383488605042, 'bagging_temperature': 2.1991660835294438}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:16,487] Trial 476 finished with value: 0.7953569891150606 and parameters: {'iterations': 182, 'objective': 'Logloss', 'colsample_bylevel': 0.08808824953871408, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.029214263476535912, 'bagging_temperature': 2.1791892738469874}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:18,742] Trial 477 finished with value: 0.7972499734445495 and parameters: {'iterations': 243, 'objective': 'Logloss', 'colsample_bylevel': 0.08609097843801607, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.025911380948305546, 'bagging_temperature': 0.7330330644647409}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:20,046] Trial 478 finished with value: 0.7952736888595692 and parameters: {'iterations': 223, 'objective': 'Logloss', 'colsample_bylevel': 0.08935086788537802, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029280818490430773, 'bagging_temperature': 1.0798297473445682}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:21,436] Trial 479 finished with value: 0.7943378188750553 and parameters: {'iterations': 269, 'objective': 'Logloss', 'colsample_bylevel': 0.09077701007090674, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 12, 'learning_rate': 0.03196036759675229, 'subsample': 0.2637084725663834}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:22,631] Trial 480 finished with value: 0.7948163760475426 and parameters: {'iterations': 201, 'objective': 'Logloss', 'colsample_bylevel': 0.09168662907593922, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02827994860339035, 'bagging_temperature': 4.463878577257376}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:23,503] Trial 481 finished with value: 0.7962928590995746 and parameters: {'iterations': 153, 'objective': 'Logloss', 'colsample_bylevel': 0.08709506669425313, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.12943072234245867, 'bagging_temperature': 2.391959551074992}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:24,655] Trial 482 finished with value: 0.800245428269535 and parameters: {'iterations': 260, 'objective': 'Logloss', 'colsample_bylevel': 0.08956316177001371, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.033574746407428246, 'bagging_temperature': 3.9772554811963996}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:25,105] Trial 483 finished with value: 0.5 and parameters: {'iterations': 100, 'objective': 'Logloss', 'colsample_bylevel': 0.010000507570388374, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.025210817948489082, 'bagging_temperature': 7.825392177427539}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:26,246] Trial 484 finished with value: 0.8003074841645655 and parameters: {'iterations': 238, 'objective': 'Logloss', 'colsample_bylevel': 0.09311074624141669, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.034549805087001305, 'bagging_temperature': 1.9568079440698611}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:28,614] Trial 485 finished with value: 0.7933807045300802 and parameters: {'iterations': 271, 'objective': 'Logloss', 'colsample_bylevel': 0.04125208272443283, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.030969396625517582, 'bagging_temperature': 2.7481738231163484}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:30,530] Trial 486 finished with value: 0.7955023452655825 and parameters: {'iterations': 209, 'objective': 'Logloss', 'colsample_bylevel': 0.09122585721548973, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03680767549200389, 'bagging_temperature': 1.662167660549879}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:32,100] Trial 487 finished with value: 0.7968547165275535 and parameters: {'iterations': 129, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09395485507013789, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.027839118022527722, 'bagging_temperature': 2.472617352969673}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:33,054] Trial 488 finished with value: 0.7934852491460327 and parameters: {'iterations': 289, 'objective': 'Logloss', 'colsample_bylevel': 0.06344764246415106, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 0, 'learning_rate': 0.036376460451173744, 'bagging_temperature': 1.3020739601085487}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:34,390] Trial 489 finished with value: 0.7999955275030608 and parameters: {'iterations': 252, 'objective': 'Logloss', 'colsample_bylevel': 0.09291696455055701, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03189366594235918, 'bagging_temperature': 2.079468837507633}. Best is trial 400 with value: 0.8040313969285127.\n",
            "[I 2024-05-15 21:37:35,600] Trial 490 finished with value: 0.8043221092295566 and parameters: {'iterations': 187, 'objective': 'Logloss', 'colsample_bylevel': 0.09528439609380669, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.034114705242209116, 'bagging_temperature': 2.614189270496462}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:36,623] Trial 491 finished with value: 0.8029909823280463 and parameters: {'iterations': 169, 'objective': 'Logloss', 'colsample_bylevel': 0.08915594149243124, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03582429033799419, 'bagging_temperature': 2.6296154249586605}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:37,541] Trial 492 finished with value: 0.7953569891150606 and parameters: {'iterations': 139, 'objective': 'Logloss', 'colsample_bylevel': 0.09098449360294189, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03724623691843645, 'bagging_temperature': 2.6519952928280475}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:38,537] Trial 493 finished with value: 0.7944831750255771 and parameters: {'iterations': 167, 'objective': 'Logloss', 'colsample_bylevel': 0.08894171242439705, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034443462607914824, 'bagging_temperature': 2.924964991497404}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:39,426] Trial 494 finished with value: 0.8020338679830717 and parameters: {'iterations': 168, 'objective': 'Logloss', 'colsample_bylevel': 0.08741031416871747, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.038217586371997216, 'bagging_temperature': 2.525437863905587}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:40,262] Trial 495 finished with value: 0.7986644006015509 and parameters: {'iterations': 136, 'objective': 'Logloss', 'colsample_bylevel': 0.08766889053865363, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0375242158722963, 'bagging_temperature': 3.12141591063046}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:41,353] Trial 496 finished with value: 0.7956689457765652 and parameters: {'iterations': 180, 'objective': 'Logloss', 'colsample_bylevel': 0.08723585090199674, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.040037253154746245, 'bagging_temperature': 2.327368586040352}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:42,299] Trial 497 finished with value: 0.7990596575185469 and parameters: {'iterations': 161, 'objective': 'Logloss', 'colsample_bylevel': 0.09012554795615883, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03672123259862703, 'bagging_temperature': 2.857486252971667}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:44,160] Trial 498 finished with value: 0.8037602518015776 and parameters: {'iterations': 185, 'objective': 'Logloss', 'colsample_bylevel': 0.08906363032217471, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.038885709548671814, 'bagging_temperature': 2.484532702635941}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:45,492] Trial 499 finished with value: 0.79662606012154 and parameters: {'iterations': 135, 'objective': 'Logloss', 'colsample_bylevel': 0.08507446944348089, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.039615043216752494, 'bagging_temperature': 2.504804478697078}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:46,466] Trial 500 finished with value: 0.7980192429180806 and parameters: {'iterations': 181, 'objective': 'Logloss', 'colsample_bylevel': 0.08641939119475794, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04171653472192623, 'bagging_temperature': 2.7818749782492045}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:47,474] Trial 501 finished with value: 0.7976452303615456 and parameters: {'iterations': 165, 'objective': 'Logloss', 'colsample_bylevel': 0.09097525730237829, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03840921831391831, 'bagging_temperature': 2.6065592074298194}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:48,617] Trial 502 finished with value: 0.7943998747700857 and parameters: {'iterations': 199, 'objective': 'Logloss', 'colsample_bylevel': 0.08812557360808221, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.035401981467702855, 'bagging_temperature': 3.0863181611029957}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:49,426] Trial 503 finished with value: 0.7990384131580859 and parameters: {'iterations': 108, 'objective': 'Logloss', 'colsample_bylevel': 0.09239293293279179, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04264644759003128, 'bagging_temperature': 2.5342637235786194}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:53,190] Trial 504 finished with value: 0.7951283327090473 and parameters: {'iterations': 145, 'objective': 'Logloss', 'colsample_bylevel': 0.09577448429511447, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03887535319638533, 'bagging_temperature': 2.8525192516319717}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:54,409] Trial 505 finished with value: 0.7947330757920512 and parameters: {'iterations': 182, 'objective': 'Logloss', 'colsample_bylevel': 0.0969691946640328, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03493371371723114, 'bagging_temperature': 8.76631698104451}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:56,713] Trial 506 finished with value: 0.7981025431735721 and parameters: {'iterations': 210, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08992405359905133, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03769950626402956, 'bagging_temperature': 2.3647847483386135}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:37:59,116] Trial 507 finished with value: 0.798497800090568 and parameters: {'iterations': 171, 'objective': 'Logloss', 'colsample_bylevel': 0.09435381454013118, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.042113715039263325, 'bagging_temperature': 5.176581490313035}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:00,274] Trial 508 finished with value: 0.7926114350565491 and parameters: {'iterations': 199, 'objective': 'Logloss', 'colsample_bylevel': 0.09216200034479362, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03364020530261227, 'bagging_temperature': 2.7944681748494076}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:01,101] Trial 509 finished with value: 0.7974165739555322 and parameters: {'iterations': 125, 'objective': 'Logloss', 'colsample_bylevel': 0.08865357021604416, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.039304553718463565, 'subsample': 0.5756733040739156}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:02,990] Trial 510 finished with value: 0.79662606012154 and parameters: {'iterations': 221, 'objective': 'Logloss', 'colsample_bylevel': 0.09813071019736845, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03630202619316724, 'bagging_temperature': 9.755750774151386}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:04,047] Trial 511 finished with value: 0.7958355462875479 and parameters: {'iterations': 146, 'objective': 'Logloss', 'colsample_bylevel': 0.09574626855426789, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.034295030646975935, 'bagging_temperature': 5.918786514221047}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:05,909] Trial 512 finished with value: 0.8017431556820278 and parameters: {'iterations': 195, 'objective': 'Logloss', 'colsample_bylevel': 0.0979918963893747, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.04025785324838157, 'bagging_temperature': 3.3476591358023193}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:07,698] Trial 513 finished with value: 0.8022837687495458 and parameters: {'iterations': 219, 'objective': 'Logloss', 'colsample_bylevel': 0.09461332713551723, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03685218413228967, 'bagging_temperature': 2.687250938177163}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:08,785] Trial 514 finished with value: 0.7970833729335668 and parameters: {'iterations': 167, 'objective': 'Logloss', 'colsample_bylevel': 0.09219385407639742, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0360169124303031, 'bagging_temperature': 2.385988143743329}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:10,885] Trial 515 finished with value: 0.7957522460320566 and parameters: {'iterations': 217, 'objective': 'Logloss', 'colsample_bylevel': 0.09422818811021019, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03307235317327345, 'bagging_temperature': 2.6007223376589783}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:12,836] Trial 516 finished with value: 0.79598090243807 and parameters: {'iterations': 182, 'objective': 'Logloss', 'colsample_bylevel': 0.08942596774434289, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.037200503561849424, 'bagging_temperature': 2.727448218806633}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:14,178] Trial 517 finished with value: 0.7975619301060541 and parameters: {'iterations': 223, 'objective': 'Logloss', 'colsample_bylevel': 0.09154288257476975, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.032999970534373084, 'bagging_temperature': 9.998960098357026}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:15,015] Trial 518 finished with value: 0.7943378188750553 and parameters: {'iterations': 104, 'objective': 'Logloss', 'colsample_bylevel': 0.08646045305700985, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03630224656685257, 'bagging_temperature': 2.1991262916043763}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:16,477] Trial 519 finished with value: 0.7967714162720619 and parameters: {'iterations': 195, 'objective': 'Logloss', 'colsample_bylevel': 0.09369445992437679, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04365131243299994, 'bagging_temperature': 2.4684440846033717}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:17,587] Trial 520 finished with value: 0.8008693415925444 and parameters: {'iterations': 156, 'objective': 'Logloss', 'colsample_bylevel': 0.09545210267602665, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03218676575285147, 'bagging_temperature': 2.6279656631683226}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:18,797] Trial 521 finished with value: 0.79598090243807 and parameters: {'iterations': 221, 'objective': 'Logloss', 'colsample_bylevel': 0.09073893404703193, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03882492749185222, 'bagging_temperature': 2.227640206284285}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:19,318] Trial 522 finished with value: 0.7739969027958696 and parameters: {'iterations': 138, 'objective': 'Logloss', 'colsample_bylevel': 0.025575258314951832, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03449142520488352, 'bagging_temperature': 2.7389089127197703}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:20,621] Trial 523 finished with value: 0.7986644006015509 and parameters: {'iterations': 190, 'objective': 'Logloss', 'colsample_bylevel': 0.0971506228821656, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 16, 'learning_rate': 0.03802589485265074, 'bagging_temperature': 0.9145916822595543}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:22,163] Trial 524 finished with value: 0.7952736888595692 and parameters: {'iterations': 157, 'objective': 'Logloss', 'colsample_bylevel': 0.051048350548309245, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0315859347276284, 'bagging_temperature': 2.9245739010222014}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:25,429] Trial 525 finished with value: 0.7997668710970477 and parameters: {'iterations': 236, 'objective': 'Logloss', 'colsample_bylevel': 0.09349899760932368, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.042600942374000375, 'bagging_temperature': 2.4554576725606503}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:26,942] Trial 526 finished with value: 0.8004740846755483 and parameters: {'iterations': 231, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09851426073612822, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.03511590536102403, 'bagging_temperature': 2.0124632707749783}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:28,412] Trial 527 finished with value: 0.7928400914625625 and parameters: {'iterations': 203, 'objective': 'Logloss', 'colsample_bylevel': 0.0884885917274304, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04597297625477885, 'bagging_temperature': 2.337433589272037}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:29,560] Trial 528 finished with value: 0.79662606012154 and parameters: {'iterations': 181, 'objective': 'Logloss', 'colsample_bylevel': 0.0954820994933975, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0669720149571837, 'bagging_temperature': 1.136859936190501}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:30,523] Trial 529 finished with value: 0.7992883139245601 and parameters: {'iterations': 131, 'objective': 'Logloss', 'colsample_bylevel': 0.09236915309259262, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 12, 'learning_rate': 0.040184278342415614, 'subsample': 0.44442572473035113}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:32,076] Trial 530 finished with value: 0.8023670690050372 and parameters: {'iterations': 240, 'objective': 'Logloss', 'colsample_bylevel': 0.09673732947448593, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0314910927780327, 'bagging_temperature': 2.948513003046116}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:33,671] Trial 531 finished with value: 0.7992883139245601 and parameters: {'iterations': 236, 'objective': 'Logloss', 'colsample_bylevel': 0.09833166752589324, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0305955792554244, 'bagging_temperature': 2.6158355644226967}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:35,041] Trial 532 finished with value: 0.7969380167830448 and parameters: {'iterations': 207, 'objective': 'Logloss', 'colsample_bylevel': 0.09696650808788325, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.031481667821711405, 'bagging_temperature': 8.984330836917392}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:37,703] Trial 533 finished with value: 0.8026790256665418 and parameters: {'iterations': 251, 'objective': 'Logloss', 'colsample_bylevel': 0.09839925635282017, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.033539807805260176, 'bagging_temperature': 2.7869390905981746}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:39,681] Trial 534 finished with value: 0.79662606012154 and parameters: {'iterations': 252, 'objective': 'Logloss', 'colsample_bylevel': 0.09838973710698723, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.12136430689092625, 'bagging_temperature': 2.7578380952171884}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:41,301] Trial 535 finished with value: 0.7944211191305467 and parameters: {'iterations': 252, 'objective': 'Logloss', 'colsample_bylevel': 0.09700037485240907, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.030467926370661143, 'bagging_temperature': 2.962705546962802}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:42,788] Trial 536 finished with value: 0.7909683514935344 and parameters: {'iterations': 224, 'objective': 'Logloss', 'colsample_bylevel': 0.09868232524077254, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.13948488424855113, 'bagging_temperature': 1.4685525389445115}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:44,765] Trial 537 finished with value: 0.801971812088041 and parameters: {'iterations': 246, 'objective': 'Logloss', 'colsample_bylevel': 0.09598140695393295, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03415509450581364, 'bagging_temperature': 9.634957924745875}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:46,547] Trial 538 finished with value: 0.8001621280140436 and parameters: {'iterations': 267, 'objective': 'Logloss', 'colsample_bylevel': 0.09947376124266202, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03287638191235963, 'bagging_temperature': 3.175874857627675}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:47,948] Trial 539 finished with value: 0.7997668710970477 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09674543030983028, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029817209608668998, 'bagging_temperature': 2.849006481382823}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:51,444] Trial 540 finished with value: 0.8010146977430661 and parameters: {'iterations': 246, 'objective': 'Logloss', 'colsample_bylevel': 0.09997241606497032, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03276145756975073, 'bagging_temperature': 1.8574682969611702}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:52,864] Trial 541 finished with value: 0.7992883139245601 and parameters: {'iterations': 204, 'objective': 'Logloss', 'colsample_bylevel': 0.09515620663914262, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03503073993908092, 'bagging_temperature': 2.223688344975801}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:55,128] Trial 542 finished with value: 0.7971666731890581 and parameters: {'iterations': 278, 'objective': 'Logloss', 'colsample_bylevel': 0.09729783290670066, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.031098866245526234, 'bagging_temperature': 2.988685441515627}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:38:59,623] Trial 543 finished with value: 0.7974786298505627 and parameters: {'iterations': 194, 'objective': 'Logloss', 'colsample_bylevel': 0.09988340267898603, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034445265964806626, 'bagging_temperature': 9.370335744189633}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:01,628] Trial 544 finished with value: 0.7996835708415563 and parameters: {'iterations': 239, 'objective': 'Logloss', 'colsample_bylevel': 0.09526485916881158, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.036803985764245206, 'bagging_temperature': 2.414774435143272}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:02,658] Trial 545 finished with value: 0.7901778376595424 and parameters: {'iterations': 289, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.03200872028795404, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029122157178275692, 'bagging_temperature': 2.631066948348853}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:06,022] Trial 546 finished with value: 0.7978738867675588 and parameters: {'iterations': 264, 'objective': 'Logloss', 'colsample_bylevel': 0.09808541265438787, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03306625145676643, 'bagging_temperature': 2.113237707433714}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:07,812] Trial 547 finished with value: 0.7978738867675588 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09494110700342302, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03531951802108516, 'bagging_temperature': 2.858523446141895}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:09,061] Trial 548 finished with value: 0.8005573849310398 and parameters: {'iterations': 188, 'objective': 'Logloss', 'colsample_bylevel': 0.0981423278786566, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04026222357042571, 'bagging_temperature': 1.3980244936648611}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:10,547] Trial 549 finished with value: 0.799850171352539 and parameters: {'iterations': 257, 'objective': 'Logloss', 'colsample_bylevel': 0.09677832672706667, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 12, 'learning_rate': 0.031009600752259493, 'subsample': 0.19741371009908654}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:12,200] Trial 550 finished with value: 0.800786041337053 and parameters: {'iterations': 227, 'objective': 'Logloss', 'colsample_bylevel': 0.09432912042220845, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03621874942162233, 'bagging_temperature': 8.302874533381017}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:13,599] Trial 551 finished with value: 0.7983524439400461 and parameters: {'iterations': 168, 'objective': 'Logloss', 'colsample_bylevel': 0.09648585753333937, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03869053929374492, 'bagging_temperature': 0.4714851112530053}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:14,920] Trial 552 finished with value: 0.7979571870230502 and parameters: {'iterations': 285, 'objective': 'Logloss', 'colsample_bylevel': 0.09492357880679474, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0423696606296186, 'bagging_temperature': 2.698133850560002}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:18,950] Trial 553 finished with value: 0.8011192423590185 and parameters: {'iterations': 388, 'objective': 'Logloss', 'colsample_bylevel': 0.09852335660368837, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03276828169687794, 'bagging_temperature': 2.4455354287187085}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:20,683] Trial 554 finished with value: 0.7983524439400461 and parameters: {'iterations': 203, 'objective': 'Logloss', 'colsample_bylevel': 0.09412752507019262, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02838123764978178, 'bagging_temperature': 1.7698080053271124}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:22,182] Trial 555 finished with value: 0.8007239854420224 and parameters: {'iterations': 304, 'objective': 'Logloss', 'colsample_bylevel': 0.09990700027336874, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03569419469304272, 'bagging_temperature': 7.499215597774291}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:24,267] Trial 556 finished with value: 0.804010152568052 and parameters: {'iterations': 243, 'objective': 'Logloss', 'colsample_bylevel': 0.09666848670014787, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03863347573874048, 'bagging_temperature': 1.976394882934833}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:25,943] Trial 557 finished with value: 0.8000788277585523 and parameters: {'iterations': 275, 'objective': 'Logloss', 'colsample_bylevel': 0.09709378178970543, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03303129631629285, 'bagging_temperature': 2.0021172645614556}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:36,741] Trial 558 finished with value: 0.7927780355675318 and parameters: {'iterations': 1203, 'objective': 'Logloss', 'colsample_bylevel': 0.09805101409539337, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03752695024559582, 'bagging_temperature': 1.625296360870367}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:38,967] Trial 559 finished with value: 0.8004740846755483 and parameters: {'iterations': 258, 'objective': 'Logloss', 'colsample_bylevel': 0.09705239179318906, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030540322191646974, 'bagging_temperature': 2.2497275119527966}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:41,777] Trial 560 finished with value: 0.7981858434290635 and parameters: {'iterations': 231, 'objective': 'Logloss', 'colsample_bylevel': 0.09575130202891434, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03499799648914481, 'bagging_temperature': 1.9536991493062152}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:45,764] Trial 561 finished with value: 0.7969380167830448 and parameters: {'iterations': 299, 'objective': 'Logloss', 'colsample_bylevel': 0.09847756364920378, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0728051853540668, 'bagging_temperature': 1.1155869274873815}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:52,269] Trial 562 finished with value: 0.7969167724225839 and parameters: {'iterations': 251, 'objective': 'Logloss', 'colsample_bylevel': 0.09592719230154215, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04544693269188688, 'bagging_temperature': 2.2442625413476756}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:55,921] Trial 563 finished with value: 0.7982691436845549 and parameters: {'iterations': 424, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09829126562281831, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.042044535533082315, 'bagging_temperature': 3.146751462042835}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:57,175] Trial 564 finished with value: 0.7939425619580592 and parameters: {'iterations': 276, 'objective': 'Logloss', 'colsample_bylevel': 0.04386660651081362, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.037553078028655665, 'bagging_temperature': 1.81967655102573}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:39:59,017] Trial 565 finished with value: 0.7990596575185469 and parameters: {'iterations': 229, 'objective': 'Logloss', 'colsample_bylevel': 0.09628800262855168, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.028477225357192073, 'bagging_temperature': 1.5417152805692258}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:01,869] Trial 566 finished with value: 0.7984357441955376 and parameters: {'iterations': 371, 'objective': 'Logloss', 'colsample_bylevel': 0.09421147582225425, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.033355595322090585, 'bagging_temperature': 2.044377751095461}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:04,350] Trial 567 finished with value: 0.8004740846755483 and parameters: {'iterations': 942, 'objective': 'Logloss', 'colsample_bylevel': 0.06723578627166517, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03073239084178664, 'bagging_temperature': 2.5269204570908874}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:07,013] Trial 568 finished with value: 0.8022837687495458 and parameters: {'iterations': 298, 'objective': 'Logloss', 'colsample_bylevel': 0.0987465677903426, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.039613303547436016, 'bagging_temperature': 2.7079595329400425}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:10,123] Trial 569 finished with value: 0.8014932549155536 and parameters: {'iterations': 282, 'objective': 'Logloss', 'colsample_bylevel': 0.09918216466395995, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.040806188953612835, 'bagging_temperature': 2.940234204375133}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:13,505] Trial 570 finished with value: 0.7971666731890581 and parameters: {'iterations': 309, 'objective': 'Logloss', 'colsample_bylevel': 0.09993242776654485, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.04351868372860992, 'subsample': 0.3289686657233365}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:15,888] Trial 571 finished with value: 0.7972499734445495 and parameters: {'iterations': 253, 'objective': 'Logloss', 'colsample_bylevel': 0.09999707071629531, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.039892865324980854, 'bagging_temperature': 2.701239636247956}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:17,833] Trial 572 finished with value: 0.8000788277585523 and parameters: {'iterations': 207, 'objective': 'Logloss', 'colsample_bylevel': 0.09687072501061843, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.044179099466841176, 'bagging_temperature': 2.8714397544718038}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:20,304] Trial 573 finished with value: 0.8003074841645655 and parameters: {'iterations': 296, 'objective': 'Logloss', 'colsample_bylevel': 0.09995000345756998, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03824657032372445, 'bagging_temperature': 3.207120463024487}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:22,453] Trial 574 finished with value: 0.8006406851865312 and parameters: {'iterations': 241, 'objective': 'Logloss', 'colsample_bylevel': 0.09474941911822748, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04808819448936937, 'bagging_temperature': 3.525877389744412}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:26,059] Trial 575 finished with value: 0.7974165739555322 and parameters: {'iterations': 269, 'objective': 'Logloss', 'colsample_bylevel': 0.0978264562806392, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03675868322098362, 'bagging_temperature': 2.6282234459696294}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:28,330] Trial 576 finished with value: 0.7941712183640725 and parameters: {'iterations': 317, 'objective': 'Logloss', 'colsample_bylevel': 0.09317391245398393, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04178769112955048, 'bagging_temperature': 2.4146967394017236}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:29,689] Trial 577 finished with value: 0.7989763572630556 and parameters: {'iterations': 213, 'objective': 'Logloss', 'colsample_bylevel': 0.09584344373713369, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03595650617445744, 'bagging_temperature': 2.7438873485784248}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:31,979] Trial 578 finished with value: 0.8013478987650319 and parameters: {'iterations': 244, 'objective': 'Logloss', 'colsample_bylevel': 0.09714094644445596, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03977322688841387, 'bagging_temperature': 2.9790745048794056}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:32,698] Trial 579 finished with value: 0.774204314841422 and parameters: {'iterations': 331, 'objective': 'Logloss', 'colsample_bylevel': 0.014330119623341048, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04630087526359415, 'bagging_temperature': 2.4736824777110957}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:35,051] Trial 580 finished with value: 0.8003907844200568 and parameters: {'iterations': 281, 'objective': 'Logloss', 'colsample_bylevel': 0.09828312978336246, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03416502799364973, 'bagging_temperature': 2.6857064116610565}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:44,628] Trial 581 finished with value: 0.7953569891150606 and parameters: {'iterations': 186, 'objective': 'Logloss', 'colsample_bylevel': 0.09207278302278203, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.038616505117534665, 'bagging_temperature': 3.013154142884699}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:45,892] Trial 582 finished with value: 0.7958355462875479 and parameters: {'iterations': 236, 'objective': 'Logloss', 'colsample_bylevel': 0.09432376844392103, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03619623149674352, 'bagging_temperature': 3.2867312142104543}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:47,815] Trial 583 finished with value: 0.800245428269535 and parameters: {'iterations': 304, 'objective': 'Logloss', 'colsample_bylevel': 0.09594829241808998, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.09632032385900326, 'bagging_temperature': 2.330052022017696}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:48,554] Trial 584 finished with value: 0.7937972058075372 and parameters: {'iterations': 267, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.03784216139766325, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03133144563158418, 'bagging_temperature': 2.5238815729399864}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:50,346] Trial 585 finished with value: 0.7942545186195639 and parameters: {'iterations': 148, 'objective': 'Logloss', 'colsample_bylevel': 0.09811177632268486, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04259964235183093, 'bagging_temperature': 1.236976251926891}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:52,890] Trial 586 finished with value: 0.7974165739555322 and parameters: {'iterations': 215, 'objective': 'Logloss', 'colsample_bylevel': 0.0935251050064904, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.027804178772137547, 'bagging_temperature': 6.381814882239139}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:55,983] Trial 587 finished with value: 0.7997881154575085 and parameters: {'iterations': 329, 'objective': 'Logloss', 'colsample_bylevel': 0.09988949810522627, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03368859253458178, 'bagging_temperature': 2.799252792162728}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:57,210] Trial 588 finished with value: 0.7999955275030608 and parameters: {'iterations': 195, 'objective': 'Logloss', 'colsample_bylevel': 0.09111543811461284, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03813873983966434, 'bagging_temperature': 0.7988006890357155}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:40:59,533] Trial 589 finished with value: 0.7980404872785416 and parameters: {'iterations': 273, 'objective': 'Logloss', 'colsample_bylevel': 0.09571704122156766, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04135040627636672, 'bagging_temperature': 2.1718031566732354}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:01,319] Trial 590 finished with value: 0.7966881160165706 and parameters: {'iterations': 297, 'objective': 'Logloss', 'colsample_bylevel': 0.09717728701291045, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.024532190545924724, 'bagging_temperature': 3.067617022630886}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:03,731] Trial 591 finished with value: 0.7951283327090473 and parameters: {'iterations': 249, 'objective': 'Logloss', 'colsample_bylevel': 0.09449606554305587, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 17, 'learning_rate': 0.0355521501091335, 'subsample': 0.10698332702318852}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:08,890] Trial 592 finished with value: 0.7923207227555054 and parameters: {'iterations': 1065, 'objective': 'Logloss', 'colsample_bylevel': 0.09817857360133836, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.031309061793117085, 'bagging_temperature': 2.5895577127122342}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:10,025] Trial 593 finished with value: 0.7971666731890581 and parameters: {'iterations': 162, 'objective': 'Logloss', 'colsample_bylevel': 0.09226187696169881, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04478105378193989, 'bagging_temperature': 2.3559884237712128}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:12,825] Trial 594 finished with value: 0.7945664752810685 and parameters: {'iterations': 343, 'objective': 'Logloss', 'colsample_bylevel': 0.09644930983666317, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0395765142782431, 'bagging_temperature': 0.9478270817232447}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:14,607] Trial 595 finished with value: 0.7995169703305735 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09372747265580539, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.033263766070574, 'bagging_temperature': 2.9588762520063194}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:18,669] Trial 596 finished with value: 0.7970213170385362 and parameters: {'iterations': 303, 'objective': 'Logloss', 'colsample_bylevel': 0.09828681741226111, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.036559817837123205, 'bagging_temperature': 2.8470475065728156}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:20,253] Trial 597 finished with value: 0.8005573849310398 and parameters: {'iterations': 257, 'objective': 'Logloss', 'colsample_bylevel': 0.09100122505995156, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02866939621858269, 'bagging_temperature': 4.597101732875191}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:21,316] Trial 598 finished with value: 0.7978738867675588 and parameters: {'iterations': 184, 'objective': 'Logloss', 'colsample_bylevel': 0.09560852841878947, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03411326029910383, 'bagging_temperature': 3.3264387678545693}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:22,319] Trial 599 finished with value: 0.7991429577740383 and parameters: {'iterations': 121, 'objective': 'Logloss', 'colsample_bylevel': 0.09996983295578508, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.031028184571301888, 'bagging_temperature': 1.3097735262342756}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:33,399] Trial 600 finished with value: 0.7972499734445495 and parameters: {'iterations': 230, 'objective': 'Logloss', 'colsample_bylevel': 0.08958782877855885, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03906878706659397, 'bagging_temperature': 2.6201360899822626}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:36,332] Trial 601 finished with value: 0.8006406851865312 and parameters: {'iterations': 341, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09722303692561673, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04169265604407884, 'bagging_temperature': 8.002510592136352}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:38,563] Trial 602 finished with value: 0.7982903880450157 and parameters: {'iterations': 288, 'objective': 'Logloss', 'colsample_bylevel': 0.09500370797093724, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.036798255969629746, 'bagging_temperature': 2.140185512129538}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:46,851] Trial 603 finished with value: 0.7965640042265095 and parameters: {'iterations': 1424, 'objective': 'Logloss', 'colsample_bylevel': 0.09302897208799993, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.0263238247935717, 'bagging_temperature': 2.407735086330326}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:47,769] Trial 604 finished with value: 0.7981237875340328 and parameters: {'iterations': 202, 'objective': 'Logloss', 'colsample_bylevel': 0.08475148738135863, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.047716146815695165, 'bagging_temperature': 1.7074044478308743}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:49,408] Trial 605 finished with value: 0.7992262580295296 and parameters: {'iterations': 250, 'objective': 'Logloss', 'colsample_bylevel': 0.0984358364643493, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03383788045772281, 'bagging_temperature': 6.685111084440463}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:50,905] Trial 606 finished with value: 0.7988930570075642 and parameters: {'iterations': 159, 'objective': 'Logloss', 'colsample_bylevel': 0.09643353510573331, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030317173546318268, 'bagging_temperature': 9.123746058994989}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:53,708] Trial 607 finished with value: 0.7974165739555322 and parameters: {'iterations': 319, 'objective': 'Logloss', 'colsample_bylevel': 0.09995765953885276, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03778770239166432, 'bagging_temperature': 2.7541690780994026}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:55,719] Trial 608 finished with value: 0.7983524439400461 and parameters: {'iterations': 376, 'objective': 'Logloss', 'colsample_bylevel': 0.09107189679464678, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.05189161777639095, 'bagging_temperature': 2.3021399233822977}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:41:59,379] Trial 609 finished with value: 0.7945664752810685 and parameters: {'iterations': 273, 'objective': 'Logloss', 'colsample_bylevel': 0.09426129797897598, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.043013927244751664, 'bagging_temperature': 1.4316943732792258}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:00,603] Trial 610 finished with value: 0.8010979979985575 and parameters: {'iterations': 223, 'objective': 'Logloss', 'colsample_bylevel': 0.09666053026314124, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03521166401321047, 'bagging_temperature': 7.001024458597643}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:02,742] Trial 611 finished with value: 0.8022837687495458 and parameters: {'iterations': 439, 'objective': 'Logloss', 'colsample_bylevel': 0.09267170520215054, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.03997778077292852, 'subsample': 0.18313089658196585}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:05,080] Trial 612 finished with value: 0.7995382146910344 and parameters: {'iterations': 418, 'objective': 'Logloss', 'colsample_bylevel': 0.09543364336278407, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04563787509903873, 'bagging_temperature': 3.7014913840222827}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:07,277] Trial 613 finished with value: 0.8009526418480357 and parameters: {'iterations': 424, 'objective': 'Logloss', 'colsample_bylevel': 0.09296852727786584, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03968905072699646, 'bagging_temperature': 1.934389176076256}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:10,190] Trial 614 finished with value: 0.8003287285250263 and parameters: {'iterations': 438, 'objective': 'Logloss', 'colsample_bylevel': 0.09809382126198425, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.04293711837541778, 'subsample': 0.37185070373175494}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:11,425] Trial 615 finished with value: 0.7978951311280196 and parameters: {'iterations': 100, 'objective': 'Logloss', 'colsample_bylevel': 0.09525506178973592, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.08643231325004934, 'subsample': 0.16557525192330508}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:14,032] Trial 616 finished with value: 0.7997668710970477 and parameters: {'iterations': 444, 'objective': 'Logloss', 'colsample_bylevel': 0.0982123492443371, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.039913609431664825, 'subsample': 0.2251261576716509}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:15,005] Trial 617 finished with value: 0.7937759614470763 and parameters: {'iterations': 484, 'objective': 'Logloss', 'colsample_bylevel': 0.046437027311540945, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.04496438412400418, 'subsample': 0.29367702801593326}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:17,245] Trial 618 finished with value: 0.800245428269535 and parameters: {'iterations': 479, 'objective': 'Logloss', 'colsample_bylevel': 0.09321596832743372, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.04203284753869686, 'subsample': 0.5051561301323838}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:19,933] Trial 619 finished with value: 0.8018052115770582 and parameters: {'iterations': 398, 'objective': 'Logloss', 'colsample_bylevel': 0.09677793189554383, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.0612851262296461, 'subsample': 0.36358323589573066}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:26,727] Trial 620 finished with value: 0.7992050136690688 and parameters: {'iterations': 183, 'objective': 'Logloss', 'colsample_bylevel': 0.09521658821982647, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.03800663622199087, 'subsample': 0.47743798776459073}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:30,081] Trial 621 finished with value: 0.79662606012154 and parameters: {'iterations': 443, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09850536292294292, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 9, 'learning_rate': 0.041206060924011335, 'subsample': 0.14442329366645168}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:31,307] Trial 622 finished with value: 0.7988310011125336 and parameters: {'iterations': 205, 'objective': 'Logloss', 'colsample_bylevel': 0.09387385420886461, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.037030403416444146, 'subsample': 0.30188589497144214}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:34,941] Trial 623 finished with value: 0.7960642026935613 and parameters: {'iterations': 463, 'objective': 'Logloss', 'colsample_bylevel': 0.09997852866062294, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0318584773725929, 'bagging_temperature': 1.0521312493897765}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:39,911] Trial 624 finished with value: 0.7978951311280196 and parameters: {'iterations': 393, 'objective': 'Logloss', 'colsample_bylevel': 0.09673856623501595, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.04793203651888476, 'subsample': 0.42656332638937455}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:40,870] Trial 625 finished with value: 0.7932353483795584 and parameters: {'iterations': 142, 'objective': 'Logloss', 'colsample_bylevel': 0.09224241285322243, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.040083184093525785, 'bagging_temperature': 1.611378805929081}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:42,812] Trial 626 finished with value: 0.7978738867675588 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.0949358612831383, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.0357453416653398, 'subsample': 0.2075160725965745}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:43,924] Trial 627 finished with value: 0.7929233917180539 and parameters: {'iterations': 179, 'objective': 'Logloss', 'colsample_bylevel': 0.08973406457404502, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.027924075235510465, 'subsample': 0.36163427009106713}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:46,177] Trial 628 finished with value: 0.7972499734445495 and parameters: {'iterations': 262, 'objective': 'Logloss', 'colsample_bylevel': 0.09725281849852324, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.043615110920313824, 'bagging_temperature': 3.0713360630654654}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:48,199] Trial 629 finished with value: 0.7989763572630556 and parameters: {'iterations': 212, 'objective': 'Logloss', 'colsample_bylevel': 0.0983611219553686, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03805255335194793, 'bagging_temperature': 4.187917065254741}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:50,941] Trial 630 finished with value: 0.7993503698195906 and parameters: {'iterations': 352, 'objective': 'Logloss', 'colsample_bylevel': 0.09218858643247262, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0324202900203946, 'bagging_temperature': 2.0915997104639477}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:53,466] Trial 631 finished with value: 0.7955235896260434 and parameters: {'iterations': 239, 'objective': 'Logloss', 'colsample_bylevel': 0.09571812037912032, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03543680588660445, 'bagging_temperature': 3.875741650122372}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:42:54,990] Trial 632 finished with value: 0.7980192429180806 and parameters: {'iterations': 286, 'objective': 'Logloss', 'colsample_bylevel': 0.09380006688142851, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 10, 'learning_rate': 0.04033565865990895, 'subsample': 0.11518901815238136}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:06,335] Trial 633 finished with value: 0.7972499734445495 and parameters: {'iterations': 1273, 'objective': 'Logloss', 'colsample_bylevel': 0.0999514392355876, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.029477461445200186, 'bagging_temperature': 3.172502584156283}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:07,920] Trial 634 finished with value: 0.7984357441955376 and parameters: {'iterations': 156, 'objective': 'Logloss', 'colsample_bylevel': 0.09793415484196981, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.024857823768232205, 'bagging_temperature': 5.449300946177269}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:09,197] Trial 635 finished with value: 0.7979571870230502 and parameters: {'iterations': 191, 'objective': 'Logloss', 'colsample_bylevel': 0.09618699394452258, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.033753462580034706, 'bagging_temperature': 1.7740879666841223}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:11,500] Trial 636 finished with value: 0.8010979979985575 and parameters: {'iterations': 372, 'objective': 'Logloss', 'colsample_bylevel': 0.08866836118374878, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03758628457938784, 'bagging_temperature': 3.3733260178958346}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:13,036] Trial 637 finished with value: 0.7977905865120675 and parameters: {'iterations': 260, 'objective': 'Logloss', 'colsample_bylevel': 0.09097528806106303, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04450020629718009, 'bagging_temperature': 0.5739038256148313}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:16,538] Trial 638 finished with value: 0.8023670690050372 and parameters: {'iterations': 411, 'objective': 'Logloss', 'colsample_bylevel': 0.09428781555899367, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03589752719311934, 'bagging_temperature': 9.30766849428413}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:22,034] Trial 639 finished with value: 0.7988097567520727 and parameters: {'iterations': 382, 'objective': 'Logloss', 'colsample_bylevel': 0.09479713120981623, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 14, 'learning_rate': 0.031149696252304522}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:24,673] Trial 640 finished with value: 0.7963974037155269 and parameters: {'iterations': 464, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09688831913244729, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.034182942214747485, 'bagging_temperature': 9.419529036040732}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:28,247] Trial 641 finished with value: 0.7959188465430393 and parameters: {'iterations': 415, 'objective': 'Logloss', 'colsample_bylevel': 0.09858223957788766, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.05586157553703242, 'bagging_temperature': 8.903274258731967}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:29,427] Trial 642 finished with value: 0.7966881160165706 and parameters: {'iterations': 418, 'objective': 'Logloss', 'colsample_bylevel': 0.05960458381468828, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.02706204750671748, 'bagging_temperature': 9.522042267857085}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:41,700] Trial 643 finished with value: 0.7970213170385362 and parameters: {'iterations': 442, 'objective': 'Logloss', 'colsample_bylevel': 0.09433729672311628, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.10146335198064364, 'bagging_temperature': 5.022820522621975}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:46,345] Trial 644 finished with value: 0.7964186480759877 and parameters: {'iterations': 408, 'objective': 'Logloss', 'colsample_bylevel': 0.09680858423856956, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04014577706888491, 'bagging_temperature': 9.767893670594336}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:49,502] Trial 645 finished with value: 0.8006406851865312 and parameters: {'iterations': 361, 'objective': 'Logloss', 'colsample_bylevel': 0.09844628623868276, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 18, 'learning_rate': 0.035473268858709606, 'bagging_temperature': 9.32742248034862}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:51,509] Trial 646 finished with value: 0.7971046172940276 and parameters: {'iterations': 380, 'objective': 'Logloss', 'colsample_bylevel': 0.09341737700461222, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.032412450589160535, 'bagging_temperature': 9.220567819747627}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:55,037] Trial 647 finished with value: 0.7935685494015239 and parameters: {'iterations': 472, 'objective': 'Logloss', 'colsample_bylevel': 0.0956464904262128, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 13, 'learning_rate': 0.04937614104048595, 'subsample': 0.16515354076216784}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:43:58,098] Trial 648 finished with value: 0.7986023447065203 and parameters: {'iterations': 437, 'objective': 'Logloss', 'colsample_bylevel': 0.09653467567413247, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029881545939145465, 'bagging_temperature': 8.986675664087276}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:00,953] Trial 649 finished with value: 0.7971666731890581 and parameters: {'iterations': 335, 'objective': 'Logloss', 'colsample_bylevel': 0.09872652034396186, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.038140602758900244, 'bagging_temperature': 8.694623685390136}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:03,707] Trial 650 finished with value: 0.794109162469042 and parameters: {'iterations': 403, 'objective': 'Logloss', 'colsample_bylevel': 0.0938879133812063, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04235381349759726, 'bagging_temperature': 9.503912920951514}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:04,535] Trial 651 finished with value: 0.7997668710970477 and parameters: {'iterations': 133, 'objective': 'Logloss', 'colsample_bylevel': 0.09583807563001367, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03579477406669508, 'bagging_temperature': 9.320474251473746}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:07,128] Trial 652 finished with value: 0.8006406851865312 and parameters: {'iterations': 355, 'objective': 'Logloss', 'colsample_bylevel': 0.09246462375127645, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03281113934637879, 'bagging_temperature': 9.067062131921011}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:08,258] Trial 653 finished with value: 0.7952736888595692 and parameters: {'iterations': 314, 'objective': 'Logloss', 'colsample_bylevel': 0.09717871763421236, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.03988161720559372}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:09,484] Trial 654 finished with value: 0.7971666731890581 and parameters: {'iterations': 183, 'objective': 'Logloss', 'colsample_bylevel': 0.09473489546355388, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04587805206335842, 'bagging_temperature': 1.3943721486640257}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:14,289] Trial 655 finished with value: 0.7984357441955376 and parameters: {'iterations': 395, 'objective': 'Logloss', 'colsample_bylevel': 0.09846726157771296, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.023463383579503505, 'bagging_temperature': 0.20183957623992277}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:15,705] Trial 656 finished with value: 0.8012645985095403 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09711144095490531, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029721222265013024, 'bagging_temperature': 1.9465867451570968}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:19,722] Trial 657 finished with value: 0.7988310011125336 and parameters: {'iterations': 452, 'objective': 'Logloss', 'colsample_bylevel': 0.09977060262115289, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.036750248777168634, 'bagging_temperature': 2.185942281939009}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:46,910] Trial 658 finished with value: 0.7943378188750553 and parameters: {'iterations': 291, 'objective': 'Logloss', 'colsample_bylevel': 0.09365544060226864, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.1493323619780129, 'bagging_temperature': 1.1779384203692835}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:49,069] Trial 659 finished with value: 0.7984357441955376 and parameters: {'iterations': 332, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.099993203236979, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.033792749918711465, 'bagging_temperature': 1.6307474199234}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:49,973] Trial 660 finished with value: 0.7969380167830448 and parameters: {'iterations': 159, 'objective': 'Logloss', 'colsample_bylevel': 0.09604176125690728, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.03878489848896729, 'subsample': 0.9449899394954057}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:51,751] Trial 661 finished with value: 0.7981025431735721 and parameters: {'iterations': 231, 'objective': 'Logloss', 'colsample_bylevel': 0.09246659137016244, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.02734045018724213, 'bagging_temperature': 2.4518191790612582}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:54,268] Trial 662 finished with value: 0.7986023447065203 and parameters: {'iterations': 266, 'objective': 'Logloss', 'colsample_bylevel': 0.09792330758354369, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03178522690636526, 'bagging_temperature': 9.592059515316274}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:55,759] Trial 663 finished with value: 0.7992050136690688 and parameters: {'iterations': 129, 'objective': 'Logloss', 'colsample_bylevel': 0.09022478715773652, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04176821472390735, 'bagging_temperature': 1.8878414908562844}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:58,070] Trial 664 finished with value: 0.7987477008570422 and parameters: {'iterations': 361, 'objective': 'Logloss', 'colsample_bylevel': 0.09487159989502485, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 12, 'learning_rate': 0.03578161735176338}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:44:59,848] Trial 665 finished with value: 0.8007239854420224 and parameters: {'iterations': 198, 'objective': 'Logloss', 'colsample_bylevel': 0.09676782569414553, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03805252554579361, 'bagging_temperature': 9.142125921766604}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:01,577] Trial 666 finished with value: 0.7977285306170367 and parameters: {'iterations': 305, 'objective': 'Logloss', 'colsample_bylevel': 0.09990723914129468, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04450374759015526, 'bagging_temperature': 2.268303117435058}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:03,520] Trial 667 finished with value: 0.800786041337053 and parameters: {'iterations': 233, 'objective': 'Logloss', 'colsample_bylevel': 0.09502384732727466, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03369196157760718, 'bagging_temperature': 2.852685977173316}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:05,300] Trial 668 finished with value: 0.7959188465430393 and parameters: {'iterations': 401, 'objective': 'Logloss', 'colsample_bylevel': 0.0922494847041289, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.04169974741684256, 'bagging_temperature': 3.5550937205525406}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:13,988] Trial 669 finished with value: 0.7917376209670657 and parameters: {'iterations': 1321, 'objective': 'Logloss', 'colsample_bylevel': 0.09800131033058855, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030115191013216743, 'bagging_temperature': 2.528054133694659}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:14,582] Trial 670 finished with value: 0.7860603451649513 and parameters: {'iterations': 167, 'objective': 'Logloss', 'colsample_bylevel': 0.02931434618389659, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.0358389566847334, 'bagging_temperature': 1.5314006059361709}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:16,943] Trial 671 finished with value: 0.796230803204544 and parameters: {'iterations': 277, 'objective': 'Logloss', 'colsample_bylevel': 0.09530503887026467, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.026817155693666293, 'bagging_temperature': 2.042858290332476}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:18,262] Trial 672 finished with value: 0.8001621280140436 and parameters: {'iterations': 200, 'objective': 'Logloss', 'colsample_bylevel': 0.09352798847297107, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0403261308339232, 'bagging_temperature': 0.9685292981737414}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:22,302] Trial 673 finished with value: 0.8008693415925444 and parameters: {'iterations': 332, 'objective': 'Logloss', 'colsample_bylevel': 0.09712643404883395, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 12, 'learning_rate': 0.032523275461892945, 'subsample': 0.8434576187918796}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:24,111] Trial 674 finished with value: 0.801576555171045 and parameters: {'iterations': 256, 'objective': 'Logloss', 'colsample_bylevel': 0.09996858197850303, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0381218711118153, 'bagging_temperature': 2.3735050337620263}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:26,927] Trial 675 finished with value: 0.7917376209670657 and parameters: {'iterations': 436, 'objective': 'Logloss', 'colsample_bylevel': 0.09070667438598132, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.11229598848285646, 'bagging_temperature': 2.77780521620263}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:28,819] Trial 676 finished with value: 0.7991429577740383 and parameters: {'iterations': 299, 'objective': 'Logloss', 'colsample_bylevel': 0.09827090926290888, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.035204115851002485, 'bagging_temperature': 3.070405554689532}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:39,472] Trial 677 finished with value: 0.7981237875340328 and parameters: {'iterations': 498, 'objective': 'Logloss', 'colsample_bylevel': 0.0943401412629794, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.04706444622252478}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:40,153] Trial 678 finished with value: 0.775161429186397 and parameters: {'iterations': 237, 'objective': 'Logloss', 'colsample_bylevel': 0.022989437980230384, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02977697629289904, 'bagging_temperature': 1.2919211091691487}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:42,068] Trial 679 finished with value: 0.799850171352539 and parameters: {'iterations': 364, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09594125364978591, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.043501212846769596, 'bagging_temperature': 2.5852659791401296}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:43,363] Trial 680 finished with value: 0.7968547165275535 and parameters: {'iterations': 209, 'objective': 'Logloss', 'colsample_bylevel': 0.09290774383640696, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03898391134995939, 'bagging_temperature': 6.0399988266923135}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:44,510] Trial 681 finished with value: 0.7958355462875479 and parameters: {'iterations': 173, 'objective': 'Logloss', 'colsample_bylevel': 0.08694163619194072, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03265428345583354, 'bagging_temperature': 2.2623317737246205}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:47,298] Trial 682 finished with value: 0.800786041337053 and parameters: {'iterations': 280, 'objective': 'Logloss', 'colsample_bylevel': 0.08904162580133732, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.05204560070219412, 'bagging_temperature': 1.7860721667010004}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:49,895] Trial 683 finished with value: 0.8002241839090741 and parameters: {'iterations': 317, 'objective': 'Logloss', 'colsample_bylevel': 0.09745076077509703, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.023883591901805074, 'bagging_temperature': 0.7004828383497042}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:50,732] Trial 684 finished with value: 0.7969380167830448 and parameters: {'iterations': 149, 'objective': 'Logloss', 'colsample_bylevel': 0.09176962361415165, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03676720306150974, 'bagging_temperature': 2.9165282525825162}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:52,748] Trial 685 finished with value: 0.801576555171045 and parameters: {'iterations': 249, 'objective': 'Logloss', 'colsample_bylevel': 0.09586924055973693, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.064593910253714, 'subsample': 0.5340713781317595}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:53,893] Trial 686 finished with value: 0.794899676303034 and parameters: {'iterations': 344, 'objective': 'Logloss', 'colsample_bylevel': 0.09792794643114118, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034979590061712834, 'bagging_temperature': 8.492392754796308}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:55,605] Trial 687 finished with value: 0.7947543201525121 and parameters: {'iterations': 387, 'objective': 'Logloss', 'colsample_bylevel': 0.09406476551886309, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0419363792042381, 'bagging_temperature': 9.244589106374232}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:45:56,453] Trial 688 finished with value: 0.796750171911601 and parameters: {'iterations': 209, 'objective': 'Logloss', 'colsample_bylevel': 0.0650659509586761, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.029394005893077575, 'bagging_temperature': 8.826301227724572}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:01,172] Trial 689 finished with value: 0.7942757629800247 and parameters: {'iterations': 431, 'objective': 'Logloss', 'colsample_bylevel': 0.09833768641475765, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.03205474955357502}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:02,256] Trial 690 finished with value: 0.7968547165275535 and parameters: {'iterations': 114, 'objective': 'Logloss', 'colsample_bylevel': 0.09654233248598815, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03839584388010964, 'bagging_temperature': 2.117593470107951}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:04,355] Trial 691 finished with value: 0.7990596575185469 and parameters: {'iterations': 278, 'objective': 'Logloss', 'colsample_bylevel': 0.09158349600219058, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034709898870611815, 'bagging_temperature': 2.6132575367682414}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:06,159] Trial 692 finished with value: 0.7989763572630556 and parameters: {'iterations': 228, 'objective': 'Logloss', 'colsample_bylevel': 0.09506041132437948, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.039495701027110426, 'bagging_temperature': 2.4514194302361165}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:07,169] Trial 693 finished with value: 0.7925281348010578 and parameters: {'iterations': 184, 'objective': 'Logloss', 'colsample_bylevel': 0.09852811643484141, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.027647892889168614, 'bagging_temperature': 9.724485067572992}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:09,373] Trial 694 finished with value: 0.7949829765585253 and parameters: {'iterations': 316, 'objective': 'Logloss', 'colsample_bylevel': 0.09989999665393352, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.04524643479638993, 'bagging_temperature': 3.2364030709108302}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:13,923] Trial 695 finished with value: 0.7915302089215133 and parameters: {'iterations': 473, 'objective': 'Logloss', 'colsample_bylevel': 0.09316127462767017, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.036641972836094605, 'bagging_temperature': 2.7536553477008665}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:15,424] Trial 696 finished with value: 0.7907184507270603 and parameters: {'iterations': 254, 'objective': 'Logloss', 'colsample_bylevel': 0.0891419590041884, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03143689682031074, 'bagging_temperature': 1.5444998821786378}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:18,215] Trial 697 finished with value: 0.8010359421035271 and parameters: {'iterations': 1539, 'objective': 'Logloss', 'colsample_bylevel': 0.05289196001143651, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.040723853426996845, 'bagging_temperature': 2.310109944340454}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:21,035] Trial 698 finished with value: 0.7924448345455665 and parameters: {'iterations': 352, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09583365384764729, 'depth': 1, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.033901625425177134, 'subsample': 0.6102822124423793}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:30,125] Trial 699 finished with value: 0.7912590637945781 and parameters: {'iterations': 1014, 'objective': 'Logloss', 'colsample_bylevel': 0.0971867265062447, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03599673093629265, 'bagging_temperature': 1.8392052239910912}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:32,324] Trial 700 finished with value: 0.795045032453556 and parameters: {'iterations': 297, 'objective': 'Logloss', 'colsample_bylevel': 0.09380792539875664, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.025330359771389768, 'bagging_temperature': 2.886981034119084}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:33,688] Trial 701 finished with value: 0.7953782334755214 and parameters: {'iterations': 214, 'objective': 'Logloss', 'colsample_bylevel': 0.095665224763573, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.07409736530942511, 'bagging_temperature': 2.532279589945544}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:35,811] Trial 702 finished with value: 0.795440289370552 and parameters: {'iterations': 417, 'objective': 'Logloss', 'colsample_bylevel': 0.09040644889084475, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 14, 'learning_rate': 0.04858532478313476}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:37,180] Trial 703 finished with value: 0.8003907844200568 and parameters: {'iterations': 145, 'objective': 'Logloss', 'colsample_bylevel': 0.09795888955043805, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.043373152399516654, 'bagging_temperature': 2.069670834692898}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:39,473] Trial 704 finished with value: 0.7967714162720619 and parameters: {'iterations': 263, 'objective': 'Logloss', 'colsample_bylevel': 0.09196881802883612, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.031035603141480878, 'bagging_temperature': 9.421324794048672}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:43,202] Trial 705 finished with value: 0.7973120293395799 and parameters: {'iterations': 386, 'objective': 'Logloss', 'colsample_bylevel': 0.09475636226117126, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03855852263391378, 'bagging_temperature': 1.3589036037509323}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:43,979] Trial 706 finished with value: 0.7958355462875479 and parameters: {'iterations': 180, 'objective': 'Logloss', 'colsample_bylevel': 0.06991187874590934, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03367011707594534, 'bagging_temperature': 2.652054494681986}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:45,964] Trial 707 finished with value: 0.7971666731890581 and parameters: {'iterations': 329, 'objective': 'Logloss', 'colsample_bylevel': 0.08562547730810313, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.041289225210701844, 'bagging_temperature': 2.4230188018398424}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:47,448] Trial 708 finished with value: 0.7945877196415294 and parameters: {'iterations': 234, 'objective': 'Logloss', 'colsample_bylevel': 0.09993216106759266, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.06909059408451966, 'bagging_temperature': 3.0719613465514146}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:49,210] Trial 709 finished with value: 0.7988930570075642 and parameters: {'iterations': 288, 'objective': 'Logloss', 'colsample_bylevel': 0.09711028218690228, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.037677569637301865, 'bagging_temperature': 1.1005309516146895}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:49,948] Trial 710 finished with value: 0.795045032453556 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.09307343022317173, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 11, 'learning_rate': 0.028052589489843256, 'subsample': 0.2607280060364303}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:51,377] Trial 711 finished with value: 0.8000788277585523 and parameters: {'iterations': 250, 'objective': 'Logloss', 'colsample_bylevel': 0.09999487771312374, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.035578639928476576, 'bagging_temperature': 2.138314128214881}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:54,894] Trial 712 finished with value: 0.7977072862565759 and parameters: {'iterations': 298, 'objective': 'Logloss', 'colsample_bylevel': 0.08783544720766402, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.031175366819422363, 'bagging_temperature': 2.85751811997952}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:55,498] Trial 713 finished with value: 0.7721872187218722 and parameters: {'iterations': 166, 'objective': 'Logloss', 'colsample_bylevel': 0.017818342521808583, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.040552660595038564, 'bagging_temperature': 3.4748597025792893}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:46:57,323] Trial 714 finished with value: 0.7978738867675588 and parameters: {'iterations': 368, 'objective': 'Logloss', 'colsample_bylevel': 0.05632259637386013, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.03289225882574544}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:14,555] Trial 715 finished with value: 0.7920495776285703 and parameters: {'iterations': 276, 'objective': 'Logloss', 'colsample_bylevel': 0.09629833235815558, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.046227394613297584, 'bagging_temperature': 2.623680033155431}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:21,192] Trial 716 finished with value: 0.7978118308725282 and parameters: {'iterations': 1103, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09442132741590759, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03774308378614976, 'bagging_temperature': 1.6923941034309942}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:22,337] Trial 717 finished with value: 0.794109162469042 and parameters: {'iterations': 102, 'objective': 'Logloss', 'colsample_bylevel': 0.09842499525176955, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03485197720851073, 'bagging_temperature': 4.837454016005636}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:24,363] Trial 718 finished with value: 0.7960021467985308 and parameters: {'iterations': 327, 'objective': 'Logloss', 'colsample_bylevel': 0.09666612369506213, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.028801880095285098, 'bagging_temperature': 9.632080172603764}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:26,240] Trial 719 finished with value: 0.7925281348010578 and parameters: {'iterations': 452, 'objective': 'Logloss', 'colsample_bylevel': 0.09180681951884273, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.12522469472609618, 'bagging_temperature': 3.093841694186844}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:27,338] Trial 720 finished with value: 0.7992883139245601 and parameters: {'iterations': 232, 'objective': 'Logloss', 'colsample_bylevel': 0.08979374544322494, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0427948242297255, 'bagging_temperature': 2.308751860212122}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:29,201] Trial 721 finished with value: 0.7986644006015509 and parameters: {'iterations': 198, 'objective': 'Logloss', 'colsample_bylevel': 0.09832721222501829, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02339363178892214, 'bagging_temperature': 2.7662489581071723}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:30,374] Trial 722 finished with value: 0.7967714162720619 and parameters: {'iterations': 409, 'objective': 'Logloss', 'colsample_bylevel': 0.09488049954484191, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03684852941714052, 'bagging_temperature': 1.8821699208062102}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:31,165] Trial 723 finished with value: 0.7980404872785416 and parameters: {'iterations': 263, 'objective': 'Logloss', 'colsample_bylevel': 0.04960052477524734, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 14, 'learning_rate': 0.032491834569191226, 'subsample': 0.7163561823715799}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:32,793] Trial 724 finished with value: 0.7973332737000408 and parameters: {'iterations': 143, 'objective': 'Logloss', 'colsample_bylevel': 0.09652878087458533, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.07975369338861892, 'bagging_temperature': 0.899296801063528}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:35,982] Trial 725 finished with value: 0.8020551123435326 and parameters: {'iterations': 313, 'objective': 'Logloss', 'colsample_bylevel': 0.093056392563758, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03957861639309096, 'bagging_temperature': 9.09288092491606}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:37,539] Trial 726 finished with value: 0.8036981959065472 and parameters: {'iterations': 228, 'objective': 'Logloss', 'colsample_bylevel': 0.09845117633135769, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03511249289641515, 'bagging_temperature': 3.3249243884888444}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:39,743] Trial 727 finished with value: 0.7964594596105574 and parameters: {'iterations': 220, 'objective': 'Logloss', 'colsample_bylevel': 0.09999116422085237, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.030454909586136305, 'bagging_temperature': 3.070226381617618}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:41,694] Trial 728 finished with value: 0.7977905865120675 and parameters: {'iterations': 204, 'objective': 'Logloss', 'colsample_bylevel': 0.09828267846795537, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.026411410943319173, 'bagging_temperature': 3.3711186881094246}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:42,758] Trial 729 finished with value: 0.799850171352539 and parameters: {'iterations': 180, 'objective': 'Logloss', 'colsample_bylevel': 0.09805270823539132, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03324697996714127, 'bagging_temperature': 3.347749142790708}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:44,379] Trial 730 finished with value: 0.7997668710970477 and parameters: {'iterations': 246, 'objective': 'Logloss', 'colsample_bylevel': 0.09856396428957417, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02994090256759035, 'bagging_temperature': 3.730255507495755}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:48,056] Trial 731 finished with value: 0.7981858434290635 and parameters: {'iterations': 243, 'objective': 'Logloss', 'colsample_bylevel': 0.09703626011362793, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.034597979032444215, 'bagging_temperature': 2.9477334392390206}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:49,752] Trial 732 finished with value: 0.798497800090568 and parameters: {'iterations': 281, 'objective': 'Logloss', 'colsample_bylevel': 0.09615458597088473, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.035740788872794055, 'bagging_temperature': 3.212153544747042}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:51,402] Trial 733 finished with value: 0.8022004684940542 and parameters: {'iterations': 172, 'objective': 'Logloss', 'colsample_bylevel': 0.09868560838755853, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03173802259285684, 'bagging_temperature': 2.7282891078238407}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:57,631] Trial 734 finished with value: 0.796230803204544 and parameters: {'iterations': 226, 'objective': 'Logloss', 'colsample_bylevel': 0.09995613893698074, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.028173063971122313, 'bagging_temperature': 3.516955930654164}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:47:59,380] Trial 735 finished with value: 0.800786041337053 and parameters: {'iterations': 198, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09671809260278115, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03675233454828704, 'bagging_temperature': 2.4899158701427595}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:02,334] Trial 736 finished with value: 0.8000788277585523 and parameters: {'iterations': 274, 'objective': 'Logloss', 'colsample_bylevel': 0.09562871020883805, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03300533222913117, 'bagging_temperature': 2.912481210587468}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:03,461] Trial 737 finished with value: 0.8007027410815615 and parameters: {'iterations': 147, 'objective': 'Logloss', 'colsample_bylevel': 0.09859273036772642, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034931541058405964, 'bagging_temperature': 3.221424055103301}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:05,394] Trial 738 finished with value: 0.7970833729335668 and parameters: {'iterations': 249, 'objective': 'Logloss', 'colsample_bylevel': 0.09484627781002539, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.026027435332135567, 'bagging_temperature': 1.2847553363796327}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:06,887] Trial 739 finished with value: 0.8001621280140436 and parameters: {'iterations': 300, 'objective': 'Logloss', 'colsample_bylevel': 0.09716398973468426, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.029929259489846805, 'bagging_temperature': 2.218064673563012}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:08,898] Trial 740 finished with value: 0.7973332737000408 and parameters: {'iterations': 218, 'objective': 'Logloss', 'colsample_bylevel': 0.0973076267843283, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 9, 'learning_rate': 0.037780116150308914}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:10,098] Trial 741 finished with value: 0.8000788277585523 and parameters: {'iterations': 190, 'objective': 'Logloss', 'colsample_bylevel': 0.09989097326729027, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03448876020120651, 'bagging_temperature': 9.274243383922997}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:18,643] Trial 742 finished with value: 0.7912803081550391 and parameters: {'iterations': 1384, 'objective': 'Logloss', 'colsample_bylevel': 0.0948519315406711, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03124824805332038, 'bagging_temperature': 2.5804719572840358}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:21,670] Trial 743 finished with value: 0.7968547165275535 and parameters: {'iterations': 339, 'objective': 'Logloss', 'colsample_bylevel': 0.09827815609933305, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03811415293804406, 'bagging_temperature': 1.4523349904458671}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:22,647] Trial 744 finished with value: 0.7963761593550659 and parameters: {'iterations': 266, 'objective': 'Logloss', 'colsample_bylevel': 0.09573084559233923, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.04159465953635158, 'bagging_temperature': 2.7955453476712195}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:24,738] Trial 745 finished with value: 0.7998289269920781 and parameters: {'iterations': 229, 'objective': 'Logloss', 'colsample_bylevel': 0.09676709408614498, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03349250502726968, 'bagging_temperature': 1.9828414669086254}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:28,109] Trial 746 finished with value: 0.799850171352539 and parameters: {'iterations': 311, 'objective': 'Logloss', 'colsample_bylevel': 0.09424202610912547, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03718437493970189, 'bagging_temperature': 2.385710411435042}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:29,700] Trial 747 finished with value: 0.7983524439400461 and parameters: {'iterations': 165, 'objective': 'Logloss', 'colsample_bylevel': 0.0985983329706667, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.02275566823225791, 'bagging_temperature': 3.1524108055374267}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:34,103] Trial 748 finished with value: 0.7952328773249995 and parameters: {'iterations': 883, 'objective': 'Logloss', 'colsample_bylevel': 0.09990069368488305, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.029246723341159388, 'bagging_temperature': 2.657418819385089}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:40,827] Trial 749 finished with value: 0.7891586674195371 and parameters: {'iterations': 1475, 'objective': 'Logloss', 'colsample_bylevel': 0.09661909339615203, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.050948115732304504, 'bagging_temperature': 3.5854078809074155}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:41,863] Trial 750 finished with value: 0.8014099546600622 and parameters: {'iterations': 122, 'objective': 'Logloss', 'colsample_bylevel': 0.0946540020373109, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 14, 'learning_rate': 0.043646900166451105, 'bagging_temperature': 3.884004114262595}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:44,199] Trial 751 finished with value: 0.8005573849310398 and parameters: {'iterations': 281, 'objective': 'Logloss', 'colsample_bylevel': 0.09813243741133942, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03496336956929464, 'bagging_temperature': 2.9859382254459508}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:45,653] Trial 752 finished with value: 0.7981237875340328 and parameters: {'iterations': 245, 'objective': 'Logloss', 'colsample_bylevel': 0.09395766023688344, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.032426107621452184}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:50,434] Trial 753 finished with value: 0.7940258622135505 and parameters: {'iterations': 207, 'objective': 'Logloss', 'colsample_bylevel': 0.0999620697424453, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 13, 'learning_rate': 0.03849560884095499, 'bagging_temperature': 2.4599926057587655}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:54,948] Trial 754 finished with value: 0.7970213170385362 and parameters: {'iterations': 357, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09609018323510665, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.027467166312091788, 'bagging_temperature': 9.482442706482885}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:56,527] Trial 755 finished with value: 0.7981858434290635 and parameters: {'iterations': 312, 'objective': 'Logloss', 'colsample_bylevel': 0.09095523770110403, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04039397193105438, 'bagging_temperature': 1.6536059948429493}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:58,705] Trial 756 finished with value: 0.7994549144355431 and parameters: {'iterations': 259, 'objective': 'Logloss', 'colsample_bylevel': 0.09749904306242409, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03527355611268747, 'bagging_temperature': 7.366510681198475}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:48:59,464] Trial 757 finished with value: 0.8027410815615723 and parameters: {'iterations': 179, 'objective': 'Logloss', 'colsample_bylevel': 0.0927436705576724, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.030653895358011714, 'bagging_temperature': 2.103518453838227}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:00,145] Trial 758 finished with value: 0.7934019488905413 and parameters: {'iterations': 142, 'objective': 'Logloss', 'colsample_bylevel': 0.08838413221208113, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.024882200243437347, 'bagging_temperature': 2.0214434990456933}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:01,071] Trial 759 finished with value: 0.7963141034600354 and parameters: {'iterations': 170, 'objective': 'Logloss', 'colsample_bylevel': 0.09075515280061404, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.027001399481018623, 'bagging_temperature': 1.7892943096037728}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:01,704] Trial 760 finished with value: 0.7931732924845278 and parameters: {'iterations': 123, 'objective': 'Logloss', 'colsample_bylevel': 0.06222612149445157, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029856576592121896, 'bagging_temperature': 2.171705111306312}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:02,464] Trial 761 finished with value: 0.7930066919735451 and parameters: {'iterations': 184, 'objective': 'Logloss', 'colsample_bylevel': 0.09218656059810937, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03114306509569387, 'bagging_temperature': 1.9551519071923091}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:03,368] Trial 762 finished with value: 0.7941924627245333 and parameters: {'iterations': 156, 'objective': 'Logloss', 'colsample_bylevel': 0.09267092789367443, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.029124462194471568, 'bagging_temperature': 1.5978046798864216}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:04,153] Trial 763 finished with value: 0.793713905552046 and parameters: {'iterations': 192, 'objective': 'Logloss', 'colsample_bylevel': 0.08957085999471992, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.02564166222427062, 'bagging_temperature': 2.245837168497339}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:05,172] Trial 764 finished with value: 0.7961475029490527 and parameters: {'iterations': 212, 'objective': 'Logloss', 'colsample_bylevel': 0.08413074131264309, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.031579978559056116}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:06,327] Trial 765 finished with value: 0.7975619301060541 and parameters: {'iterations': 187, 'objective': 'Logloss', 'colsample_bylevel': 0.09342478356866038, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.09221184073916999, 'bagging_temperature': 2.154569033769676}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:07,380] Trial 766 finished with value: 0.7937972058075372 and parameters: {'iterations': 145, 'objective': 'Logloss', 'colsample_bylevel': 0.08712183500574286, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.022012154050747166, 'bagging_temperature': 1.1563831191725344}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:08,616] Trial 767 finished with value: 0.7973953295950713 and parameters: {'iterations': 222, 'objective': 'Logloss', 'colsample_bylevel': 0.09089647120922739, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03359821920064094, 'bagging_temperature': 1.9257720811954335}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:09,715] Trial 768 finished with value: 0.7930066919735451 and parameters: {'iterations': 167, 'objective': 'Logloss', 'colsample_bylevel': 0.09339334637314402, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029115145747524315, 'bagging_temperature': 1.4069228600793995}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:10,864] Trial 769 finished with value: 0.7971046172940276 and parameters: {'iterations': 215, 'objective': 'Logloss', 'colsample_bylevel': 0.09487271729607842, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03282838992849961, 'bagging_temperature': 9.802073982013978}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:11,493] Trial 770 finished with value: 0.7981858434290635 and parameters: {'iterations': 101, 'objective': 'Logloss', 'colsample_bylevel': 0.09246305721363046, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03639082894862278, 'bagging_temperature': 2.350752314270352}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:12,253] Trial 771 finished with value: 0.793713905552046 and parameters: {'iterations': 186, 'objective': 'Logloss', 'colsample_bylevel': 0.09045464768907827, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03166552540182615, 'bagging_temperature': 1.771964800802638}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:15,269] Trial 772 finished with value: 0.7953569891150606 and parameters: {'iterations': 232, 'objective': 'Logloss', 'colsample_bylevel': 0.09444248455625597, 'depth': 8, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.027084488386764374, 'bagging_temperature': 2.1322734697331773}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:15,970] Trial 773 finished with value: 0.7987477008570422 and parameters: {'iterations': 144, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09531018977222605, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03446520652587438, 'bagging_temperature': 2.3863731246350137}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:17,079] Trial 774 finished with value: 0.7967714162720619 and parameters: {'iterations': 245, 'objective': 'Logloss', 'colsample_bylevel': 0.08812174283412214, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029516149023223782, 'bagging_temperature': 2.513381305219502}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:18,527] Trial 775 finished with value: 0.7922994783950444 and parameters: {'iterations': 201, 'objective': 'Logloss', 'colsample_bylevel': 0.09178779766537257, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.1187795781337399, 'bagging_temperature': 5.631232779202119}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:20,003] Trial 776 finished with value: 0.7980404872785416 and parameters: {'iterations': 168, 'objective': 'Logloss', 'colsample_bylevel': 0.09584816574096172, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.036280970884908566, 'bagging_temperature': 8.924333845002813}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:21,962] Trial 777 finished with value: 0.7978738867675588 and parameters: {'iterations': 228, 'objective': 'Logloss', 'colsample_bylevel': 0.09363570327251726, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.03251979493202219}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:22,802] Trial 778 finished with value: 0.8000788277585523 and parameters: {'iterations': 260, 'objective': 'Logloss', 'colsample_bylevel': 0.08585325187173466, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0341727794924199, 'bagging_temperature': 4.269525633448873}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:23,699] Trial 779 finished with value: 0.7983524439400461 and parameters: {'iterations': 125, 'objective': 'Logloss', 'colsample_bylevel': 0.09603542612638134, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.02535858203560335, 'bagging_temperature': 1.527617765131986}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:24,502] Trial 780 finished with value: 0.7998289269920781 and parameters: {'iterations': 201, 'objective': 'Logloss', 'colsample_bylevel': 0.09368651996515129, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03695473517899779, 'bagging_temperature': 9.274255508890272}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:26,061] Trial 781 finished with value: 0.7999955275030608 and parameters: {'iterations': 282, 'objective': 'Logloss', 'colsample_bylevel': 0.08971688026156603, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0303580410808399, 'bagging_temperature': 1.0319428038864102}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:27,189] Trial 782 finished with value: 0.7970833729335668 and parameters: {'iterations': 164, 'objective': 'Logloss', 'colsample_bylevel': 0.09705158275301834, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.033600319669122594, 'bagging_temperature': 3.2990429625145654}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:28,341] Trial 783 finished with value: 0.7972499734445495 and parameters: {'iterations': 238, 'objective': 'Logloss', 'colsample_bylevel': 0.09180440456812684, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03822309402699482, 'bagging_temperature': 2.9105193557548388}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:29,601] Trial 784 finished with value: 0.8003907844200568 and parameters: {'iterations': 193, 'objective': 'Logloss', 'colsample_bylevel': 0.09542271846386224, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.02899348339075522, 'bagging_temperature': 2.0014601228384605}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:30,897] Trial 785 finished with value: 0.7978738867675588 and parameters: {'iterations': 263, 'objective': 'Logloss', 'colsample_bylevel': 0.09734589579113968, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0356894215008577, 'bagging_temperature': 9.583810971346969}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:31,982] Trial 786 finished with value: 0.7943378188750553 and parameters: {'iterations': 214, 'objective': 'Logloss', 'colsample_bylevel': 0.09325551170647266, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.02236758168065915, 'bagging_temperature': 2.2830647956662182}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:35,192] Trial 787 finished with value: 0.794109162469042 and parameters: {'iterations': 336, 'objective': 'Logloss', 'colsample_bylevel': 0.09527159554089816, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.05829540791398284, 'bagging_temperature': 1.8439890546708957}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:37,153] Trial 788 finished with value: 0.7977905865120675 and parameters: {'iterations': 287, 'objective': 'Logloss', 'colsample_bylevel': 0.09830399500340994, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 12, 'learning_rate': 0.032097884540060094}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:38,514] Trial 789 finished with value: 0.7975619301060541 and parameters: {'iterations': 251, 'objective': 'Logloss', 'colsample_bylevel': 0.09098900199968404, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04024926596489479, 'bagging_temperature': 2.6657670641607303}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:39,578] Trial 790 finished with value: 0.799850171352539 and parameters: {'iterations': 133, 'objective': 'Logloss', 'colsample_bylevel': 0.0970767180388691, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.027208141275066728, 'bagging_temperature': 1.6647767765395396}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:47,703] Trial 791 finished with value: 0.7976452303615456 and parameters: {'iterations': 371, 'objective': 'Logloss', 'colsample_bylevel': 0.0944370815545635, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03606365308328402, 'bagging_temperature': 0.7959083346022527}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:49,165] Trial 792 finished with value: 0.7989763572630556 and parameters: {'iterations': 176, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08904703351811777, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.031527975487936385, 'bagging_temperature': 1.235270595578469}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:50,227] Trial 793 finished with value: 0.7975619301060541 and parameters: {'iterations': 213, 'objective': 'Logloss', 'colsample_bylevel': 0.09264221283421893, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.10806196762628756, 'bagging_temperature': 2.491180710073663}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:51,913] Trial 794 finished with value: 0.8027623259220331 and parameters: {'iterations': 240, 'objective': 'Logloss', 'colsample_bylevel': 0.09995870197177789, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0384843988697299, 'bagging_temperature': 2.8320393878765104}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:53,797] Trial 795 finished with value: 0.7980404872785416 and parameters: {'iterations': 289, 'objective': 'Logloss', 'colsample_bylevel': 0.09991916492516026, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.044672190688712636, 'bagging_temperature': 3.051571687321041}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:55,758] Trial 796 finished with value: 0.795045032453556 and parameters: {'iterations': 322, 'objective': 'Logloss', 'colsample_bylevel': 0.09852060355045238, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.042096165314184385, 'bagging_temperature': 3.3906024545354274}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:49:57,528] Trial 797 finished with value: 0.7959188465430393 and parameters: {'iterations': 268, 'objective': 'Logloss', 'colsample_bylevel': 0.09853539165484287, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03965126117338513, 'bagging_temperature': 5.26561718654597}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:04,475] Trial 798 finished with value: 0.7915089645610525 and parameters: {'iterations': 966, 'objective': 'Logloss', 'colsample_bylevel': 0.09861382313225199, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03842709155516125, 'bagging_temperature': 2.9198536339753263}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:06,706] Trial 799 finished with value: 0.8007239854420224 and parameters: {'iterations': 347, 'objective': 'Logloss', 'colsample_bylevel': 0.09703678172318911, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04766255552302988, 'bagging_temperature': 3.184452684017597}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:08,405] Trial 800 finished with value: 0.7976452303615456 and parameters: {'iterations': 247, 'objective': 'Logloss', 'colsample_bylevel': 0.09858392811702629, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.033708800916308033}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:09,962] Trial 801 finished with value: 0.7981025431735721 and parameters: {'iterations': 298, 'objective': 'Logloss', 'colsample_bylevel': 0.09997222084512193, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04325018941978291, 'bagging_temperature': 2.793428890037755}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:13,044] Trial 802 finished with value: 0.8026169697715113 and parameters: {'iterations': 383, 'objective': 'Logloss', 'colsample_bylevel': 0.0972200274783506, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03558906459362998, 'bagging_temperature': 2.1048031905050197}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:15,957] Trial 803 finished with value: 0.7990596575185469 and parameters: {'iterations': 406, 'objective': 'Logloss', 'colsample_bylevel': 0.09747201935128678, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.039343531670894924, 'bagging_temperature': 2.1599166813530335}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:18,373] Trial 804 finished with value: 0.7993716141800515 and parameters: {'iterations': 383, 'objective': 'Logloss', 'colsample_bylevel': 0.09995234653628593, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03684722576864152, 'bagging_temperature': 2.0781047904765697}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:20,779] Trial 805 finished with value: 0.7977285306170367 and parameters: {'iterations': 378, 'objective': 'Logloss', 'colsample_bylevel': 0.09993725624071735, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04097901423943379, 'bagging_temperature': 2.4032863803916995}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:23,178] Trial 806 finished with value: 0.7997668710970477 and parameters: {'iterations': 403, 'objective': 'Logloss', 'colsample_bylevel': 0.09696390066670828, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.035535664961522176, 'bagging_temperature': 2.253715358874521}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:24,426] Trial 807 finished with value: 0.7962928590995746 and parameters: {'iterations': 368, 'objective': 'Logloss', 'colsample_bylevel': 0.0979291370210683, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.044911684088288745, 'bagging_temperature': 0.011316089288317066}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:28,291] Trial 808 finished with value: 0.799850171352539 and parameters: {'iterations': 350, 'objective': 'Logloss', 'colsample_bylevel': 0.09995934615432478, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.041691144619750355, 'bagging_temperature': 2.5532534751695533}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:30,605] Trial 809 finished with value: 0.8009526418480357 and parameters: {'iterations': 400, 'objective': 'Logloss', 'colsample_bylevel': 0.09612660687959614, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03812839377701331, 'bagging_temperature': 0.4057800469182843}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:44,787] Trial 810 finished with value: 0.7945664752810685 and parameters: {'iterations': 419, 'objective': 'Logloss', 'colsample_bylevel': 0.09691195584981742, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03567588875021291, 'bagging_temperature': 2.0455613527068532}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:46,996] Trial 811 finished with value: 0.7994549144355431 and parameters: {'iterations': 351, 'objective': 'Logloss', 'colsample_bylevel': 0.09811339917524918, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03425251051657525, 'bagging_temperature': 6.138246323945359}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:48,881] Trial 812 finished with value: 0.7961475029490527 and parameters: {'iterations': 327, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09642229020466282, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 10, 'learning_rate': 0.037441981839502154}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:51,203] Trial 813 finished with value: 0.7935473050410631 and parameters: {'iterations': 382, 'objective': 'Logloss', 'colsample_bylevel': 0.09836183089635872, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.0412974966996315, 'bagging_temperature': 2.2938472390253555}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:53,105] Trial 814 finished with value: 0.7991429577740383 and parameters: {'iterations': 328, 'objective': 'Logloss', 'colsample_bylevel': 0.09578011070767928, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04694862027951969, 'bagging_temperature': 2.574597115679917}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:56,216] Trial 815 finished with value: 0.8013478987650319 and parameters: {'iterations': 425, 'objective': 'Logloss', 'colsample_bylevel': 0.0981411940354867, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03305487623659977, 'bagging_temperature': 4.51859601378926}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:57,384] Trial 816 finished with value: 0.7956901901370262 and parameters: {'iterations': 360, 'objective': 'Logloss', 'colsample_bylevel': 0.09607795773548768, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03910743590747185, 'bagging_temperature': 9.380196373604651}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:50:59,258] Trial 817 finished with value: 0.7964594596105574 and parameters: {'iterations': 308, 'objective': 'Logloss', 'colsample_bylevel': 0.09877870884391625, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03455421700036362, 'bagging_temperature': 2.8079541654516205}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:01,956] Trial 818 finished with value: 0.7958355462875479 and parameters: {'iterations': 455, 'objective': 'Logloss', 'colsample_bylevel': 0.09718423332153171, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.04374553549662985, 'bagging_temperature': 1.92121484824255}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:03,035] Trial 819 finished with value: 0.7990596575185469 and parameters: {'iterations': 149, 'objective': 'Logloss', 'colsample_bylevel': 0.09489988436708399, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03027510712744097, 'bagging_temperature': 2.379866339314879}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:03,993] Trial 820 finished with value: 0.800245428269535 and parameters: {'iterations': 274, 'objective': 'Logloss', 'colsample_bylevel': 0.0984629984509603, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.05443590205457501, 'bagging_temperature': 9.043488151980211}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:06,779] Trial 821 finished with value: 0.7975619301060541 and parameters: {'iterations': 382, 'objective': 'Logloss', 'colsample_bylevel': 0.09573070510796176, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.037774803759051574, 'bagging_temperature': 3.08059436673919}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:08,427] Trial 822 finished with value: 0.7994549144355431 and parameters: {'iterations': 183, 'objective': 'Logloss', 'colsample_bylevel': 0.09706305939195235, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.0768228567152382, 'bagging_temperature': 3.532819564734859}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:10,666] Trial 823 finished with value: 0.7916755650720352 and parameters: {'iterations': 335, 'objective': 'Logloss', 'colsample_bylevel': 0.09982370266579847, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.049495251972937644, 'bagging_temperature': 2.696431778281043}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:12,068] Trial 824 finished with value: 0.7963761593550659 and parameters: {'iterations': 309, 'objective': 'Logloss', 'colsample_bylevel': 0.09504247467073126, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.0325159729931907}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:12,861] Trial 825 finished with value: 0.7948163760475426 and parameters: {'iterations': 234, 'objective': 'Logloss', 'colsample_bylevel': 0.09818390058357913, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04088144264427922, 'bagging_temperature': 3.7044321558075852}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:13,904] Trial 826 finished with value: 0.8003074841645655 and parameters: {'iterations': 129, 'objective': 'Logloss', 'colsample_bylevel': 0.09987444067768807, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03595353941642039, 'bagging_temperature': 2.152277836743469}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:16,235] Trial 827 finished with value: 0.7963141034600354 and parameters: {'iterations': 400, 'objective': 'Logloss', 'colsample_bylevel': 0.09658235731184805, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.02847857418709758, 'bagging_temperature': 2.934217940898073}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:17,739] Trial 828 finished with value: 0.7981858434290635 and parameters: {'iterations': 165, 'objective': 'Logloss', 'colsample_bylevel': 0.09432484822975337, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03847634264158969, 'bagging_temperature': 2.4805134667847826}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:24,825] Trial 829 finished with value: 0.7934019488905413 and parameters: {'iterations': 216, 'objective': 'Logloss', 'colsample_bylevel': 0.09708864269602765, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03450671250083451, 'bagging_temperature': 9.76713462528606}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:27,225] Trial 830 finished with value: 0.7997668710970477 and parameters: {'iterations': 279, 'objective': 'Logloss', 'colsample_bylevel': 0.09836452716390583, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03141227726693353, 'bagging_temperature': 3.272137481981111}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:29,262] Trial 831 finished with value: 0.7944423634910074 and parameters: {'iterations': 365, 'objective': 'Logloss', 'colsample_bylevel': 0.09547418879200664, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.04274308431554729, 'bagging_temperature': 8.611500624779945}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:29,906] Trial 832 finished with value: 0.7934231932510021 and parameters: {'iterations': 196, 'objective': 'Logloss', 'colsample_bylevel': 0.034512086253237775, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03687087263490114, 'bagging_temperature': 2.2624629872159474}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:32,068] Trial 833 finished with value: 0.7978738867675588 and parameters: {'iterations': 255, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0999632267693781, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03986060398498316, 'bagging_temperature': 2.7259614035026725}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:33,438] Trial 834 finished with value: 0.7955644011606129 and parameters: {'iterations': 335, 'objective': 'Logloss', 'colsample_bylevel': 0.038977802216386824, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.025266427691336576, 'bagging_temperature': 1.9315640012282023}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:35,281] Trial 835 finished with value: 0.7953569891150606 and parameters: {'iterations': 291, 'objective': 'Logloss', 'colsample_bylevel': 0.09387889107898464, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.033860711803179885, 'bagging_temperature': 9.166329519141437}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:36,185] Trial 836 finished with value: 0.79598090243807 and parameters: {'iterations': 228, 'objective': 'Logloss', 'colsample_bylevel': 0.09614111679373154, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03000463099439875, 'bagging_temperature': 2.534901564529445}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:37,939] Trial 837 finished with value: 0.7941924627245333 and parameters: {'iterations': 177, 'objective': 'Logloss', 'colsample_bylevel': 0.09838800097980928, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.035314028482653345}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:39,346] Trial 838 finished with value: 0.7957522460320566 and parameters: {'iterations': 312, 'objective': 'Logloss', 'colsample_bylevel': 0.0970957618528795, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.020330419248410232, 'bagging_temperature': 9.549772908062426}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:41,640] Trial 839 finished with value: 0.794899676303034 and parameters: {'iterations': 427, 'objective': 'Logloss', 'colsample_bylevel': 0.0944120697329411, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.08216737409394237, 'bagging_temperature': 2.343391848929895}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:43,954] Trial 840 finished with value: 0.795045032453556 and parameters: {'iterations': 263, 'objective': 'Logloss', 'colsample_bylevel': 0.0969490977191726, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.045156804407222834, 'bagging_temperature': 3.007679850920655}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:44,764] Trial 841 finished with value: 0.7899491812535292 and parameters: {'iterations': 156, 'objective': 'Logloss', 'colsample_bylevel': 0.021365430551321223, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03868637511605385, 'bagging_temperature': 2.0810546353591213}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:47,922] Trial 842 finished with value: 0.800245428269535 and parameters: {'iterations': 206, 'objective': 'Logloss', 'colsample_bylevel': 0.09994911787732158, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03209947347614196, 'bagging_temperature': 2.7723575879031093}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:50,345] Trial 843 finished with value: 0.7982691436845549 and parameters: {'iterations': 345, 'objective': 'Logloss', 'colsample_bylevel': 0.0931893137491595, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.027833071231787875, 'bagging_temperature': 1.8427174321118267}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:51,344] Trial 844 finished with value: 0.7977905865120675 and parameters: {'iterations': 126, 'objective': 'Logloss', 'colsample_bylevel': 0.09576350271468156, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.041295490290068154, 'bagging_temperature': 3.182077799017127}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:53,466] Trial 845 finished with value: 0.7999955275030608 and parameters: {'iterations': 237, 'objective': 'Logloss', 'colsample_bylevel': 0.09825192657392104, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.037179224025278776, 'bagging_temperature': 2.574387105099351}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:55,410] Trial 846 finished with value: 0.794899676303034 and parameters: {'iterations': 298, 'objective': 'Logloss', 'colsample_bylevel': 0.09997892698630743, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.03339372469503393, 'bagging_temperature': 4.089895993599642}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:51:57,024] Trial 847 finished with value: 0.7979571870230502 and parameters: {'iterations': 390, 'objective': 'Logloss', 'colsample_bylevel': 0.0940929215652078, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.030277476378933528, 'bagging_temperature': 2.8955364466218594}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:00,601] Trial 848 finished with value: 0.7933186486350499 and parameters: {'iterations': 190, 'objective': 'Logloss', 'colsample_bylevel': 0.09581162355843269, 'depth': 7, 'boosting_type': 'Ordered', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 11, 'learning_rate': 0.0361625618487344}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:03,421] Trial 849 finished with value: 0.7934019488905413 and parameters: {'iterations': 477, 'objective': 'Logloss', 'colsample_bylevel': 0.09775546796076262, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.0424674988063153, 'bagging_temperature': 6.992178814758793}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:05,491] Trial 850 finished with value: 0.8037814961620384 and parameters: {'iterations': 268, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09275903129357747, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03837084528946865, 'bagging_temperature': 8.776240102277319}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:07,134] Trial 851 finished with value: 0.7955856455210739 and parameters: {'iterations': 239, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09158513333292106, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.046195468802431784, 'bagging_temperature': 9.319966058748387}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:08,660] Trial 852 finished with value: 0.8001621280140436 and parameters: {'iterations': 218, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08741398971337856, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03988794939994539, 'bagging_temperature': 9.179583181427345}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:14,181] Trial 853 finished with value: 0.800100072119013 and parameters: {'iterations': 1167, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08986160657036793, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03379317434252623, 'bagging_temperature': 8.574529533977586}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:15,758] Trial 854 finished with value: 0.7960642026935613 and parameters: {'iterations': 264, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09246160525729534, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.02436139409077693, 'bagging_temperature': 8.502863342070658}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:16,489] Trial 855 finished with value: 0.7944831750255771 and parameters: {'iterations': 101, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09035966753826292, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03923366394347318, 'bagging_temperature': 8.765165896863264}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:17,892] Trial 856 finished with value: 0.8020551123435326 and parameters: {'iterations': 159, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09292063085319792, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.02811406328175735, 'bagging_temperature': 9.03413523775167}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:18,681] Trial 857 finished with value: 0.7950237880930949 and parameters: {'iterations': 207, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09155561122858548, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03234469672101736, 'bagging_temperature': 8.662929813765023}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:20,136] Trial 858 finished with value: 0.7996835708415563 and parameters: {'iterations': 261, 'objective': 'Logloss', 'colsample_bylevel': 0.09342926737800991, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.04378129688309806, 'bagging_temperature': 7.573042549108154}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:21,142] Trial 859 finished with value: 0.7978738867675588 and parameters: {'iterations': 188, 'objective': 'Logloss', 'colsample_bylevel': 0.08365560718008017, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03570503517043603, 'bagging_temperature': 8.177773276526512}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:22,853] Trial 860 finished with value: 0.7958355462875479 and parameters: {'iterations': 224, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08800271927751586, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 5, 'learning_rate': 0.037536163784773625}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:27,805] Trial 861 finished with value: 0.7964807039710183 and parameters: {'iterations': 432, 'objective': 'Logloss', 'colsample_bylevel': 0.09474463661052661, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03056929494757065, 'bagging_temperature': 8.437618354226043}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:29,339] Trial 862 finished with value: 0.7953569891150606 and parameters: {'iterations': 279, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08967748862860372, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0417088217220539, 'bagging_temperature': 8.845713547488893}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:30,533] Trial 863 finished with value: 0.7985811003460594 and parameters: {'iterations': 246, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09209249040100638, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.034128576405806015, 'bagging_temperature': 9.092382385145571}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:31,883] Trial 864 finished with value: 0.7968547165275535 and parameters: {'iterations': 162, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09319205919951043, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0392039070602066, 'bagging_temperature': 9.41215014753759}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:33,681] Trial 865 finished with value: 0.7925493791615187 and parameters: {'iterations': 315, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09519879920104368, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.05104095017413222, 'bagging_temperature': 8.977075721855574}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:34,472] Trial 866 finished with value: 0.7901990820200031 and parameters: {'iterations': 198, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.06818509863319196, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.027149943284226527, 'bagging_temperature': 8.805280311138304}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:35,676] Trial 867 finished with value: 0.7986644006015509 and parameters: {'iterations': 363, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09564839308414623, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.036867491028978766, 'bagging_temperature': 7.874355368392134}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:37,468] Trial 868 finished with value: 0.8007027410815615 and parameters: {'iterations': 238, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09382532175456426, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03053245779903472, 'bagging_temperature': 0.6477439486284553}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:45,404] Trial 869 finished with value: 0.8002241839090741 and parameters: {'iterations': 281, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09089265279486027, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.1390356132666395, 'bagging_temperature': 9.929152303772403}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:48,382] Trial 870 finished with value: 0.799850171352539 and parameters: {'iterations': 333, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09683870269944632, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.03395313135018506, 'bagging_temperature': 9.622380579340284}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:49,465] Trial 871 finished with value: 0.7961475029490527 and parameters: {'iterations': 144, 'objective': 'Logloss', 'colsample_bylevel': 0.0943526380415775, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 11, 'learning_rate': 0.043554838825303005, 'bagging_temperature': 8.284831782311901}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:51,921] Trial 872 finished with value: 0.7988097567520727 and parameters: {'iterations': 382, 'objective': 'Logloss', 'colsample_bylevel': 0.0857759221361443, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 8, 'learning_rate': 0.04670579415199511}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:54,193] Trial 873 finished with value: 0.7966881160165706 and parameters: {'iterations': 178, 'objective': 'Logloss', 'colsample_bylevel': 0.09687721642099487, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03954103651134755, 'bagging_temperature': 2.1771407191568586}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:56,018] Trial 874 finished with value: 0.8032196387340598 and parameters: {'iterations': 216, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09191335408771499, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03586124623070199, 'bagging_temperature': 2.0206243153296537}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:57,087] Trial 875 finished with value: 0.8000788277585523 and parameters: {'iterations': 219, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08906104824155169, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03293959121878502, 'bagging_temperature': 1.8006786672495634}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:58,337] Trial 876 finished with value: 0.7992883139245601 and parameters: {'iterations': 212, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08799632308890548, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03618775342725315, 'bagging_temperature': 2.0765930284479945}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:52:59,470] Trial 877 finished with value: 0.7988930570075642 and parameters: {'iterations': 236, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0905639711994187, 'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.022247141352127492, 'bagging_temperature': 1.9952561848558523}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:53:00,635] Trial 878 finished with value: 0.7971666731890581 and parameters: {'iterations': 193, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0915130166645083, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.02981421127553121, 'bagging_temperature': 1.706608340395732}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:53:02,345] Trial 879 finished with value: 0.7971666731890581 and parameters: {'iterations': 260, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0865224904244671, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.041668638486588465, 'bagging_temperature': 2.247603839275346}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:53:03,419] Trial 880 finished with value: 0.8007027410815615 and parameters: {'iterations': 174, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08994104820001804, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03177962818230223, 'bagging_temperature': 1.8237720246838294}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:53:06,224] Trial 881 finished with value: 0.8000788277585523 and parameters: {'iterations': 250, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09257308136664755, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.02675846891257359, 'bagging_temperature': 2.3681018959574818}. Best is trial 490 with value: 0.8043221092295566.\n",
            "[I 2024-05-15 21:53:08,660] Trial 882 finished with value: 0.8044887097405393 and parameters: {'iterations': 192, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09853109024911047, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03966323710608413, 'bagging_temperature': 2.016588205604849}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:09,989] Trial 883 finished with value: 0.7976452303615456 and parameters: {'iterations': 126, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09999098165843325, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.045069646986049365, 'bagging_temperature': 1.914684897473647}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:11,646] Trial 884 finished with value: 0.7989763572630556 and parameters: {'iterations': 163, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09845150232889537, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.0419485708319437}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:12,431] Trial 885 finished with value: 0.7944211191305467 and parameters: {'iterations': 137, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.07191463393760983, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.047476308510809556, 'bagging_temperature': 1.707563276966836}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:14,212] Trial 886 finished with value: 0.7989763572630556 and parameters: {'iterations': 190, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09846864200257287, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04004321101210198, 'bagging_temperature': 2.0796196186467295}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:15,448] Trial 887 finished with value: 0.7989763572630556 and parameters: {'iterations': 148, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08861910502483342, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.043611052759550496, 'bagging_temperature': 1.910234000257163}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:28,980] Trial 888 finished with value: 0.7981858434290635 and parameters: {'iterations': 199, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09832378217777452, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.039572834008024936, 'bagging_temperature': 2.1214377643121147}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:30,710] Trial 889 finished with value: 0.8006194408260702 and parameters: {'iterations': 162, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0999535527043225, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03810133915359512, 'bagging_temperature': 1.6172747206179192}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:32,022] Trial 890 finished with value: 0.7977285306170367 and parameters: {'iterations': 102, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09586073917769895, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.042171388890236444, 'bagging_temperature': 2.2950674456495115}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:35,178] Trial 891 finished with value: 0.7995169703305735 and parameters: {'iterations': 204, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09760587682802621, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.038296053736654075, 'bagging_temperature': 1.9716801786452984}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:36,716] Trial 892 finished with value: 0.7964594596105574 and parameters: {'iterations': 172, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09352482321638415, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03614721503471391, 'bagging_temperature': 2.1179692918264164}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:38,832] Trial 893 finished with value: 0.8022004684940542 and parameters: {'iterations': 221, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09180269203197826, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.04474526751997524, 'bagging_temperature': 1.7077514898619266}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:40,173] Trial 894 finished with value: 0.8004740846755483 and parameters: {'iterations': 126, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09814347082417285, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04991664959242533, 'bagging_temperature': 2.3311197576487515}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:42,077] Trial 895 finished with value: 0.8036148956510559 and parameters: {'iterations': 188, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09999024212125497, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0404869822339134, 'bagging_temperature': 1.9293044255450176}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:43,924] Trial 896 finished with value: 0.8001621280140436 and parameters: {'iterations': 180, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09917842907185134, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'MVS', 'l2_leaf_reg': 6, 'learning_rate': 0.040283540924516754}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:46,166] Trial 897 finished with value: 0.8033862392450424 and parameters: {'iterations': 153, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09987015103302824, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03753544880306499, 'bagging_temperature': 1.9867165493955001}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:48,272] Trial 898 finished with value: 0.794504419386038 and parameters: {'iterations': 121, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0997277973247612, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0375213345349956, 'bagging_temperature': 1.8913876482075325}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:49,740] Trial 899 finished with value: 0.7999955275030608 and parameters: {'iterations': 143, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09974562607451082, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03684515854529657, 'bagging_temperature': 1.8793704604440917}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:51,256] Trial 900 finished with value: 0.8000788277585523 and parameters: {'iterations': 157, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09856571231922731, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03991869059195489, 'bagging_temperature': 2.0874239971605815}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:52,615] Trial 901 finished with value: 0.7995382146910344 and parameters: {'iterations': 133, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09992859267104684, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03548454698616307, 'bagging_temperature': 2.0362084950775787}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:54,308] Trial 902 finished with value: 0.7979571870230502 and parameters: {'iterations': 169, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09808627528591604, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.038554321193118836, 'bagging_temperature': 2.1727933474756767}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:55,612] Trial 903 finished with value: 0.801181298254049 and parameters: {'iterations': 117, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09989112793177171, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03473080347910146, 'bagging_temperature': 1.8660390963723725}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:57,375] Trial 904 finished with value: 0.791820921222557 and parameters: {'iterations': 188, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09797837791924913, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04158282331800027, 'bagging_temperature': 1.7107170704195855}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:53:59,462] Trial 905 finished with value: 0.8004740846755483 and parameters: {'iterations': 150, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09993991295101035, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03454680093668968, 'bagging_temperature': 2.391846977630676}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:02,218] Trial 906 finished with value: 0.801576555171045 and parameters: {'iterations': 195, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09996832301572345, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03814713627385695, 'bagging_temperature': 2.2287023483803123}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:12,919] Trial 907 finished with value: 0.7972499734445495 and parameters: {'iterations': 167, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09692574611398044, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04060054381294022, 'bagging_temperature': 2.0362230528900573}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:14,632] Trial 908 finished with value: 0.7964594596105574 and parameters: {'iterations': 100, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09814046570716188, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03606248329156405, 'bagging_temperature': 1.7712623199406359}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:16,770] Trial 909 finished with value: 0.7966881160165706 and parameters: {'iterations': 203, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09757690049076248, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.033597578354797306, 'bagging_temperature': 2.210876146399395}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:18,370] Trial 910 finished with value: 0.7984357441955376 and parameters: {'iterations': 145, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09996958704128969, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.037953640464402735, 'bagging_temperature': 2.4081886798083434}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:20,233] Trial 911 finished with value: 0.802512425155559 and parameters: {'iterations': 215, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09670659402574669, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.040878587505581064, 'bagging_temperature': 1.9862744972097}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:22,046] Trial 912 finished with value: 0.8012645985095403 and parameters: {'iterations': 186, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0983223031805326, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03395957663011515, 'bagging_temperature': 2.2408573258016284}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:23,891] Trial 913 finished with value: 0.7975619301060541 and parameters: {'iterations': 224, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09645938197710753, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03661993510609078, 'bagging_temperature': 1.5979835908573712}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:25,529] Trial 914 finished with value: 0.8009526418480357 and parameters: {'iterations': 162, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09626572729621714, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03287407355806771, 'bagging_temperature': 5.8342907705890354}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:28,953] Trial 915 finished with value: 0.7982691436845549 and parameters: {'iterations': 220, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09742451485408696, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.042334159487720804, 'bagging_temperature': 2.460150855341633}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:30,698] Trial 916 finished with value: 0.800786041337053 and parameters: {'iterations': 184, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09844660589814407, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03903073097539212, 'bagging_temperature': 1.9562105586293863}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:31,359] Trial 917 finished with value: 0.7941924627245333 and parameters: {'iterations': 133, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0955193130216609, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03548371366847116, 'bagging_temperature': 2.165255013887051}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:32,061] Trial 918 finished with value: 0.7904685499605861 and parameters: {'iterations': 245, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09866173229936141, 'depth': 1, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03231215075108609, 'bagging_temperature': 2.4601167958818393}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:33,875] Trial 919 finished with value: 0.7916543207115743 and parameters: {'iterations': 203, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09577023193897347, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.1326085439460584, 'bagging_temperature': 1.803367366589074}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:35,607] Trial 920 finished with value: 0.8028243818170637 and parameters: {'iterations': 161, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09999786847035101, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.0382308088523358, 'bagging_temperature': 2.068904017606778}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:36,957] Trial 921 finished with value: 0.7993716141800515 and parameters: {'iterations': 129, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09844520799068511, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.04344771331227849, 'bagging_temperature': 1.9808548668233619}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:38,460] Trial 922 finished with value: 0.7983524439400461 and parameters: {'iterations': 152, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09866199979620403, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.040840730596569175, 'bagging_temperature': 2.1467491821248106}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:39,654] Trial 923 finished with value: 0.7978738867675588 and parameters: {'iterations': 106, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08639535196763576, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03826833072234141, 'bagging_temperature': 1.634472458490911}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:42,324] Trial 924 finished with value: 0.7932353483795584 and parameters: {'iterations': 151, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09999954621718735, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.04274949576696224, 'bagging_temperature': 2.299461735887293}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:43,492] Trial 925 finished with value: 0.8003907844200568 and parameters: {'iterations': 176, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09987487853767076, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04002197579803441, 'bagging_temperature': 1.8312531038114013}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:44,675] Trial 926 finished with value: 0.7979571870230502 and parameters: {'iterations': 130, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09694995901025637, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.037402251124318726, 'bagging_temperature': 2.067796542341644}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:54:45,910] Trial 927 finished with value: 0.7974786298505627 and parameters: {'iterations': 174, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09550759186108737, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03977375714061715, 'bagging_temperature': 2.5263743029395678}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:04,099] Trial 928 finished with value: 0.7943378188750553 and parameters: {'iterations': 102, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09989648932275014, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.04383913607188127, 'bagging_temperature': 1.5328147437825246}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:05,198] Trial 929 finished with value: 0.7987477008570422 and parameters: {'iterations': 155, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09750851020529637, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03626443543058253, 'bagging_temperature': 2.3079626887823435}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:06,876] Trial 930 finished with value: 0.7947330757920512 and parameters: {'iterations': 192, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09501632162805285, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.09937989402215142, 'bagging_temperature': 1.9865293302007347}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:08,372] Trial 931 finished with value: 0.7985811003460594 and parameters: {'iterations': 140, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09994907285114318, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.038222385609975186, 'bagging_temperature': 1.7895169783187872}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:10,194] Trial 932 finished with value: 0.7947330757920512 and parameters: {'iterations': 166, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09755368013052401, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.04117238644363071, 'bagging_temperature': 2.255967377078394}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:12,459] Trial 933 finished with value: 0.800245428269535 and parameters: {'iterations': 212, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08874235562596391, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03578161656372807, 'bagging_temperature': 2.530224198780802}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:14,153] Trial 934 finished with value: 0.7979571870230502 and parameters: {'iterations': 182, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09649087789924224, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.035053113402919084, 'bagging_temperature': 2.0661422618708367}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:15,934] Trial 935 finished with value: 0.8016386110660757 and parameters: {'iterations': 229, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09440857574158233, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03856375758708751, 'bagging_temperature': 2.380774971434508}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:17,077] Trial 936 finished with value: 0.7991217134135774 and parameters: {'iterations': 200, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09605492064441262, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.043578853460271766, 'bagging_temperature': 1.7595693025853707}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:17,904] Trial 937 finished with value: 0.7952736888595692 and parameters: {'iterations': 130, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0841602310428487, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03438173691602977, 'bagging_temperature': 2.6045145686742437}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:18,502] Trial 938 finished with value: 0.7706699241352707 and parameters: {'iterations': 169, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.012962931155409044, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.040536792588941065, 'bagging_temperature': 2.179376684991769}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:20,083] Trial 939 finished with value: 0.799850171352539 and parameters: {'iterations': 225, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09821664834681722, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03779016676707597, 'bagging_temperature': 2.464860403519189}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:20,997] Trial 940 finished with value: 0.7965427598660487 and parameters: {'iterations': 251, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09809183844064838, 'depth': 6, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03588962687837147, 'bagging_temperature': 8.079392234752849}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:22,753] Trial 941 finished with value: 0.8000788277585523 and parameters: {'iterations': 191, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09048748692754138, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03257957312584236, 'bagging_temperature': 6.516563068425633}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:23,619] Trial 942 finished with value: 0.7923615342900749 and parameters: {'iterations': 153, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.02650046377920345, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.040927825966083804, 'bagging_temperature': 1.9111810221539869}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:26,678] Trial 943 finished with value: 0.7996835708415563 and parameters: {'iterations': 275, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09328353576694336, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03744493421641372, 'bagging_temperature': 1.5558081081929656}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:28,067] Trial 944 finished with value: 0.8022004684940542 and parameters: {'iterations': 212, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09682482333816628, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03341715016698577, 'bagging_temperature': 2.155688938453797}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:28,846] Trial 945 finished with value: 0.7929233917180539 and parameters: {'iterations': 268, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09488970499008706, 'depth': 2, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.044307984475448946, 'bagging_temperature': 2.2953013072220436}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:29,895] Trial 946 finished with value: 0.7933186486350499 and parameters: {'iterations': 133, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09984247976509465, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03902580970269619, 'bagging_temperature': 1.9912641005646317}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:38,794] Trial 947 finished with value: 0.7991217134135774 and parameters: {'iterations': 187, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08704441482186454, 'depth': 12, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.036423283728859494, 'bagging_temperature': 2.622893021025793}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:42,236] Trial 948 finished with value: 0.7931732924845278 and parameters: {'iterations': 842, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09805471238072822, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.042869337093554415, 'bagging_temperature': 3.8294725368236073}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:44,128] Trial 949 finished with value: 0.7970833729335668 and parameters: {'iterations': 231, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09226085697589174, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03341248388049539, 'bagging_temperature': 1.8047952529348656}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:44,926] Trial 950 finished with value: 0.7964594596105574 and parameters: {'iterations': 165, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09996663702476878, 'depth': 7, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0310910986051799, 'bagging_temperature': 2.3929862600165754}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:46,665] Trial 951 finished with value: 0.801576555171045 and parameters: {'iterations': 280, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09534570525388884, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04052250304007826, 'bagging_temperature': 2.13619182523197}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:48,898] Trial 952 finished with value: 0.8008693415925444 and parameters: {'iterations': 232, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09997551318282429, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.03505008777434, 'bagging_temperature': 2.5653369582410264}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:51,347] Trial 953 finished with value: 0.7990384131580859 and parameters: {'iterations': 206, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09699369980395958, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 0, 'learning_rate': 0.038267355363251834, 'bagging_temperature': 1.6711145167793908}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:52,689] Trial 954 finished with value: 0.7992883139245601 and parameters: {'iterations': 119, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09348565626870287, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.045580243484913845, 'bagging_temperature': 2.69126352122617}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:54,604] Trial 955 finished with value: 0.8009313974875749 and parameters: {'iterations': 254, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0898732978498595, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03635402266549927, 'bagging_temperature': 1.9403609380905549}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:55:55,568] Trial 956 finished with value: 0.7967714162720619 and parameters: {'iterations': 298, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09682781141594994, 'depth': 5, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.04127405020215137, 'bagging_temperature': 2.2673080004032977}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:00,135] Trial 957 finished with value: 0.7921541222445226 and parameters: {'iterations': 924, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09829515564150193, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.031650600138566055, 'bagging_temperature': 2.3987403108451293}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:00,775] Trial 958 finished with value: 0.7920495776285703 and parameters: {'iterations': 173, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0444128294910451, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.034156304071155746, 'bagging_temperature': 1.4786934869940414}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:03,048] Trial 959 finished with value: 0.7983524439400461 and parameters: {'iterations': 210, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09489844285420801, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.03838478442051206, 'bagging_temperature': 2.0826968368248417}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:05,055] Trial 960 finished with value: 0.7993716141800515 and parameters: {'iterations': 149, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0983180911253362, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.02987778377288651, 'bagging_temperature': 1.8958527923170165}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:07,113] Trial 961 finished with value: 0.7994336700750819 and parameters: {'iterations': 253, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09104508491687736, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03527216826635419, 'bagging_temperature': 2.6059568155129234}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:08,416] Trial 962 finished with value: 0.7992883139245601 and parameters: {'iterations': 192, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09589545518756863, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 1, 'learning_rate': 0.04210373349370521, 'bagging_temperature': 2.2400264633737104}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:09,957] Trial 963 finished with value: 0.8000788277585523 and parameters: {'iterations': 286, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09998024794664523, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.038880714056775, 'bagging_temperature': 1.7697754590899966}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:11,742] Trial 964 finished with value: 0.7995169703305735 and parameters: {'iterations': 227, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09394078144381925, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 4, 'learning_rate': 0.03263812675063331, 'bagging_temperature': 2.7223845822596324}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:13,092] Trial 965 finished with value: 0.7999955275030608 and parameters: {'iterations': 148, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09220268063680184, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.0369895066617908, 'bagging_temperature': 2.3766182122929274}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:24,826] Trial 966 finished with value: 0.7967093603770316 and parameters: {'iterations': 1065, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08802306553761431, 'depth': 7, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.04475942760226002, 'bagging_temperature': 2.051984070163982}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:26,127] Trial 967 finished with value: 0.7979571870230502 and parameters: {'iterations': 184, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09699510040553044, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.03434852267506485, 'bagging_temperature': 2.5322305326246397}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:28,610] Trial 968 finished with value: 0.8016598554265365 and parameters: {'iterations': 301, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09836288964770566, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.04015721010624461, 'bagging_temperature': 1.6102560423138519}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:31,370] Trial 969 finished with value: 0.8000788277585523 and parameters: {'iterations': 260, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09546684367043512, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 15, 'learning_rate': 0.031718419334500286, 'bagging_temperature': 2.087955193559014}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:32,350] Trial 970 finished with value: 0.7963974037155269 and parameters: {'iterations': 101, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0590432992003479, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.03684223409289607, 'bagging_temperature': 2.270295319274283}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:33,709] Trial 971 finished with value: 0.8008693415925444 and parameters: {'iterations': 216, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0935645973452039, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 10, 'learning_rate': 0.029223756182907568, 'bagging_temperature': 1.908873220105204}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:35,417] Trial 972 finished with value: 0.7956689457765652 and parameters: {'iterations': 170, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09694000656412728, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 5, 'learning_rate': 0.042235227086915635, 'bagging_temperature': 2.786944797425591}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:36,781] Trial 973 finished with value: 0.7986644006015509 and parameters: {'iterations': 244, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09853986999876806, 'depth': 10, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.0344488066508925, 'bagging_temperature': 0.2773002869773338}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:38,452] Trial 974 finished with value: 0.8003287285250263 and parameters: {'iterations': 200, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.08926578051179077, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.06944566267799147, 'bagging_temperature': 4.401276233078146}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:40,139] Trial 975 finished with value: 0.8016598554265365 and parameters: {'iterations': 273, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09571775849610487, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 12, 'learning_rate': 0.039030555884163265, 'bagging_temperature': 2.4433016417595046}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:41,226] Trial 976 finished with value: 0.7949617321980645 and parameters: {'iterations': 133, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09256414887241082, 'depth': 12, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 3, 'learning_rate': 0.03647689445742468, 'bagging_temperature': 1.7559324413995443}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:44,077] Trial 977 finished with value: 0.8026790256665418 and parameters: {'iterations': 308, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0999896676024378, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03158789952718781, 'bagging_temperature': 1.3976053777343873}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:46,409] Trial 978 finished with value: 0.7926947353120405 and parameters: {'iterations': 241, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09992912706912768, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.08503655728204026, 'bagging_temperature': 1.3707454532371421}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:47,701] Trial 979 finished with value: 0.7952949332200301 and parameters: {'iterations': 174, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09990060462017086, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.02909932939435646, 'bagging_temperature': 1.3922653811940937}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:48,390] Trial 980 finished with value: 0.7899279368930683 and parameters: {'iterations': 214, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.036222341315042365, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.028478667396847974, 'bagging_temperature': 1.3950447308721134}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:49,252] Trial 981 finished with value: 0.795440289370552 and parameters: {'iterations': 284, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.05435063533426423, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.03079639641315788, 'bagging_temperature': 1.5219465041009677}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:50,136] Trial 982 finished with value: 0.7944211191305467 and parameters: {'iterations': 100, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09842233934298661, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.031813756342039104, 'bagging_temperature': 1.618726634085405}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:52,132] Trial 983 finished with value: 0.7973953295950713 and parameters: {'iterations': 311, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09989813150535688, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.031965463888289795, 'bagging_temperature': 1.167457744211646}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:56,773] Trial 984 finished with value: 0.7947330757920512 and parameters: {'iterations': 147, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09839515170857654, 'depth': 11, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 9, 'learning_rate': 0.028049837973469473, 'bagging_temperature': 1.339418377177741}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:56:59,287] Trial 985 finished with value: 0.7989763572630556 and parameters: {'iterations': 234, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09826874305814795, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 8, 'learning_rate': 0.03389717643885455, 'bagging_temperature': 1.5446609025641236}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:00,680] Trial 986 finished with value: 0.8025957254110503 and parameters: {'iterations': 197, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09718079798074582, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 6, 'learning_rate': 0.030525742844679106, 'bagging_temperature': 1.7545572144202826}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:02,518] Trial 987 finished with value: 0.8033029389895512 and parameters: {'iterations': 273, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09803710388258981, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.033687798542672745, 'bagging_temperature': 4.85093417656128}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:04,202] Trial 988 finished with value: 0.8039268523125606 and parameters: {'iterations': 260, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09701619345198094, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.027207504441110947, 'bagging_temperature': 6.852508532199393}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:05,845] Trial 989 finished with value: 0.7979571870230502 and parameters: {'iterations': 259, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09808396044869358, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.024909437500544314, 'bagging_temperature': 4.217543350342443}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:07,494] Trial 990 finished with value: 0.7965427598660487 and parameters: {'iterations': 254, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09627005447082922, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.026401161921694893, 'bagging_temperature': 4.734826588608666}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:09,174] Trial 991 finished with value: 0.7985811003460594 and parameters: {'iterations': 222, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0998991933777164, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.023512452097741113, 'bagging_temperature': 6.825753090164097}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:12,126] Trial 992 finished with value: 0.7943378188750553 and parameters: {'iterations': 264, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09687073861364692, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.02469973684071141, 'bagging_temperature': 7.8550699591089375}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:13,824] Trial 993 finished with value: 0.7967093603770316 and parameters: {'iterations': 244, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09999555041337925, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.026991698293167714, 'bagging_temperature': 7.313792140244422}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:15,160] Trial 994 finished with value: 0.7985811003460594 and parameters: {'iterations': 213, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09431083745212214, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.028140544710792567, 'bagging_temperature': 5.242221374481264}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:16,920] Trial 995 finished with value: 0.7968547165275535 and parameters: {'iterations': 272, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09737220952491844, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.029016485354376988, 'bagging_temperature': 7.154036178567029}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:18,415] Trial 996 finished with value: 0.800786041337053 and parameters: {'iterations': 231, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.096138249156259, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.029245606215617025, 'bagging_temperature': 4.936168050916199}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:19,856] Trial 997 finished with value: 0.7973332737000408 and parameters: {'iterations': 193, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09821712156213802, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.025414151282773047, 'bagging_temperature': 5.942776374306929}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:21,120] Trial 998 finished with value: 0.7942757629800247 and parameters: {'iterations': 241, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.0854715680632413, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.030710630212688553, 'bagging_temperature': 6.459996214836456}. Best is trial 882 with value: 0.8044887097405393.\n",
            "[I 2024-05-15 21:57:23,398] Trial 999 finished with value: 0.7989763572630556 and parameters: {'iterations': 280, 'objective': 'CrossEntropy', 'colsample_bylevel': 0.09503873731491352, 'depth': 11, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'l2_leaf_reg': 7, 'learning_rate': 0.028073177288089366, 'bagging_temperature': 7.7123416301698935}. Best is trial 882 with value: 0.8044887097405393.\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(study_name=\"catboost_optuna\", storage=\"sqlite:///db.sqlite3\", direction='maximize')\n",
        "study.optimize(objective, n_trials=1000)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:12:40.572085Z",
          "end_time": "2024-05-14T18:38:26.661997Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wpgPs2UlLVe",
        "outputId": "1b651cb0-bc49-40a1-e755-d1ee7e88a24f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6824264\ttotal: 1.47ms\tremaining: 282ms\n",
            "1:\tlearn: 0.6609275\ttotal: 10.5ms\tremaining: 998ms\n",
            "2:\tlearn: 0.6589184\ttotal: 11.8ms\tremaining: 743ms\n",
            "3:\tlearn: 0.6414663\ttotal: 14.8ms\tremaining: 697ms\n",
            "4:\tlearn: 0.6263693\ttotal: 16.8ms\tremaining: 629ms\n",
            "5:\tlearn: 0.6100118\ttotal: 40.3ms\tremaining: 1.25s\n",
            "6:\tlearn: 0.5967095\ttotal: 44ms\tremaining: 1.16s\n",
            "7:\tlearn: 0.5956027\ttotal: 45.1ms\tremaining: 1.04s\n",
            "8:\tlearn: 0.5846592\ttotal: 46.7ms\tremaining: 949ms\n",
            "9:\tlearn: 0.5776898\ttotal: 48.5ms\tremaining: 883ms\n",
            "10:\tlearn: 0.5659615\ttotal: 50.5ms\tremaining: 831ms\n",
            "11:\tlearn: 0.5587905\ttotal: 52.3ms\tremaining: 784ms\n",
            "12:\tlearn: 0.5521252\ttotal: 53.9ms\tremaining: 742ms\n",
            "13:\tlearn: 0.5410820\ttotal: 81.1ms\tremaining: 1.03s\n",
            "14:\tlearn: 0.5344731\ttotal: 83ms\tremaining: 980ms\n",
            "15:\tlearn: 0.5306864\ttotal: 85.8ms\tremaining: 943ms\n",
            "16:\tlearn: 0.5243201\ttotal: 89.8ms\tremaining: 925ms\n",
            "17:\tlearn: 0.5197967\ttotal: 95.3ms\tremaining: 921ms\n",
            "18:\tlearn: 0.5123116\ttotal: 136ms\tremaining: 1.24s\n",
            "19:\tlearn: 0.5081119\ttotal: 142ms\tremaining: 1.22s\n",
            "20:\tlearn: 0.5078379\ttotal: 144ms\tremaining: 1.17s\n",
            "21:\tlearn: 0.5013329\ttotal: 212ms\tremaining: 1.64s\n",
            "22:\tlearn: 0.5011070\ttotal: 213ms\tremaining: 1.57s\n",
            "23:\tlearn: 0.4950611\ttotal: 267ms\tremaining: 1.87s\n",
            "24:\tlearn: 0.4917652\ttotal: 274ms\tremaining: 1.83s\n",
            "25:\tlearn: 0.4887818\ttotal: 280ms\tremaining: 1.79s\n",
            "26:\tlearn: 0.4837714\ttotal: 315ms\tremaining: 1.92s\n",
            "27:\tlearn: 0.4821306\ttotal: 316ms\tremaining: 1.85s\n",
            "28:\tlearn: 0.4783552\ttotal: 325ms\tremaining: 1.83s\n",
            "29:\tlearn: 0.4743923\ttotal: 334ms\tremaining: 1.81s\n",
            "30:\tlearn: 0.4738338\ttotal: 336ms\tremaining: 1.74s\n",
            "31:\tlearn: 0.4722721\ttotal: 337ms\tremaining: 1.69s\n",
            "32:\tlearn: 0.4703840\ttotal: 339ms\tremaining: 1.63s\n",
            "33:\tlearn: 0.4681198\ttotal: 341ms\tremaining: 1.58s\n",
            "34:\tlearn: 0.4656244\ttotal: 345ms\tremaining: 1.55s\n",
            "35:\tlearn: 0.4636355\ttotal: 348ms\tremaining: 1.51s\n",
            "36:\tlearn: 0.4635660\ttotal: 350ms\tremaining: 1.46s\n",
            "37:\tlearn: 0.4618602\ttotal: 352ms\tremaining: 1.43s\n",
            "38:\tlearn: 0.4608147\ttotal: 354ms\tremaining: 1.39s\n",
            "39:\tlearn: 0.4583640\ttotal: 412ms\tremaining: 1.57s\n",
            "40:\tlearn: 0.4583232\ttotal: 414ms\tremaining: 1.53s\n",
            "41:\tlearn: 0.4579811\ttotal: 417ms\tremaining: 1.49s\n",
            "42:\tlearn: 0.4553418\ttotal: 461ms\tremaining: 1.6s\n",
            "43:\tlearn: 0.4541611\ttotal: 464ms\tremaining: 1.56s\n",
            "44:\tlearn: 0.4516042\ttotal: 516ms\tremaining: 1.69s\n",
            "45:\tlearn: 0.4515653\ttotal: 518ms\tremaining: 1.64s\n",
            "46:\tlearn: 0.4503303\ttotal: 521ms\tremaining: 1.61s\n",
            "47:\tlearn: 0.4491311\ttotal: 523ms\tremaining: 1.57s\n",
            "48:\tlearn: 0.4468848\ttotal: 557ms\tremaining: 1.63s\n",
            "49:\tlearn: 0.4462552\ttotal: 563ms\tremaining: 1.6s\n",
            "50:\tlearn: 0.4448164\ttotal: 580ms\tremaining: 1.6s\n",
            "51:\tlearn: 0.4443618\ttotal: 584ms\tremaining: 1.57s\n",
            "52:\tlearn: 0.4437645\ttotal: 586ms\tremaining: 1.54s\n",
            "53:\tlearn: 0.4429556\ttotal: 589ms\tremaining: 1.5s\n",
            "54:\tlearn: 0.4429437\ttotal: 591ms\tremaining: 1.47s\n",
            "55:\tlearn: 0.4424909\ttotal: 593ms\tremaining: 1.44s\n",
            "56:\tlearn: 0.4418819\ttotal: 596ms\tremaining: 1.41s\n",
            "57:\tlearn: 0.4409171\ttotal: 610ms\tremaining: 1.41s\n",
            "58:\tlearn: 0.4397585\ttotal: 621ms\tremaining: 1.4s\n",
            "59:\tlearn: 0.4384002\ttotal: 645ms\tremaining: 1.42s\n",
            "60:\tlearn: 0.4377525\ttotal: 651ms\tremaining: 1.4s\n",
            "61:\tlearn: 0.4360405\ttotal: 678ms\tremaining: 1.42s\n",
            "62:\tlearn: 0.4349633\ttotal: 691ms\tremaining: 1.41s\n",
            "63:\tlearn: 0.4326746\ttotal: 730ms\tremaining: 1.46s\n",
            "64:\tlearn: 0.4319437\ttotal: 739ms\tremaining: 1.44s\n",
            "65:\tlearn: 0.4317057\ttotal: 745ms\tremaining: 1.42s\n",
            "66:\tlearn: 0.4316707\ttotal: 747ms\tremaining: 1.39s\n",
            "67:\tlearn: 0.4312275\ttotal: 752ms\tremaining: 1.37s\n",
            "68:\tlearn: 0.4309318\ttotal: 758ms\tremaining: 1.35s\n",
            "69:\tlearn: 0.4302014\ttotal: 764ms\tremaining: 1.33s\n",
            "70:\tlearn: 0.4288980\ttotal: 794ms\tremaining: 1.35s\n",
            "71:\tlearn: 0.4281688\ttotal: 808ms\tremaining: 1.35s\n",
            "72:\tlearn: 0.4280429\ttotal: 810ms\tremaining: 1.32s\n",
            "73:\tlearn: 0.4268697\ttotal: 862ms\tremaining: 1.37s\n",
            "74:\tlearn: 0.4267172\ttotal: 869ms\tremaining: 1.36s\n",
            "75:\tlearn: 0.4267152\ttotal: 872ms\tremaining: 1.33s\n",
            "76:\tlearn: 0.4267140\ttotal: 875ms\tremaining: 1.31s\n",
            "77:\tlearn: 0.4267131\ttotal: 877ms\tremaining: 1.28s\n",
            "78:\tlearn: 0.4266866\ttotal: 878ms\tremaining: 1.26s\n",
            "79:\tlearn: 0.4262869\ttotal: 881ms\tremaining: 1.23s\n",
            "80:\tlearn: 0.4253202\ttotal: 892ms\tremaining: 1.22s\n",
            "81:\tlearn: 0.4252648\ttotal: 894ms\tremaining: 1.2s\n",
            "82:\tlearn: 0.4252639\ttotal: 895ms\tremaining: 1.18s\n",
            "83:\tlearn: 0.4243865\ttotal: 951ms\tremaining: 1.22s\n",
            "84:\tlearn: 0.4243853\ttotal: 953ms\tremaining: 1.2s\n",
            "85:\tlearn: 0.4240180\ttotal: 956ms\tremaining: 1.18s\n",
            "86:\tlearn: 0.4238800\ttotal: 958ms\tremaining: 1.16s\n",
            "87:\tlearn: 0.4238790\ttotal: 960ms\tremaining: 1.13s\n",
            "88:\tlearn: 0.4229263\ttotal: 999ms\tremaining: 1.16s\n",
            "89:\tlearn: 0.4227340\ttotal: 1s\tremaining: 1.13s\n",
            "90:\tlearn: 0.4226135\ttotal: 1.01s\tremaining: 1.12s\n",
            "91:\tlearn: 0.4221971\ttotal: 1.01s\tremaining: 1.1s\n",
            "92:\tlearn: 0.4218452\ttotal: 1.02s\tremaining: 1.08s\n",
            "93:\tlearn: 0.4212483\ttotal: 1.06s\tremaining: 1.1s\n",
            "94:\tlearn: 0.4203571\ttotal: 1.08s\tremaining: 1.1s\n",
            "95:\tlearn: 0.4203441\ttotal: 1.09s\tremaining: 1.09s\n",
            "96:\tlearn: 0.4203441\ttotal: 1.09s\tremaining: 1.07s\n",
            "97:\tlearn: 0.4203029\ttotal: 1.09s\tremaining: 1.05s\n",
            "98:\tlearn: 0.4203017\ttotal: 1.1s\tremaining: 1.03s\n",
            "99:\tlearn: 0.4202040\ttotal: 1.1s\tremaining: 1.01s\n",
            "100:\tlearn: 0.4201590\ttotal: 1.1s\tremaining: 994ms\n",
            "101:\tlearn: 0.4200131\ttotal: 1.11s\tremaining: 977ms\n",
            "102:\tlearn: 0.4189878\ttotal: 1.12s\tremaining: 971ms\n",
            "103:\tlearn: 0.4184289\ttotal: 1.13s\tremaining: 958ms\n",
            "104:\tlearn: 0.4184284\ttotal: 1.14s\tremaining: 941ms\n",
            "105:\tlearn: 0.4169641\ttotal: 1.18s\tremaining: 954ms\n",
            "106:\tlearn: 0.4168428\ttotal: 1.18s\tremaining: 939ms\n",
            "107:\tlearn: 0.4167074\ttotal: 1.2s\tremaining: 931ms\n",
            "108:\tlearn: 0.4166857\ttotal: 1.2s\tremaining: 915ms\n",
            "109:\tlearn: 0.4166743\ttotal: 1.21s\tremaining: 902ms\n",
            "110:\tlearn: 0.4156340\ttotal: 1.26s\tremaining: 919ms\n",
            "111:\tlearn: 0.4155467\ttotal: 1.26s\tremaining: 903ms\n",
            "112:\tlearn: 0.4155070\ttotal: 1.27s\tremaining: 886ms\n",
            "113:\tlearn: 0.4155072\ttotal: 1.27s\tremaining: 871ms\n",
            "114:\tlearn: 0.4155072\ttotal: 1.27s\tremaining: 854ms\n",
            "115:\tlearn: 0.4143298\ttotal: 1.33s\tremaining: 870ms\n",
            "116:\tlearn: 0.4134047\ttotal: 1.37s\tremaining: 878ms\n",
            "117:\tlearn: 0.4133971\ttotal: 1.38s\tremaining: 864ms\n",
            "118:\tlearn: 0.4121158\ttotal: 1.42s\tremaining: 873ms\n",
            "119:\tlearn: 0.4120929\ttotal: 1.43s\tremaining: 858ms\n",
            "120:\tlearn: 0.4120925\ttotal: 1.43s\tremaining: 840ms\n",
            "121:\tlearn: 0.4114312\ttotal: 1.44s\tremaining: 829ms\n",
            "122:\tlearn: 0.4104971\ttotal: 1.46s\tremaining: 820ms\n",
            "123:\tlearn: 0.4098806\ttotal: 1.5s\tremaining: 825ms\n",
            "124:\tlearn: 0.4092166\ttotal: 1.51s\tremaining: 811ms\n",
            "125:\tlearn: 0.4086630\ttotal: 1.54s\tremaining: 808ms\n",
            "126:\tlearn: 0.4085209\ttotal: 1.55s\tremaining: 791ms\n",
            "127:\tlearn: 0.4085211\ttotal: 1.55s\tremaining: 775ms\n",
            "128:\tlearn: 0.4085114\ttotal: 1.55s\tremaining: 758ms\n",
            "129:\tlearn: 0.4083053\ttotal: 1.56s\tremaining: 744ms\n",
            "130:\tlearn: 0.4081678\ttotal: 1.56s\tremaining: 728ms\n",
            "131:\tlearn: 0.4081458\ttotal: 1.57s\tremaining: 713ms\n",
            "132:\tlearn: 0.4070159\ttotal: 1.61s\tremaining: 714ms\n",
            "133:\tlearn: 0.4068458\ttotal: 1.64s\tremaining: 712ms\n",
            "134:\tlearn: 0.4068418\ttotal: 1.65s\tremaining: 696ms\n",
            "135:\tlearn: 0.4068295\ttotal: 1.65s\tremaining: 681ms\n",
            "136:\tlearn: 0.4067821\ttotal: 1.66s\tremaining: 665ms\n",
            "137:\tlearn: 0.4067824\ttotal: 1.66s\tremaining: 650ms\n",
            "138:\tlearn: 0.4064928\ttotal: 1.67s\tremaining: 635ms\n",
            "139:\tlearn: 0.4064557\ttotal: 1.67s\tremaining: 621ms\n",
            "140:\tlearn: 0.4051466\ttotal: 1.75s\tremaining: 633ms\n",
            "141:\tlearn: 0.4050851\ttotal: 1.76s\tremaining: 619ms\n",
            "142:\tlearn: 0.4047622\ttotal: 1.76s\tremaining: 604ms\n",
            "143:\tlearn: 0.4047214\ttotal: 1.77s\tremaining: 589ms\n",
            "144:\tlearn: 0.4036565\ttotal: 1.83s\tremaining: 594ms\n",
            "145:\tlearn: 0.4036535\ttotal: 1.83s\tremaining: 578ms\n",
            "146:\tlearn: 0.4032761\ttotal: 1.88s\tremaining: 577ms\n",
            "147:\tlearn: 0.4032757\ttotal: 1.89s\tremaining: 562ms\n",
            "148:\tlearn: 0.4032578\ttotal: 1.89s\tremaining: 547ms\n",
            "149:\tlearn: 0.4017504\ttotal: 1.93s\tremaining: 541ms\n",
            "150:\tlearn: 0.4016774\ttotal: 1.93s\tremaining: 525ms\n",
            "151:\tlearn: 0.4016166\ttotal: 1.94s\tremaining: 509ms\n",
            "152:\tlearn: 0.4016112\ttotal: 1.94s\tremaining: 494ms\n",
            "153:\tlearn: 0.4015993\ttotal: 1.94s\tremaining: 478ms\n",
            "154:\tlearn: 0.4004231\ttotal: 1.97s\tremaining: 469ms\n",
            "155:\tlearn: 0.4003735\ttotal: 1.97s\tremaining: 454ms\n",
            "156:\tlearn: 0.3988413\ttotal: 2.01s\tremaining: 448ms\n",
            "157:\tlearn: 0.3988246\ttotal: 2.01s\tremaining: 433ms\n",
            "158:\tlearn: 0.3988246\ttotal: 2.01s\tremaining: 418ms\n",
            "159:\tlearn: 0.3981901\ttotal: 2.03s\tremaining: 407ms\n",
            "160:\tlearn: 0.3981846\ttotal: 2.04s\tremaining: 392ms\n",
            "161:\tlearn: 0.3979143\ttotal: 2.05s\tremaining: 380ms\n",
            "162:\tlearn: 0.3966716\ttotal: 2.07s\tremaining: 369ms\n",
            "163:\tlearn: 0.3959756\ttotal: 2.1s\tremaining: 358ms\n",
            "164:\tlearn: 0.3959744\ttotal: 2.1s\tremaining: 344ms\n",
            "165:\tlearn: 0.3958422\ttotal: 2.1s\tremaining: 329ms\n",
            "166:\tlearn: 0.3958418\ttotal: 2.1s\tremaining: 315ms\n",
            "167:\tlearn: 0.3957929\ttotal: 2.11s\tremaining: 301ms\n",
            "168:\tlearn: 0.3957256\ttotal: 2.11s\tremaining: 287ms\n",
            "169:\tlearn: 0.3954137\ttotal: 2.11s\tremaining: 274ms\n",
            "170:\tlearn: 0.3942126\ttotal: 2.14s\tremaining: 263ms\n",
            "171:\tlearn: 0.3941593\ttotal: 2.15s\tremaining: 250ms\n",
            "172:\tlearn: 0.3927062\ttotal: 2.17s\tremaining: 238ms\n",
            "173:\tlearn: 0.3927060\ttotal: 2.17s\tremaining: 225ms\n",
            "174:\tlearn: 0.3918501\ttotal: 2.19s\tremaining: 213ms\n",
            "175:\tlearn: 0.3911947\ttotal: 2.21s\tremaining: 201ms\n",
            "176:\tlearn: 0.3911119\ttotal: 2.22s\tremaining: 188ms\n",
            "177:\tlearn: 0.3910991\ttotal: 2.22s\tremaining: 175ms\n",
            "178:\tlearn: 0.3910898\ttotal: 2.22s\tremaining: 161ms\n",
            "179:\tlearn: 0.3909996\ttotal: 2.22s\tremaining: 148ms\n",
            "180:\tlearn: 0.3909162\ttotal: 2.23s\tremaining: 135ms\n",
            "181:\tlearn: 0.3909138\ttotal: 2.23s\tremaining: 122ms\n",
            "182:\tlearn: 0.3909050\ttotal: 2.23s\tremaining: 110ms\n",
            "183:\tlearn: 0.3900331\ttotal: 2.27s\tremaining: 98.6ms\n",
            "184:\tlearn: 0.3898954\ttotal: 2.27s\tremaining: 86ms\n",
            "185:\tlearn: 0.3897574\ttotal: 2.27s\tremaining: 73.4ms\n",
            "186:\tlearn: 0.3897247\ttotal: 2.28s\tremaining: 60.9ms\n",
            "187:\tlearn: 0.3897246\ttotal: 2.28s\tremaining: 48.5ms\n",
            "188:\tlearn: 0.3896907\ttotal: 2.28s\tremaining: 36.2ms\n",
            "189:\tlearn: 0.3886911\ttotal: 2.31s\tremaining: 24.3ms\n",
            "190:\tlearn: 0.3886792\ttotal: 2.31s\tremaining: 12.1ms\n",
            "191:\tlearn: 0.3886554\ttotal: 2.31s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7c3132ce0ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "params = study.best_params\n",
        "model8 = CatBoostClassifier(**params)\n",
        "model8.fit(x_train, y_train)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:39:51.835075Z",
          "end_time": "2024-05-14T18:39:52.752234Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGtrpI5TlLVe",
        "outputId": "a023a3a2-ab5b-4c50-8c68-617e479f6331"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing performance\n",
            "RMSE: 0.422\n",
            "R2: 0.226\n",
            "ROC AUC: 0.80449\n",
            "Score: 0.8220\n",
            "Local Score: 0.8313\n",
            "Best params:  {'iterations': 192, 'learning_rate': 0.03966323710608413, 'depth': 12, 'l2_leaf_reg': 6, 'bagging_temperature': 2.016588205604849, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'colsample_bylevel': 0.09853109024911047, 'objective': 'CrossEntropy'}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model8)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:39:59.846227Z",
          "end_time": "2024-05-14T18:39:59.914954Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZTZOmeflLVe",
        "outputId": "9af78313-e41b-409b-eaa8-ef09a325717b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T23:04:02.328645Z",
          "end_time": "2024-05-13T23:04:02.393407Z"
        },
        "id": "Sbc1LhuFlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    param = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 300, 1200),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 40, 3000, step=10),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 99, step=3),\n",
        "        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 99, step=3),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1, step=0.1),\n",
        "        \"objective\": \"binary\",\n",
        "        \"boosting_type\": \"dart\"\n",
        "    }\n",
        "\n",
        "    optuna_model = LGBMClassifier(**param,verbosity=-1)\n",
        "    optuna_model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
        "\n",
        "    preds = optuna_model.predict(x_test)\n",
        "    accuracy = roc_auc_score(y_test, preds)\n",
        "    return accuracy"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T19:23:15.621294Z",
          "end_time": "2024-05-14T19:23:15.694978Z"
        },
        "id": "DVplROCjlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-15 21:57:26,071] A new study created in RDB with name: lgbm_optuna\n",
            "[I 2024-05-15 21:57:26,512] Trial 0 finished with value: 0.79263267941701 and parameters: {'iterations': 1178, 'learning_rate': 0.14717009938815415, 'num_leaves': 1630, 'max_depth': 8, 'lambda_l1': 63, 'lambda_l2': 39, 'feature_fraction': 0.6000000000000001}. Best is trial 0 with value: 0.79263267941701.\n",
            "[I 2024-05-15 21:57:26,937] Trial 1 finished with value: 0.7913015525155 and parameters: {'iterations': 524, 'learning_rate': 0.1674024629691993, 'num_leaves': 1030, 'max_depth': 9, 'lambda_l1': 72, 'lambda_l2': 39, 'feature_fraction': 0.8}. Best is trial 0 with value: 0.79263267941701.\n",
            "[I 2024-05-15 21:57:27,319] Trial 2 finished with value: 0.7934231932510021 and parameters: {'iterations': 536, 'learning_rate': 0.1911093969009781, 'num_leaves': 570, 'max_depth': 12, 'lambda_l1': 60, 'lambda_l2': 45, 'feature_fraction': 0.4}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:27,730] Trial 3 finished with value: 0.7920087660940006 and parameters: {'iterations': 654, 'learning_rate': 0.14905762244957357, 'num_leaves': 1110, 'max_depth': 12, 'lambda_l1': 54, 'lambda_l2': 51, 'feature_fraction': 0.5}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:28,242] Trial 4 finished with value: 0.7928205242884537 and parameters: {'iterations': 440, 'learning_rate': 0.07844232461240994, 'num_leaves': 670, 'max_depth': 12, 'lambda_l1': 21, 'lambda_l2': 30, 'feature_fraction': 0.7}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:28,658] Trial 5 finished with value: 0.7902823822754945 and parameters: {'iterations': 474, 'learning_rate': 0.09761157313279817, 'num_leaves': 1440, 'max_depth': 12, 'lambda_l1': 54, 'lambda_l2': 90, 'feature_fraction': 0.5}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:28,996] Trial 6 finished with value: 0.7911970078995477 and parameters: {'iterations': 814, 'learning_rate': 0.24616502032530518, 'num_leaves': 260, 'max_depth': 5, 'lambda_l1': 66, 'lambda_l2': 36, 'feature_fraction': 0.30000000000000004}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:29,397] Trial 7 finished with value: 0.7920087660940006 and parameters: {'iterations': 822, 'learning_rate': 0.14589712286132936, 'num_leaves': 820, 'max_depth': 6, 'lambda_l1': 81, 'lambda_l2': 54, 'feature_fraction': 0.6000000000000001}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:29,757] Trial 8 finished with value: 0.7934231932510021 and parameters: {'iterations': 310, 'learning_rate': 0.06992635185030972, 'num_leaves': 1680, 'max_depth': 3, 'lambda_l1': 33, 'lambda_l2': 60, 'feature_fraction': 0.5}. Best is trial 2 with value: 0.7934231932510021.\n",
            "[I 2024-05-15 21:57:30,146] Trial 9 finished with value: 0.7943590632355161 and parameters: {'iterations': 798, 'learning_rate': 0.23310625383819264, 'num_leaves': 2520, 'max_depth': 6, 'lambda_l1': 27, 'lambda_l2': 93, 'feature_fraction': 0.4}. Best is trial 9 with value: 0.7943590632355161.\n",
            "[I 2024-05-15 21:57:30,573] Trial 10 finished with value: 0.7944211191305467 and parameters: {'iterations': 1010, 'learning_rate': 0.2934505558535198, 'num_leaves': 2720, 'max_depth': 3, 'lambda_l1': 6, 'lambda_l2': 3, 'feature_fraction': 1.0}. Best is trial 10 with value: 0.7944211191305467.\n",
            "[I 2024-05-15 21:57:30,995] Trial 11 finished with value: 0.8004120287805178 and parameters: {'iterations': 1084, 'learning_rate': 0.2987045313222469, 'num_leaves': 2770, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 3, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:31,408] Trial 12 finished with value: 0.7911349520045172 and parameters: {'iterations': 1076, 'learning_rate': 0.2932151732959324, 'num_leaves': 2960, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:31,871] Trial 13 finished with value: 0.794504419386038 and parameters: {'iterations': 1000, 'learning_rate': 0.2992299509458582, 'num_leaves': 2290, 'max_depth': 4, 'lambda_l1': 0, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:32,415] Trial 14 finished with value: 0.7923827786505357 and parameters: {'iterations': 975, 'learning_rate': 0.25258371909413696, 'num_leaves': 2210, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 18, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:32,835] Trial 15 finished with value: 0.7836692364888663 and parameters: {'iterations': 1198, 'learning_rate': 0.018627789704016295, 'num_leaves': 2110, 'max_depth': 4, 'lambda_l1': 96, 'lambda_l2': 18, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:33,342] Trial 16 finished with value: 0.7956281342419956 and parameters: {'iterations': 948, 'learning_rate': 0.21574459860340917, 'num_leaves': 2200, 'max_depth': 10, 'lambda_l1': 36, 'lambda_l2': 12, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:33,816] Trial 17 finished with value: 0.7946089640019903 and parameters: {'iterations': 874, 'learning_rate': 0.20990215175551774, 'num_leaves': 3000, 'max_depth': 10, 'lambda_l1': 42, 'lambda_l2': 18, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:34,297] Trial 18 finished with value: 0.7952541216854605 and parameters: {'iterations': 709, 'learning_rate': 0.26213118767288945, 'num_leaves': 1900, 'max_depth': 10, 'lambda_l1': 39, 'lambda_l2': 75, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:34,715] Trial 19 finished with value: 0.7893873238255503 and parameters: {'iterations': 922, 'learning_rate': 0.19753698585207038, 'num_leaves': 2590, 'max_depth': 8, 'lambda_l1': 21, 'lambda_l2': 12, 'feature_fraction': 0.2}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:35,342] Trial 20 finished with value: 0.7956068898815348 and parameters: {'iterations': 1094, 'learning_rate': 0.22222965584073595, 'num_leaves': 2430, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 27, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:36,073] Trial 21 finished with value: 0.7986023447065203 and parameters: {'iterations': 1095, 'learning_rate': 0.225335226251068, 'num_leaves': 2420, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 27, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:36,817] Trial 22 finished with value: 0.7900324815090204 and parameters: {'iterations': 1098, 'learning_rate': 0.27057494275363436, 'num_leaves': 1960, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 9, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:37,540] Trial 23 finished with value: 0.7955448339865042 and parameters: {'iterations': 926, 'learning_rate': 0.17770909872601184, 'num_leaves': 2790, 'max_depth': 11, 'lambda_l1': 33, 'lambda_l2': 24, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:38,357] Trial 24 finished with value: 0.7973332737000408 and parameters: {'iterations': 1065, 'learning_rate': 0.22462566425387165, 'num_leaves': 2400, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 9, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:39,091] Trial 25 finished with value: 0.7956901901370262 and parameters: {'iterations': 1129, 'learning_rate': 0.12185644070270857, 'num_leaves': 2740, 'max_depth': 7, 'lambda_l1': 9, 'lambda_l2': 6, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:39,789] Trial 26 finished with value: 0.793713905552046 and parameters: {'iterations': 1041, 'learning_rate': 0.27302460999502326, 'num_leaves': 2400, 'max_depth': 7, 'lambda_l1': 0, 'lambda_l2': 21, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:40,301] Trial 27 finished with value: 0.7954615337310129 and parameters: {'iterations': 1148, 'learning_rate': 0.23547774114984382, 'num_leaves': 1890, 'max_depth': 9, 'lambda_l1': 24, 'lambda_l2': 30, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:40,838] Trial 28 finished with value: 0.7944636078514684 and parameters: {'iterations': 1045, 'learning_rate': 0.27025812088987056, 'num_leaves': 2590, 'max_depth': 6, 'lambda_l1': 15, 'lambda_l2': 12, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:41,331] Trial 29 finished with value: 0.7972712178050104 and parameters: {'iterations': 1175, 'learning_rate': 0.19276780001876156, 'num_leaves': 1610, 'max_depth': 8, 'lambda_l1': 27, 'lambda_l2': 36, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:42,056] Trial 30 finished with value: 0.7931112365894974 and parameters: {'iterations': 872, 'learning_rate': 0.1320522463841467, 'num_leaves': 1400, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:42,557] Trial 31 finished with value: 0.7958567906480088 and parameters: {'iterations': 1158, 'learning_rate': 0.18770761176959042, 'num_leaves': 1680, 'max_depth': 8, 'lambda_l1': 27, 'lambda_l2': 39, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:43,224] Trial 32 finished with value: 0.7963974037155269 and parameters: {'iterations': 1175, 'learning_rate': 0.16757641361891412, 'num_leaves': 1270, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 33, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:43,831] Trial 33 finished with value: 0.7945256637464989 and parameters: {'iterations': 1074, 'learning_rate': 0.21010820312834189, 'num_leaves': 2060, 'max_depth': 9, 'lambda_l1': 45, 'lambda_l2': 48, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:44,615] Trial 34 finished with value: 0.7922161781395531 and parameters: {'iterations': 1123, 'learning_rate': 0.23635964575637258, 'num_leaves': 40, 'max_depth': 8, 'lambda_l1': 6, 'lambda_l2': 42, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:45,337] Trial 35 finished with value: 0.7962520475650049 and parameters: {'iterations': 1193, 'learning_rate': 0.19637734873839374, 'num_leaves': 2840, 'max_depth': 11, 'lambda_l1': 30, 'lambda_l2': 27, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:45,956] Trial 36 finished with value: 0.7929446360785146 and parameters: {'iterations': 651, 'learning_rate': 0.161761899074989, 'num_leaves': 2340, 'max_depth': 5, 'lambda_l1': 21, 'lambda_l2': 6, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:46,457] Trial 37 finished with value: 0.7887013546075105 and parameters: {'iterations': 1034, 'learning_rate': 0.17858683172526632, 'num_leaves': 1760, 'max_depth': 4, 'lambda_l1': 6, 'lambda_l2': 36, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:46,890] Trial 38 finished with value: 0.7961687473095135 and parameters: {'iterations': 963, 'learning_rate': 0.28363943896229915, 'num_leaves': 1030, 'max_depth': 8, 'lambda_l1': 51, 'lambda_l2': 15, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:47,422] Trial 39 finished with value: 0.7947543201525121 and parameters: {'iterations': 1125, 'learning_rate': 0.25715391027334106, 'num_leaves': 1210, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 24, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:47,876] Trial 40 finished with value: 0.7936730940174763 and parameters: {'iterations': 549, 'learning_rate': 0.22706217896415198, 'num_leaves': 1480, 'max_depth': 6, 'lambda_l1': 63, 'lambda_l2': 54, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:48,437] Trial 41 finished with value: 0.7951495770695082 and parameters: {'iterations': 1198, 'learning_rate': 0.15008485386314413, 'num_leaves': 800, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 33, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:49,074] Trial 42 finished with value: 0.7950662768140169 and parameters: {'iterations': 1161, 'learning_rate': 0.17010822123329264, 'num_leaves': 1250, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 33, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:49,728] Trial 43 finished with value: 0.795316177580491 and parameters: {'iterations': 1049, 'learning_rate': 0.10338088965198011, 'num_leaves': 890, 'max_depth': 5, 'lambda_l1': 24, 'lambda_l2': 42, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:50,531] Trial 44 finished with value: 0.793713905552046 and parameters: {'iterations': 1103, 'learning_rate': 0.19889500250041495, 'num_leaves': 450, 'max_depth': 6, 'lambda_l1': 3, 'lambda_l2': 48, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:51,073] Trial 45 finished with value: 0.7894918684415025 and parameters: {'iterations': 986, 'learning_rate': 0.24312524847444342, 'num_leaves': 2580, 'max_depth': 7, 'lambda_l1': 78, 'lambda_l2': 6, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:51,843] Trial 46 finished with value: 0.7929233917180539 and parameters: {'iterations': 1154, 'learning_rate': 0.1365767022440102, 'num_leaves': 1350, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 24, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:52,435] Trial 47 finished with value: 0.7935897937619849 and parameters: {'iterations': 1076, 'learning_rate': 0.18287908693926655, 'num_leaves': 1770, 'max_depth': 12, 'lambda_l1': 18, 'lambda_l2': 0, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:52,942] Trial 48 finished with value: 0.7958780350084699 and parameters: {'iterations': 360, 'learning_rate': 0.1613530133590021, 'num_leaves': 2470, 'max_depth': 9, 'lambda_l1': 30, 'lambda_l2': 36, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:53,428] Trial 49 finished with value: 0.7994761587960039 and parameters: {'iterations': 1013, 'learning_rate': 0.20780768160638086, 'num_leaves': 1630, 'max_depth': 4, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:53,842] Trial 50 finished with value: 0.7967714162720619 and parameters: {'iterations': 896, 'learning_rate': 0.20713269727296718, 'num_leaves': 1530, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:54,259] Trial 51 finished with value: 0.7963761593550659 and parameters: {'iterations': 1025, 'learning_rate': 0.20637073192899377, 'num_leaves': 2180, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 72, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:54,720] Trial 52 finished with value: 0.7919254658385094 and parameters: {'iterations': 899, 'learning_rate': 0.22233797947477257, 'num_leaves': 1540, 'max_depth': 4, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:55,145] Trial 53 finished with value: 0.7913211196896087 and parameters: {'iterations': 993, 'learning_rate': 0.24843429014657326, 'num_leaves': 1560, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 75, 'feature_fraction': 0.2}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:55,624] Trial 54 finished with value: 0.789553924336533 and parameters: {'iterations': 808, 'learning_rate': 0.21577481653611127, 'num_leaves': 2010, 'max_depth': 4, 'lambda_l1': 3, 'lambda_l2': 84, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:56,081] Trial 55 finished with value: 0.7905322830419689 and parameters: {'iterations': 864, 'learning_rate': 0.026567462885213416, 'num_leaves': 2880, 'max_depth': 3, 'lambda_l1': 12, 'lambda_l2': 57, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:56,489] Trial 56 finished with value: 0.7935685494015239 and parameters: {'iterations': 730, 'learning_rate': 0.2018199588999485, 'num_leaves': 2690, 'max_depth': 4, 'lambda_l1': 93, 'lambda_l2': 69, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:56,935] Trial 57 finished with value: 0.7943590632355161 and parameters: {'iterations': 939, 'learning_rate': 0.1907324161941457, 'num_leaves': 2310, 'max_depth': 3, 'lambda_l1': 24, 'lambda_l2': 66, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:57,500] Trial 58 finished with value: 0.7989976016235163 and parameters: {'iterations': 1005, 'learning_rate': 0.23284953485146898, 'num_leaves': 1850, 'max_depth': 5, 'lambda_l1': 9, 'lambda_l2': 54, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:58,006] Trial 59 finished with value: 0.794504419386038 and parameters: {'iterations': 1063, 'learning_rate': 0.23019047249812916, 'num_leaves': 2690, 'max_depth': 5, 'lambda_l1': 18, 'lambda_l2': 51, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:58,469] Trial 60 finished with value: 0.7941304068295029 and parameters: {'iterations': 1113, 'learning_rate': 0.2840868836447621, 'num_leaves': 1790, 'max_depth': 4, 'lambda_l1': 36, 'lambda_l2': 57, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:58,913] Trial 61 finished with value: 0.7938805060630287 and parameters: {'iterations': 1013, 'learning_rate': 0.24194153538529456, 'num_leaves': 1590, 'max_depth': 3, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:59,430] Trial 62 finished with value: 0.7941304068295029 and parameters: {'iterations': 976, 'learning_rate': 0.21503554879691172, 'num_leaves': 1900, 'max_depth': 4, 'lambda_l1': 0, 'lambda_l2': 57, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:57:59,969] Trial 63 finished with value: 0.7854772433765116 and parameters: {'iterations': 1086, 'learning_rate': 0.2543035757606271, 'num_leaves': 1660, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 45, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:00,428] Trial 64 finished with value: 0.7921541222445226 and parameters: {'iterations': 1008, 'learning_rate': 0.26244717764134196, 'num_leaves': 1130, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 3, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:00,911] Trial 65 finished with value: 0.7945877196415294 and parameters: {'iterations': 922, 'learning_rate': 0.22215895708932876, 'num_leaves': 2120, 'max_depth': 4, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:01,579] Trial 66 finished with value: 0.794504419386038 and parameters: {'iterations': 846, 'learning_rate': 0.20643506238918466, 'num_leaves': 1840, 'max_depth': 6, 'lambda_l1': 6, 'lambda_l2': 15, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:02,333] Trial 67 finished with value: 0.7957734903925175 and parameters: {'iterations': 1141, 'learning_rate': 0.2386380638961175, 'num_leaves': 1380, 'max_depth': 10, 'lambda_l1': 21, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:03,031] Trial 68 finished with value: 0.7946710198970207 and parameters: {'iterations': 897, 'learning_rate': 0.17552436387425283, 'num_leaves': 2520, 'max_depth': 5, 'lambda_l1': 9, 'lambda_l2': 54, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:03,641] Trial 69 finished with value: 0.7923827786505357 and parameters: {'iterations': 1064, 'learning_rate': 0.18961631075492352, 'num_leaves': 2930, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 99, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:04,374] Trial 70 finished with value: 0.7887013546075105 and parameters: {'iterations': 1103, 'learning_rate': 0.28107308044835594, 'num_leaves': 1470, 'max_depth': 4, 'lambda_l1': 6, 'lambda_l2': 9, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:05,045] Trial 71 finished with value: 0.7949829765585253 and parameters: {'iterations': 1168, 'learning_rate': 0.16849208760959455, 'num_leaves': 1320, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 30, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:05,600] Trial 72 finished with value: 0.7958567906480088 and parameters: {'iterations': 763, 'learning_rate': 0.19595943764051618, 'num_leaves': 1700, 'max_depth': 8, 'lambda_l1': 18, 'lambda_l2': 45, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:06,123] Trial 73 finished with value: 0.7938184501679981 and parameters: {'iterations': 1181, 'learning_rate': 0.2139039090116409, 'num_leaves': 1260, 'max_depth': 6, 'lambda_l1': 15, 'lambda_l2': 21, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:06,722] Trial 74 finished with value: 0.7945256637464989 and parameters: {'iterations': 959, 'learning_rate': 0.15666074458203222, 'num_leaves': 1610, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 42, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:07,249] Trial 75 finished with value: 0.7959400909035003 and parameters: {'iterations': 1046, 'learning_rate': 0.18385199663338092, 'num_leaves': 1030, 'max_depth': 11, 'lambda_l1': 27, 'lambda_l2': 60, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:08,060] Trial 76 finished with value: 0.7947330757920512 and parameters: {'iterations': 1144, 'learning_rate': 0.2267131633622385, 'num_leaves': 2020, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 33, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:08,515] Trial 77 finished with value: 0.7898038251030071 and parameters: {'iterations': 1123, 'learning_rate': 0.20331093679001663, 'num_leaves': 2260, 'max_depth': 3, 'lambda_l1': 21, 'lambda_l2': 51, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:09,173] Trial 78 finished with value: 0.7946710198970207 and parameters: {'iterations': 1178, 'learning_rate': 0.29801278010741405, 'num_leaves': 1430, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 27, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:09,645] Trial 79 finished with value: 0.7916543207115743 and parameters: {'iterations': 1087, 'learning_rate': 0.14186435436504735, 'num_leaves': 1170, 'max_depth': 6, 'lambda_l1': 12, 'lambda_l2': 3, 'feature_fraction': 0.2}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:10,238] Trial 80 finished with value: 0.7957734903925175 and parameters: {'iterations': 1024, 'learning_rate': 0.23311104234825888, 'num_leaves': 2410, 'max_depth': 5, 'lambda_l1': 6, 'lambda_l2': 15, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:10,683] Trial 81 finished with value: 0.7961475029490527 and parameters: {'iterations': 1034, 'learning_rate': 0.1743798718180326, 'num_leaves': 2210, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 75, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:11,117] Trial 82 finished with value: 0.795440289370552 and parameters: {'iterations': 995, 'learning_rate': 0.22033555158759052, 'num_leaves': 2160, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 78, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:11,546] Trial 83 finished with value: 0.7934852491460327 and parameters: {'iterations': 1062, 'learning_rate': 0.21082192453624032, 'num_leaves': 1720, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:12,002] Trial 84 finished with value: 0.7885968099915581 and parameters: {'iterations': 637, 'learning_rate': 0.20807345300238747, 'num_leaves': 1950, 'max_depth': 4, 'lambda_l1': 9, 'lambda_l2': 39, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:12,466] Trial 85 finished with value: 0.7905730945765383 and parameters: {'iterations': 1020, 'learning_rate': 0.19655917666156883, 'num_leaves': 1520, 'max_depth': 4, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:12,891] Trial 86 finished with value: 0.7911757635390868 and parameters: {'iterations': 1108, 'learning_rate': 0.18270104085619865, 'num_leaves': 2630, 'max_depth': 3, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 0.2}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:13,572] Trial 87 finished with value: 0.7888254663975713 and parameters: {'iterations': 1128, 'learning_rate': 0.12452601352225294, 'num_leaves': 2350, 'max_depth': 7, 'lambda_l1': 3, 'lambda_l2': 9, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:14,001] Trial 88 finished with value: 0.7911349520045172 and parameters: {'iterations': 971, 'learning_rate': 0.2035259347466353, 'num_leaves': 1830, 'max_depth': 3, 'lambda_l1': 54, 'lambda_l2': 72, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:14,501] Trial 89 finished with value: 0.7944423634910074 and parameters: {'iterations': 1199, 'learning_rate': 0.16146434988896638, 'num_leaves': 2800, 'max_depth': 4, 'lambda_l1': 24, 'lambda_l2': 87, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:15,061] Trial 90 finished with value: 0.7956281342419956 and parameters: {'iterations': 1077, 'learning_rate': 0.2618533507083824, 'num_leaves': 2470, 'max_depth': 8, 'lambda_l1': 45, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:15,804] Trial 91 finished with value: 0.7944423634910074 and parameters: {'iterations': 1150, 'learning_rate': 0.19206236870804277, 'num_leaves': 2800, 'max_depth': 11, 'lambda_l1': 30, 'lambda_l2': 21, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:16,625] Trial 92 finished with value: 0.7965640042265095 and parameters: {'iterations': 1173, 'learning_rate': 0.24744090556583967, 'num_leaves': 2550, 'max_depth': 12, 'lambda_l1': 39, 'lambda_l2': 27, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:17,413] Trial 93 finished with value: 0.7957947347529785 and parameters: {'iterations': 1173, 'learning_rate': 0.24724957069518186, 'num_leaves': 2540, 'max_depth': 12, 'lambda_l1': 36, 'lambda_l2': 39, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:18,378] Trial 94 finished with value: 0.7920300104544616 and parameters: {'iterations': 1094, 'learning_rate': 0.22811500089705616, 'num_leaves': 2660, 'max_depth': 11, 'lambda_l1': 45, 'lambda_l2': 30, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:19,593] Trial 95 finished with value: 0.7950875211744778 and parameters: {'iterations': 1055, 'learning_rate': 0.21792741569382007, 'num_leaves': 2260, 'max_depth': 10, 'lambda_l1': 21, 'lambda_l2': 36, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:20,275] Trial 96 finished with value: 0.7958567906480088 and parameters: {'iterations': 1136, 'learning_rate': 0.23721095401753312, 'num_leaves': 2080, 'max_depth': 10, 'lambda_l1': 33, 'lambda_l2': 27, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:20,748] Trial 97 finished with value: 0.7954827780914738 and parameters: {'iterations': 937, 'learning_rate': 0.24392831328910464, 'num_leaves': 2350, 'max_depth': 6, 'lambda_l1': 39, 'lambda_l2': 18, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:21,321] Trial 98 finished with value: 0.7929446360785146 and parameters: {'iterations': 1112, 'learning_rate': 0.23170651574273726, 'num_leaves': 1640, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:22,124] Trial 99 finished with value: 0.7938805060630287 and parameters: {'iterations': 1033, 'learning_rate': 0.22397372964084916, 'num_leaves': 2460, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:22,651] Trial 100 finished with value: 0.7911137076440564 and parameters: {'iterations': 900, 'learning_rate': 0.2749591359022446, 'num_leaves': 2760, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 54, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:23,213] Trial 101 finished with value: 0.7940471065740113 and parameters: {'iterations': 1187, 'learning_rate': 0.19682552021231006, 'num_leaves': 2930, 'max_depth': 11, 'lambda_l1': 30, 'lambda_l2': 27, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:23,794] Trial 102 finished with value: 0.7944423634910074 and parameters: {'iterations': 1151, 'learning_rate': 0.20998714805981786, 'num_leaves': 2600, 'max_depth': 12, 'lambda_l1': 27, 'lambda_l2': 33, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:24,329] Trial 103 finished with value: 0.794920920663495 and parameters: {'iterations': 1165, 'learning_rate': 0.2023100115564296, 'num_leaves': 1310, 'max_depth': 12, 'lambda_l1': 33, 'lambda_l2': 24, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:24,968] Trial 104 finished with value: 0.7930279363340061 and parameters: {'iterations': 1197, 'learning_rate': 0.1876308775050035, 'num_leaves': 2830, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 21, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:25,522] Trial 105 finished with value: 0.7926539237774709 and parameters: {'iterations': 996, 'learning_rate': 0.2172529175615699, 'num_leaves': 2400, 'max_depth': 11, 'lambda_l1': 39, 'lambda_l2': 30, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:26,275] Trial 106 finished with value: 0.794899676303034 and parameters: {'iterations': 1080, 'learning_rate': 0.2554766686748654, 'num_leaves': 2860, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 48, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:26,736] Trial 107 finished with value: 0.7878275405180268 and parameters: {'iterations': 1129, 'learning_rate': 0.05755901079931751, 'num_leaves': 1510, 'max_depth': 3, 'lambda_l1': 0, 'lambda_l2': 45, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:27,192] Trial 108 finished with value: 0.7932565927400194 and parameters: {'iterations': 1160, 'learning_rate': 0.29046882591203804, 'num_leaves': 2720, 'max_depth': 4, 'lambda_l1': 48, 'lambda_l2': 63, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:28,093] Trial 109 finished with value: 0.7927567912070711 and parameters: {'iterations': 1011, 'learning_rate': 0.2501978022959336, 'num_leaves': 2510, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 6, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:28,819] Trial 110 finished with value: 0.7946710198970207 and parameters: {'iterations': 1098, 'learning_rate': 0.18012133281720533, 'num_leaves': 1580, 'max_depth': 5, 'lambda_l1': 21, 'lambda_l2': 12, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:29,369] Trial 111 finished with value: 0.7908850512380431 and parameters: {'iterations': 958, 'learning_rate': 0.2855945095859267, 'num_leaves': 1010, 'max_depth': 8, 'lambda_l1': 72, 'lambda_l2': 15, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:29,948] Trial 112 finished with value: 0.7916135091770046 and parameters: {'iterations': 980, 'learning_rate': 0.2943126959703832, 'num_leaves': 870, 'max_depth': 8, 'lambda_l1': 57, 'lambda_l2': 9, 'feature_fraction': 0.4}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:30,519] Trial 113 finished with value: 0.7920495776285703 and parameters: {'iterations': 1048, 'learning_rate': 0.2626158636924168, 'num_leaves': 1030, 'max_depth': 7, 'lambda_l1': 51, 'lambda_l2': 33, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:31,125] Trial 114 finished with value: 0.794920920663495 and parameters: {'iterations': 958, 'learning_rate': 0.26976802458667803, 'num_leaves': 970, 'max_depth': 9, 'lambda_l1': 48, 'lambda_l2': 18, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:31,621] Trial 115 finished with value: 0.7930279363340061 and parameters: {'iterations': 913, 'learning_rate': 0.21280530530314212, 'num_leaves': 650, 'max_depth': 7, 'lambda_l1': 42, 'lambda_l2': 27, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:32,015] Trial 116 finished with value: 0.7960642026935613 and parameters: {'iterations': 837, 'learning_rate': 0.27849415413964174, 'num_leaves': 1250, 'max_depth': 3, 'lambda_l1': 60, 'lambda_l2': 24, 'feature_fraction': 0.2}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:32,606] Trial 117 finished with value: 0.7903444381705251 and parameters: {'iterations': 1181, 'learning_rate': 0.28761273771125834, 'num_leaves': 1450, 'max_depth': 8, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 0.30000000000000004}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:33,142] Trial 118 finished with value: 0.7941304068295029 and parameters: {'iterations': 1024, 'learning_rate': 0.2399308262535825, 'num_leaves': 1750, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 78, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:33,583] Trial 119 finished with value: 0.7934231932510021 and parameters: {'iterations': 1071, 'learning_rate': 0.2986431211524604, 'num_leaves': 2570, 'max_depth': 3, 'lambda_l1': 24, 'lambda_l2': 36, 'feature_fraction': 0.5}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:34,207] Trial 120 finished with value: 0.7966473044820009 and parameters: {'iterations': 1117, 'learning_rate': 0.19401590089253629, 'num_leaves': 2970, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 57, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:34,884] Trial 121 finished with value: 0.7942757629800247 and parameters: {'iterations': 786, 'learning_rate': 0.19311413760215287, 'num_leaves': 2980, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 57, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:35,524] Trial 122 finished with value: 0.7916347535374656 and parameters: {'iterations': 1140, 'learning_rate': 0.17242544061458115, 'num_leaves': 3000, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 54, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:36,112] Trial 123 finished with value: 0.7949829765585253 and parameters: {'iterations': 1117, 'learning_rate': 0.20615418189724555, 'num_leaves': 2900, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:36,976] Trial 124 finished with value: 0.7891374230590761 and parameters: {'iterations': 1168, 'learning_rate': 0.22640989590664698, 'num_leaves': 2640, 'max_depth': 8, 'lambda_l1': 0, 'lambda_l2': 3, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:37,643] Trial 125 finished with value: 0.7972091619099798 and parameters: {'iterations': 693, 'learning_rate': 0.21935098859859775, 'num_leaves': 1370, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:38,360] Trial 126 finished with value: 0.7959400909035003 and parameters: {'iterations': 569, 'learning_rate': 0.2004437323006401, 'num_leaves': 1400, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:38,978] Trial 127 finished with value: 0.7974378183159931 and parameters: {'iterations': 629, 'learning_rate': 0.23233187279786557, 'num_leaves': 2740, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:39,642] Trial 128 finished with value: 0.7973545180605016 and parameters: {'iterations': 671, 'learning_rate': 0.21919145120049413, 'num_leaves': 2760, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:40,299] Trial 129 finished with value: 0.7999334716080304 and parameters: {'iterations': 596, 'learning_rate': 0.2334731342712423, 'num_leaves': 2750, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:40,958] Trial 130 finished with value: 0.7978330752329892 and parameters: {'iterations': 614, 'learning_rate': 0.22144632656380944, 'num_leaves': 2700, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:41,862] Trial 131 finished with value: 0.7933611373559717 and parameters: {'iterations': 610, 'learning_rate': 0.23383598123656751, 'num_leaves': 2750, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:42,751] Trial 132 finished with value: 0.7959400909035003 and parameters: {'iterations': 693, 'learning_rate': 0.22004631540324027, 'num_leaves': 2680, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:43,707] Trial 133 finished with value: 0.7941304068295029 and parameters: {'iterations': 672, 'learning_rate': 0.2263596330358744, 'num_leaves': 2850, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:44,431] Trial 134 finished with value: 0.794920920663495 and parameters: {'iterations': 621, 'learning_rate': 0.2369740622389248, 'num_leaves': 2770, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:45,151] Trial 135 finished with value: 0.7940471065740113 and parameters: {'iterations': 590, 'learning_rate': 0.2305510064800243, 'num_leaves': 2720, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:45,844] Trial 136 finished with value: 0.794920920663495 and parameters: {'iterations': 717, 'learning_rate': 0.22078961986719006, 'num_leaves': 2600, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:46,477] Trial 137 finished with value: 0.7954827780914738 and parameters: {'iterations': 663, 'learning_rate': 0.21381473631973488, 'num_leaves': 2940, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:47,260] Trial 138 finished with value: 0.7934852491460327 and parameters: {'iterations': 639, 'learning_rate': 0.24028864949943227, 'num_leaves': 2510, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:47,942] Trial 139 finished with value: 0.7958355462875479 and parameters: {'iterations': 552, 'learning_rate': 0.24521778413854967, 'num_leaves': 2880, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 45, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:48,589] Trial 140 finished with value: 0.7928613358230233 and parameters: {'iterations': 505, 'learning_rate': 0.2312441023346209, 'num_leaves': 2650, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:49,220] Trial 141 finished with value: 0.795711434497487 and parameters: {'iterations': 595, 'learning_rate': 0.21735929771484844, 'num_leaves': 1120, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:49,897] Trial 142 finished with value: 0.7941304068295029 and parameters: {'iterations': 694, 'learning_rate': 0.22302964409626247, 'num_leaves': 1190, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:50,635] Trial 143 finished with value: 0.7911349520045172 and parameters: {'iterations': 746, 'learning_rate': 0.21354087701556468, 'num_leaves': 1320, 'max_depth': 8, 'lambda_l1': 6, 'lambda_l2': 42, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:51,242] Trial 144 finished with value: 0.7953782334755214 and parameters: {'iterations': 644, 'learning_rate': 0.2067409177892375, 'num_leaves': 1660, 'max_depth': 8, 'lambda_l1': 18, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:51,795] Trial 145 finished with value: 0.7950042209189864 and parameters: {'iterations': 683, 'learning_rate': 0.18578433132049016, 'num_leaves': 2780, 'max_depth': 9, 'lambda_l1': 21, 'lambda_l2': 54, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:52,394] Trial 146 finished with value: 0.7923207227555054 and parameters: {'iterations': 618, 'learning_rate': 0.22508210874285423, 'num_leaves': 1530, 'max_depth': 8, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:53,224] Trial 147 finished with value: 0.791842165583018 and parameters: {'iterations': 573, 'learning_rate': 0.14871664163568346, 'num_leaves': 1470, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:53,883] Trial 148 finished with value: 0.7965640042265095 and parameters: {'iterations': 667, 'learning_rate': 0.23549801735161327, 'num_leaves': 2700, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:54,729] Trial 149 finished with value: 0.7913015525155 and parameters: {'iterations': 658, 'learning_rate': 0.2485671380939085, 'num_leaves': 2710, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:55,696] Trial 150 finished with value: 0.7904277384260165 and parameters: {'iterations': 627, 'learning_rate': 0.2365092557635189, 'num_leaves': 2840, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 42, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:56,609] Trial 151 finished with value: 0.7949421650239558 and parameters: {'iterations': 707, 'learning_rate': 0.23033373277158892, 'num_leaves': 2550, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:57,418] Trial 152 finished with value: 0.7950662768140169 and parameters: {'iterations': 668, 'learning_rate': 0.20953635019967448, 'num_leaves': 2650, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 54, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:58,100] Trial 153 finished with value: 0.7936730940174763 and parameters: {'iterations': 721, 'learning_rate': 0.2212432963626353, 'num_leaves': 1370, 'max_depth': 7, 'lambda_l1': 9, 'lambda_l2': 45, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:58,932] Trial 154 finished with value: 0.7935685494015239 and parameters: {'iterations': 588, 'learning_rate': 0.2422785698584637, 'num_leaves': 2410, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 57, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:58:59,547] Trial 155 finished with value: 0.7966473044820009 and parameters: {'iterations': 606, 'learning_rate': 0.20079724265213303, 'num_leaves': 2800, 'max_depth': 6, 'lambda_l1': 9, 'lambda_l2': 36, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:00,197] Trial 156 finished with value: 0.7948163760475426 and parameters: {'iterations': 604, 'learning_rate': 0.19912368657965282, 'num_leaves': 2810, 'max_depth': 6, 'lambda_l1': 6, 'lambda_l2': 39, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:00,859] Trial 157 finished with value: 0.7917588653275266 and parameters: {'iterations': 633, 'learning_rate': 0.21641335712429063, 'num_leaves': 2740, 'max_depth': 6, 'lambda_l1': 6, 'lambda_l2': 51, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:01,443] Trial 158 finished with value: 0.7956068898815348 and parameters: {'iterations': 515, 'learning_rate': 0.25204129665524605, 'num_leaves': 2870, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:01,919] Trial 159 finished with value: 0.7905110386815078 and parameters: {'iterations': 571, 'learning_rate': 0.234545933616016, 'num_leaves': 2610, 'max_depth': 8, 'lambda_l1': 90, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:02,508] Trial 160 finished with value: 0.7938805060630287 and parameters: {'iterations': 406, 'learning_rate': 0.21027947375489278, 'num_leaves': 2710, 'max_depth': 5, 'lambda_l1': 9, 'lambda_l2': 54, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:03,057] Trial 161 finished with value: 0.7942757629800247 and parameters: {'iterations': 1091, 'learning_rate': 0.19112890113093411, 'num_leaves': 1600, 'max_depth': 6, 'lambda_l1': 18, 'lambda_l2': 33, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:03,636] Trial 162 finished with value: 0.7958567906480088 and parameters: {'iterations': 648, 'learning_rate': 0.20256601728827833, 'num_leaves': 2930, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 36, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:04,260] Trial 163 finished with value: 0.7963974037155269 and parameters: {'iterations': 544, 'learning_rate': 0.16482206981255618, 'num_leaves': 2790, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 30, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:04,909] Trial 164 finished with value: 0.794899676303034 and parameters: {'iterations': 675, 'learning_rate': 0.2249013926560467, 'num_leaves': 2480, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 36, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:05,479] Trial 165 finished with value: 0.7921541222445226 and parameters: {'iterations': 769, 'learning_rate': 0.2289386858470241, 'num_leaves': 2690, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:06,181] Trial 166 finished with value: 0.7887846548630019 and parameters: {'iterations': 1111, 'learning_rate': 0.1947913832703318, 'num_leaves': 2550, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:06,913] Trial 167 finished with value: 0.79662606012154 and parameters: {'iterations': 624, 'learning_rate': 0.21981911713647645, 'num_leaves': 2900, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 57, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:07,928] Trial 168 finished with value: 0.7959188465430393 and parameters: {'iterations': 614, 'learning_rate': 0.21566881809886246, 'num_leaves': 2870, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 57, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:08,919] Trial 169 finished with value: 0.7950662768140169 and parameters: {'iterations': 654, 'learning_rate': 0.20541029489690282, 'num_leaves': 2820, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:09,664] Trial 170 finished with value: 0.7934852491460327 and parameters: {'iterations': 744, 'learning_rate': 0.23510751340076402, 'num_leaves': 2960, 'max_depth': 4, 'lambda_l1': 9, 'lambda_l2': 54, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:10,591] Trial 171 finished with value: 0.7952116329645387 and parameters: {'iterations': 700, 'learning_rate': 0.21903364093604005, 'num_leaves': 2920, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 57, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:11,247] Trial 172 finished with value: 0.7960021467985308 and parameters: {'iterations': 581, 'learning_rate': 0.22290279241975516, 'num_leaves': 1830, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 51, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:11,892] Trial 173 finished with value: 0.79396380631852 and parameters: {'iterations': 632, 'learning_rate': 0.15627441084511612, 'num_leaves': 3000, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 39, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:12,446] Trial 174 finished with value: 0.79263267941701 and parameters: {'iterations': 608, 'learning_rate': 0.22791274276708465, 'num_leaves': 1280, 'max_depth': 9, 'lambda_l1': 21, 'lambda_l2': 54, 'feature_fraction': 0.6000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:13,213] Trial 175 finished with value: 0.7942757629800247 and parameters: {'iterations': 1056, 'learning_rate': 0.21228951235876234, 'num_leaves': 2770, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:13,781] Trial 176 finished with value: 0.7954615337310129 and parameters: {'iterations': 1136, 'learning_rate': 0.23999583427473994, 'num_leaves': 2600, 'max_depth': 8, 'lambda_l1': 18, 'lambda_l2': 63, 'feature_fraction': 0.7}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:14,361] Trial 177 finished with value: 0.7950875211744778 and parameters: {'iterations': 685, 'learning_rate': 0.17859185008738354, 'num_leaves': 2660, 'max_depth': 6, 'lambda_l1': 15, 'lambda_l2': 33, 'feature_fraction': 0.9000000000000001}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:14,997] Trial 178 finished with value: 0.7937972058075372 and parameters: {'iterations': 603, 'learning_rate': 0.19927915740069926, 'num_leaves': 1420, 'max_depth': 5, 'lambda_l1': 0, 'lambda_l2': 6, 'feature_fraction': 0.8}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:15,631] Trial 179 finished with value: 0.7974590626764542 and parameters: {'iterations': 1077, 'learning_rate': 0.20927219926020368, 'num_leaves': 190, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 11 with value: 0.8004120287805178.\n",
            "[I 2024-05-15 21:59:16,304] Trial 180 finished with value: 0.8007664741629442 and parameters: {'iterations': 1074, 'learning_rate': 0.20777662756402576, 'num_leaves': 1950, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:16,988] Trial 181 finished with value: 0.7983116324054765 and parameters: {'iterations': 1076, 'learning_rate': 0.20773488685433208, 'num_leaves': 430, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:17,689] Trial 182 finished with value: 0.7963974037155269 and parameters: {'iterations': 1067, 'learning_rate': 0.20599690953313962, 'num_leaves': 320, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:18,334] Trial 183 finished with value: 0.7969805055039666 and parameters: {'iterations': 1090, 'learning_rate': 0.21162780083369728, 'num_leaves': 130, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:18,984] Trial 184 finished with value: 0.793049180694467 and parameters: {'iterations': 1085, 'learning_rate': 0.21113961182274316, 'num_leaves': 40, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:19,735] Trial 185 finished with value: 0.7954827780914738 and parameters: {'iterations': 1042, 'learning_rate': 0.19976422944846983, 'num_leaves': 170, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:20,467] Trial 186 finished with value: 0.7972924621654712 and parameters: {'iterations': 1076, 'learning_rate': 0.20703677910863208, 'num_leaves': 160, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:21,330] Trial 187 finished with value: 0.7919254658385094 and parameters: {'iterations': 1074, 'learning_rate': 0.20609329064839338, 'num_leaves': 220, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:22,217] Trial 188 finished with value: 0.7942757629800247 and parameters: {'iterations': 1097, 'learning_rate': 0.18968612472114635, 'num_leaves': 160, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:23,071] Trial 189 finished with value: 0.7953994778359825 and parameters: {'iterations': 1057, 'learning_rate': 0.2115296949051202, 'num_leaves': 370, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:23,787] Trial 190 finished with value: 0.7965019483314792 and parameters: {'iterations': 1035, 'learning_rate': 0.19537839720210798, 'num_leaves': 90, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:24,497] Trial 191 finished with value: 0.7994761587960039 and parameters: {'iterations': 1107, 'learning_rate': 0.2185105520048688, 'num_leaves': 530, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:25,187] Trial 192 finished with value: 0.7974378183159931 and parameters: {'iterations': 1096, 'learning_rate': 0.21569251199532175, 'num_leaves': 510, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:25,821] Trial 193 finished with value: 0.7969592611435057 and parameters: {'iterations': 1107, 'learning_rate': 0.2164875782982409, 'num_leaves': 590, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:26,472] Trial 194 finished with value: 0.7946089640019903 and parameters: {'iterations': 1100, 'learning_rate': 0.21895669663611422, 'num_leaves': 570, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:27,082] Trial 195 finished with value: 0.7954827780914738 and parameters: {'iterations': 1079, 'learning_rate': 0.21389141300984763, 'num_leaves': 520, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:27,749] Trial 196 finished with value: 0.7928825801834842 and parameters: {'iterations': 1064, 'learning_rate': 0.22290174996374, 'num_leaves': 470, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:28,397] Trial 197 finished with value: 0.7912394966204697 and parameters: {'iterations': 1111, 'learning_rate': 0.0971089777665467, 'num_leaves': 710, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:29,113] Trial 198 finished with value: 0.7981450318944939 and parameters: {'iterations': 1086, 'learning_rate': 0.20866834964222616, 'num_leaves': 360, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:29,839] Trial 199 finished with value: 0.7969592611435057 and parameters: {'iterations': 1089, 'learning_rate': 0.21705321343949743, 'num_leaves': 320, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:30,492] Trial 200 finished with value: 0.8003499728854874 and parameters: {'iterations': 1128, 'learning_rate': 0.20962819314449355, 'num_leaves': 440, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:31,143] Trial 201 finished with value: 0.7963565921809572 and parameters: {'iterations': 1131, 'learning_rate': 0.20982904239612585, 'num_leaves': 390, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:31,807] Trial 202 finished with value: 0.796668548842462 and parameters: {'iterations': 1104, 'learning_rate': 0.22514250937471234, 'num_leaves': 410, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:32,526] Trial 203 finished with value: 0.7990188459839773 and parameters: {'iterations': 1087, 'learning_rate': 0.2161000091143496, 'num_leaves': 630, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:33,236] Trial 204 finished with value: 0.795711434497487 and parameters: {'iterations': 1079, 'learning_rate': 0.20707204263020393, 'num_leaves': 460, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:34,197] Trial 205 finished with value: 0.7967926606325229 and parameters: {'iterations': 1055, 'learning_rate': 0.2130381617713598, 'num_leaves': 260, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:35,100] Trial 206 finished with value: 0.7979163754884805 and parameters: {'iterations': 1085, 'learning_rate': 0.2279766619366269, 'num_leaves': 510, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:36,127] Trial 207 finished with value: 0.7998714157129999 and parameters: {'iterations': 1126, 'learning_rate': 0.2295947511676319, 'num_leaves': 520, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:36,961] Trial 208 finished with value: 0.7952328773249995 and parameters: {'iterations': 1116, 'learning_rate': 0.23042889685080398, 'num_leaves': 530, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:37,721] Trial 209 finished with value: 0.7979163754884805 and parameters: {'iterations': 1152, 'learning_rate': 0.22794187338388647, 'num_leaves': 500, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:38,544] Trial 210 finished with value: 0.7970213170385362 and parameters: {'iterations': 1128, 'learning_rate': 0.23231148145456512, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:39,283] Trial 211 finished with value: 0.7975831744665149 and parameters: {'iterations': 1149, 'learning_rate': 0.22616728073776377, 'num_leaves': 630, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:40,026] Trial 212 finished with value: 0.7995594590514953 and parameters: {'iterations': 1149, 'learning_rate': 0.22609343458985315, 'num_leaves': 630, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:40,763] Trial 213 finished with value: 0.7975211185714844 and parameters: {'iterations': 1151, 'learning_rate': 0.22865165010085967, 'num_leaves': 600, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:41,500] Trial 214 finished with value: 0.7946922642574819 and parameters: {'iterations': 1151, 'learning_rate': 0.227514033031133, 'num_leaves': 640, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:42,293] Trial 215 finished with value: 0.794920920663495 and parameters: {'iterations': 1151, 'learning_rate': 0.22867514936594313, 'num_leaves': 610, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:43,019] Trial 216 finished with value: 0.7987068893224726 and parameters: {'iterations': 1143, 'learning_rate': 0.2234412899537584, 'num_leaves': 500, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:43,772] Trial 217 finished with value: 0.8007452298024834 and parameters: {'iterations': 1137, 'learning_rate': 0.22503556834587504, 'num_leaves': 530, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:44,532] Trial 218 finished with value: 0.795316177580491 and parameters: {'iterations': 1145, 'learning_rate': 0.2368660312261252, 'num_leaves': 750, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:45,340] Trial 219 finished with value: 0.7955235896260434 and parameters: {'iterations': 1130, 'learning_rate': 0.22471900579097778, 'num_leaves': 480, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:46,068] Trial 220 finished with value: 0.794899676303034 and parameters: {'iterations': 1163, 'learning_rate': 0.24141082467665523, 'num_leaves': 590, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:46,914] Trial 221 finished with value: 0.7975831744665149 and parameters: {'iterations': 1123, 'learning_rate': 0.2234103806321016, 'num_leaves': 530, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:47,881] Trial 222 finished with value: 0.7946922642574819 and parameters: {'iterations': 1136, 'learning_rate': 0.224607450407436, 'num_leaves': 420, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:49,015] Trial 223 finished with value: 0.7981450318944939 and parameters: {'iterations': 1156, 'learning_rate': 0.22903418947048954, 'num_leaves': 560, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:50,053] Trial 224 finished with value: 0.7913636084105304 and parameters: {'iterations': 1158, 'learning_rate': 0.22991437931373043, 'num_leaves': 550, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:50,825] Trial 225 finished with value: 0.7975211185714844 and parameters: {'iterations': 1124, 'learning_rate': 0.22207603132064851, 'num_leaves': 690, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:51,590] Trial 226 finished with value: 0.797978431383511 and parameters: {'iterations': 1183, 'learning_rate': 0.22236707627828464, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:52,453] Trial 227 finished with value: 0.7941924627245333 and parameters: {'iterations': 1182, 'learning_rate': 0.23483598163461508, 'num_leaves': 520, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:53,243] Trial 228 finished with value: 0.7967306047374924 and parameters: {'iterations': 1165, 'learning_rate': 0.2272393709405244, 'num_leaves': 620, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:54,008] Trial 229 finished with value: 0.7953994778359825 and parameters: {'iterations': 1143, 'learning_rate': 0.22143762056962019, 'num_leaves': 330, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:54,715] Trial 230 finished with value: 0.7938184501679981 and parameters: {'iterations': 1198, 'learning_rate': 0.2323357877116736, 'num_leaves': 450, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:55,496] Trial 231 finished with value: 0.7976664747220064 and parameters: {'iterations': 1123, 'learning_rate': 0.2230140554234929, 'num_leaves': 670, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:56,293] Trial 232 finished with value: 0.7977497749774978 and parameters: {'iterations': 1120, 'learning_rate': 0.224007156409273, 'num_leaves': 670, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:57,079] Trial 233 finished with value: 0.800100072119013 and parameters: {'iterations': 1118, 'learning_rate': 0.22118403729768296, 'num_leaves': 650, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:57,935] Trial 234 finished with value: 0.794504419386038 and parameters: {'iterations': 1117, 'learning_rate': 0.21897863092982484, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:58,710] Trial 235 finished with value: 0.7966473044820009 and parameters: {'iterations': 1110, 'learning_rate': 0.21844680230180205, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 21:59:59,555] Trial 236 finished with value: 0.7956901901370262 and parameters: {'iterations': 1138, 'learning_rate': 0.22458731435353882, 'num_leaves': 750, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:00,623] Trial 237 finished with value: 0.7974998742110235 and parameters: {'iterations': 1178, 'learning_rate': 0.23803523837934487, 'num_leaves': 660, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:01,721] Trial 238 finished with value: 0.7975831744665149 and parameters: {'iterations': 1161, 'learning_rate': 0.23173291921079364, 'num_leaves': 580, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:02,762] Trial 239 finished with value: 0.7965852485869706 and parameters: {'iterations': 1122, 'learning_rate': 0.21618093338990854, 'num_leaves': 490, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:03,729] Trial 240 finished with value: 0.794109162469042 and parameters: {'iterations': 1099, 'learning_rate': 0.24459148357709862, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:04,468] Trial 241 finished with value: 0.7950042209189864 and parameters: {'iterations': 1136, 'learning_rate': 0.224538482142365, 'num_leaves': 530, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:05,217] Trial 242 finished with value: 0.7960021467985308 and parameters: {'iterations': 1117, 'learning_rate': 0.22225665256807053, 'num_leaves': 440, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:05,996] Trial 243 finished with value: 0.7982903880450157 and parameters: {'iterations': 1144, 'learning_rate': 0.22511890739753918, 'num_leaves': 560, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:06,774] Trial 244 finished with value: 0.7972091619099798 and parameters: {'iterations': 1148, 'learning_rate': 0.2318207084261317, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:07,537] Trial 245 finished with value: 0.7950875211744778 and parameters: {'iterations': 1174, 'learning_rate': 0.2164002379399826, 'num_leaves': 580, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 180 with value: 0.8007664741629442.\n",
            "[I 2024-05-15 22:00:08,327] Trial 246 finished with value: 0.8009738862084966 and parameters: {'iterations': 1098, 'learning_rate': 0.22849085243435588, 'num_leaves': 1950, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:09,185] Trial 247 finished with value: 0.7960854470540222 and parameters: {'iterations': 1095, 'learning_rate': 0.2291033759394305, 'num_leaves': 1920, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:09,966] Trial 248 finished with value: 0.7979996757439719 and parameters: {'iterations': 1104, 'learning_rate': 0.23808716144107306, 'num_leaves': 1980, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:10,823] Trial 249 finished with value: 0.7960021467985308 and parameters: {'iterations': 1101, 'learning_rate': 0.23624819693091992, 'num_leaves': 2060, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:11,615] Trial 250 finished with value: 0.7976044188269759 and parameters: {'iterations': 1082, 'learning_rate': 0.24078708030037116, 'num_leaves': 1980, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:12,467] Trial 251 finished with value: 0.7942970073404857 and parameters: {'iterations': 1110, 'learning_rate': 0.23531957050690255, 'num_leaves': 380, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:13,231] Trial 252 finished with value: 0.7987068893224726 and parameters: {'iterations': 489, 'learning_rate': 0.21433425063138367, 'num_leaves': 1850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:14,239] Trial 253 finished with value: 0.7996427593069867 and parameters: {'iterations': 304, 'learning_rate': 0.21355560992715733, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:15,274] Trial 254 finished with value: 0.7961687473095135 and parameters: {'iterations': 303, 'learning_rate': 0.2141463595228467, 'num_leaves': 1820, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:16,266] Trial 255 finished with value: 0.7836479921284053 and parameters: {'iterations': 347, 'learning_rate': 0.03531669179187412, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:17,045] Trial 256 finished with value: 0.7968972052484752 and parameters: {'iterations': 435, 'learning_rate': 0.21185910201312502, 'num_leaves': 1970, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:17,827] Trial 257 finished with value: 0.7998093598179694 and parameters: {'iterations': 1071, 'learning_rate': 0.20670206931982027, 'num_leaves': 1760, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:18,603] Trial 258 finished with value: 0.7965640042265095 and parameters: {'iterations': 365, 'learning_rate': 0.2087197552228243, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:19,145] Trial 259 finished with value: 0.791696809432496 and parameters: {'iterations': 1055, 'learning_rate': 0.2035036609119785, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 72, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:19,874] Trial 260 finished with value: 0.7958567906480088 and parameters: {'iterations': 441, 'learning_rate': 0.21449259330018547, 'num_leaves': 1990, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:20,772] Trial 261 finished with value: 0.7974998742110235 and parameters: {'iterations': 483, 'learning_rate': 0.20784154580573397, 'num_leaves': 1690, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:21,484] Trial 262 finished with value: 0.800661929546992 and parameters: {'iterations': 1076, 'learning_rate': 0.20269819315203824, 'num_leaves': 1740, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:22,273] Trial 263 finished with value: 0.7986856449620117 and parameters: {'iterations': 1067, 'learning_rate': 0.20248223322280592, 'num_leaves': 1750, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:22,995] Trial 264 finished with value: 0.7991021462394686 and parameters: {'iterations': 1064, 'learning_rate': 0.20151363035361114, 'num_leaves': 1760, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:23,716] Trial 265 finished with value: 0.7979163754884805 and parameters: {'iterations': 1037, 'learning_rate': 0.19963996036093018, 'num_leaves': 1770, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:24,472] Trial 266 finished with value: 0.8002046167349655 and parameters: {'iterations': 1062, 'learning_rate': 0.20271522547429863, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 246 with value: 0.8009738862084966.\n",
            "[I 2024-05-15 22:00:25,242] Trial 267 finished with value: 0.8012858428700013 and parameters: {'iterations': 1067, 'learning_rate': 0.2025328723544829, 'num_leaves': 1730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:26,019] Trial 268 finished with value: 0.7997881154575085 and parameters: {'iterations': 1018, 'learning_rate': 0.2027886401367828, 'num_leaves': 1730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:26,889] Trial 269 finished with value: 0.7986235890669813 and parameters: {'iterations': 1010, 'learning_rate': 0.20063277333201537, 'num_leaves': 1730, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:27,868] Trial 270 finished with value: 0.7968972052484752 and parameters: {'iterations': 998, 'learning_rate': 0.20059745644307053, 'num_leaves': 1730, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:28,960] Trial 271 finished with value: 0.8004953290360091 and parameters: {'iterations': 1023, 'learning_rate': 0.19506497089907684, 'num_leaves': 1870, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:29,877] Trial 272 finished with value: 0.7993308026454821 and parameters: {'iterations': 1013, 'learning_rate': 0.1984662927484186, 'num_leaves': 1750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:30,420] Trial 273 finished with value: 0.7913848527709915 and parameters: {'iterations': 1030, 'learning_rate': 0.19565675193705903, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 99, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 267 with value: 0.8012858428700013.\n",
            "[I 2024-05-15 22:00:31,178] Trial 274 finished with value: 0.8019097561930105 and parameters: {'iterations': 1012, 'learning_rate': 0.18857523987250596, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:31,959] Trial 275 finished with value: 0.7982283321499852 and parameters: {'iterations': 989, 'learning_rate': 0.19343356763342553, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:32,718] Trial 276 finished with value: 0.8001833723745044 and parameters: {'iterations': 1015, 'learning_rate': 0.19235977450879488, 'num_leaves': 1680, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:33,472] Trial 277 finished with value: 0.7957947347529785 and parameters: {'iterations': 1031, 'learning_rate': 0.1875451976125669, 'num_leaves': 1640, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:34,227] Trial 278 finished with value: 0.7997881154575085 and parameters: {'iterations': 1013, 'learning_rate': 0.18581617694001085, 'num_leaves': 1690, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:34,994] Trial 279 finished with value: 0.7970425613989971 and parameters: {'iterations': 1001, 'learning_rate': 0.18902269220029308, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:35,702] Trial 280 finished with value: 0.7958567906480088 and parameters: {'iterations': 1016, 'learning_rate': 0.18603111008010434, 'num_leaves': 1670, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:36,424] Trial 281 finished with value: 0.7950042209189864 and parameters: {'iterations': 1011, 'learning_rate': 0.1805019699920163, 'num_leaves': 1710, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:37,178] Trial 282 finished with value: 0.7972712178050104 and parameters: {'iterations': 981, 'learning_rate': 0.19596712007429268, 'num_leaves': 1600, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:37,877] Trial 283 finished with value: 0.7964186480759877 and parameters: {'iterations': 1042, 'learning_rate': 0.19413272798369774, 'num_leaves': 1760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:38,640] Trial 284 finished with value: 0.7958567906480088 and parameters: {'iterations': 1024, 'learning_rate': 0.20280837597721868, 'num_leaves': 1670, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:39,505] Trial 285 finished with value: 0.7961687473095135 and parameters: {'iterations': 1051, 'learning_rate': 0.19127766224629297, 'num_leaves': 1880, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:40,586] Trial 286 finished with value: 0.7961066914144831 and parameters: {'iterations': 1015, 'learning_rate': 0.19763306608689415, 'num_leaves': 1790, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:41,580] Trial 287 finished with value: 0.7923827786505357 and parameters: {'iterations': 969, 'learning_rate': 0.1857545129854108, 'num_leaves': 1690, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:42,576] Trial 288 finished with value: 0.8000167718635217 and parameters: {'iterations': 1045, 'learning_rate': 0.20454419059672846, 'num_leaves': 1800, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:43,324] Trial 289 finished with value: 0.7969592611435057 and parameters: {'iterations': 1047, 'learning_rate': 0.2018578625688453, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:43,854] Trial 290 finished with value: 0.7921133107099531 and parameters: {'iterations': 1063, 'learning_rate': 0.20529124490085118, 'num_leaves': 1610, 'max_depth': 11, 'lambda_l1': 84, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:44,663] Trial 291 finished with value: 0.7993928585405125 and parameters: {'iterations': 1031, 'learning_rate': 0.1957108140651658, 'num_leaves': 1920, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:45,575] Trial 292 finished with value: 0.7919467101989702 and parameters: {'iterations': 1034, 'learning_rate': 0.18188227472857055, 'num_leaves': 1720, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:46,391] Trial 293 finished with value: 0.7949829765585253 and parameters: {'iterations': 987, 'learning_rate': 0.191454782233549, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:47,204] Trial 294 finished with value: 0.7975831744665149 and parameters: {'iterations': 1019, 'learning_rate': 0.19744708829910523, 'num_leaves': 1940, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:47,875] Trial 295 finished with value: 0.79396380631852 and parameters: {'iterations': 1061, 'learning_rate': 0.2023621878939633, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:48,742] Trial 296 finished with value: 0.7955448339865042 and parameters: {'iterations': 1054, 'learning_rate': 0.19371072692828006, 'num_leaves': 1560, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:49,462] Trial 297 finished with value: 0.7974165739555322 and parameters: {'iterations': 1036, 'learning_rate': 0.19755437147663538, 'num_leaves': 1850, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:50,254] Trial 298 finished with value: 0.7995594590514953 and parameters: {'iterations': 1044, 'learning_rate': 0.20467627989368464, 'num_leaves': 1640, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:51,121] Trial 299 finished with value: 0.7962520475650049 and parameters: {'iterations': 1003, 'learning_rate': 0.1857798603683615, 'num_leaves': 1630, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:51,957] Trial 300 finished with value: 0.7986856449620117 and parameters: {'iterations': 1018, 'learning_rate': 0.20618634576107597, 'num_leaves': 2050, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:52,978] Trial 301 finished with value: 0.7941924627245333 and parameters: {'iterations': 1042, 'learning_rate': 0.19175185473633247, 'num_leaves': 1690, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:54,206] Trial 302 finished with value: 0.7952328773249995 and parameters: {'iterations': 1003, 'learning_rate': 0.17739059244394847, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:55,268] Trial 303 finished with value: 0.801597799531506 and parameters: {'iterations': 1035, 'learning_rate': 0.20551775660260396, 'num_leaves': 1720, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:56,178] Trial 304 finished with value: 0.7951495770695082 and parameters: {'iterations': 1042, 'learning_rate': 0.20655390151987243, 'num_leaves': 1640, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:57,064] Trial 305 finished with value: 0.7977285306170367 and parameters: {'iterations': 1028, 'learning_rate': 0.20573489212655882, 'num_leaves': 1540, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:57,795] Trial 306 finished with value: 0.7965640042265095 and parameters: {'iterations': 1069, 'learning_rate': 0.21139498546823063, 'num_leaves': 2120, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:58,561] Trial 307 finished with value: 0.7983736883005071 and parameters: {'iterations': 1047, 'learning_rate': 0.17319568048997588, 'num_leaves': 1930, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:00:59,381] Trial 308 finished with value: 0.795316177580491 and parameters: {'iterations': 1073, 'learning_rate': 0.203483462471212, 'num_leaves': 1710, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:00,082] Trial 309 finished with value: 0.7936730940174763 and parameters: {'iterations': 981, 'learning_rate': 0.13463392156953297, 'num_leaves': 1890, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:00,785] Trial 310 finished with value: 0.7952541216854605 and parameters: {'iterations': 861, 'learning_rate': 0.20969602956501532, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:01,587] Trial 311 finished with value: 0.7997881154575085 and parameters: {'iterations': 1053, 'learning_rate': 0.19742617727419823, 'num_leaves': 1820, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:02,329] Trial 312 finished with value: 0.8000167718635217 and parameters: {'iterations': 1052, 'learning_rate': 0.20245256201171027, 'num_leaves': 1800, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:03,081] Trial 313 finished with value: 0.7993928585405125 and parameters: {'iterations': 1054, 'learning_rate': 0.18776299577756964, 'num_leaves': 1820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:03,965] Trial 314 finished with value: 0.7958567906480088 and parameters: {'iterations': 1066, 'learning_rate': 0.1986032534284466, 'num_leaves': 1740, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:04,527] Trial 315 finished with value: 0.7901157817645119 and parameters: {'iterations': 1052, 'learning_rate': 0.20138908432097402, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 66, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:05,265] Trial 316 finished with value: 0.7927992799279928 and parameters: {'iterations': 949, 'learning_rate': 0.11510788851959389, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:06,215] Trial 317 finished with value: 0.7998714157129999 and parameters: {'iterations': 1029, 'learning_rate': 0.20714874864945748, 'num_leaves': 1690, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:07,241] Trial 318 finished with value: 0.7987689452175031 and parameters: {'iterations': 1030, 'learning_rate': 0.1910154897577136, 'num_leaves': 1710, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:08,418] Trial 319 finished with value: 0.7999334716080304 and parameters: {'iterations': 1088, 'learning_rate': 0.20433568812024494, 'num_leaves': 1790, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:09,422] Trial 320 finished with value: 0.7987689452175031 and parameters: {'iterations': 1090, 'learning_rate': 0.21002169827321907, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:10,307] Trial 321 finished with value: 0.7956901901370262 and parameters: {'iterations': 1080, 'learning_rate': 0.1966580619051916, 'num_leaves': 1800, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:11,144] Trial 322 finished with value: 0.7958567906480088 and parameters: {'iterations': 1072, 'learning_rate': 0.26834763292450087, 'num_leaves': 1800, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:11,916] Trial 323 finished with value: 0.7988522454729946 and parameters: {'iterations': 1089, 'learning_rate': 0.18269241352929794, 'num_leaves': 2010, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:12,684] Trial 324 finished with value: 0.7986856449620117 and parameters: {'iterations': 1066, 'learning_rate': 0.2035089138307283, 'num_leaves': 1740, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:13,544] Trial 325 finished with value: 0.7986856449620117 and parameters: {'iterations': 1024, 'learning_rate': 0.20894368646564151, 'num_leaves': 1940, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:14,274] Trial 326 finished with value: 0.7965640042265095 and parameters: {'iterations': 1054, 'learning_rate': 0.19194413311250008, 'num_leaves': 1880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:15,002] Trial 327 finished with value: 0.7987689452175031 and parameters: {'iterations': 1000, 'learning_rate': 0.1970318540289678, 'num_leaves': 1710, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:15,834] Trial 328 finished with value: 0.7989976016235163 and parameters: {'iterations': 1096, 'learning_rate': 0.20403181808872872, 'num_leaves': 1820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:17,132] Trial 329 finished with value: 0.7959188465430393 and parameters: {'iterations': 1043, 'learning_rate': 0.21131081177005984, 'num_leaves': 1770, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:18,120] Trial 330 finished with value: 0.7945469081069597 and parameters: {'iterations': 1077, 'learning_rate': 0.2001233029740482, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:19,254] Trial 331 finished with value: 0.7961687473095135 and parameters: {'iterations': 1021, 'learning_rate': 0.2116123065345189, 'num_leaves': 1710, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:20,286] Trial 332 finished with value: 0.7967926606325229 and parameters: {'iterations': 1058, 'learning_rate': 0.18766827721072551, 'num_leaves': 1590, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:21,384] Trial 333 finished with value: 0.8004953290360091 and parameters: {'iterations': 1088, 'learning_rate': 0.20547155794500976, 'num_leaves': 1950, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:22,358] Trial 334 finished with value: 0.7994761587960039 and parameters: {'iterations': 799, 'learning_rate': 0.20509932628612623, 'num_leaves': 2040, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:23,102] Trial 335 finished with value: 0.7972712178050104 and parameters: {'iterations': 1090, 'learning_rate': 0.1936358994135017, 'num_leaves': 1940, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:24,032] Trial 336 finished with value: 0.7935685494015239 and parameters: {'iterations': 1074, 'learning_rate': 0.1990421231709577, 'num_leaves': 1980, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:24,773] Trial 337 finished with value: 0.7994974031564647 and parameters: {'iterations': 1105, 'learning_rate': 0.2079596206333828, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:25,604] Trial 338 finished with value: 0.7980617316390025 and parameters: {'iterations': 1040, 'learning_rate': 0.20186523668213757, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:26,334] Trial 339 finished with value: 0.7974378183159931 and parameters: {'iterations': 1063, 'learning_rate': 0.2115922852683671, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:27,017] Trial 340 finished with value: 0.7937351499125067 and parameters: {'iterations': 995, 'learning_rate': 0.19380353284611612, 'num_leaves': 1670, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:27,775] Trial 341 finished with value: 0.7999547159684913 and parameters: {'iterations': 1082, 'learning_rate': 0.1823338409727168, 'num_leaves': 2110, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:28,601] Trial 342 finished with value: 0.7947543201525121 and parameters: {'iterations': 1096, 'learning_rate': 0.1781443408472656, 'num_leaves': 1770, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:29,369] Trial 343 finished with value: 0.7992475023899906 and parameters: {'iterations': 1078, 'learning_rate': 0.16956741149877272, 'num_leaves': 1690, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:30,191] Trial 344 finished with value: 0.7961687473095135 and parameters: {'iterations': 1051, 'learning_rate': 0.18199019642347164, 'num_leaves': 2110, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:30,944] Trial 345 finished with value: 0.7972712178050104 and parameters: {'iterations': 1108, 'learning_rate': 0.1897412653408263, 'num_leaves': 2210, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:31,844] Trial 346 finished with value: 0.7937351499125067 and parameters: {'iterations': 1082, 'learning_rate': 0.18585691293733392, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:32,855] Trial 347 finished with value: 0.7950042209189864 and parameters: {'iterations': 1031, 'learning_rate': 0.1956492434812947, 'num_leaves': 1970, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:33,587] Trial 348 finished with value: 0.7913015525155 and parameters: {'iterations': 1062, 'learning_rate': 0.1989849441023867, 'num_leaves': 1580, 'max_depth': 11, 'lambda_l1': 78, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:34,746] Trial 349 finished with value: 0.7969592611435057 and parameters: {'iterations': 1016, 'learning_rate': 0.1893397498143356, 'num_leaves': 1740, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:35,565] Trial 350 finished with value: 0.7974378183159931 and parameters: {'iterations': 1094, 'learning_rate': 0.20152520225484388, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:36,512] Trial 351 finished with value: 0.7921541222445226 and parameters: {'iterations': 1045, 'learning_rate': 0.1836986824196166, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:37,312] Trial 352 finished with value: 0.7998926600734608 and parameters: {'iterations': 1123, 'learning_rate': 0.20528551856324273, 'num_leaves': 1750, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:38,073] Trial 353 finished with value: 0.7983949326609681 and parameters: {'iterations': 1120, 'learning_rate': 0.20743050187238038, 'num_leaves': 1730, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:38,788] Trial 354 finished with value: 0.7961687473095135 and parameters: {'iterations': 1122, 'learning_rate': 0.17411323995036265, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:39,561] Trial 355 finished with value: 0.7948588647684645 and parameters: {'iterations': 1099, 'learning_rate': 0.20518887907286232, 'num_leaves': 2020, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:40,286] Trial 356 finished with value: 0.7945256637464989 and parameters: {'iterations': 1125, 'learning_rate': 0.21416662338989906, 'num_leaves': 1500, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:40,980] Trial 357 finished with value: 0.7985402888114899 and parameters: {'iterations': 1074, 'learning_rate': 0.20593490827465422, 'num_leaves': 1760, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:41,747] Trial 358 finished with value: 0.7983949326609681 and parameters: {'iterations': 975, 'learning_rate': 0.21502839061128093, 'num_leaves': 1690, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:42,526] Trial 359 finished with value: 0.7982903880450157 and parameters: {'iterations': 1112, 'learning_rate': 0.19375185457838429, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:43,387] Trial 360 finished with value: 0.7945256637464989 and parameters: {'iterations': 1086, 'learning_rate': 0.2012364889296284, 'num_leaves': 2150, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:44,153] Trial 361 finished with value: 0.8004953290360091 and parameters: {'iterations': 1010, 'learning_rate': 0.20729011833807595, 'num_leaves': 1820, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:44,843] Trial 362 finished with value: 0.793985050678981 and parameters: {'iterations': 1104, 'learning_rate': 0.21055816088106286, 'num_leaves': 1820, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:45,822] Trial 363 finished with value: 0.800121316479474 and parameters: {'iterations': 1069, 'learning_rate': 0.20635392801994357, 'num_leaves': 1920, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:46,777] Trial 364 finished with value: 0.7974998742110235 and parameters: {'iterations': 1068, 'learning_rate': 0.2171266559539217, 'num_leaves': 1990, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:47,816] Trial 365 finished with value: 0.7974378183159931 and parameters: {'iterations': 1084, 'learning_rate': 0.21049266750625903, 'num_leaves': 2070, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:48,700] Trial 366 finished with value: 0.7999547159684913 and parameters: {'iterations': 1133, 'learning_rate': 0.20643698080353695, 'num_leaves': 1910, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:49,386] Trial 367 finished with value: 0.7927992799279928 and parameters: {'iterations': 1111, 'learning_rate': 0.2805451489404986, 'num_leaves': 1940, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:50,121] Trial 368 finished with value: 0.7973545180605016 and parameters: {'iterations': 1135, 'learning_rate': 0.2601544426108092, 'num_leaves': 1940, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:50,776] Trial 369 finished with value: 0.7983949326609681 and parameters: {'iterations': 1099, 'learning_rate': 0.20430776023376646, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 18, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:51,516] Trial 370 finished with value: 0.7971258616544884 and parameters: {'iterations': 1126, 'learning_rate': 0.21743485146947863, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:52,258] Trial 371 finished with value: 0.7987689452175031 and parameters: {'iterations': 1134, 'learning_rate': 0.2083601032554459, 'num_leaves': 2010, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:52,955] Trial 372 finished with value: 0.7997881154575085 and parameters: {'iterations': 1089, 'learning_rate': 0.1971977704429464, 'num_leaves': 1940, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:53,712] Trial 373 finished with value: 0.7960021467985308 and parameters: {'iterations': 1116, 'learning_rate': 0.2939486802781706, 'num_leaves': 2320, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:54,446] Trial 374 finished with value: 0.7942757629800247 and parameters: {'iterations': 1043, 'learning_rate': 0.2149315549648115, 'num_leaves': 1840, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:55,173] Trial 375 finished with value: 0.7962520475650049 and parameters: {'iterations': 1069, 'learning_rate': 0.2008221236824845, 'num_leaves': 2040, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:55,978] Trial 376 finished with value: 0.7933611373559717 and parameters: {'iterations': 823, 'learning_rate': 0.08911598627198289, 'num_leaves': 1820, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:56,737] Trial 377 finished with value: 0.7972712178050104 and parameters: {'iterations': 1103, 'learning_rate': 0.20556983541703425, 'num_leaves': 1900, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:57,715] Trial 378 finished with value: 0.7935685494015239 and parameters: {'iterations': 1132, 'learning_rate': 0.21171454267156964, 'num_leaves': 1970, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:58,550] Trial 379 finished with value: 0.7991021462394686 and parameters: {'iterations': 1060, 'learning_rate': 0.19467406920412822, 'num_leaves': 1800, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:01:59,721] Trial 380 finished with value: 0.7955448339865042 and parameters: {'iterations': 1031, 'learning_rate': 0.15262519408145436, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:00,846] Trial 381 finished with value: 0.7946710198970207 and parameters: {'iterations': 1088, 'learning_rate': 0.21803242854556495, 'num_leaves': 1940, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:01,712] Trial 382 finished with value: 0.8005786292915006 and parameters: {'iterations': 1053, 'learning_rate': 0.16294978680113525, 'num_leaves': 1790, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:02,409] Trial 383 finished with value: 0.7958567906480088 and parameters: {'iterations': 1078, 'learning_rate': 0.19002925001187929, 'num_leaves': 1850, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:03,153] Trial 384 finished with value: 0.7943590632355161 and parameters: {'iterations': 1058, 'learning_rate': 0.14849763350434175, 'num_leaves': 2100, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:03,875] Trial 385 finished with value: 0.7965640042265095 and parameters: {'iterations': 1097, 'learning_rate': 0.16381289810444735, 'num_leaves': 1770, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:04,545] Trial 386 finished with value: 0.7955448339865042 and parameters: {'iterations': 1116, 'learning_rate': 0.1633482829743754, 'num_leaves': 1830, 'max_depth': 12, 'lambda_l1': 18, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:05,465] Trial 387 finished with value: 0.7948376204080034 and parameters: {'iterations': 1052, 'learning_rate': 0.14489335497834735, 'num_leaves': 1920, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:06,198] Trial 388 finished with value: 0.7948376204080034 and parameters: {'iterations': 1073, 'learning_rate': 0.17614076345065569, 'num_leaves': 2010, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:07,052] Trial 389 finished with value: 0.7958567906480088 and parameters: {'iterations': 1105, 'learning_rate': 0.19866876103119152, 'num_leaves': 1800, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:07,779] Trial 390 finished with value: 0.7940471065740113 and parameters: {'iterations': 1138, 'learning_rate': 0.1673807613531613, 'num_leaves': 1830, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:08,559] Trial 391 finished with value: 0.7991429577740383 and parameters: {'iterations': 1084, 'learning_rate': 0.20190075383299644, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:09,270] Trial 392 finished with value: 0.7941304068295029 and parameters: {'iterations': 1046, 'learning_rate': 0.1563627108929441, 'num_leaves': 1780, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:10,015] Trial 393 finished with value: 0.7959400909035003 and parameters: {'iterations': 1000, 'learning_rate': 0.14074766900955218, 'num_leaves': 1940, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:10,594] Trial 394 finished with value: 0.7920920663494921 and parameters: {'iterations': 1120, 'learning_rate': 0.19249547846463336, 'num_leaves': 1740, 'max_depth': 12, 'lambda_l1': 66, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:11,299] Trial 395 finished with value: 0.7976877190824673 and parameters: {'iterations': 1070, 'learning_rate': 0.21011635622269578, 'num_leaves': 290, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:12,443] Trial 396 finished with value: 0.7951495770695082 and parameters: {'iterations': 1037, 'learning_rate': 0.1807937826125508, 'num_leaves': 900, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:13,253] Trial 397 finished with value: 0.7898250694634681 and parameters: {'iterations': 1091, 'learning_rate': 0.19845129535228342, 'num_leaves': 1990, 'max_depth': 11, 'lambda_l1': 42, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:14,325] Trial 398 finished with value: 0.7945256637464989 and parameters: {'iterations': 1059, 'learning_rate': 0.21564144687765163, 'num_leaves': 1870, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:15,213] Trial 399 finished with value: 0.7960233911589917 and parameters: {'iterations': 1158, 'learning_rate': 0.12086658759096738, 'num_leaves': 410, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:16,260] Trial 400 finished with value: 0.7861844569550124 and parameters: {'iterations': 1132, 'learning_rate': 0.20460542329678205, 'num_leaves': 2080, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:17,069] Trial 401 finished with value: 0.7933398929955107 and parameters: {'iterations': 1111, 'learning_rate': 0.18967420768390103, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:17,668] Trial 402 finished with value: 0.7937351499125067 and parameters: {'iterations': 1084, 'learning_rate': 0.19652489347321958, 'num_leaves': 1910, 'max_depth': 11, 'lambda_l1': 57, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:18,411] Trial 403 finished with value: 0.796668548842462 and parameters: {'iterations': 1025, 'learning_rate': 0.2098409402978126, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:19,082] Trial 404 finished with value: 0.7934231932510021 and parameters: {'iterations': 1060, 'learning_rate': 0.20353173767657032, 'num_leaves': 1730, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:19,891] Trial 405 finished with value: 0.7964186480759877 and parameters: {'iterations': 1106, 'learning_rate': 0.274396747842597, 'num_leaves': 1620, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:20,756] Trial 406 finished with value: 0.797978431383511 and parameters: {'iterations': 1043, 'learning_rate': 0.21847906348806773, 'num_leaves': 2260, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:21,496] Trial 407 finished with value: 0.7968139049929838 and parameters: {'iterations': 990, 'learning_rate': 0.19422902585800256, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:22,272] Trial 408 finished with value: 0.800121316479474 and parameters: {'iterations': 527, 'learning_rate': 0.21272533934276505, 'num_leaves': 2040, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:22,993] Trial 409 finished with value: 0.795711434497487 and parameters: {'iterations': 462, 'learning_rate': 0.21196002210605086, 'num_leaves': 2010, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:23,701] Trial 410 finished with value: 0.7954615337310129 and parameters: {'iterations': 403, 'learning_rate': 0.20635962039761221, 'num_leaves': 2160, 'max_depth': 12, 'lambda_l1': 18, 'lambda_l2': 63, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:24,466] Trial 411 finished with value: 0.7962732919254659 and parameters: {'iterations': 1006, 'learning_rate': 0.2129167246678691, 'num_leaves': 1970, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:25,439] Trial 412 finished with value: 0.7962520475650049 and parameters: {'iterations': 490, 'learning_rate': 0.20294590002673982, 'num_leaves': 2040, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:26,461] Trial 413 finished with value: 0.7976877190824673 and parameters: {'iterations': 539, 'learning_rate': 0.218795707031017, 'num_leaves': 2050, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:27,484] Trial 414 finished with value: 0.7952949332200301 and parameters: {'iterations': 1072, 'learning_rate': 0.20849669290170053, 'num_leaves': 1930, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:28,378] Trial 415 finished with value: 0.8002046167349655 and parameters: {'iterations': 1037, 'learning_rate': 0.19880948426543302, 'num_leaves': 1970, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:29,116] Trial 416 finished with value: 0.7983949326609681 and parameters: {'iterations': 1028, 'learning_rate': 0.19010123783879404, 'num_leaves': 2010, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:30,089] Trial 417 finished with value: 0.7910516517490258 and parameters: {'iterations': 1039, 'learning_rate': 0.19647562170036423, 'num_leaves': 1960, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:30,967] Trial 418 finished with value: 0.7954615337310129 and parameters: {'iterations': 508, 'learning_rate': 0.20118474692559818, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:31,636] Trial 419 finished with value: 0.7944423634910074 and parameters: {'iterations': 1015, 'learning_rate': 0.18902173419297022, 'num_leaves': 2080, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 63, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:32,460] Trial 420 finished with value: 0.7972712178050104 and parameters: {'iterations': 528, 'learning_rate': 0.19771459738991043, 'num_leaves': 2200, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:33,216] Trial 421 finished with value: 0.7932778371004803 and parameters: {'iterations': 551, 'learning_rate': 0.2125074373472008, 'num_leaves': 1960, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:34,076] Trial 422 finished with value: 0.7941304068295029 and parameters: {'iterations': 1057, 'learning_rate': 0.18578267759353562, 'num_leaves': 2110, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:34,871] Trial 423 finished with value: 0.7955448339865042 and parameters: {'iterations': 1045, 'learning_rate': 0.2878308784945278, 'num_leaves': 1910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:35,822] Trial 424 finished with value: 0.7934852491460327 and parameters: {'iterations': 778, 'learning_rate': 0.18237292544315822, 'num_leaves': 1860, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:36,562] Trial 425 finished with value: 0.7940471065740113 and parameters: {'iterations': 1025, 'learning_rate': 0.17163473449368635, 'num_leaves': 1940, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:37,270] Trial 426 finished with value: 0.7942757629800247 and parameters: {'iterations': 1063, 'learning_rate': 0.21866296345812392, 'num_leaves': 2030, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:37,880] Trial 427 finished with value: 0.7732488776827994 and parameters: {'iterations': 1079, 'learning_rate': 0.011954813232944367, 'num_leaves': 1840, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:39,061] Trial 428 finished with value: 0.7924660789060273 and parameters: {'iterations': 567, 'learning_rate': 0.2012335812258421, 'num_leaves': 1980, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:40,048] Trial 429 finished with value: 0.7877867289834575 and parameters: {'iterations': 1045, 'learning_rate': 0.05847349989439367, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:41,254] Trial 430 finished with value: 0.7960854470540222 and parameters: {'iterations': 1007, 'learning_rate': 0.19507355111556127, 'num_leaves': 1810, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:42,102] Trial 431 finished with value: 0.7968759608880143 and parameters: {'iterations': 979, 'learning_rate': 0.20570571667790447, 'num_leaves': 1880, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:42,890] Trial 432 finished with value: 0.7958780350084699 and parameters: {'iterations': 1084, 'learning_rate': 0.15863848269488964, 'num_leaves': 1710, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:43,625] Trial 433 finished with value: 0.7968759608880143 and parameters: {'iterations': 1064, 'learning_rate': 0.21286107060360046, 'num_leaves': 2080, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:44,375] Trial 434 finished with value: 0.7977497749774978 and parameters: {'iterations': 959, 'learning_rate': 0.19274107890897718, 'num_leaves': 1790, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:45,153] Trial 435 finished with value: 0.797978431383511 and parameters: {'iterations': 1038, 'learning_rate': 0.1999350230692091, 'num_leaves': 1980, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:45,819] Trial 436 finished with value: 0.794899676303034 and parameters: {'iterations': 457, 'learning_rate': 0.17773753977743398, 'num_leaves': 1910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.4}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:46,714] Trial 437 finished with value: 0.7945256637464989 and parameters: {'iterations': 1093, 'learning_rate': 0.20804104864305062, 'num_leaves': 2140, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:47,453] Trial 438 finished with value: 0.7995594590514953 and parameters: {'iterations': 1015, 'learning_rate': 0.21982092856039417, 'num_leaves': 1540, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:48,433] Trial 439 finished with value: 0.7930279363340061 and parameters: {'iterations': 1054, 'learning_rate': 0.2087823355110415, 'num_leaves': 1670, 'max_depth': 12, 'lambda_l1': 0, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:49,191] Trial 440 finished with value: 0.7990809018790077 and parameters: {'iterations': 1078, 'learning_rate': 0.20025562961998006, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:49,893] Trial 441 finished with value: 0.7954615337310129 and parameters: {'iterations': 1032, 'learning_rate': 0.1843123752340192, 'num_leaves': 1740, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:50,758] Trial 442 finished with value: 0.7977285306170367 and parameters: {'iterations': 1067, 'learning_rate': 0.2144251349305724, 'num_leaves': 1800, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:51,817] Trial 443 finished with value: 0.7983116324054765 and parameters: {'iterations': 990, 'learning_rate': 0.1957508100599714, 'num_leaves': 1990, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:52,746] Trial 444 finished with value: 0.7981450318944939 and parameters: {'iterations': 1094, 'learning_rate': 0.20570239362755752, 'num_leaves': 1590, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:53,800] Trial 445 finished with value: 0.7968759608880143 and parameters: {'iterations': 1051, 'learning_rate': 0.1918337709910602, 'num_leaves': 2050, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:54,876] Trial 446 finished with value: 0.7923827786505357 and parameters: {'iterations': 1024, 'learning_rate': 0.2193245469869037, 'num_leaves': 1860, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:55,464] Trial 447 finished with value: 0.7916135091770046 and parameters: {'iterations': 1105, 'learning_rate': 0.21200164218830334, 'num_leaves': 1940, 'max_depth': 11, 'lambda_l1': 51, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:56,343] Trial 448 finished with value: 0.7958567906480088 and parameters: {'iterations': 1071, 'learning_rate': 0.25506983269813577, 'num_leaves': 1780, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:56,882] Trial 449 finished with value: 0.7897417692079768 and parameters: {'iterations': 999, 'learning_rate': 0.20233279309983557, 'num_leaves': 1660, 'max_depth': 10, 'lambda_l1': 93, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:57,423] Trial 450 finished with value: 0.7939425619580592 and parameters: {'iterations': 1051, 'learning_rate': 0.188803914421792, 'num_leaves': 1910, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 0.2}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:58,034] Trial 451 finished with value: 0.7968139049929838 and parameters: {'iterations': 938, 'learning_rate': 0.2083715898585416, 'num_leaves': 1090, 'max_depth': 11, 'lambda_l1': 36, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:58,769] Trial 452 finished with value: 0.7969592611435057 and parameters: {'iterations': 875, 'learning_rate': 0.19852298881828045, 'num_leaves': 1730, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:02:59,562] Trial 453 finished with value: 0.7936730940174763 and parameters: {'iterations': 1089, 'learning_rate': 0.21589504788154978, 'num_leaves': 1820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:00,251] Trial 454 finished with value: 0.7980617316390025 and parameters: {'iterations': 1036, 'learning_rate': 0.20353765556253292, 'num_leaves': 2000, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:01,094] Trial 455 finished with value: 0.7968139049929838 and parameters: {'iterations': 1070, 'learning_rate': 0.19417462017634216, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:01,846] Trial 456 finished with value: 0.7958567906480088 and parameters: {'iterations': 1109, 'learning_rate': 0.2685973766346619, 'num_leaves': 1750, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 3, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:02,595] Trial 457 finished with value: 0.7961066914144831 and parameters: {'iterations': 1012, 'learning_rate': 0.20917726108566692, 'num_leaves': 1920, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:03,479] Trial 458 finished with value: 0.7912803081550391 and parameters: {'iterations': 1094, 'learning_rate': 0.19959463504366218, 'num_leaves': 1690, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:04,326] Trial 459 finished with value: 0.7993928585405125 and parameters: {'iterations': 1048, 'learning_rate': 0.18401800961692077, 'num_leaves': 2030, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:05,251] Trial 460 finished with value: 0.7938184501679981 and parameters: {'iterations': 1080, 'learning_rate': 0.22054952371233108, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:06,333] Trial 461 finished with value: 0.7948376204080034 and parameters: {'iterations': 1026, 'learning_rate': 0.21435120344823094, 'num_leaves': 1910, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:07,458] Trial 462 finished with value: 0.789699280487055 and parameters: {'iterations': 1144, 'learning_rate': 0.20241507728449265, 'num_leaves': 1960, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 12, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:08,178] Trial 463 finished with value: 0.7950042209189864 and parameters: {'iterations': 410, 'learning_rate': 0.19033808330335802, 'num_leaves': 1780, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:08,921] Trial 464 finished with value: 0.7947543201525121 and parameters: {'iterations': 521, 'learning_rate': 0.20742848579549564, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:09,683] Trial 465 finished with value: 0.7965019483314792 and parameters: {'iterations': 1061, 'learning_rate': 0.1967301325786366, 'num_leaves': 2370, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:10,537] Trial 466 finished with value: 0.7910516517490258 and parameters: {'iterations': 729, 'learning_rate': 0.1731174130099205, 'num_leaves': 1870, 'max_depth': 8, 'lambda_l1': 0, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:11,378] Trial 467 finished with value: 0.7976044188269759 and parameters: {'iterations': 1106, 'learning_rate': 0.21412292895037058, 'num_leaves': 1720, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:12,105] Trial 468 finished with value: 0.7922586668604749 and parameters: {'iterations': 1078, 'learning_rate': 0.12872981357487803, 'num_leaves': 2080, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:13,007] Trial 469 finished with value: 0.7976664747220064 and parameters: {'iterations': 1169, 'learning_rate': 0.2048090362785751, 'num_leaves': 2200, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:13,752] Trial 470 finished with value: 0.7956281342419956 and parameters: {'iterations': 1038, 'learning_rate': 0.22183685931310393, 'num_leaves': 1840, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:14,648] Trial 471 finished with value: 0.7922374225000139 and parameters: {'iterations': 1127, 'learning_rate': 0.2980438227440734, 'num_leaves': 1970, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:15,393] Trial 472 finished with value: 0.7965640042265095 and parameters: {'iterations': 1059, 'learning_rate': 0.19584022118749564, 'num_leaves': 1480, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:15,933] Trial 473 finished with value: 0.7905943389369993 and parameters: {'iterations': 1003, 'learning_rate': 0.17988702213057983, 'num_leaves': 360, 'max_depth': 11, 'lambda_l1': 87, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:16,665] Trial 474 finished with value: 0.7959613352639612 and parameters: {'iterations': 1098, 'learning_rate': 0.21036258512994466, 'num_leaves': 1760, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:17,496] Trial 475 finished with value: 0.7933398929955107 and parameters: {'iterations': 1019, 'learning_rate': 0.2018304507683649, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:18,230] Trial 476 finished with value: 0.7898871253584985 and parameters: {'iterations': 972, 'learning_rate': 0.18702949463088545, 'num_leaves': 440, 'max_depth': 10, 'lambda_l1': 63, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:19,183] Trial 477 finished with value: 0.7837312923838967 and parameters: {'iterations': 1082, 'learning_rate': 0.031537842196479454, 'num_leaves': 1880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:20,300] Trial 478 finished with value: 0.7937351499125067 and parameters: {'iterations': 1046, 'learning_rate': 0.20610350935079472, 'num_leaves': 1810, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:21,169] Trial 479 finished with value: 0.7949829765585253 and parameters: {'iterations': 1064, 'learning_rate': 0.21718379290910683, 'num_leaves': 2030, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:22,104] Trial 480 finished with value: 0.7952116329645387 and parameters: {'iterations': 1114, 'learning_rate': 0.191734982199747, 'num_leaves': 1920, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:22,760] Trial 481 finished with value: 0.7935685494015239 and parameters: {'iterations': 1142, 'learning_rate': 0.19877121917751917, 'num_leaves': 1750, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:23,486] Trial 482 finished with value: 0.7968139049929838 and parameters: {'iterations': 1035, 'learning_rate': 0.21244213571126322, 'num_leaves': 1690, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:24,281] Trial 483 finished with value: 0.7996427593069867 and parameters: {'iterations': 1090, 'learning_rate': 0.22177661071769422, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:24,898] Trial 484 finished with value: 0.7981237875340328 and parameters: {'iterations': 1069, 'learning_rate': 0.16683208521081927, 'num_leaves': 2110, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:25,783] Trial 485 finished with value: 0.7982903880450157 and parameters: {'iterations': 991, 'learning_rate': 0.2054056899437368, 'num_leaves': 1960, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:26,346] Trial 486 finished with value: 0.7945877196415294 and parameters: {'iterations': 1054, 'learning_rate': 0.1944372030439838, 'num_leaves': 1810, 'max_depth': 12, 'lambda_l1': 45, 'lambda_l2': 96, 'feature_fraction': 0.5}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:27,224] Trial 487 finished with value: 0.7931732924845278 and parameters: {'iterations': 1121, 'learning_rate': 0.21171013882277429, 'num_leaves': 1890, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:27,953] Trial 488 finished with value: 0.7977497749774978 and parameters: {'iterations': 1099, 'learning_rate': 0.2026395704313477, 'num_leaves': 2000, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 21, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:28,615] Trial 489 finished with value: 0.7937351499125067 and parameters: {'iterations': 1035, 'learning_rate': 0.18652925939663825, 'num_leaves': 260, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:29,417] Trial 490 finished with value: 0.7989143013680249 and parameters: {'iterations': 1017, 'learning_rate': 0.20789973989622293, 'num_leaves': 2450, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:30,135] Trial 491 finished with value: 0.7966473044820009 and parameters: {'iterations': 1078, 'learning_rate': 0.1984414137424796, 'num_leaves': 2830, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:30,887] Trial 492 finished with value: 0.7955660783469651 and parameters: {'iterations': 1055, 'learning_rate': 0.21826613366823153, 'num_leaves': 1580, 'max_depth': 9, 'lambda_l1': 27, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:32,060] Trial 493 finished with value: 0.7978951311280196 and parameters: {'iterations': 1159, 'learning_rate': 0.19332305027894262, 'num_leaves': 1710, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:33,207] Trial 494 finished with value: 0.7967926606325229 and parameters: {'iterations': 1110, 'learning_rate': 0.24758053307808792, 'num_leaves': 1770, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:34,272] Trial 495 finished with value: 0.794920920663495 and parameters: {'iterations': 1132, 'learning_rate': 0.22506326118482484, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:35,080] Trial 496 finished with value: 0.7998093598179694 and parameters: {'iterations': 1085, 'learning_rate': 0.2095450661628321, 'num_leaves': 1920, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:35,752] Trial 497 finished with value: 0.7982903880450157 and parameters: {'iterations': 1007, 'learning_rate': 0.18009790392066324, 'num_leaves': 1640, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:36,575] Trial 498 finished with value: 0.7944423634910074 and parameters: {'iterations': 1067, 'learning_rate': 0.20230721677673935, 'num_leaves': 1820, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:37,384] Trial 499 finished with value: 0.7964186480759877 and parameters: {'iterations': 1044, 'learning_rate': 0.21458417773403918, 'num_leaves': 2140, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:38,134] Trial 500 finished with value: 0.7939017504234896 and parameters: {'iterations': 1098, 'learning_rate': 0.1079667838503465, 'num_leaves': 2030, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:38,710] Trial 501 finished with value: 0.7913015525155 and parameters: {'iterations': 1027, 'learning_rate': 0.19902991082807228, 'num_leaves': 1950, 'max_depth': 12, 'lambda_l1': 75, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:39,448] Trial 502 finished with value: 0.7993928585405125 and parameters: {'iterations': 1074, 'learning_rate': 0.18963004926351007, 'num_leaves': 1750, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:39,981] Trial 503 finished with value: 0.7967306047374924 and parameters: {'iterations': 494, 'learning_rate': 0.20456792531258391, 'num_leaves': 1880, 'max_depth': 4, 'lambda_l1': 30, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:40,855] Trial 504 finished with value: 0.7924660789060273 and parameters: {'iterations': 1055, 'learning_rate': 0.16082608672459298, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:41,657] Trial 505 finished with value: 0.7910941404699476 and parameters: {'iterations': 1113, 'learning_rate': 0.07404807248509741, 'num_leaves': 1680, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:42,386] Trial 506 finished with value: 0.7945256637464989 and parameters: {'iterations': 1136, 'learning_rate': 0.2095578991755597, 'num_leaves': 1950, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:43,274] Trial 507 finished with value: 0.7938805060630287 and parameters: {'iterations': 1085, 'learning_rate': 0.2155379396253068, 'num_leaves': 2070, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:44,252] Trial 508 finished with value: 0.7990188459839773 and parameters: {'iterations': 1025, 'learning_rate': 0.19687218028473052, 'num_leaves': 1840, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:45,202] Trial 509 finished with value: 0.7953782334755214 and parameters: {'iterations': 1051, 'learning_rate': 0.20657468273716686, 'num_leaves': 1730, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:46,436] Trial 510 finished with value: 0.7975831744665149 and parameters: {'iterations': 1097, 'learning_rate': 0.22212837621997886, 'num_leaves': 880, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:47,263] Trial 511 finished with value: 0.7913848527709915 and parameters: {'iterations': 751, 'learning_rate': 0.18508885024380492, 'num_leaves': 2760, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:47,984] Trial 512 finished with value: 0.7981450318944939 and parameters: {'iterations': 1001, 'learning_rate': 0.19232846822798352, 'num_leaves': 950, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:48,604] Trial 513 finished with value: 0.7936730940174763 and parameters: {'iterations': 1068, 'learning_rate': 0.20070771426065603, 'num_leaves': 1890, 'max_depth': 11, 'lambda_l1': 33, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:49,361] Trial 514 finished with value: 0.7956281342419956 and parameters: {'iterations': 924, 'learning_rate': 0.17673538454458385, 'num_leaves': 2000, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:50,079] Trial 515 finished with value: 0.7945256637464989 and parameters: {'iterations': 1035, 'learning_rate': 0.15051589526517212, 'num_leaves': 1790, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:50,907] Trial 516 finished with value: 0.7937972058075372 and parameters: {'iterations': 1151, 'learning_rate': 0.21296587884574786, 'num_leaves': 1630, 'max_depth': 7, 'lambda_l1': 3, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:51,719] Trial 517 finished with value: 0.7960854470540222 and parameters: {'iterations': 1113, 'learning_rate': 0.20374684079892905, 'num_leaves': 1550, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:52,485] Trial 518 finished with value: 0.7947543201525121 and parameters: {'iterations': 1082, 'learning_rate': 0.19499544055397036, 'num_leaves': 1920, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:53,223] Trial 519 finished with value: 0.7954827780914738 and parameters: {'iterations': 571, 'learning_rate': 0.22765120272029496, 'num_leaves': 1710, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:53,993] Trial 520 finished with value: 0.795316177580491 and parameters: {'iterations': 471, 'learning_rate': 0.21611748411920229, 'num_leaves': 1240, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:54,675] Trial 521 finished with value: 0.7951495770695082 and parameters: {'iterations': 430, 'learning_rate': 0.20853838849977957, 'num_leaves': 450, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:55,504] Trial 522 finished with value: 0.7941304068295029 and parameters: {'iterations': 981, 'learning_rate': 0.1999217515879192, 'num_leaves': 1840, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:56,214] Trial 523 finished with value: 0.8011192423590185 and parameters: {'iterations': 1051, 'learning_rate': 0.2208142002473251, 'num_leaves': 1770, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:56,908] Trial 524 finished with value: 0.7957734903925175 and parameters: {'iterations': 1015, 'learning_rate': 0.23525453033355653, 'num_leaves': 1970, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:57,889] Trial 525 finished with value: 0.7960233911589917 and parameters: {'iterations': 1040, 'learning_rate': 0.22729692523941952, 'num_leaves': 2660, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:58,798] Trial 526 finished with value: 0.7931112365894974 and parameters: {'iterations': 1054, 'learning_rate': 0.1402271279483392, 'num_leaves': 1760, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:03:59,774] Trial 527 finished with value: 0.7962520475650049 and parameters: {'iterations': 1025, 'learning_rate': 0.22073146251599426, 'num_leaves': 1690, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:00,572] Trial 528 finished with value: 0.7956901901370262 and parameters: {'iterations': 834, 'learning_rate': 0.23110453691029145, 'num_leaves': 370, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:01,280] Trial 529 finished with value: 0.7972712178050104 and parameters: {'iterations': 1047, 'learning_rate': 0.2209713841810229, 'num_leaves': 2040, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:01,991] Trial 530 finished with value: 0.7974378183159931 and parameters: {'iterations': 993, 'learning_rate': 0.224735785268897, 'num_leaves': 1880, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:02,721] Trial 531 finished with value: 0.7962732919254659 and parameters: {'iterations': 1064, 'learning_rate': 0.21419273607663508, 'num_leaves': 1410, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:03,395] Trial 532 finished with value: 0.7941304068295029 and parameters: {'iterations': 1035, 'learning_rate': 0.16931731060630886, 'num_leaves': 1830, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:04,166] Trial 533 finished with value: 0.7984357441955376 and parameters: {'iterations': 1006, 'learning_rate': 0.24277816551216622, 'num_leaves': 2240, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:04,798] Trial 534 finished with value: 0.7944423634910074 and parameters: {'iterations': 343, 'learning_rate': 0.21849028945490662, 'num_leaves': 1930, 'max_depth': 8, 'lambda_l1': 18, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:05,547] Trial 535 finished with value: 0.795316177580491 and parameters: {'iterations': 1178, 'learning_rate': 0.21072746372429077, 'num_leaves': 1770, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:06,219] Trial 536 finished with value: 0.7876413728329354 and parameters: {'iterations': 1066, 'learning_rate': 0.043936003241741475, 'num_leaves': 2300, 'max_depth': 6, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:06,964] Trial 537 finished with value: 0.7948376204080034 and parameters: {'iterations': 1024, 'learning_rate': 0.23275676640396034, 'num_leaves': 1630, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 60, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:07,667] Trial 538 finished with value: 0.799704815202017 and parameters: {'iterations': 1049, 'learning_rate': 0.19041698643658417, 'num_leaves': 1870, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:08,376] Trial 539 finished with value: 0.7967926606325229 and parameters: {'iterations': 1074, 'learning_rate': 0.18367734419869408, 'num_leaves': 1720, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:09,093] Trial 540 finished with value: 0.800245428269535 and parameters: {'iterations': 962, 'learning_rate': 0.21849253552809733, 'num_leaves': 1980, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:09,884] Trial 541 finished with value: 0.7944636078514684 and parameters: {'iterations': 946, 'learning_rate': 0.21217439648818656, 'num_leaves': 2130, 'max_depth': 8, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:10,836] Trial 542 finished with value: 0.7978951311280196 and parameters: {'iterations': 981, 'learning_rate': 0.21874303504210094, 'num_leaves': 2070, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:11,870] Trial 543 finished with value: 0.7975831744665149 and parameters: {'iterations': 1018, 'learning_rate': 0.20793076875697364, 'num_leaves': 1950, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.8}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:13,175] Trial 544 finished with value: 0.7964807039710183 and parameters: {'iterations': 878, 'learning_rate': 0.21740726681850553, 'num_leaves': 2010, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:14,179] Trial 545 finished with value: 0.7961066914144831 and parameters: {'iterations': 998, 'learning_rate': 0.20324077350691439, 'num_leaves': 1990, 'max_depth': 7, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:14,946] Trial 546 finished with value: 0.7985402888114899 and parameters: {'iterations': 1038, 'learning_rate': 0.19584326051028692, 'num_leaves': 1910, 'max_depth': 8, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:15,771] Trial 547 finished with value: 0.7989355457284859 and parameters: {'iterations': 958, 'learning_rate': 0.21076743588912195, 'num_leaves': 2060, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:16,492] Trial 548 finished with value: 0.7945256637464989 and parameters: {'iterations': 1058, 'learning_rate': 0.2244250995269395, 'num_leaves': 1170, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:17,215] Trial 549 finished with value: 0.7966473044820009 and parameters: {'iterations': 969, 'learning_rate': 0.28023748879987126, 'num_leaves': 1970, 'max_depth': 7, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:18,087] Trial 550 finished with value: 0.7956281342419956 and parameters: {'iterations': 1007, 'learning_rate': 0.19962270449367825, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:18,667] Trial 551 finished with value: 0.7951495770695082 and parameters: {'iterations': 1126, 'learning_rate': 0.2063302986734372, 'num_leaves': 1810, 'max_depth': 10, 'lambda_l1': 54, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:19,360] Trial 552 finished with value: 0.7983116324054765 and parameters: {'iterations': 1095, 'learning_rate': 0.18775528657543708, 'num_leaves': 1990, 'max_depth': 12, 'lambda_l1': 21, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:20,020] Trial 553 finished with value: 0.7978330752329892 and parameters: {'iterations': 1034, 'learning_rate': 0.21516314821008123, 'num_leaves': 1850, 'max_depth': 11, 'lambda_l1': 24, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:20,755] Trial 554 finished with value: 0.7981237875340328 and parameters: {'iterations': 1075, 'learning_rate': 0.19403003123213525, 'num_leaves': 2090, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:21,493] Trial 555 finished with value: 0.7952949332200301 and parameters: {'iterations': 1046, 'learning_rate': 0.2624738004538573, 'num_leaves': 2160, 'max_depth': 7, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:22,447] Trial 556 finished with value: 0.7940471065740113 and parameters: {'iterations': 1020, 'learning_rate': 0.20451421277963713, 'num_leaves': 1780, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:23,142] Trial 557 finished with value: 0.7970425613989971 and parameters: {'iterations': 1101, 'learning_rate': 0.21065945326244093, 'num_leaves': 1930, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:24,111] Trial 558 finished with value: 0.7969805055039666 and parameters: {'iterations': 1061, 'learning_rate': 0.1547938244643965, 'num_leaves': 1720, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:25,275] Trial 559 finished with value: 0.7952541216854605 and parameters: {'iterations': 901, 'learning_rate': 0.0868073070458518, 'num_leaves': 1830, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:26,134] Trial 560 finished with value: 0.7938396945284591 and parameters: {'iterations': 1136, 'learning_rate': 0.2192675120509419, 'num_leaves': 1660, 'max_depth': 11, 'lambda_l1': 42, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:27,155] Trial 561 finished with value: 0.7972091619099798 and parameters: {'iterations': 1080, 'learning_rate': 0.17779008646552988, 'num_leaves': 1910, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:27,694] Trial 562 finished with value: 0.7896372245920245 and parameters: {'iterations': 989, 'learning_rate': 0.19570484786929443, 'num_leaves': 1600, 'max_depth': 3, 'lambda_l1': 12, 'lambda_l2': 18, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:28,382] Trial 563 finished with value: 0.7935897937619849 and parameters: {'iterations': 1156, 'learning_rate': 0.2019380875050491, 'num_leaves': 480, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 45, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:29,252] Trial 564 finished with value: 0.7939017504234896 and parameters: {'iterations': 1111, 'learning_rate': 0.21456923657160248, 'num_leaves': 1750, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:30,048] Trial 565 finished with value: 0.7986023447065203 and parameters: {'iterations': 1044, 'learning_rate': 0.20647105308938216, 'num_leaves': 2030, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:30,873] Trial 566 finished with value: 0.7991021462394686 and parameters: {'iterations': 1014, 'learning_rate': 0.1883613438952369, 'num_leaves': 1870, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:31,618] Trial 567 finished with value: 0.7950042209189864 and parameters: {'iterations': 1068, 'learning_rate': 0.2237512071335436, 'num_leaves': 1080, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:32,498] Trial 568 finished with value: 0.7969592611435057 and parameters: {'iterations': 1083, 'learning_rate': 0.19987843496670754, 'num_leaves': 1960, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:33,222] Trial 569 finished with value: 0.7954615337310129 and parameters: {'iterations': 1030, 'learning_rate': 0.21115696018270846, 'num_leaves': 1690, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:33,982] Trial 570 finished with value: 0.7957947347529785 and parameters: {'iterations': 1126, 'learning_rate': 0.18264009746217202, 'num_leaves': 1830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:34,917] Trial 571 finished with value: 0.7941924627245333 and parameters: {'iterations': 1056, 'learning_rate': 0.19853001221907346, 'num_leaves': 1490, 'max_depth': 8, 'lambda_l1': 3, 'lambda_l2': 9, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:35,689] Trial 572 finished with value: 0.7942970073404857 and parameters: {'iterations': 1103, 'learning_rate': 0.2054839071083043, 'num_leaves': 1760, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 42, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:36,354] Trial 573 finished with value: 0.7995382146910344 and parameters: {'iterations': 383, 'learning_rate': 0.19237021158946813, 'num_leaves': 2020, 'max_depth': 5, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:37,569] Trial 574 finished with value: 0.7946922642574819 and parameters: {'iterations': 1091, 'learning_rate': 0.21904043839017318, 'num_leaves': 1900, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:38,352] Trial 575 finished with value: 0.7879108407735184 and parameters: {'iterations': 1045, 'learning_rate': 0.21145628757374046, 'num_leaves': 1790, 'max_depth': 11, 'lambda_l1': 69, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:39,318] Trial 576 finished with value: 0.7960233911589917 and parameters: {'iterations': 321, 'learning_rate': 0.20717808210050065, 'num_leaves': 1950, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 0.7}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:40,176] Trial 577 finished with value: 0.7973545180605016 and parameters: {'iterations': 1016, 'learning_rate': 0.172733019383228, 'num_leaves': 2090, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 0.4}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:40,830] Trial 578 finished with value: 0.7961899916699745 and parameters: {'iterations': 1068, 'learning_rate': 0.2004706821967699, 'num_leaves': 410, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:41,801] Trial 579 finished with value: 0.7968759608880143 and parameters: {'iterations': 1119, 'learning_rate': 0.22708250349434395, 'num_leaves': 1830, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:42,633] Trial 580 finished with value: 0.7958567906480088 and parameters: {'iterations': 1143, 'learning_rate': 0.2173774927359078, 'num_leaves': 1550, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:43,386] Trial 581 finished with value: 0.7978951311280196 and parameters: {'iterations': 1040, 'learning_rate': 0.19047844552044282, 'num_leaves': 1880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:44,016] Trial 582 finished with value: 0.7946710198970207 and parameters: {'iterations': 992, 'learning_rate': 0.19777377719784875, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 0.30000000000000004}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:44,777] Trial 583 finished with value: 0.7968972052484752 and parameters: {'iterations': 1089, 'learning_rate': 0.21441902180974415, 'num_leaves': 1990, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:45,590] Trial 584 finished with value: 0.7954615337310129 and parameters: {'iterations': 970, 'learning_rate': 0.20581264268473984, 'num_leaves': 1720, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:46,313] Trial 585 finished with value: 0.7966473044820009 and parameters: {'iterations': 1063, 'learning_rate': 0.22396516043721926, 'num_leaves': 1790, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:47,240] Trial 586 finished with value: 0.795316177580491 and parameters: {'iterations': 1027, 'learning_rate': 0.20965438879651885, 'num_leaves': 1930, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:47,938] Trial 587 finished with value: 0.7943590632355161 and parameters: {'iterations': 1200, 'learning_rate': 0.19451736948470738, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:48,914] Trial 588 finished with value: 0.7884726982014971 and parameters: {'iterations': 1077, 'learning_rate': 0.18535011169615503, 'num_leaves': 2010, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:49,666] Trial 589 finished with value: 0.7981450318944939 and parameters: {'iterations': 1106, 'learning_rate': 0.20358608701706635, 'num_leaves': 1770, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:50,713] Trial 590 finished with value: 0.7972712178050104 and parameters: {'iterations': 1166, 'learning_rate': 0.2138407316619274, 'num_leaves': 1690, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:51,493] Trial 591 finished with value: 0.793444437611463 and parameters: {'iterations': 1005, 'learning_rate': 0.22086179375241027, 'num_leaves': 1820, 'max_depth': 10, 'lambda_l1': 57, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:52,683] Trial 592 finished with value: 0.7979996757439719 and parameters: {'iterations': 1053, 'learning_rate': 0.16085593584634975, 'num_leaves': 1910, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:54,141] Trial 593 finished with value: 0.7936518496570153 and parameters: {'iterations': 1032, 'learning_rate': 0.19926996610454872, 'num_leaves': 310, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:55,104] Trial 594 finished with value: 0.7927992799279928 and parameters: {'iterations': 1091, 'learning_rate': 0.20878659352812196, 'num_leaves': 1950, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:56,131] Trial 595 finished with value: 0.7929658804389756 and parameters: {'iterations': 1123, 'learning_rate': 0.20279946118086098, 'num_leaves': 2070, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:56,925] Trial 596 finished with value: 0.7953782334755214 and parameters: {'iterations': 1056, 'learning_rate': 0.19144258315803744, 'num_leaves': 1740, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:57,657] Trial 597 finished with value: 0.7978951311280196 and parameters: {'iterations': 1078, 'learning_rate': 0.18073421800193296, 'num_leaves': 2150, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 0.8}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:58,256] Trial 598 finished with value: 0.7930899922290366 and parameters: {'iterations': 1013, 'learning_rate': 0.21547476551551978, 'num_leaves': 810, 'max_depth': 4, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:59,053] Trial 599 finished with value: 0.8004120287805178 and parameters: {'iterations': 1144, 'learning_rate': 0.2311329877317847, 'num_leaves': 1610, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:04:59,903] Trial 600 finished with value: 0.7951495770695082 and parameters: {'iterations': 1149, 'learning_rate': 0.23038538820674323, 'num_leaves': 1560, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:00,834] Trial 601 finished with value: 0.7969592611435057 and parameters: {'iterations': 1168, 'learning_rate': 0.23325151636102684, 'num_leaves': 1670, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:01,666] Trial 602 finished with value: 0.7972091619099798 and parameters: {'iterations': 1041, 'learning_rate': 0.2404524830941757, 'num_leaves': 1590, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:02,617] Trial 603 finished with value: 0.7982903880450157 and parameters: {'iterations': 1068, 'learning_rate': 0.22899830636965432, 'num_leaves': 1500, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:03,640] Trial 604 finished with value: 0.7993095582850209 and parameters: {'iterations': 986, 'learning_rate': 0.22391865466143523, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:04,842] Trial 605 finished with value: 0.7973332737000408 and parameters: {'iterations': 1027, 'learning_rate': 0.23578939656319484, 'num_leaves': 1730, 'max_depth': 6, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:05,834] Trial 606 finished with value: 0.7945256637464989 and parameters: {'iterations': 1100, 'learning_rate': 0.2505883306833789, 'num_leaves': 1630, 'max_depth': 9, 'lambda_l1': 48, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:07,609] Trial 607 finished with value: 0.7961687473095135 and parameters: {'iterations': 1054, 'learning_rate': 0.221235889503151, 'num_leaves': 1320, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:08,995] Trial 608 finished with value: 0.7956901901370262 and parameters: {'iterations': 816, 'learning_rate': 0.22458180399413036, 'num_leaves': 1640, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:09,744] Trial 609 finished with value: 0.7896584689524854 and parameters: {'iterations': 1080, 'learning_rate': 0.2268621038463226, 'num_leaves': 1540, 'max_depth': 12, 'lambda_l1': 81, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:10,682] Trial 610 finished with value: 0.7963353478204963 and parameters: {'iterations': 1006, 'learning_rate': 0.13134144974383632, 'num_leaves': 1600, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:11,853] Trial 611 finished with value: 0.7987689452175031 and parameters: {'iterations': 1040, 'learning_rate': 0.21784568589532488, 'num_leaves': 1750, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:13,068] Trial 612 finished with value: 0.7977497749774978 and parameters: {'iterations': 1114, 'learning_rate': 0.2310480149592414, 'num_leaves': 1430, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:13,922] Trial 613 finished with value: 0.7946922642574819 and parameters: {'iterations': 1184, 'learning_rate': 0.27497617388083206, 'num_leaves': 1670, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:14,629] Trial 614 finished with value: 0.7982283321499852 and parameters: {'iterations': 1064, 'learning_rate': 0.1878037165954332, 'num_leaves': 540, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:15,509] Trial 615 finished with value: 0.8008072856975138 and parameters: {'iterations': 1092, 'learning_rate': 0.1955542480287931, 'num_leaves': 1710, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:16,379] Trial 616 finished with value: 0.7965640042265095 and parameters: {'iterations': 1137, 'learning_rate': 0.1978795900360194, 'num_leaves': 1590, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:17,310] Trial 617 finished with value: 0.7942757629800247 and parameters: {'iterations': 1099, 'learning_rate': 0.23919986651068936, 'num_leaves': 1690, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:18,204] Trial 618 finished with value: 0.7964807039710183 and parameters: {'iterations': 1120, 'learning_rate': 0.2118610952940924, 'num_leaves': 1680, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:19,077] Trial 619 finished with value: 0.7994761587960039 and parameters: {'iterations': 1026, 'learning_rate': 0.2024582041429685, 'num_leaves': 1740, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:20,279] Trial 620 finished with value: 0.7968759608880143 and parameters: {'iterations': 1154, 'learning_rate': 0.22012104715064507, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:21,547] Trial 621 finished with value: 0.7967926606325229 and parameters: {'iterations': 1054, 'learning_rate': 0.20808687058231679, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:22,579] Trial 622 finished with value: 0.7920920663494921 and parameters: {'iterations': 1091, 'learning_rate': 0.12266996524313123, 'num_leaves': 1700, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:23,449] Trial 623 finished with value: 0.7944423634910074 and parameters: {'iterations': 1005, 'learning_rate': 0.195474370302327, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:24,158] Trial 624 finished with value: 0.7913636084105304 and parameters: {'iterations': 1073, 'learning_rate': 0.2143523616470857, 'num_leaves': 1690, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 0.5}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:25,040] Trial 625 finished with value: 0.7956901901370262 and parameters: {'iterations': 1044, 'learning_rate': 0.28821136980939577, 'num_leaves': 1810, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:25,747] Trial 626 finished with value: 0.7958780350084699 and parameters: {'iterations': 1108, 'learning_rate': 0.20199290989585605, 'num_leaves': 1740, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:26,559] Trial 627 finished with value: 0.7965640042265095 and parameters: {'iterations': 1134, 'learning_rate': 0.20645528648764638, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:27,319] Trial 628 finished with value: 0.7965852485869706 and parameters: {'iterations': 966, 'learning_rate': 0.2578309330983028, 'num_leaves': 1640, 'max_depth': 7, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:28,011] Trial 629 finished with value: 0.7995594590514953 and parameters: {'iterations': 1063, 'learning_rate': 0.2274208635108475, 'num_leaves': 1840, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:28,805] Trial 630 finished with value: 0.7995594590514953 and parameters: {'iterations': 1022, 'learning_rate': 0.19390638939563717, 'num_leaves': 1770, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:29,695] Trial 631 finished with value: 0.7950662768140169 and parameters: {'iterations': 1091, 'learning_rate': 0.21610078282207912, 'num_leaves': 1700, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:30,474] Trial 632 finished with value: 0.7963974037155269 and parameters: {'iterations': 989, 'learning_rate': 0.20965289405882204, 'num_leaves': 1850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 15, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:31,234] Trial 633 finished with value: 0.7980829759994633 and parameters: {'iterations': 794, 'learning_rate': 0.2208484308681902, 'num_leaves': 1780, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:32,020] Trial 634 finished with value: 0.7975831744665149 and parameters: {'iterations': 1040, 'learning_rate': 0.19916277649881547, 'num_leaves': 1620, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:33,078] Trial 635 finished with value: 0.7922374225000139 and parameters: {'iterations': 1073, 'learning_rate': 0.2128836694660034, 'num_leaves': 1730, 'max_depth': 8, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:33,877] Trial 636 finished with value: 0.7975211185714844 and parameters: {'iterations': 1117, 'learning_rate': 0.24545660116851703, 'num_leaves': 430, 'max_depth': 9, 'lambda_l1': 33, 'lambda_l2': 24, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:34,888] Trial 637 finished with value: 0.7948376204080034 and parameters: {'iterations': 1050, 'learning_rate': 0.2050789720301458, 'num_leaves': 1500, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:35,640] Trial 638 finished with value: 0.7948588647684645 and parameters: {'iterations': 1016, 'learning_rate': 0.23447809790461652, 'num_leaves': 490, 'max_depth': 12, 'lambda_l1': 39, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:36,446] Trial 639 finished with value: 0.7936518496570153 and parameters: {'iterations': 1099, 'learning_rate': 0.1923478525572143, 'num_leaves': 60, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:37,102] Trial 640 finished with value: 0.7963974037155269 and parameters: {'iterations': 1080, 'learning_rate': 0.19977818605376657, 'num_leaves': 1870, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:37,818] Trial 641 finished with value: 0.7944423634910074 and parameters: {'iterations': 1028, 'learning_rate': 0.22183293755501726, 'num_leaves': 1850, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:38,665] Trial 642 finished with value: 0.79396380631852 and parameters: {'iterations': 1061, 'learning_rate': 0.21295956547329306, 'num_leaves': 1790, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:39,486] Trial 643 finished with value: 0.7950042209189864 and parameters: {'iterations': 1145, 'learning_rate': 0.2052411024872051, 'num_leaves': 1710, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:40,303] Trial 644 finished with value: 0.7992475023899906 and parameters: {'iterations': 1001, 'learning_rate': 0.1461649959269819, 'num_leaves': 230, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:41,060] Trial 645 finished with value: 0.7947543201525121 and parameters: {'iterations': 917, 'learning_rate': 0.1944160587273823, 'num_leaves': 600, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:41,823] Trial 646 finished with value: 0.7971258616544884 and parameters: {'iterations': 945, 'learning_rate': 0.20903465621974093, 'num_leaves': 1660, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:42,549] Trial 647 finished with value: 0.7906155832974602 and parameters: {'iterations': 1125, 'learning_rate': 0.11561702036268726, 'num_leaves': 1810, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:43,263] Trial 648 finished with value: 0.7946710198970207 and parameters: {'iterations': 1040, 'learning_rate': 0.21759720372813865, 'num_leaves': 1900, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:44,142] Trial 649 finished with value: 0.7941924627245333 and parameters: {'iterations': 1088, 'learning_rate': 0.2653071385927149, 'num_leaves': 1750, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 0.8}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:44,959] Trial 650 finished with value: 0.7937972058075372 and parameters: {'iterations': 1063, 'learning_rate': 0.29935262101080606, 'num_leaves': 1570, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:46,082] Trial 651 finished with value: 0.7922994783950444 and parameters: {'iterations': 1104, 'learning_rate': 0.22897715666904153, 'num_leaves': 2400, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:47,053] Trial 652 finished with value: 0.7953994778359825 and parameters: {'iterations': 1165, 'learning_rate': 0.20115370440705288, 'num_leaves': 350, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:48,129] Trial 653 finished with value: 0.7959613352639612 and parameters: {'iterations': 1025, 'learning_rate': 0.19033897553287865, 'num_leaves': 1830, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:49,086] Trial 654 finished with value: 0.7969805055039666 and parameters: {'iterations': 1077, 'learning_rate': 0.20940117313037535, 'num_leaves': 1900, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:50,017] Trial 655 finished with value: 0.7934852491460327 and parameters: {'iterations': 1052, 'learning_rate': 0.19929556848245053, 'num_leaves': 1650, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:50,733] Trial 656 finished with value: 0.7942970073404857 and parameters: {'iterations': 983, 'learning_rate': 0.13921874504984558, 'num_leaves': 1740, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 60, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:51,695] Trial 657 finished with value: 0.7905110386815078 and parameters: {'iterations': 861, 'learning_rate': 0.2220286045291734, 'num_leaves': 1970, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 12, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:52,581] Trial 658 finished with value: 0.7896159802315635 and parameters: {'iterations': 1110, 'learning_rate': 0.21499463799789173, 'num_leaves': 960, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 6, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:53,322] Trial 659 finished with value: 0.7971879175495189 and parameters: {'iterations': 1037, 'learning_rate': 0.2059151913721759, 'num_leaves': 1800, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:54,088] Trial 660 finished with value: 0.7989143013680249 and parameters: {'iterations': 1141, 'learning_rate': 0.19512505016151313, 'num_leaves': 1880, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:54,858] Trial 661 finished with value: 0.8004953290360091 and parameters: {'iterations': 1010, 'learning_rate': 0.20328096358191627, 'num_leaves': 1710, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:55,633] Trial 662 finished with value: 0.7989355457284859 and parameters: {'iterations': 978, 'learning_rate': 0.21214078060540167, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:56,309] Trial 663 finished with value: 0.79396380631852 and parameters: {'iterations': 958, 'learning_rate': 0.22749692809466954, 'num_leaves': 1700, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 0.5}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:57,210] Trial 664 finished with value: 0.7920708219890312 and parameters: {'iterations': 999, 'learning_rate': 0.18643301498737022, 'num_leaves': 1670, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:58,025] Trial 665 finished with value: 0.800266672629996 and parameters: {'iterations': 1008, 'learning_rate': 0.217497500202368, 'num_leaves': 1540, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:05:59,034] Trial 666 finished with value: 0.7938805060630287 and parameters: {'iterations': 987, 'learning_rate': 0.2336404408652303, 'num_leaves': 1620, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:00,217] Trial 667 finished with value: 0.7951495770695082 and parameters: {'iterations': 1031, 'learning_rate': 0.21957618632956505, 'num_leaves': 1370, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 63, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:01,521] Trial 668 finished with value: 0.794899676303034 and parameters: {'iterations': 1012, 'learning_rate': 0.22538425131209225, 'num_leaves': 1520, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:02,489] Trial 669 finished with value: 0.800100072119013 and parameters: {'iterations': 1007, 'learning_rate': 0.2156981244597468, 'num_leaves': 1430, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:03,481] Trial 670 finished with value: 0.791030407388565 and parameters: {'iterations': 1011, 'learning_rate': 0.2213580142897139, 'num_leaves': 1460, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 30, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:04,426] Trial 671 finished with value: 0.7906563948320298 and parameters: {'iterations': 990, 'learning_rate': 0.2386480036464935, 'num_leaves': 1630, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:05,265] Trial 672 finished with value: 0.7990809018790077 and parameters: {'iterations': 995, 'learning_rate': 0.2296178615190876, 'num_leaves': 1550, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:05,875] Trial 673 finished with value: 0.7985402888114899 and parameters: {'iterations': 964, 'learning_rate': 0.21743654061813536, 'num_leaves': 1530, 'max_depth': 10, 'lambda_l1': 30, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:06,662] Trial 674 finished with value: 0.8009526418480357 and parameters: {'iterations': 759, 'learning_rate': 0.22564984153212608, 'num_leaves': 1510, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:07,371] Trial 675 finished with value: 0.795440289370552 and parameters: {'iterations': 972, 'learning_rate': 0.23238703405259528, 'num_leaves': 1490, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:08,037] Trial 676 finished with value: 0.793985050678981 and parameters: {'iterations': 1004, 'learning_rate': 0.22460116508423403, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:08,753] Trial 677 finished with value: 0.7990188459839773 and parameters: {'iterations': 695, 'learning_rate': 0.21051395176898424, 'num_leaves': 1450, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 274 with value: 0.8019097561930105.\n",
            "[I 2024-05-15 22:06:09,536] Trial 678 finished with value: 0.8032621274549816 and parameters: {'iterations': 718, 'learning_rate': 0.24460090178023103, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:10,282] Trial 679 finished with value: 0.7928613358230233 and parameters: {'iterations': 738, 'learning_rate': 0.22851542506267958, 'num_leaves': 1550, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:11,067] Trial 680 finished with value: 0.7965640042265095 and parameters: {'iterations': 725, 'learning_rate': 0.24495776137439196, 'num_leaves': 1580, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:11,845] Trial 681 finished with value: 0.7989143013680249 and parameters: {'iterations': 1021, 'learning_rate': 0.23762471332730062, 'num_leaves': 1540, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:13,022] Trial 682 finished with value: 0.7964186480759877 and parameters: {'iterations': 678, 'learning_rate': 0.23521952775831287, 'num_leaves': 1590, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:14,133] Trial 683 finished with value: 0.799726059562478 and parameters: {'iterations': 949, 'learning_rate': 0.23855532706412497, 'num_leaves': 1540, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:15,335] Trial 684 finished with value: 0.7973332737000408 and parameters: {'iterations': 762, 'learning_rate': 0.25333960513730513, 'num_leaves': 1350, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:16,153] Trial 685 finished with value: 0.796230803204544 and parameters: {'iterations': 981, 'learning_rate': 0.24305054336930984, 'num_leaves': 1490, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:16,745] Trial 686 finished with value: 0.7923207227555054 and parameters: {'iterations': 712, 'learning_rate': 0.2859829733727392, 'num_leaves': 1410, 'max_depth': 10, 'lambda_l1': 60, 'lambda_l2': 72, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:17,571] Trial 687 finished with value: 0.7913848527709915 and parameters: {'iterations': 678, 'learning_rate': 0.2485437788730268, 'num_leaves': 2480, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:18,324] Trial 688 finished with value: 0.7994141029009734 and parameters: {'iterations': 887, 'learning_rate': 0.23081853071207845, 'num_leaves': 1200, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:19,175] Trial 689 finished with value: 0.7993928585405125 and parameters: {'iterations': 1015, 'learning_rate': 0.22798796515046194, 'num_leaves': 1660, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:20,000] Trial 690 finished with value: 0.7977497749774978 and parameters: {'iterations': 709, 'learning_rate': 0.22545583512955564, 'num_leaves': 1280, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:20,551] Trial 691 finished with value: 0.7897205248475159 and parameters: {'iterations': 999, 'learning_rate': 0.23995716639663534, 'num_leaves': 1490, 'max_depth': 10, 'lambda_l1': 99, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:21,292] Trial 692 finished with value: 0.7965852485869706 and parameters: {'iterations': 1034, 'learning_rate': 0.22148926403769453, 'num_leaves': 1580, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:22,205] Trial 693 finished with value: 0.7916755650720352 and parameters: {'iterations': 732, 'learning_rate': 0.2785368398828653, 'num_leaves': 1680, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:22,937] Trial 694 finished with value: 0.7971258616544884 and parameters: {'iterations': 708, 'learning_rate': 0.1976807529846258, 'num_leaves': 1060, 'max_depth': 7, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:23,708] Trial 695 finished with value: 0.7978330752329892 and parameters: {'iterations': 702, 'learning_rate': 0.1876072374481219, 'num_leaves': 1600, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:24,425] Trial 696 finished with value: 0.7971258616544884 and parameters: {'iterations': 749, 'learning_rate': 0.2036215697235965, 'num_leaves': 1470, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 39, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:25,098] Trial 697 finished with value: 0.7911137076440564 and parameters: {'iterations': 716, 'learning_rate': 0.23480259689169258, 'num_leaves': 1150, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 0.30000000000000004}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:26,066] Trial 698 finished with value: 0.791446908666022 and parameters: {'iterations': 771, 'learning_rate': 0.1925201762659892, 'num_leaves': 2570, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:27,215] Trial 699 finished with value: 0.7882227974350229 and parameters: {'iterations': 1017, 'learning_rate': 0.21718515375353323, 'num_leaves': 1710, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 3, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:28,478] Trial 700 finished with value: 0.7958567906480088 and parameters: {'iterations': 925, 'learning_rate': 0.19864877746770643, 'num_leaves': 1640, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:29,029] Trial 701 finished with value: 0.7893873238255503 and parameters: {'iterations': 1053, 'learning_rate': 0.29117012841304396, 'num_leaves': 1720, 'max_depth': 3, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:29,792] Trial 702 finished with value: 0.7980617316390025 and parameters: {'iterations': 974, 'learning_rate': 0.20731952188069652, 'num_leaves': 1520, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:30,433] Trial 703 finished with value: 0.7984569885559986 and parameters: {'iterations': 846, 'learning_rate': 0.22126481458877342, 'num_leaves': 1650, 'max_depth': 9, 'lambda_l1': 27, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:31,313] Trial 704 finished with value: 0.7919042214780485 and parameters: {'iterations': 747, 'learning_rate': 0.27296935243626325, 'num_leaves': 1720, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:31,991] Trial 705 finished with value: 0.7954827780914738 and parameters: {'iterations': 1026, 'learning_rate': 0.22527678578366708, 'num_leaves': 1590, 'max_depth': 8, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:32,744] Trial 706 finished with value: 0.7972091619099798 and parameters: {'iterations': 1073, 'learning_rate': 0.20302978280744952, 'num_leaves': 1750, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:33,479] Trial 707 finished with value: 0.7976452303615456 and parameters: {'iterations': 662, 'learning_rate': 0.23116749691248126, 'num_leaves': 1630, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:34,084] Trial 708 finished with value: 0.7947543201525121 and parameters: {'iterations': 1000, 'learning_rate': 0.17711243572090277, 'num_leaves': 850, 'max_depth': 4, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:34,932] Trial 709 finished with value: 0.7936518496570153 and parameters: {'iterations': 1091, 'learning_rate': 0.21103795194632613, 'num_leaves': 1700, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:35,759] Trial 710 finished with value: 0.7987689452175031 and parameters: {'iterations': 1046, 'learning_rate': 0.1895602536793697, 'num_leaves': 1790, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:36,548] Trial 711 finished with value: 0.7914060971314523 and parameters: {'iterations': 1063, 'learning_rate': 0.062127076878332924, 'num_leaves': 1460, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:37,288] Trial 712 finished with value: 0.7960233911589917 and parameters: {'iterations': 1023, 'learning_rate': 0.1950483420619754, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:37,952] Trial 713 finished with value: 0.7966473044820009 and parameters: {'iterations': 996, 'learning_rate': 0.21521740883375287, 'num_leaves': 1670, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:38,783] Trial 714 finished with value: 0.7969592611435057 and parameters: {'iterations': 1082, 'learning_rate': 0.25755087088740986, 'num_leaves': 1750, 'max_depth': 6, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:39,482] Trial 715 finished with value: 0.7924448345455665 and parameters: {'iterations': 1178, 'learning_rate': 0.20592928853326817, 'num_leaves': 1370, 'max_depth': 8, 'lambda_l1': 21, 'lambda_l2': 99, 'feature_fraction': 0.2}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:40,846] Trial 716 finished with value: 0.788618054352019 and parameters: {'iterations': 1039, 'learning_rate': 0.1974435771476774, 'num_leaves': 1840, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:41,873] Trial 717 finished with value: 0.8004953290360091 and parameters: {'iterations': 1102, 'learning_rate': 0.18483180515610342, 'num_leaves': 2260, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:42,777] Trial 718 finished with value: 0.7957734903925175 and parameters: {'iterations': 1125, 'learning_rate': 0.1792876276573542, 'num_leaves': 2340, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:43,640] Trial 719 finished with value: 0.7976664747220064 and parameters: {'iterations': 1100, 'learning_rate': 0.16887508811482183, 'num_leaves': 2540, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:44,469] Trial 720 finished with value: 0.79396380631852 and parameters: {'iterations': 1158, 'learning_rate': 0.18212864486802205, 'num_leaves': 2410, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:45,303] Trial 721 finished with value: 0.7938184501679981 and parameters: {'iterations': 1112, 'learning_rate': 0.18375086153256592, 'num_leaves': 1680, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:46,214] Trial 722 finished with value: 0.7943590632355161 and parameters: {'iterations': 1135, 'learning_rate': 0.1875189150711873, 'num_leaves': 2230, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:47,105] Trial 723 finished with value: 0.79396380631852 and parameters: {'iterations': 1120, 'learning_rate': 0.18601174193203898, 'num_leaves': 1600, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:47,935] Trial 724 finished with value: 0.7991642021344993 and parameters: {'iterations': 1098, 'learning_rate': 0.17420674748177706, 'num_leaves': 2990, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:48,767] Trial 725 finished with value: 0.7969592611435057 and parameters: {'iterations': 1011, 'learning_rate': 0.18903530842907448, 'num_leaves': 2300, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:49,514] Trial 726 finished with value: 0.7905110386815078 and parameters: {'iterations': 724, 'learning_rate': 0.1654132313022687, 'num_leaves': 1750, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:50,343] Trial 727 finished with value: 0.7972712178050104 and parameters: {'iterations': 1141, 'learning_rate': 0.19576064138727356, 'num_leaves': 1510, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:51,246] Trial 728 finished with value: 0.8004120287805178 and parameters: {'iterations': 1088, 'learning_rate': 0.19143908969387896, 'num_leaves': 2240, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:52,426] Trial 729 finished with value: 0.7916135091770046 and parameters: {'iterations': 1092, 'learning_rate': 0.1854064474245681, 'num_leaves': 2210, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:53,700] Trial 730 finished with value: 0.7983736883005071 and parameters: {'iterations': 1110, 'learning_rate': 0.17418911081686095, 'num_leaves': 2350, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:54,821] Trial 731 finished with value: 0.7927159796725015 and parameters: {'iterations': 1088, 'learning_rate': 0.1554123279412998, 'num_leaves': 2170, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:55,745] Trial 732 finished with value: 0.7943590632355161 and parameters: {'iterations': 1119, 'learning_rate': 0.23136178093338597, 'num_leaves': 2220, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:56,640] Trial 733 finished with value: 0.7965640042265095 and parameters: {'iterations': 1078, 'learning_rate': 0.1938954038869138, 'num_leaves': 2490, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:57,553] Trial 734 finished with value: 0.7931112365894974 and parameters: {'iterations': 1102, 'learning_rate': 0.2005705496900299, 'num_leaves': 2100, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:58,305] Trial 735 finished with value: 0.788515186922419 and parameters: {'iterations': 1071, 'learning_rate': 0.04458889433794684, 'num_leaves': 2030, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:06:59,196] Trial 736 finished with value: 0.7992262580295296 and parameters: {'iterations': 1135, 'learning_rate': 0.22342773598996316, 'num_leaves': 2260, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:00,101] Trial 737 finished with value: 0.7883060976905144 and parameters: {'iterations': 1092, 'learning_rate': 0.24397981528427756, 'num_leaves': 2530, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:00,901] Trial 738 finished with value: 0.7930279363340061 and parameters: {'iterations': 1110, 'learning_rate': 0.18183852152744678, 'num_leaves': 1940, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:01,749] Trial 739 finished with value: 0.7990809018790077 and parameters: {'iterations': 785, 'learning_rate': 0.19081664941984258, 'num_leaves': 2440, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:02,546] Trial 740 finished with value: 0.80120254261451 and parameters: {'iterations': 1064, 'learning_rate': 0.2167817158098192, 'num_leaves': 2660, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:03,391] Trial 741 finished with value: 0.7990809018790077 and parameters: {'iterations': 1079, 'learning_rate': 0.21914799697746437, 'num_leaves': 2680, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:04,298] Trial 742 finished with value: 0.7974998742110235 and parameters: {'iterations': 1070, 'learning_rate': 0.22453231339978738, 'num_leaves': 2790, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:05,445] Trial 743 finished with value: 0.7986023447065203 and parameters: {'iterations': 1091, 'learning_rate': 0.217677108848822, 'num_leaves': 2630, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:06,337] Trial 744 finished with value: 0.7937563942729676 and parameters: {'iterations': 1149, 'learning_rate': 0.23384346893459315, 'num_leaves': 2450, 'max_depth': 10, 'lambda_l1': 36, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:07,672] Trial 745 finished with value: 0.797978431383511 and parameters: {'iterations': 1127, 'learning_rate': 0.22687579992143164, 'num_leaves': 2720, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:08,532] Trial 746 finished with value: 0.7943590632355161 and parameters: {'iterations': 1095, 'learning_rate': 0.21332055200465264, 'num_leaves': 2320, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:09,302] Trial 747 finished with value: 0.8011192423590185 and parameters: {'iterations': 1060, 'learning_rate': 0.2233285035092775, 'num_leaves': 2840, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:10,124] Trial 748 finished with value: 0.7922586668604749 and parameters: {'iterations': 1076, 'learning_rate': 0.22691120377982096, 'num_leaves': 2630, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:10,923] Trial 749 finished with value: 0.799704815202017 and parameters: {'iterations': 1059, 'learning_rate': 0.23859769021438665, 'num_leaves': 2310, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:11,839] Trial 750 finished with value: 0.7986023447065203 and parameters: {'iterations': 1162, 'learning_rate': 0.23143180379439177, 'num_leaves': 2830, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:12,636] Trial 751 finished with value: 0.7989143013680249 and parameters: {'iterations': 1106, 'learning_rate': 0.22389152888178931, 'num_leaves': 2830, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:13,572] Trial 752 finished with value: 0.7987477008570422 and parameters: {'iterations': 1057, 'learning_rate': 0.23134872008064258, 'num_leaves': 2360, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:14,314] Trial 753 finished with value: 0.7963974037155269 and parameters: {'iterations': 1116, 'learning_rate': 0.22154243197754503, 'num_leaves': 2900, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:15,148] Trial 754 finished with value: 0.7961687473095135 and parameters: {'iterations': 1083, 'learning_rate': 0.21785598885592292, 'num_leaves': 2740, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:15,731] Trial 755 finished with value: 0.7925493791615187 and parameters: {'iterations': 1065, 'learning_rate': 0.22634637568637364, 'num_leaves': 2870, 'max_depth': 9, 'lambda_l1': 48, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:16,545] Trial 756 finished with value: 0.7958567906480088 and parameters: {'iterations': 1098, 'learning_rate': 0.2498839772382226, 'num_leaves': 2950, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:17,397] Trial 757 finished with value: 0.7994761587960039 and parameters: {'iterations': 693, 'learning_rate': 0.22084005357468087, 'num_leaves': 2740, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:18,325] Trial 758 finished with value: 0.7996215149465257 and parameters: {'iterations': 1126, 'learning_rate': 0.23594217242041993, 'num_leaves': 2890, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:19,307] Trial 759 finished with value: 0.7966473044820009 and parameters: {'iterations': 1187, 'learning_rate': 0.2139745528449753, 'num_leaves': 2790, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:20,604] Trial 760 finished with value: 0.7959400909035003 and parameters: {'iterations': 1050, 'learning_rate': 0.21851625265292135, 'num_leaves': 2810, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:21,479] Trial 761 finished with value: 0.7950662768140169 and parameters: {'iterations': 904, 'learning_rate': 0.2266224738428272, 'num_leaves': 2710, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:22,315] Trial 762 finished with value: 0.7986235890669813 and parameters: {'iterations': 1087, 'learning_rate': 0.2158611172457264, 'num_leaves': 2180, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:23,078] Trial 763 finished with value: 0.7978118308725282 and parameters: {'iterations': 1069, 'learning_rate': 0.23135596493740984, 'num_leaves': 2760, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:23,858] Trial 764 finished with value: 0.7982903880450157 and parameters: {'iterations': 1102, 'learning_rate': 0.24117353481374185, 'num_leaves': 2590, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:24,690] Trial 765 finished with value: 0.7957734903925175 and parameters: {'iterations': 1149, 'learning_rate': 0.22205947812133603, 'num_leaves': 2730, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:25,412] Trial 766 finished with value: 0.7971046172940276 and parameters: {'iterations': 1048, 'learning_rate': 0.21111024440711615, 'num_leaves': 2880, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:26,220] Trial 767 finished with value: 0.7953994778359825 and parameters: {'iterations': 1117, 'learning_rate': 0.217935623609163, 'num_leaves': 2830, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:27,122] Trial 768 finished with value: 0.7962520475650049 and parameters: {'iterations': 1078, 'learning_rate': 0.22838170951876272, 'num_leaves': 1420, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:27,955] Trial 769 finished with value: 0.7916755650720352 and parameters: {'iterations': 761, 'learning_rate': 0.21100808985594863, 'num_leaves': 2960, 'max_depth': 9, 'lambda_l1': 0, 'lambda_l2': 99, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:28,679] Trial 770 finished with value: 0.79662606012154 and parameters: {'iterations': 1063, 'learning_rate': 0.22484764322592232, 'num_leaves': 2870, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:29,524] Trial 771 finished with value: 0.7964807039710183 and parameters: {'iterations': 934, 'learning_rate': 0.21438363212777012, 'num_leaves': 2640, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:30,229] Trial 772 finished with value: 0.7956901901370262 and parameters: {'iterations': 1131, 'learning_rate': 0.23804488248885822, 'num_leaves': 2940, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:31,003] Trial 773 finished with value: 0.7974378183159931 and parameters: {'iterations': 1044, 'learning_rate': 0.20834930088281892, 'num_leaves': 2700, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:32,180] Trial 774 finished with value: 0.7963974037155269 and parameters: {'iterations': 1086, 'learning_rate': 0.2644052026095656, 'num_leaves': 2810, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:33,231] Trial 775 finished with value: 0.7978330752329892 and parameters: {'iterations': 1104, 'learning_rate': 0.21939065036600247, 'num_leaves': 2790, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:34,364] Trial 776 finished with value: 0.795316177580491 and parameters: {'iterations': 804, 'learning_rate': 0.14867212526921467, 'num_leaves': 2910, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:34,939] Trial 777 finished with value: 0.7913227968759611 and parameters: {'iterations': 1035, 'learning_rate': 0.2343230805470392, 'num_leaves': 1020, 'max_depth': 10, 'lambda_l1': 90, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:35,697] Trial 778 finished with value: 0.7960021467985308 and parameters: {'iterations': 1065, 'learning_rate': 0.29392686643484317, 'num_leaves': 1570, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:36,270] Trial 779 finished with value: 0.7901157817645119 and parameters: {'iterations': 1081, 'learning_rate': 0.22260187660224268, 'num_leaves': 140, 'max_depth': 10, 'lambda_l1': 69, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:36,991] Trial 780 finished with value: 0.7953782334755214 and parameters: {'iterations': 1172, 'learning_rate': 0.21091984128828128, 'num_leaves': 1730, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:37,658] Trial 781 finished with value: 0.7968972052484752 and parameters: {'iterations': 733, 'learning_rate': 0.17785769828250125, 'num_leaves': 2670, 'max_depth': 10, 'lambda_l1': 24, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:38,483] Trial 782 finished with value: 0.7957947347529785 and parameters: {'iterations': 1113, 'learning_rate': 0.16235796051362417, 'num_leaves': 2390, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:39,385] Trial 783 finished with value: 0.800266672629996 and parameters: {'iterations': 1052, 'learning_rate': 0.16964609739662306, 'num_leaves': 1640, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:40,275] Trial 784 finished with value: 0.7944423634910074 and parameters: {'iterations': 1058, 'learning_rate': 0.17188438299725173, 'num_leaves': 1630, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:41,016] Trial 785 finished with value: 0.7963141034600354 and parameters: {'iterations': 1050, 'learning_rate': 0.17539430420954558, 'num_leaves': 1570, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 99, 'feature_fraction': 0.4}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:41,892] Trial 786 finished with value: 0.7965640042265095 and parameters: {'iterations': 1080, 'learning_rate': 0.15791675323802756, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:42,855] Trial 787 finished with value: 0.789699280487055 and parameters: {'iterations': 1093, 'learning_rate': 0.16568307354134532, 'num_leaves': 1670, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 21, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:43,755] Trial 788 finished with value: 0.7935685494015239 and parameters: {'iterations': 1036, 'learning_rate': 0.17430197672023137, 'num_leaves': 1660, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:44,751] Trial 789 finished with value: 0.7971258616544884 and parameters: {'iterations': 1059, 'learning_rate': 0.18176307001675426, 'num_leaves': 1500, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:45,982] Trial 790 finished with value: 0.7938184501679981 and parameters: {'iterations': 1147, 'learning_rate': 0.16656565254306518, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:47,293] Trial 791 finished with value: 0.7971879175495189 and parameters: {'iterations': 1069, 'learning_rate': 0.16422649126757394, 'num_leaves': 1700, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:48,333] Trial 792 finished with value: 0.794899676303034 and parameters: {'iterations': 648, 'learning_rate': 0.1722014519485859, 'num_leaves': 1610, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:49,234] Trial 793 finished with value: 0.7930279363340061 and parameters: {'iterations': 1128, 'learning_rate': 0.1818181335953893, 'num_leaves': 1530, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:50,069] Trial 794 finished with value: 0.7972712178050104 and parameters: {'iterations': 1101, 'learning_rate': 0.17120917044418482, 'num_leaves': 2580, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:50,908] Trial 795 finished with value: 0.7961687473095135 and parameters: {'iterations': 1037, 'learning_rate': 0.1536986073337816, 'num_leaves': 300, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 45, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:51,915] Trial 796 finished with value: 0.7846654851820586 and parameters: {'iterations': 1086, 'learning_rate': 0.16405100859464916, 'num_leaves': 2850, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 15, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:52,755] Trial 797 finished with value: 0.7994761587960039 and parameters: {'iterations': 1048, 'learning_rate': 0.15555805028904574, 'num_leaves': 1620, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:53,463] Trial 798 finished with value: 0.7961899916699745 and parameters: {'iterations': 1073, 'learning_rate': 0.16189497284201396, 'num_leaves': 1740, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:54,304] Trial 799 finished with value: 0.7995594590514953 and parameters: {'iterations': 1110, 'learning_rate': 0.19034511193649445, 'num_leaves': 1690, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:55,205] Trial 800 finished with value: 0.7974998742110235 and parameters: {'iterations': 1021, 'learning_rate': 0.17838275744231918, 'num_leaves': 3000, 'max_depth': 10, 'lambda_l1': 0, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:55,911] Trial 801 finished with value: 0.7937563942729676 and parameters: {'iterations': 1061, 'learning_rate': 0.18297082882499735, 'num_leaves': 1760, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:56,734] Trial 802 finished with value: 0.7955448339865042 and parameters: {'iterations': 1128, 'learning_rate': 0.15748983766217312, 'num_leaves': 1650, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:57,506] Trial 803 finished with value: 0.7955448339865042 and parameters: {'iterations': 1086, 'learning_rate': 0.1879429030548245, 'num_leaves': 1790, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:58,215] Trial 804 finished with value: 0.7894085681860111 and parameters: {'iterations': 1034, 'learning_rate': 0.20386185293241418, 'num_leaves': 2270, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 42, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:07:59,237] Trial 805 finished with value: 0.7933398929955107 and parameters: {'iterations': 1104, 'learning_rate': 0.17021179614642004, 'num_leaves': 2500, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:00,351] Trial 806 finished with value: 0.794109162469042 and parameters: {'iterations': 1068, 'learning_rate': 0.28309847318434983, 'num_leaves': 1700, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:01,284] Trial 807 finished with value: 0.7993928585405125 and parameters: {'iterations': 1050, 'learning_rate': 0.19401753903661775, 'num_leaves': 1490, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:02,202] Trial 808 finished with value: 0.7990809018790077 and parameters: {'iterations': 1149, 'learning_rate': 0.20172227036154486, 'num_leaves': 1760, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:02,964] Trial 809 finished with value: 0.7993095582850209 and parameters: {'iterations': 1027, 'learning_rate': 0.19395523382475252, 'num_leaves': 1580, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:03,794] Trial 810 finished with value: 0.7965640042265095 and parameters: {'iterations': 1092, 'learning_rate': 0.2074664772824835, 'num_leaves': 1460, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:04,560] Trial 811 finished with value: 0.7941304068295029 and parameters: {'iterations': 1167, 'learning_rate': 0.24344939009150152, 'num_leaves': 2770, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:05,242] Trial 812 finished with value: 0.7925493791615187 and parameters: {'iterations': 1118, 'learning_rate': 0.25423636368477787, 'num_leaves': 770, 'max_depth': 6, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:06,000] Trial 813 finished with value: 0.7936730940174763 and parameters: {'iterations': 1073, 'learning_rate': 0.16863520803949117, 'num_leaves': 1660, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:06,778] Trial 814 finished with value: 0.7977497749774978 and parameters: {'iterations': 1047, 'learning_rate': 0.2010913567850344, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:07,702] Trial 815 finished with value: 0.7934852491460327 and parameters: {'iterations': 1139, 'learning_rate': 0.23000334198784395, 'num_leaves': 1720, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:08,540] Trial 816 finished with value: 0.7957734903925175 and parameters: {'iterations': 1104, 'learning_rate': 0.18590129940193778, 'num_leaves': 1530, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:09,297] Trial 817 finished with value: 0.7967926606325229 and parameters: {'iterations': 1021, 'learning_rate': 0.20938792221624347, 'num_leaves': 1630, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:10,043] Trial 818 finished with value: 0.7958780350084699 and parameters: {'iterations': 1078, 'learning_rate': 0.17811094601511218, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:11,012] Trial 819 finished with value: 0.7922374225000139 and parameters: {'iterations': 1058, 'learning_rate': 0.19781387455145305, 'num_leaves': 1710, 'max_depth': 9, 'lambda_l1': 0, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:12,218] Trial 820 finished with value: 0.7986856449620117 and parameters: {'iterations': 1094, 'learning_rate': 0.21361122432765525, 'num_leaves': 1830, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:13,226] Trial 821 finished with value: 0.7955660783469651 and parameters: {'iterations': 1120, 'learning_rate': 0.1610587101133954, 'num_leaves': 2900, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:14,056] Trial 822 finished with value: 0.7941924627245333 and parameters: {'iterations': 1048, 'learning_rate': 0.1907906159379354, 'num_leaves': 1660, 'max_depth': 4, 'lambda_l1': 3, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:14,867] Trial 823 finished with value: 0.7993095582850209 and parameters: {'iterations': 1073, 'learning_rate': 0.20533205146339684, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:15,686] Trial 824 finished with value: 0.7962732919254659 and parameters: {'iterations': 1032, 'learning_rate': 0.22605737803439577, 'num_leaves': 2680, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:16,564] Trial 825 finished with value: 0.7973545180605016 and parameters: {'iterations': 1003, 'learning_rate': 0.23541355284941637, 'num_leaves': 1390, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:17,303] Trial 826 finished with value: 0.7967926606325229 and parameters: {'iterations': 1088, 'learning_rate': 0.2723127265648349, 'num_leaves': 380, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:18,158] Trial 827 finished with value: 0.7988522454729946 and parameters: {'iterations': 1059, 'learning_rate': 0.15044041066689032, 'num_leaves': 1810, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:19,010] Trial 828 finished with value: 0.800100072119013 and parameters: {'iterations': 828, 'learning_rate': 0.199740733813221, 'num_leaves': 2770, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:19,632] Trial 829 finished with value: 0.7948376204080034 and parameters: {'iterations': 1017, 'learning_rate': 0.21508333129696958, 'num_leaves': 1750, 'max_depth': 10, 'lambda_l1': 45, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:20,308] Trial 830 finished with value: 0.7919254658385094 and parameters: {'iterations': 1112, 'learning_rate': 0.20712635384017758, 'num_leaves': 1690, 'max_depth': 5, 'lambda_l1': 12, 'lambda_l2': 27, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:21,114] Trial 831 finished with value: 0.7983116324054765 and parameters: {'iterations': 1140, 'learning_rate': 0.220901618409851, 'num_leaves': 1600, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:22,048] Trial 832 finished with value: 0.7952949332200301 and parameters: {'iterations': 1037, 'learning_rate': 0.19420971930337327, 'num_leaves': 2860, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:23,127] Trial 833 finished with value: 0.794775564512973 and parameters: {'iterations': 1200, 'learning_rate': 0.23152340667213236, 'num_leaves': 1830, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:24,336] Trial 834 finished with value: 0.7971879175495189 and parameters: {'iterations': 693, 'learning_rate': 0.18643873178889908, 'num_leaves': 1740, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:25,696] Trial 835 finished with value: 0.7981237875340328 and parameters: {'iterations': 1073, 'learning_rate': 0.2480113321016482, 'num_leaves': 1640, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:27,071] Trial 836 finished with value: 0.7986856449620117 and parameters: {'iterations': 1161, 'learning_rate': 0.21115048716788118, 'num_leaves': 1460, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:28,012] Trial 837 finished with value: 0.7980829759994633 and parameters: {'iterations': 1093, 'learning_rate': 0.2014867743852453, 'num_leaves': 1790, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:28,713] Trial 838 finished with value: 0.7960854470540222 and parameters: {'iterations': 985, 'learning_rate': 0.21726176042819706, 'num_leaves': 1510, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:29,573] Trial 839 finished with value: 0.7978118308725282 and parameters: {'iterations': 1056, 'learning_rate': 0.22535807536917296, 'num_leaves': 1700, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:30,646] Trial 840 finished with value: 0.7838962157085274 and parameters: {'iterations': 1128, 'learning_rate': 0.2046334384959141, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 24, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:31,393] Trial 841 finished with value: 0.7988734898334555 and parameters: {'iterations': 1107, 'learning_rate': 0.19671059222211598, 'num_leaves': 1580, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:32,294] Trial 842 finished with value: 0.7951495770695082 and parameters: {'iterations': 715, 'learning_rate': 0.21399970406317853, 'num_leaves': 1760, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:33,079] Trial 843 finished with value: 0.7997881154575085 and parameters: {'iterations': 1043, 'learning_rate': 0.17780113270698228, 'num_leaves': 420, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:33,819] Trial 844 finished with value: 0.7972712178050104 and parameters: {'iterations': 1070, 'learning_rate': 0.20841068963777343, 'num_leaves': 2590, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:34,708] Trial 845 finished with value: 0.7996215149465257 and parameters: {'iterations': 1015, 'learning_rate': 0.19036424235690588, 'num_leaves': 1630, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:35,607] Trial 846 finished with value: 0.7907396950875212 and parameters: {'iterations': 1088, 'learning_rate': 0.23683133168416315, 'num_leaves': 1860, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:36,434] Trial 847 finished with value: 0.793049180694467 and parameters: {'iterations': 1030, 'learning_rate': 0.22859582760311817, 'num_leaves': 1310, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:37,301] Trial 848 finished with value: 0.7958567906480088 and parameters: {'iterations': 1057, 'learning_rate': 0.2196527885115402, 'num_leaves': 2810, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:38,561] Trial 849 finished with value: 0.7893873238255503 and parameters: {'iterations': 1107, 'learning_rate': 0.19849082685018388, 'num_leaves': 1690, 'max_depth': 9, 'lambda_l1': 0, 'lambda_l2': 33, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:39,693] Trial 850 finished with value: 0.7943590632355161 and parameters: {'iterations': 1080, 'learning_rate': 0.18409309289105596, 'num_leaves': 2430, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:40,600] Trial 851 finished with value: 0.7905943389369993 and parameters: {'iterations': 856, 'learning_rate': 0.2234276088489028, 'num_leaves': 2260, 'max_depth': 11, 'lambda_l1': 81, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:41,353] Trial 852 finished with value: 0.7940471065740113 and parameters: {'iterations': 1000, 'learning_rate': 0.20600317251477418, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:41,963] Trial 853 finished with value: 0.7910941404699476 and parameters: {'iterations': 1132, 'learning_rate': 0.1673278039518177, 'num_leaves': 1730, 'max_depth': 10, 'lambda_l1': 42, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:42,755] Trial 854 finished with value: 0.7946089640019903 and parameters: {'iterations': 1064, 'learning_rate': 0.2143144835280568, 'num_leaves': 1540, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:43,596] Trial 855 finished with value: 0.7986856449620117 and parameters: {'iterations': 1042, 'learning_rate': 0.1915140589967054, 'num_leaves': 1850, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:44,286] Trial 856 finished with value: 0.7809857383253852 and parameters: {'iterations': 1092, 'learning_rate': 0.02226149797936039, 'num_leaves': 2740, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:45,249] Trial 857 finished with value: 0.8000167718635217 and parameters: {'iterations': 1179, 'learning_rate': 0.2029335760652203, 'num_leaves': 1680, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:46,026] Trial 858 finished with value: 0.7987689452175031 and parameters: {'iterations': 1024, 'learning_rate': 0.20963950928852404, 'num_leaves': 1610, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:46,881] Trial 859 finished with value: 0.7965640042265095 and parameters: {'iterations': 1120, 'learning_rate': 0.1970581875560818, 'num_leaves': 1810, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:47,601] Trial 860 finished with value: 0.7967093603770316 and parameters: {'iterations': 738, 'learning_rate': 0.22001493086077165, 'num_leaves': 250, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:48,256] Trial 861 finished with value: 0.7936518496570153 and parameters: {'iterations': 1149, 'learning_rate': 0.23083218280615514, 'num_leaves': 1430, 'max_depth': 4, 'lambda_l1': 3, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:49,037] Trial 862 finished with value: 0.7958567906480088 and parameters: {'iterations': 1075, 'learning_rate': 0.21371778749031986, 'num_leaves': 1740, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:49,759] Trial 863 finished with value: 0.7978951311280196 and parameters: {'iterations': 1052, 'learning_rate': 0.2426806930732735, 'num_leaves': 1880, 'max_depth': 6, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:50,596] Trial 864 finished with value: 0.7976664747220064 and parameters: {'iterations': 1100, 'learning_rate': 0.20385233606465947, 'num_leaves': 460, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:51,823] Trial 865 finished with value: 0.7974998742110235 and parameters: {'iterations': 1011, 'learning_rate': 0.18778866675360675, 'num_leaves': 1660, 'max_depth': 9, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:52,648] Trial 866 finished with value: 0.7930279363340061 and parameters: {'iterations': 1036, 'learning_rate': 0.22322941398780535, 'num_leaves': 2510, 'max_depth': 3, 'lambda_l1': 6, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:53,803] Trial 867 finished with value: 0.7994549144355431 and parameters: {'iterations': 1081, 'learning_rate': 0.2098265710151465, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:54,765] Trial 868 finished with value: 0.7919875217335397 and parameters: {'iterations': 1056, 'learning_rate': 0.19540965618388895, 'num_leaves': 2680, 'max_depth': 11, 'lambda_l1': 0, 'lambda_l2': 69, 'feature_fraction': 0.5}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:55,453] Trial 869 finished with value: 0.7916347535374656 and parameters: {'iterations': 1105, 'learning_rate': 0.10159706658727327, 'num_leaves': 2180, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:56,065] Trial 870 finished with value: 0.7906155832974602 and parameters: {'iterations': 995, 'learning_rate': 0.1601524664771456, 'num_leaves': 1530, 'max_depth': 10, 'lambda_l1': 75, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:56,864] Trial 871 finished with value: 0.7987689452175031 and parameters: {'iterations': 1123, 'learning_rate': 0.2157981981324841, 'num_leaves': 2910, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:57,618] Trial 872 finished with value: 0.7940471065740113 and parameters: {'iterations': 1073, 'learning_rate': 0.18090747394435436, 'num_leaves': 1600, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:58,384] Trial 873 finished with value: 0.7974998742110235 and parameters: {'iterations': 1031, 'learning_rate': 0.26854615197992676, 'num_leaves': 1720, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:59,104] Trial 874 finished with value: 0.7919875217335397 and parameters: {'iterations': 1094, 'learning_rate': 0.14456724499431886, 'num_leaves': 1770, 'max_depth': 5, 'lambda_l1': 6, 'lambda_l2': 36, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:08:59,910] Trial 875 finished with value: 0.7973545180605016 and parameters: {'iterations': 979, 'learning_rate': 0.17265875251563106, 'num_leaves': 1890, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:00,710] Trial 876 finished with value: 0.7984569885559986 and parameters: {'iterations': 1052, 'learning_rate': 0.20175355370701006, 'num_leaves': 1620, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:01,605] Trial 877 finished with value: 0.7956901901370262 and parameters: {'iterations': 1068, 'learning_rate': 0.23721182794736662, 'num_leaves': 1830, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:02,512] Trial 878 finished with value: 0.7967926606325229 and parameters: {'iterations': 1137, 'learning_rate': 0.22787829648130678, 'num_leaves': 2840, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:03,213] Trial 879 finished with value: 0.793194536844989 and parameters: {'iterations': 1020, 'learning_rate': 0.19171162577276665, 'num_leaves': 340, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:04,090] Trial 880 finished with value: 0.7993928585405125 and parameters: {'iterations': 1155, 'learning_rate': 0.20595191915365274, 'num_leaves': 1680, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:05,175] Trial 881 finished with value: 0.7968139049929838 and parameters: {'iterations': 1112, 'learning_rate': 0.21725427881516948, 'num_leaves': 1740, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:06,440] Trial 882 finished with value: 0.7941924627245333 and parameters: {'iterations': 1081, 'learning_rate': 0.19805315970551615, 'num_leaves': 1860, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:07,569] Trial 883 finished with value: 0.7911349520045172 and parameters: {'iterations': 1046, 'learning_rate': 0.22291511024711438, 'num_leaves': 1660, 'max_depth': 9, 'lambda_l1': 0, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:08,377] Trial 884 finished with value: 0.802138412599024 and parameters: {'iterations': 759, 'learning_rate': 0.2101251796625083, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:09,174] Trial 885 finished with value: 0.7968759608880143 and parameters: {'iterations': 717, 'learning_rate': 0.21039815150082294, 'num_leaves': 1560, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:09,843] Trial 886 finished with value: 0.7885347540965277 and parameters: {'iterations': 802, 'learning_rate': 0.2094072231669837, 'num_leaves': 1240, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 0.4}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:10,608] Trial 887 finished with value: 0.7939017504234896 and parameters: {'iterations': 740, 'learning_rate': 0.21820573923095016, 'num_leaves': 2140, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:11,434] Trial 888 finished with value: 0.7934231932510021 and parameters: {'iterations': 730, 'learning_rate': 0.1334035014770886, 'num_leaves': 1400, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:12,177] Trial 889 finished with value: 0.793194536844989 and parameters: {'iterations': 780, 'learning_rate': 0.21065383824054607, 'num_leaves': 2640, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:12,907] Trial 890 finished with value: 0.7935685494015239 and parameters: {'iterations': 744, 'learning_rate': 0.20249648948151083, 'num_leaves': 1510, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:13,727] Trial 891 finished with value: 0.7947543201525121 and parameters: {'iterations': 784, 'learning_rate': 0.21358758679204107, 'num_leaves': 1460, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:14,474] Trial 892 finished with value: 0.7946710198970207 and parameters: {'iterations': 764, 'learning_rate': 0.2045835816972265, 'num_leaves': 1530, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:15,105] Trial 893 finished with value: 0.7890753671640457 and parameters: {'iterations': 750, 'learning_rate': 0.2181473347711062, 'num_leaves': 2370, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 21, 'feature_fraction': 0.2}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:15,885] Trial 894 finished with value: 0.7995807034119562 and parameters: {'iterations': 769, 'learning_rate': 0.22459556697884786, 'num_leaves': 1470, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 66, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:16,738] Trial 895 finished with value: 0.7974998742110235 and parameters: {'iterations': 722, 'learning_rate': 0.20686070100556092, 'num_leaves': 1360, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:17,716] Trial 896 finished with value: 0.79662606012154 and parameters: {'iterations': 752, 'learning_rate': 0.19960856108672712, 'num_leaves': 1580, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:18,452] Trial 897 finished with value: 0.7947330757920512 and parameters: {'iterations': 707, 'learning_rate': 0.215281116492436, 'num_leaves': 1890, 'max_depth': 9, 'lambda_l1': 51, 'lambda_l2': 72, 'feature_fraction': 0.30000000000000004}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:19,693] Trial 898 finished with value: 0.7968972052484752 and parameters: {'iterations': 691, 'learning_rate': 0.2336006492053641, 'num_leaves': 2730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:20,727] Trial 899 finished with value: 0.8009526418480357 and parameters: {'iterations': 774, 'learning_rate': 0.29909729263244994, 'num_leaves': 1800, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:21,324] Trial 900 finished with value: 0.7901990820200031 and parameters: {'iterations': 790, 'learning_rate': 0.2947641034974974, 'num_leaves': 1820, 'max_depth': 10, 'lambda_l1': 60, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:22,063] Trial 901 finished with value: 0.7984357441955376 and parameters: {'iterations': 766, 'learning_rate': 0.2772590442219213, 'num_leaves': 1770, 'max_depth': 6, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:22,941] Trial 902 finished with value: 0.7977285306170367 and parameters: {'iterations': 750, 'learning_rate': 0.2952968024993065, 'num_leaves': 1920, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:23,732] Trial 903 finished with value: 0.7958567906480088 and parameters: {'iterations': 751, 'learning_rate': 0.19206929001618206, 'num_leaves': 1840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:24,438] Trial 904 finished with value: 0.7949829765585253 and parameters: {'iterations': 756, 'learning_rate': 0.25972082332425994, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:25,253] Trial 905 finished with value: 0.7930066919735451 and parameters: {'iterations': 794, 'learning_rate': 0.29764289554600154, 'num_leaves': 1830, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 0, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:25,998] Trial 906 finished with value: 0.7948376204080034 and parameters: {'iterations': 732, 'learning_rate': 0.1987556102201844, 'num_leaves': 1900, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 66, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:26,884] Trial 907 finished with value: 0.791696809432496 and parameters: {'iterations': 717, 'learning_rate': 0.28748164733161397, 'num_leaves': 1730, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:27,703] Trial 908 finished with value: 0.7941304068295029 and parameters: {'iterations': 770, 'learning_rate': 0.29414558004832736, 'num_leaves': 1780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:28,448] Trial 909 finished with value: 0.8011192423590185 and parameters: {'iterations': 809, 'learning_rate': 0.2933185324353382, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:29,195] Trial 910 finished with value: 0.7963974037155269 and parameters: {'iterations': 744, 'learning_rate': 0.29884016507213823, 'num_leaves': 620, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:30,103] Trial 911 finished with value: 0.7961475029490527 and parameters: {'iterations': 810, 'learning_rate': 0.29916445709201767, 'num_leaves': 1860, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:31,151] Trial 912 finished with value: 0.8023883133654981 and parameters: {'iterations': 761, 'learning_rate': 0.29083176129285754, 'num_leaves': 1010, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:32,162] Trial 913 finished with value: 0.7984357441955376 and parameters: {'iterations': 769, 'learning_rate': 0.28761521487965286, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:33,206] Trial 914 finished with value: 0.7925493791615187 and parameters: {'iterations': 803, 'learning_rate': 0.252939978857245, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:34,070] Trial 915 finished with value: 0.8012858428700013 and parameters: {'iterations': 803, 'learning_rate': 0.2905810622006571, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:34,792] Trial 916 finished with value: 0.7966473044820009 and parameters: {'iterations': 784, 'learning_rate': 0.29218550241925023, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:35,498] Trial 917 finished with value: 0.7935685494015239 and parameters: {'iterations': 790, 'learning_rate': 0.2906196318234194, 'num_leaves': 1200, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:36,231] Trial 918 finished with value: 0.7978951311280196 and parameters: {'iterations': 772, 'learning_rate': 0.2905087533886734, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:36,986] Trial 919 finished with value: 0.7987689452175031 and parameters: {'iterations': 800, 'learning_rate': 0.28171593913163073, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:37,703] Trial 920 finished with value: 0.7956068898815348 and parameters: {'iterations': 772, 'learning_rate': 0.29961380197421483, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:38,413] Trial 921 finished with value: 0.7936518496570153 and parameters: {'iterations': 806, 'learning_rate': 0.2904927919085312, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 9, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:39,168] Trial 922 finished with value: 0.7988310011125336 and parameters: {'iterations': 829, 'learning_rate': 0.294277775920954, 'num_leaves': 970, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:39,928] Trial 923 finished with value: 0.7971879175495189 and parameters: {'iterations': 819, 'learning_rate': 0.28194178405005, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:40,616] Trial 924 finished with value: 0.7954615337310129 and parameters: {'iterations': 761, 'learning_rate': 0.28351884655387505, 'num_leaves': 530, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:41,374] Trial 925 finished with value: 0.7986235890669813 and parameters: {'iterations': 826, 'learning_rate': 0.2872458598099827, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:42,083] Trial 926 finished with value: 0.7941304068295029 and parameters: {'iterations': 780, 'learning_rate': 0.2932708963676134, 'num_leaves': 550, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:42,859] Trial 927 finished with value: 0.800100072119013 and parameters: {'iterations': 778, 'learning_rate': 0.2986638416284762, 'num_leaves': 1120, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:43,668] Trial 928 finished with value: 0.7988310011125336 and parameters: {'iterations': 845, 'learning_rate': 0.2858576569970882, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:44,676] Trial 929 finished with value: 0.7973332737000408 and parameters: {'iterations': 781, 'learning_rate': 0.28413396731848234, 'num_leaves': 1050, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:45,744] Trial 930 finished with value: 0.7986235890669813 and parameters: {'iterations': 761, 'learning_rate': 0.2941128486706948, 'num_leaves': 610, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:46,904] Trial 931 finished with value: 0.799850171352539 and parameters: {'iterations': 796, 'learning_rate': 0.2918415410584506, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:47,714] Trial 932 finished with value: 0.7985190444510288 and parameters: {'iterations': 742, 'learning_rate': 0.29769271976411643, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:48,330] Trial 933 finished with value: 0.7945256637464989 and parameters: {'iterations': 784, 'learning_rate': 0.2988088586737941, 'num_leaves': 540, 'max_depth': 11, 'lambda_l1': 39, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:49,007] Trial 934 finished with value: 0.7944211191305467 and parameters: {'iterations': 763, 'learning_rate': 0.28629616587020024, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:49,779] Trial 935 finished with value: 0.8029289264330159 and parameters: {'iterations': 810, 'learning_rate': 0.2793393290540805, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:50,448] Trial 936 finished with value: 0.7961687473095135 and parameters: {'iterations': 801, 'learning_rate': 0.28223902504794757, 'num_leaves': 590, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:51,116] Trial 937 finished with value: 0.7926947353120405 and parameters: {'iterations': 840, 'learning_rate': 0.2760559873989893, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:51,841] Trial 938 finished with value: 0.7953782334755214 and parameters: {'iterations': 776, 'learning_rate': 0.2892113171877984, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:52,568] Trial 939 finished with value: 0.7940471065740113 and parameters: {'iterations': 821, 'learning_rate': 0.278791943772008, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:53,318] Trial 940 finished with value: 0.8025336695160199 and parameters: {'iterations': 818, 'learning_rate': 0.2872912479375809, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:54,009] Trial 941 finished with value: 0.7947543201525121 and parameters: {'iterations': 824, 'learning_rate': 0.2842374627939268, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:54,775] Trial 942 finished with value: 0.797978431383511 and parameters: {'iterations': 813, 'learning_rate': 0.2897265187566307, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:55,509] Trial 943 finished with value: 0.7944423634910074 and parameters: {'iterations': 862, 'learning_rate': 0.2802539397902605, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:56,272] Trial 944 finished with value: 0.7974378183159931 and parameters: {'iterations': 818, 'learning_rate': 0.2748957292119663, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:57,145] Trial 945 finished with value: 0.7951495770695082 and parameters: {'iterations': 832, 'learning_rate': 0.28880308112331127, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:58,142] Trial 946 finished with value: 0.79662606012154 and parameters: {'iterations': 809, 'learning_rate': 0.2996962370263011, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:09:59,238] Trial 947 finished with value: 0.799704815202017 and parameters: {'iterations': 794, 'learning_rate': 0.2907975345296788, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:00,206] Trial 948 finished with value: 0.8001833723745044 and parameters: {'iterations': 855, 'learning_rate': 0.28736014191415077, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:00,968] Trial 949 finished with value: 0.7981237875340328 and parameters: {'iterations': 808, 'learning_rate': 0.2812343556824396, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:01,725] Trial 950 finished with value: 0.7987477008570422 and parameters: {'iterations': 836, 'learning_rate': 0.2920250008346498, 'num_leaves': 560, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:02,439] Trial 951 finished with value: 0.7965640042265095 and parameters: {'iterations': 800, 'learning_rate': 0.2682191182345815, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:03,156] Trial 952 finished with value: 0.7974998742110235 and parameters: {'iterations': 791, 'learning_rate': 0.2939511053141634, 'num_leaves': 480, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:03,919] Trial 953 finished with value: 0.7960021467985308 and parameters: {'iterations': 756, 'learning_rate': 0.2996019920599711, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:04,609] Trial 954 finished with value: 0.7958567906480088 and parameters: {'iterations': 816, 'learning_rate': 0.2839389911848825, 'num_leaves': 560, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:05,424] Trial 955 finished with value: 0.7986023447065203 and parameters: {'iterations': 770, 'learning_rate': 0.2923614366945248, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:06,043] Trial 956 finished with value: 0.793444437611463 and parameters: {'iterations': 782, 'learning_rate': 0.28664311397095743, 'num_leaves': 580, 'max_depth': 11, 'lambda_l1': 54, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:06,811] Trial 957 finished with value: 0.7982903880450157 and parameters: {'iterations': 761, 'learning_rate': 0.2703391457486365, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:07,468] Trial 958 finished with value: 0.7968759608880143 and parameters: {'iterations': 792, 'learning_rate': 0.2927921376367946, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:08,208] Trial 959 finished with value: 0.7965640042265095 and parameters: {'iterations': 844, 'learning_rate': 0.2784229976199245, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:09,013] Trial 960 finished with value: 0.7988310011125336 and parameters: {'iterations': 738, 'learning_rate': 0.27833552886809165, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:09,779] Trial 961 finished with value: 0.7964807039710183 and parameters: {'iterations': 816, 'learning_rate': 0.29999394908866356, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:10,934] Trial 962 finished with value: 0.7938184501679981 and parameters: {'iterations': 787, 'learning_rate': 0.29109982627971925, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:12,041] Trial 963 finished with value: 0.7999334716080304 and parameters: {'iterations': 741, 'learning_rate': 0.2734783901623909, 'num_leaves': 530, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:13,194] Trial 964 finished with value: 0.7977497749774978 and parameters: {'iterations': 768, 'learning_rate': 0.2680535660094767, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:13,936] Trial 965 finished with value: 0.7967306047374924 and parameters: {'iterations': 818, 'learning_rate': 0.29451490086074505, 'num_leaves': 520, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:14,700] Trial 966 finished with value: 0.7967926606325229 and parameters: {'iterations': 748, 'learning_rate': 0.28599308295988807, 'num_leaves': 610, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:15,529] Trial 967 finished with value: 0.7950875211744778 and parameters: {'iterations': 793, 'learning_rate': 0.2999969164368126, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:16,319] Trial 968 finished with value: 0.7986856449620117 and parameters: {'iterations': 831, 'learning_rate': 0.2847963593348317, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:17,006] Trial 969 finished with value: 0.7944423634910074 and parameters: {'iterations': 775, 'learning_rate': 0.28760194761437774, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:17,598] Trial 970 finished with value: 0.7900324815090204 and parameters: {'iterations': 725, 'learning_rate': 0.2802056252457361, 'num_leaves': 590, 'max_depth': 11, 'lambda_l1': 84, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:18,404] Trial 971 finished with value: 0.7985190444510288 and parameters: {'iterations': 753, 'learning_rate': 0.28539274416564764, 'num_leaves': 500, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:19,244] Trial 972 finished with value: 0.8026169697715113 and parameters: {'iterations': 876, 'learning_rate': 0.294267242577931, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:20,003] Trial 973 finished with value: 0.7982070877895242 and parameters: {'iterations': 836, 'learning_rate': 0.2953645068980795, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:20,729] Trial 974 finished with value: 0.7970213170385362 and parameters: {'iterations': 897, 'learning_rate': 0.2930705474122068, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:21,453] Trial 975 finished with value: 0.7883060976905144 and parameters: {'iterations': 851, 'learning_rate': 0.2882318228996291, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.5}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:22,217] Trial 976 finished with value: 0.7978118308725282 and parameters: {'iterations': 877, 'learning_rate': 0.2948400503263014, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:23,047] Trial 977 finished with value: 0.7961066914144831 and parameters: {'iterations': 809, 'learning_rate': 0.29146505948341156, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:24,116] Trial 978 finished with value: 0.7989976016235163 and parameters: {'iterations': 800, 'learning_rate': 0.27970083850866895, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:25,187] Trial 979 finished with value: 0.7967926606325229 and parameters: {'iterations': 880, 'learning_rate': 0.29418866959550993, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:26,120] Trial 980 finished with value: 0.7913636084105304 and parameters: {'iterations': 850, 'learning_rate': 0.29938151340156216, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 0.30000000000000004}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:26,957] Trial 981 finished with value: 0.7945256637464989 and parameters: {'iterations': 777, 'learning_rate': 0.2843560563041463, 'num_leaves': 1260, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:27,690] Trial 982 finished with value: 0.7974165739555322 and parameters: {'iterations': 806, 'learning_rate': 0.2884416253787309, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:28,264] Trial 983 finished with value: 0.7912182522600086 and parameters: {'iterations': 870, 'learning_rate': 0.29015638023135654, 'num_leaves': 580, 'max_depth': 11, 'lambda_l1': 96, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:28,969] Trial 984 finished with value: 0.7986023447065203 and parameters: {'iterations': 727, 'learning_rate': 0.2841589453717062, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:29,740] Trial 985 finished with value: 0.7972712178050104 and parameters: {'iterations': 787, 'learning_rate': 0.2730829444460503, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:30,559] Trial 986 finished with value: 0.7971046172940276 and parameters: {'iterations': 759, 'learning_rate': 0.2803486412329325, 'num_leaves': 490, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:31,324] Trial 987 finished with value: 0.7935064935064936 and parameters: {'iterations': 827, 'learning_rate': 0.2943888277310238, 'num_leaves': 610, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:32,138] Trial 988 finished with value: 0.7974378183159931 and parameters: {'iterations': 813, 'learning_rate': 0.2999655008120109, 'num_leaves': 560, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:32,896] Trial 989 finished with value: 0.7973332737000408 and parameters: {'iterations': 786, 'learning_rate': 0.29981760605475116, 'num_leaves': 530, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 69, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:33,723] Trial 990 finished with value: 0.7919254658385094 and parameters: {'iterations': 759, 'learning_rate': 0.28991453636105363, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 72, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:34,478] Trial 991 finished with value: 0.794920920663495 and parameters: {'iterations': 732, 'learning_rate': 0.2950528780988999, 'num_leaves': 1300, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:35,317] Trial 992 finished with value: 0.801597799531506 and parameters: {'iterations': 835, 'learning_rate': 0.28665916064014285, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:36,201] Trial 993 finished with value: 0.7957734903925175 and parameters: {'iterations': 838, 'learning_rate': 0.2748535039496059, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:37,391] Trial 994 finished with value: 0.7952116329645387 and parameters: {'iterations': 818, 'learning_rate': 0.287287411732652, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:38,578] Trial 995 finished with value: 0.800245428269535 and parameters: {'iterations': 845, 'learning_rate': 0.27712543575114496, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:39,556] Trial 996 finished with value: 0.7968759608880143 and parameters: {'iterations': 850, 'learning_rate': 0.28081191617586154, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:40,433] Trial 997 finished with value: 0.7957734903925175 and parameters: {'iterations': 827, 'learning_rate': 0.2865217886327691, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:41,212] Trial 998 finished with value: 0.8003287285250263 and parameters: {'iterations': 795, 'learning_rate': 0.2820306711086854, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:42,029] Trial 999 finished with value: 0.7952328773249995 and parameters: {'iterations': 827, 'learning_rate': 0.29204443481635517, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:42,845] Trial 1000 finished with value: 0.7939017504234896 and parameters: {'iterations': 812, 'learning_rate': 0.29415791281008274, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:43,631] Trial 1001 finished with value: 0.7967306047374924 and parameters: {'iterations': 776, 'learning_rate': 0.2777742351328242, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:44,529] Trial 1002 finished with value: 0.7919875217335397 and parameters: {'iterations': 804, 'learning_rate': 0.28750062553143024, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:45,369] Trial 1003 finished with value: 0.7991429577740383 and parameters: {'iterations': 858, 'learning_rate': 0.28318095874977234, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:46,153] Trial 1004 finished with value: 0.7964807039710183 and parameters: {'iterations': 799, 'learning_rate': 0.2999429978114656, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:47,043] Trial 1005 finished with value: 0.7974165739555322 and parameters: {'iterations': 841, 'learning_rate': 0.29043724762697193, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:47,865] Trial 1006 finished with value: 0.7973332737000408 and parameters: {'iterations': 865, 'learning_rate': 0.29281141623109846, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:48,752] Trial 1007 finished with value: 0.7989143013680249 and parameters: {'iterations': 776, 'learning_rate': 0.27514008824640784, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:49,762] Trial 1008 finished with value: 0.7981237875340328 and parameters: {'iterations': 822, 'learning_rate': 0.28443659625733386, 'num_leaves': 1090, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:50,925] Trial 1009 finished with value: 0.7988310011125336 and parameters: {'iterations': 748, 'learning_rate': 0.2868811613963201, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:51,977] Trial 1010 finished with value: 0.7946710198970207 and parameters: {'iterations': 796, 'learning_rate': 0.29314088128742016, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:52,869] Trial 1011 finished with value: 0.7997881154575085 and parameters: {'iterations': 819, 'learning_rate': 0.2821306484913237, 'num_leaves': 1000, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:53,685] Trial 1012 finished with value: 0.7964807039710183 and parameters: {'iterations': 764, 'learning_rate': 0.2952532538086762, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:54,575] Trial 1013 finished with value: 0.7929446360785146 and parameters: {'iterations': 898, 'learning_rate': 0.2710939453366208, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:55,355] Trial 1014 finished with value: 0.7971046172940276 and parameters: {'iterations': 875, 'learning_rate': 0.2647612724241443, 'num_leaves': 620, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:56,185] Trial 1015 finished with value: 0.7968759608880143 and parameters: {'iterations': 705, 'learning_rate': 0.2950239137013132, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:57,087] Trial 1016 finished with value: 0.7935064935064936 and parameters: {'iterations': 785, 'learning_rate': 0.299881134084835, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:57,856] Trial 1017 finished with value: 0.8011192423590185 and parameters: {'iterations': 836, 'learning_rate': 0.29119936201608115, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:58,543] Trial 1018 finished with value: 0.7935685494015239 and parameters: {'iterations': 852, 'learning_rate': 0.2855759061658155, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:59,263] Trial 1019 finished with value: 0.7985190444510288 and parameters: {'iterations': 846, 'learning_rate': 0.2926210769523464, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:10:59,982] Trial 1020 finished with value: 0.7941924627245333 and parameters: {'iterations': 833, 'learning_rate': 0.2884886922956969, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:00,713] Trial 1021 finished with value: 0.7989976016235163 and parameters: {'iterations': 838, 'learning_rate': 0.2793187328225939, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:01,478] Trial 1022 finished with value: 0.7970213170385362 and parameters: {'iterations': 805, 'learning_rate': 0.28701326268532773, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:02,257] Trial 1023 finished with value: 0.7978951311280196 and parameters: {'iterations': 870, 'learning_rate': 0.29325353459573356, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:03,282] Trial 1024 finished with value: 0.7963974037155269 and parameters: {'iterations': 827, 'learning_rate': 0.28196161031949535, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:04,137] Trial 1025 finished with value: 0.7920300104544616 and parameters: {'iterations': 884, 'learning_rate': 0.2770411556475478, 'num_leaves': 1150, 'max_depth': 11, 'lambda_l1': 69, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:05,233] Trial 1026 finished with value: 0.7915514532819742 and parameters: {'iterations': 796, 'learning_rate': 0.2891148703601165, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:06,020] Trial 1027 finished with value: 0.7969805055039666 and parameters: {'iterations': 808, 'learning_rate': 0.2952114282193, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 30, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:06,618] Trial 1028 finished with value: 0.7863085687450733 and parameters: {'iterations': 838, 'learning_rate': 0.2887846417251122, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.2}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:07,306] Trial 1029 finished with value: 0.7930279363340061 and parameters: {'iterations': 867, 'learning_rate': 0.29500569395095255, 'num_leaves': 620, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:08,014] Trial 1030 finished with value: 0.7956901901370262 and parameters: {'iterations': 913, 'learning_rate': 0.273240199004398, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:08,613] Trial 1031 finished with value: 0.7920300104544616 and parameters: {'iterations': 824, 'learning_rate': 0.2831506223167123, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 63, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:09,453] Trial 1032 finished with value: 0.7960021467985308 and parameters: {'iterations': 786, 'learning_rate': 0.29606410430739794, 'num_leaves': 1030, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:10,240] Trial 1033 finished with value: 0.800245428269535 and parameters: {'iterations': 767, 'learning_rate': 0.2866408471117278, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:10,972] Trial 1034 finished with value: 0.7935685494015239 and parameters: {'iterations': 812, 'learning_rate': 0.2895952742174461, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:11,762] Trial 1035 finished with value: 0.8004332731409788 and parameters: {'iterations': 757, 'learning_rate': 0.2598052258448674, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:12,574] Trial 1036 finished with value: 0.7949829765585253 and parameters: {'iterations': 846, 'learning_rate': 0.29921003517221617, 'num_leaves': 610, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:13,339] Trial 1037 finished with value: 0.7955235896260434 and parameters: {'iterations': 735, 'learning_rate': 0.2840431103026689, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:14,113] Trial 1038 finished with value: 0.7996215149465257 and parameters: {'iterations': 785, 'learning_rate': 0.277188919306586, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:15,014] Trial 1039 finished with value: 0.7944211191305467 and parameters: {'iterations': 862, 'learning_rate': 0.29156297207226073, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:15,933] Trial 1040 finished with value: 0.7999334716080304 and parameters: {'iterations': 813, 'learning_rate': 0.28098238756195393, 'num_leaves': 660, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:16,931] Trial 1041 finished with value: 0.7949829765585253 and parameters: {'iterations': 831, 'learning_rate': 0.2905358316314477, 'num_leaves': 460, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:18,106] Trial 1042 finished with value: 0.7982070877895242 and parameters: {'iterations': 741, 'learning_rate': 0.295120616791868, 'num_leaves': 520, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:18,997] Trial 1043 finished with value: 0.7953782334755214 and parameters: {'iterations': 799, 'learning_rate': 0.2997453011557396, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:19,903] Trial 1044 finished with value: 0.7906563948320298 and parameters: {'iterations': 766, 'learning_rate': 0.26861185505318813, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:20,677] Trial 1045 finished with value: 0.7972712178050104 and parameters: {'iterations': 719, 'learning_rate': 0.288570665268819, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:21,419] Trial 1046 finished with value: 0.793444437611463 and parameters: {'iterations': 816, 'learning_rate': 0.08868347135863322, 'num_leaves': 640, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:22,135] Trial 1047 finished with value: 0.7965640042265095 and parameters: {'iterations': 778, 'learning_rate': 0.2826258033519669, 'num_leaves': 580, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:22,929] Trial 1048 finished with value: 0.793713905552046 and parameters: {'iterations': 800, 'learning_rate': 0.29493229435261026, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 0.5}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:23,766] Trial 1049 finished with value: 0.7989143013680249 and parameters: {'iterations': 887, 'learning_rate': 0.27695700816670626, 'num_leaves': 1320, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:24,584] Trial 1050 finished with value: 0.7989976016235163 and parameters: {'iterations': 843, 'learning_rate': 0.28845985424982734, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:25,504] Trial 1051 finished with value: 0.7981237875340328 and parameters: {'iterations': 751, 'learning_rate': 0.26382825130526344, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:26,293] Trial 1052 finished with value: 0.7970213170385362 and parameters: {'iterations': 827, 'learning_rate': 0.2843741050242972, 'num_leaves': 480, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:27,133] Trial 1053 finished with value: 0.7971046172940276 and parameters: {'iterations': 682, 'learning_rate': 0.2950260531043421, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:27,829] Trial 1054 finished with value: 0.7937351499125067 and parameters: {'iterations': 809, 'learning_rate': 0.2904038655051774, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:28,781] Trial 1055 finished with value: 0.7910516517490258 and parameters: {'iterations': 784, 'learning_rate': 0.279535184257079, 'num_leaves': 610, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:29,893] Trial 1056 finished with value: 0.7953782334755214 and parameters: {'iterations': 765, 'learning_rate': 0.27202440634401703, 'num_leaves': 550, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:31,103] Trial 1057 finished with value: 0.7929446360785146 and parameters: {'iterations': 855, 'learning_rate': 0.298878007293, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:32,006] Trial 1058 finished with value: 0.7936518496570153 and parameters: {'iterations': 730, 'learning_rate': 0.2898409535623673, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:32,896] Trial 1059 finished with value: 0.7990809018790077 and parameters: {'iterations': 790, 'learning_rate': 0.2512918422790427, 'num_leaves': 1200, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:33,605] Trial 1060 finished with value: 0.7960854470540222 and parameters: {'iterations': 813, 'learning_rate': 0.2995864775019504, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 0.4}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:34,247] Trial 1061 finished with value: 0.7979996757439719 and parameters: {'iterations': 743, 'learning_rate': 0.28471666637075455, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 36, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:34,995] Trial 1062 finished with value: 0.7963353478204963 and parameters: {'iterations': 829, 'learning_rate': 0.2934105912145975, 'num_leaves': 650, 'max_depth': 8, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:35,826] Trial 1063 finished with value: 0.7936730940174763 and parameters: {'iterations': 705, 'learning_rate': 0.13809009033753478, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:36,608] Trial 1064 finished with value: 0.7951495770695082 and parameters: {'iterations': 777, 'learning_rate': 0.28091154586400285, 'num_leaves': 1110, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:37,288] Trial 1065 finished with value: 0.7984569885559986 and parameters: {'iterations': 860, 'learning_rate': 0.28904044426359166, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 27, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:38,158] Trial 1066 finished with value: 0.7966473044820009 and parameters: {'iterations': 752, 'learning_rate': 0.24625572524970918, 'num_leaves': 600, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:38,968] Trial 1067 finished with value: 0.7908442397034735 and parameters: {'iterations': 796, 'learning_rate': 0.11685355041186996, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:39,604] Trial 1068 finished with value: 0.791446908666022 and parameters: {'iterations': 834, 'learning_rate': 0.28508966254032503, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 0.4}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:40,305] Trial 1069 finished with value: 0.7949829765585253 and parameters: {'iterations': 767, 'learning_rate': 0.2753337666921021, 'num_leaves': 510, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:41,091] Trial 1070 finished with value: 0.8011192423590185 and parameters: {'iterations': 810, 'learning_rate': 0.2888100734772816, 'num_leaves': 1360, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:41,852] Trial 1071 finished with value: 0.7971046172940276 and parameters: {'iterations': 813, 'learning_rate': 0.29390828310286415, 'num_leaves': 1230, 'max_depth': 10, 'lambda_l1': 21, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:42,924] Trial 1072 finished with value: 0.7973332737000408 and parameters: {'iterations': 804, 'learning_rate': 0.2883323439003937, 'num_leaves': 1400, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:43,998] Trial 1073 finished with value: 0.7934019488905413 and parameters: {'iterations': 841, 'learning_rate': 0.2809041458500483, 'num_leaves': 1360, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:45,122] Trial 1074 finished with value: 0.799704815202017 and parameters: {'iterations': 820, 'learning_rate': 0.29374517589671006, 'num_leaves': 1290, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:45,848] Trial 1075 finished with value: 0.7945256637464989 and parameters: {'iterations': 792, 'learning_rate': 0.283560041055971, 'num_leaves': 1020, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:46,632] Trial 1076 finished with value: 0.7993095582850209 and parameters: {'iterations': 829, 'learning_rate': 0.2897963533710302, 'num_leaves': 1410, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:47,437] Trial 1077 finished with value: 0.7977285306170367 and parameters: {'iterations': 786, 'learning_rate': 0.2770658728995938, 'num_leaves': 1500, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:48,190] Trial 1078 finished with value: 0.7971046172940276 and parameters: {'iterations': 804, 'learning_rate': 0.299421896823099, 'num_leaves': 920, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:48,955] Trial 1079 finished with value: 0.7936518496570153 and parameters: {'iterations': 855, 'learning_rate': 0.1290736177179948, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:49,942] Trial 1080 finished with value: 0.79396380631852 and parameters: {'iterations': 773, 'learning_rate': 0.284078091031896, 'num_leaves': 1400, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:51,047] Trial 1081 finished with value: 0.7978951311280196 and parameters: {'iterations': 822, 'learning_rate': 0.27105574240744557, 'num_leaves': 1300, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:52,182] Trial 1082 finished with value: 0.7989143013680249 and parameters: {'iterations': 839, 'learning_rate': 0.29344215960082287, 'num_leaves': 1430, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:52,954] Trial 1083 finished with value: 0.7946710198970207 and parameters: {'iterations': 799, 'learning_rate': 0.28884236974412214, 'num_leaves': 1460, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:53,797] Trial 1084 finished with value: 0.7971879175495189 and parameters: {'iterations': 729, 'learning_rate': 0.2949075893487045, 'num_leaves': 1350, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:54,631] Trial 1085 finished with value: 0.7947543201525121 and parameters: {'iterations': 764, 'learning_rate': 0.276129244456703, 'num_leaves': 1120, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:55,868] Trial 1086 finished with value: 0.7951283327090473 and parameters: {'iterations': 791, 'learning_rate': 0.28391480815723935, 'num_leaves': 1170, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:56,865] Trial 1087 finished with value: 0.7933398929955107 and parameters: {'iterations': 812, 'learning_rate': 0.29386891998398723, 'num_leaves': 820, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:57,946] Trial 1088 finished with value: 0.7986023447065203 and parameters: {'iterations': 777, 'learning_rate': 0.28726953243889347, 'num_leaves': 670, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:58,683] Trial 1089 finished with value: 0.7986023447065203 and parameters: {'iterations': 755, 'learning_rate': 0.2816766349924618, 'num_leaves': 1470, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:11:59,582] Trial 1090 finished with value: 0.7952949332200301 and parameters: {'iterations': 713, 'learning_rate': 0.2917003933308124, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:00,404] Trial 1091 finished with value: 0.8006406851865312 and parameters: {'iterations': 852, 'learning_rate': 0.28684412978339857, 'num_leaves': 740, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:01,280] Trial 1092 finished with value: 0.7985190444510288 and parameters: {'iterations': 847, 'learning_rate': 0.282730150498088, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:02,102] Trial 1093 finished with value: 0.8004740846755483 and parameters: {'iterations': 881, 'learning_rate': 0.2763703623545234, 'num_leaves': 620, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:02,770] Trial 1094 finished with value: 0.7968972052484752 and parameters: {'iterations': 868, 'learning_rate': 0.2702347509527079, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 24, 'lambda_l2': 51, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:03,628] Trial 1095 finished with value: 0.8006406851865312 and parameters: {'iterations': 895, 'learning_rate': 0.2856952913883897, 'num_leaves': 740, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:04,435] Trial 1096 finished with value: 0.7960854470540222 and parameters: {'iterations': 868, 'learning_rate': 0.2783407370363634, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:05,366] Trial 1097 finished with value: 0.7963353478204963 and parameters: {'iterations': 868, 'learning_rate': 0.29903477630933706, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:06,201] Trial 1098 finished with value: 0.8005573849310398 and parameters: {'iterations': 850, 'learning_rate': 0.28984388879198575, 'num_leaves': 830, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:07,028] Trial 1099 finished with value: 0.7974165739555322 and parameters: {'iterations': 846, 'learning_rate': 0.2885505021259639, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:07,633] Trial 1100 finished with value: 0.7902203263804641 and parameters: {'iterations': 831, 'learning_rate': 0.2658113887270945, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 57, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:08,834] Trial 1101 finished with value: 0.800100072119013 and parameters: {'iterations': 911, 'learning_rate': 0.28074993601662934, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:10,029] Trial 1102 finished with value: 0.8004120287805178 and parameters: {'iterations': 859, 'learning_rate': 0.2937421960634901, 'num_leaves': 600, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:11,142] Trial 1103 finished with value: 0.8022217128545153 and parameters: {'iterations': 817, 'learning_rate': 0.2741193029425086, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:11,862] Trial 1104 finished with value: 0.7947543201525121 and parameters: {'iterations': 828, 'learning_rate': 0.2751554685005673, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:12,600] Trial 1105 finished with value: 0.7915089645610525 and parameters: {'iterations': 817, 'learning_rate': 0.29983992467442533, 'num_leaves': 880, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:13,384] Trial 1106 finished with value: 0.7959188465430393 and parameters: {'iterations': 808, 'learning_rate': 0.2594982801895193, 'num_leaves': 900, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:14,129] Trial 1107 finished with value: 0.7953782334755214 and parameters: {'iterations': 794, 'learning_rate': 0.27447364069532826, 'num_leaves': 1010, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:14,909] Trial 1108 finished with value: 0.7942757629800247 and parameters: {'iterations': 776, 'learning_rate': 0.26896137577268137, 'num_leaves': 1050, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:15,691] Trial 1109 finished with value: 0.7937351499125067 and parameters: {'iterations': 821, 'learning_rate': 0.2783745273747655, 'num_leaves': 950, 'max_depth': 8, 'lambda_l1': 12, 'lambda_l2': 57, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:16,471] Trial 1110 finished with value: 0.7931732924845278 and parameters: {'iterations': 800, 'learning_rate': 0.254322238811485, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:17,077] Trial 1111 finished with value: 0.78997042561399 and parameters: {'iterations': 809, 'learning_rate': 0.26274061466601845, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 75, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:17,779] Trial 1112 finished with value: 0.7954615337310129 and parameters: {'iterations': 833, 'learning_rate': 0.2734217661514626, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:18,560] Trial 1113 finished with value: 0.7912394966204697 and parameters: {'iterations': 780, 'learning_rate': 0.08119161921313095, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 75, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:19,423] Trial 1114 finished with value: 0.7963974037155269 and parameters: {'iterations': 748, 'learning_rate': 0.2415028660561735, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:20,218] Trial 1115 finished with value: 0.7993928585405125 and parameters: {'iterations': 790, 'learning_rate': 0.2824409008951603, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:20,943] Trial 1116 finished with value: 0.77528721816281 and parameters: {'iterations': 815, 'learning_rate': 0.013368712425300217, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:22,088] Trial 1117 finished with value: 0.8011192423590185 and parameters: {'iterations': 771, 'learning_rate': 0.28833323075118217, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:23,402] Trial 1118 finished with value: 0.7960854470540222 and parameters: {'iterations': 762, 'learning_rate': 0.28050666087495124, 'num_leaves': 960, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:24,491] Trial 1119 finished with value: 0.7952949332200301 and parameters: {'iterations': 735, 'learning_rate': 0.2885798302136898, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:25,244] Trial 1120 finished with value: 0.7973332737000408 and parameters: {'iterations': 763, 'learning_rate': 0.2841812017093453, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:26,045] Trial 1121 finished with value: 0.7970213170385362 and parameters: {'iterations': 770, 'learning_rate': 0.294350023237331, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:26,935] Trial 1122 finished with value: 0.7985190444510288 and parameters: {'iterations': 744, 'learning_rate': 0.26984692968517443, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:27,583] Trial 1123 finished with value: 0.7957947347529785 and parameters: {'iterations': 783, 'learning_rate': 0.29004354015857997, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 45, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:28,428] Trial 1124 finished with value: 0.7967926606325229 and parameters: {'iterations': 789, 'learning_rate': 0.2997309158589586, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:29,166] Trial 1125 finished with value: 0.7947543201525121 and parameters: {'iterations': 753, 'learning_rate': 0.2806767588802314, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:30,017] Trial 1126 finished with value: 0.7982070877895242 and parameters: {'iterations': 802, 'learning_rate': 0.286885043545467, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:30,802] Trial 1127 finished with value: 0.797978431383511 and parameters: {'iterations': 718, 'learning_rate': 0.2750472904733371, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:31,709] Trial 1128 finished with value: 0.7974165739555322 and parameters: {'iterations': 784, 'learning_rate': 0.2944049998106474, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:32,498] Trial 1129 finished with value: 0.8007239854420224 and parameters: {'iterations': 773, 'learning_rate': 0.28399166890867655, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:33,344] Trial 1130 finished with value: 0.7984357441955376 and parameters: {'iterations': 811, 'learning_rate': 0.27849415923714105, 'num_leaves': 1050, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:34,134] Trial 1131 finished with value: 0.7990809018790077 and parameters: {'iterations': 828, 'learning_rate': 0.2907948285553069, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:35,239] Trial 1132 finished with value: 0.7983736883005071 and parameters: {'iterations': 737, 'learning_rate': 0.2946407693791686, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:36,276] Trial 1133 finished with value: 0.7940471065740113 and parameters: {'iterations': 800, 'learning_rate': 0.2652225250258689, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:37,515] Trial 1134 finished with value: 0.7948163760475426 and parameters: {'iterations': 757, 'learning_rate': 0.28546147240551156, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:38,294] Trial 1135 finished with value: 0.8006406851865312 and parameters: {'iterations': 699, 'learning_rate': 0.29016333283695783, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:39,050] Trial 1136 finished with value: 0.7959400909035003 and parameters: {'iterations': 820, 'learning_rate': 0.27172727216965076, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:39,894] Trial 1137 finished with value: 0.7986023447065203 and parameters: {'iterations': 772, 'learning_rate': 0.2995367214134726, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:40,823] Trial 1138 finished with value: 0.7982070877895242 and parameters: {'iterations': 794, 'learning_rate': 0.2795325062305176, 'num_leaves': 1340, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:41,541] Trial 1139 finished with value: 0.7945877196415294 and parameters: {'iterations': 832, 'learning_rate': 0.2862141279782874, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:42,385] Trial 1140 finished with value: 0.7986023447065203 and parameters: {'iterations': 668, 'learning_rate': 0.2948994035411705, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:43,184] Trial 1141 finished with value: 0.799850171352539 and parameters: {'iterations': 730, 'learning_rate': 0.29018848644749823, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:44,008] Trial 1142 finished with value: 0.7967926606325229 and parameters: {'iterations': 775, 'learning_rate': 0.23577715917087969, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:44,647] Trial 1143 finished with value: 0.7901157817645119 and parameters: {'iterations': 748, 'learning_rate': 0.2291473980553742, 'num_leaves': 2980, 'max_depth': 11, 'lambda_l1': 78, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:45,602] Trial 1144 finished with value: 0.7932353483795584 and parameters: {'iterations': 802, 'learning_rate': 0.28243362597141636, 'num_leaves': 1440, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:46,448] Trial 1145 finished with value: 0.799850171352539 and parameters: {'iterations': 815, 'learning_rate': 0.2910732847898444, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:47,135] Trial 1146 finished with value: 0.7920920663494921 and parameters: {'iterations': 785, 'learning_rate': 0.2743993501229319, 'num_leaves': 540, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:48,068] Trial 1147 finished with value: 0.794920920663495 and parameters: {'iterations': 835, 'learning_rate': 0.2951475798665109, 'num_leaves': 1240, 'max_depth': 8, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:49,147] Trial 1148 finished with value: 0.7953782334755214 and parameters: {'iterations': 759, 'learning_rate': 0.2998732176002286, 'num_leaves': 440, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:50,416] Trial 1149 finished with value: 0.7968139049929838 and parameters: {'iterations': 842, 'learning_rate': 0.24544367263785594, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:51,344] Trial 1150 finished with value: 0.7952949332200301 and parameters: {'iterations': 804, 'learning_rate': 0.2880146549133045, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:52,121] Trial 1151 finished with value: 0.8018264559375193 and parameters: {'iterations': 780, 'learning_rate': 0.28242375769303574, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:52,811] Trial 1152 finished with value: 0.7956281342419956 and parameters: {'iterations': 774, 'learning_rate': 0.2764018315247672, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 21, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:53,559] Trial 1153 finished with value: 0.7968759608880143 and parameters: {'iterations': 756, 'learning_rate': 0.27088721361393125, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:54,304] Trial 1154 finished with value: 0.7974998742110235 and parameters: {'iterations': 789, 'learning_rate': 0.2794416780542062, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:55,111] Trial 1155 finished with value: 0.7974165739555322 and parameters: {'iterations': 738, 'learning_rate': 0.28434091353822327, 'num_leaves': 1080, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:55,869] Trial 1156 finished with value: 0.7990809018790077 and parameters: {'iterations': 774, 'learning_rate': 0.27956510052232963, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:56,590] Trial 1157 finished with value: 0.7964807039710183 and parameters: {'iterations': 791, 'learning_rate': 0.28266116011236025, 'num_leaves': 870, 'max_depth': 9, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:57,382] Trial 1158 finished with value: 0.7927992799279928 and parameters: {'iterations': 814, 'learning_rate': 0.2707814425547281, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:58,121] Trial 1159 finished with value: 0.7957734903925175 and parameters: {'iterations': 763, 'learning_rate': 0.2599862629172431, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:58,756] Trial 1160 finished with value: 0.7916347535374656 and parameters: {'iterations': 723, 'learning_rate': 0.28879732163687066, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 54, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:12:59,549] Trial 1161 finished with value: 0.7954615337310129 and parameters: {'iterations': 798, 'learning_rate': 0.2771742424214857, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:00,346] Trial 1162 finished with value: 0.7981237875340328 and parameters: {'iterations': 749, 'learning_rate': 0.285583292911637, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:01,075] Trial 1163 finished with value: 0.7889300110135238 and parameters: {'iterations': 783, 'learning_rate': 0.2927153805960859, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 87, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:02,112] Trial 1164 finished with value: 0.7974998742110235 and parameters: {'iterations': 827, 'learning_rate': 0.28474181335956666, 'num_leaves': 1000, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:03,284] Trial 1165 finished with value: 0.7975831744665149 and parameters: {'iterations': 805, 'learning_rate': 0.26662496911642886, 'num_leaves': 1510, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:04,293] Trial 1166 finished with value: 0.8007239854420224 and parameters: {'iterations': 759, 'learning_rate': 0.29398590582754214, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:05,024] Trial 1167 finished with value: 0.7946710198970207 and parameters: {'iterations': 774, 'learning_rate': 0.2789941175466107, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:05,760] Trial 1168 finished with value: 0.7982070877895242 and parameters: {'iterations': 817, 'learning_rate': 0.291885890938638, 'num_leaves': 1370, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:06,617] Trial 1169 finished with value: 0.7996215149465257 and parameters: {'iterations': 794, 'learning_rate': 0.2859418178778814, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:07,407] Trial 1170 finished with value: 0.7967306047374924 and parameters: {'iterations': 745, 'learning_rate': 0.2778535649174607, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:08,308] Trial 1171 finished with value: 0.79662606012154 and parameters: {'iterations': 838, 'learning_rate': 0.29489710152497667, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:09,279] Trial 1172 finished with value: 0.7960854470540222 and parameters: {'iterations': 784, 'learning_rate': 0.2883168737194379, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:10,126] Trial 1173 finished with value: 0.7970213170385362 and parameters: {'iterations': 712, 'learning_rate': 0.2731513360689952, 'num_leaves': 1560, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:10,887] Trial 1174 finished with value: 0.7960021467985308 and parameters: {'iterations': 817, 'learning_rate': 0.29989288841711087, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:11,670] Trial 1175 finished with value: 0.8017431556820278 and parameters: {'iterations': 730, 'learning_rate': 0.2858853230903651, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:12,337] Trial 1176 finished with value: 0.794109162469042 and parameters: {'iterations': 679, 'learning_rate': 0.2822236352052264, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.30000000000000004}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:13,127] Trial 1177 finished with value: 0.7960021467985308 and parameters: {'iterations': 696, 'learning_rate': 0.28347245739440835, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:14,077] Trial 1178 finished with value: 0.8014311990205232 and parameters: {'iterations': 725, 'learning_rate': 0.2730585971115703, 'num_leaves': 820, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:15,269] Trial 1179 finished with value: 0.8015144992760146 and parameters: {'iterations': 695, 'learning_rate': 0.26628668376025716, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:16,388] Trial 1180 finished with value: 0.7948163760475426 and parameters: {'iterations': 684, 'learning_rate': 0.25598256615712944, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:17,372] Trial 1181 finished with value: 0.7957734903925175 and parameters: {'iterations': 660, 'learning_rate': 0.2656728063486407, 'num_leaves': 880, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:18,220] Trial 1182 finished with value: 0.8022217128545153 and parameters: {'iterations': 708, 'learning_rate': 0.26312566707552826, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:19,015] Trial 1183 finished with value: 0.7987689452175031 and parameters: {'iterations': 686, 'learning_rate': 0.26194657143285294, 'num_leaves': 880, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:19,883] Trial 1184 finished with value: 0.7967306047374924 and parameters: {'iterations': 678, 'learning_rate': 0.26807071670836863, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:20,541] Trial 1185 finished with value: 0.7955660783469651 and parameters: {'iterations': 643, 'learning_rate': 0.25547489456627004, 'num_leaves': 920, 'max_depth': 10, 'lambda_l1': 48, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:21,410] Trial 1186 finished with value: 0.7963353478204963 and parameters: {'iterations': 723, 'learning_rate': 0.2626419311841017, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:22,173] Trial 1187 finished with value: 0.799850171352539 and parameters: {'iterations': 701, 'learning_rate': 0.26695277171536713, 'num_leaves': 990, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:22,968] Trial 1188 finished with value: 0.7963974037155269 and parameters: {'iterations': 694, 'learning_rate': 0.26149161967367124, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:23,806] Trial 1189 finished with value: 0.7978118308725282 and parameters: {'iterations': 712, 'learning_rate': 0.26829710586425576, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:24,601] Trial 1190 finished with value: 0.7978330752329892 and parameters: {'iterations': 723, 'learning_rate': 0.27101742464864437, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:25,416] Trial 1191 finished with value: 0.790906295598504 and parameters: {'iterations': 662, 'learning_rate': 0.2658481827801462, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:26,269] Trial 1192 finished with value: 0.7943590632355161 and parameters: {'iterations': 694, 'learning_rate': 0.258880724283522, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:26,919] Trial 1193 finished with value: 0.7980829759994633 and parameters: {'iterations': 698, 'learning_rate': 0.27088412247367644, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 36, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:27,981] Trial 1194 finished with value: 0.7971879175495189 and parameters: {'iterations': 713, 'learning_rate': 0.27313712408266444, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:29,116] Trial 1195 finished with value: 0.7952949332200301 and parameters: {'iterations': 680, 'learning_rate': 0.25796468955765917, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:30,243] Trial 1196 finished with value: 0.7978118308725282 and parameters: {'iterations': 717, 'learning_rate': 0.2634325600551758, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:30,969] Trial 1197 finished with value: 0.7965640042265095 and parameters: {'iterations': 663, 'learning_rate': 0.2734665235042046, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:31,754] Trial 1198 finished with value: 0.7978118308725282 and parameters: {'iterations': 702, 'learning_rate': 0.2736946113528821, 'num_leaves': 740, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:32,606] Trial 1199 finished with value: 0.7963974037155269 and parameters: {'iterations': 703, 'learning_rate': 0.27018172832511417, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:33,356] Trial 1200 finished with value: 0.7974165739555322 and parameters: {'iterations': 722, 'learning_rate': 0.26319782063929315, 'num_leaves': 1000, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:34,202] Trial 1201 finished with value: 0.7950875211744778 and parameters: {'iterations': 676, 'learning_rate': 0.2742740834421591, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:35,006] Trial 1202 finished with value: 0.7964807039710183 and parameters: {'iterations': 703, 'learning_rate': 0.2773295690198367, 'num_leaves': 740, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:35,861] Trial 1203 finished with value: 0.7970425613989971 and parameters: {'iterations': 710, 'learning_rate': 0.26752523305729475, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:36,635] Trial 1204 finished with value: 0.7934019488905413 and parameters: {'iterations': 690, 'learning_rate': 0.2569389427853746, 'num_leaves': 900, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:37,432] Trial 1205 finished with value: 0.7967926606325229 and parameters: {'iterations': 724, 'learning_rate': 0.27493300978515167, 'num_leaves': 1040, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:38,276] Trial 1206 finished with value: 0.8007452298024834 and parameters: {'iterations': 712, 'learning_rate': 0.26958835681200916, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:39,123] Trial 1207 finished with value: 0.7965019483314792 and parameters: {'iterations': 736, 'learning_rate': 0.27633091414952143, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:39,931] Trial 1208 finished with value: 0.7954615337310129 and parameters: {'iterations': 729, 'learning_rate': 0.2624737723230004, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:40,990] Trial 1209 finished with value: 0.7994761587960039 and parameters: {'iterations': 730, 'learning_rate': 0.2767782378405589, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:42,145] Trial 1210 finished with value: 0.8005786292915006 and parameters: {'iterations': 651, 'learning_rate': 0.2542032657519721, 'num_leaves': 890, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:43,257] Trial 1211 finished with value: 0.7928613358230233 and parameters: {'iterations': 679, 'learning_rate': 0.26658404079363285, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:44,010] Trial 1212 finished with value: 0.7989143013680249 and parameters: {'iterations': 694, 'learning_rate': 0.2800639887276151, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:44,816] Trial 1213 finished with value: 0.7971046172940276 and parameters: {'iterations': 729, 'learning_rate': 0.2706164973915745, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:45,591] Trial 1214 finished with value: 0.7951495770695082 and parameters: {'iterations': 709, 'learning_rate': 0.24746161980551465, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:46,277] Trial 1215 finished with value: 0.7916347535374656 and parameters: {'iterations': 628, 'learning_rate': 0.2802170556065175, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 51, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:46,949] Trial 1216 finished with value: 0.7963353478204963 and parameters: {'iterations': 670, 'learning_rate': 0.2517450596147669, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 39, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:47,797] Trial 1217 finished with value: 0.7971046172940276 and parameters: {'iterations': 740, 'learning_rate': 0.2802388861889871, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:48,662] Trial 1218 finished with value: 0.7968759608880143 and parameters: {'iterations': 691, 'learning_rate': 0.27380848744036895, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:49,584] Trial 1219 finished with value: 0.7974998742110235 and parameters: {'iterations': 739, 'learning_rate': 0.26588096759377927, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:50,399] Trial 1220 finished with value: 0.7953782334755214 and parameters: {'iterations': 734, 'learning_rate': 0.27892361900851564, 'num_leaves': 690, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:51,265] Trial 1221 finished with value: 0.7954615337310129 and parameters: {'iterations': 709, 'learning_rate': 0.2605821429266172, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:52,042] Trial 1222 finished with value: 0.8019930564485022 and parameters: {'iterations': 709, 'learning_rate': 0.27286908172011576, 'num_leaves': 950, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:52,842] Trial 1223 finished with value: 0.7985190444510288 and parameters: {'iterations': 685, 'learning_rate': 0.2724937373121264, 'num_leaves': 1040, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:53,868] Trial 1224 finished with value: 0.7948163760475426 and parameters: {'iterations': 715, 'learning_rate': 0.2679205088638273, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:54,962] Trial 1225 finished with value: 0.7968139049929838 and parameters: {'iterations': 692, 'learning_rate': 0.2744405574015711, 'num_leaves': 1030, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:56,004] Trial 1226 finished with value: 0.7938805060630287 and parameters: {'iterations': 704, 'learning_rate': 0.26393614147666705, 'num_leaves': 990, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:56,682] Trial 1227 finished with value: 0.7959613352639612 and parameters: {'iterations': 655, 'learning_rate': 0.2697123625225596, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 33, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:57,419] Trial 1228 finished with value: 0.7937972058075372 and parameters: {'iterations': 724, 'learning_rate': 0.279692255228025, 'num_leaves': 970, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:58,212] Trial 1229 finished with value: 0.7956901901370262 and parameters: {'iterations': 712, 'learning_rate': 0.257094873326311, 'num_leaves': 1100, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:58,960] Trial 1230 finished with value: 0.7975831744665149 and parameters: {'iterations': 741, 'learning_rate': 0.2740082645883433, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:13:59,759] Trial 1231 finished with value: 0.8004953290360091 and parameters: {'iterations': 667, 'learning_rate': 0.2809287024860045, 'num_leaves': 1010, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:00,574] Trial 1232 finished with value: 0.7945877196415294 and parameters: {'iterations': 715, 'learning_rate': 0.2645956680473642, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:01,381] Trial 1233 finished with value: 0.7989143013680249 and parameters: {'iterations': 724, 'learning_rate': 0.2770350226209325, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:02,104] Trial 1234 finished with value: 0.794504419386038 and parameters: {'iterations': 677, 'learning_rate': 0.2844860240736054, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:02,862] Trial 1235 finished with value: 0.7956068898815348 and parameters: {'iterations': 695, 'learning_rate': 0.2709336247791738, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:03,668] Trial 1236 finished with value: 0.7974998742110235 and parameters: {'iterations': 741, 'learning_rate': 0.2817459428866924, 'num_leaves': 1070, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:04,356] Trial 1237 finished with value: 0.7979163754884805 and parameters: {'iterations': 701, 'learning_rate': 0.27631601862558736, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:05,094] Trial 1238 finished with value: 0.79662606012154 and parameters: {'iterations': 746, 'learning_rate': 0.28389173097148274, 'num_leaves': 890, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:05,883] Trial 1239 finished with value: 0.8020551123435326 and parameters: {'iterations': 714, 'learning_rate': 0.26812453187497826, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:06,895] Trial 1240 finished with value: 0.7952328773249995 and parameters: {'iterations': 680, 'learning_rate': 0.2689362896824604, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:07,963] Trial 1241 finished with value: 0.7944423634910074 and parameters: {'iterations': 701, 'learning_rate': 0.26171139890411393, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:09,069] Trial 1242 finished with value: 0.7971879175495189 and parameters: {'iterations': 739, 'learning_rate': 0.2719079961021696, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:10,019] Trial 1243 finished with value: 0.7958567906480088 and parameters: {'iterations': 720, 'learning_rate': 0.25278139008230005, 'num_leaves': 640, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:10,618] Trial 1244 finished with value: 0.7902203263804641 and parameters: {'iterations': 687, 'learning_rate': 0.2677443185367323, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 72, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:11,332] Trial 1245 finished with value: 0.7952328773249995 and parameters: {'iterations': 718, 'learning_rate': 0.260655873862317, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:12,120] Trial 1246 finished with value: 0.7955235896260434 and parameters: {'iterations': 705, 'learning_rate': 0.258271512843116, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:12,862] Trial 1247 finished with value: 0.7937972058075372 and parameters: {'iterations': 669, 'learning_rate': 0.26539376828139416, 'num_leaves': 640, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:13,668] Trial 1248 finished with value: 0.7957734903925175 and parameters: {'iterations': 721, 'learning_rate': 0.25410304858337185, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:14,452] Trial 1249 finished with value: 0.7978951311280196 and parameters: {'iterations': 685, 'learning_rate': 0.2663612248690385, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:15,190] Trial 1250 finished with value: 0.7952949332200301 and parameters: {'iterations': 699, 'learning_rate': 0.2727969198045248, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:15,993] Trial 1251 finished with value: 0.79662606012154 and parameters: {'iterations': 724, 'learning_rate': 0.2621295312686367, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:16,746] Trial 1252 finished with value: 0.7944423634910074 and parameters: {'iterations': 661, 'learning_rate': 0.27648837027926404, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:17,538] Trial 1253 finished with value: 0.7931112365894974 and parameters: {'iterations': 639, 'learning_rate': 0.270655418190091, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:18,348] Trial 1254 finished with value: 0.797978431383511 and parameters: {'iterations': 855, 'learning_rate': 0.2589537450145479, 'num_leaves': 740, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:19,249] Trial 1255 finished with value: 0.7984357441955376 and parameters: {'iterations': 705, 'learning_rate': 0.25149800987978643, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:20,273] Trial 1256 finished with value: 0.7934852491460327 and parameters: {'iterations': 733, 'learning_rate': 0.27760448642802416, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:21,406] Trial 1257 finished with value: 0.7992262580295296 and parameters: {'iterations': 712, 'learning_rate': 0.2664183792305178, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:22,447] Trial 1258 finished with value: 0.7963974037155269 and parameters: {'iterations': 934, 'learning_rate': 0.27652821172902625, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:23,182] Trial 1259 finished with value: 0.7952949332200301 and parameters: {'iterations': 734, 'learning_rate': 0.2727539356885537, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:24,025] Trial 1260 finished with value: 0.8027623259220331 and parameters: {'iterations': 880, 'learning_rate': 0.2829228838457025, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:24,857] Trial 1261 finished with value: 0.794899676303034 and parameters: {'iterations': 870, 'learning_rate': 0.26862727527470825, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:25,713] Trial 1262 finished with value: 0.7985190444510288 and parameters: {'iterations': 853, 'learning_rate': 0.2799034960120367, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:26,561] Trial 1263 finished with value: 0.7962520475650049 and parameters: {'iterations': 892, 'learning_rate': 0.2630902059362059, 'num_leaves': 820, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:27,287] Trial 1264 finished with value: 0.7890133112690151 and parameters: {'iterations': 864, 'learning_rate': 0.27467401508990674, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.4}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:27,825] Trial 1265 finished with value: 0.7938592617025677 and parameters: {'iterations': 878, 'learning_rate': 0.2818405316325331, 'num_leaves': 600, 'max_depth': 9, 'lambda_l1': 60, 'lambda_l2': 84, 'feature_fraction': 0.2}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:28,662] Trial 1266 finished with value: 0.8023883133654981 and parameters: {'iterations': 686, 'learning_rate': 0.2700804137908099, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:29,577] Trial 1267 finished with value: 0.7946089640019903 and parameters: {'iterations': 642, 'learning_rate': 0.2590544257979145, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:30,429] Trial 1268 finished with value: 0.7985190444510288 and parameters: {'iterations': 670, 'learning_rate': 0.2647901363982831, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:31,264] Trial 1269 finished with value: 0.7974998742110235 and parameters: {'iterations': 676, 'learning_rate': 0.2694288275384094, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:31,877] Trial 1270 finished with value: 0.7911561963649781 and parameters: {'iterations': 659, 'learning_rate': 0.2568430658418865, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 66, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:33,042] Trial 1271 finished with value: 0.7971258616544884 and parameters: {'iterations': 712, 'learning_rate': 0.2651526364291202, 'num_leaves': 950, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:34,275] Trial 1272 finished with value: 0.794380307595977 and parameters: {'iterations': 929, 'learning_rate': 0.272332120241271, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:35,533] Trial 1273 finished with value: 0.7956068898815348 and parameters: {'iterations': 686, 'learning_rate': 0.24988400674680245, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:36,393] Trial 1274 finished with value: 0.7972712178050104 and parameters: {'iterations': 688, 'learning_rate': 0.26995657491059133, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:37,227] Trial 1275 finished with value: 0.7979163754884805 and parameters: {'iterations': 703, 'learning_rate': 0.2670976333801667, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:38,145] Trial 1276 finished with value: 0.8000380162239827 and parameters: {'iterations': 653, 'learning_rate': 0.2768892731634334, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:38,990] Trial 1277 finished with value: 0.7982070877895242 and parameters: {'iterations': 696, 'learning_rate': 0.27511186459452297, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:39,883] Trial 1278 finished with value: 0.7980617316390025 and parameters: {'iterations': 686, 'learning_rate': 0.26549804438855495, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:40,495] Trial 1279 finished with value: 0.791446908666022 and parameters: {'iterations': 671, 'learning_rate': 0.25864658237938687, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 93, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:41,396] Trial 1280 finished with value: 0.7931112365894974 and parameters: {'iterations': 726, 'learning_rate': 0.2724412750815163, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:42,167] Trial 1281 finished with value: 0.7951495770695082 and parameters: {'iterations': 688, 'learning_rate': 0.2785281281225646, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:42,906] Trial 1282 finished with value: 0.7934852491460327 and parameters: {'iterations': 712, 'learning_rate': 0.27016085722352623, 'num_leaves': 680, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 0.5}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:43,697] Trial 1283 finished with value: 0.795316177580491 and parameters: {'iterations': 895, 'learning_rate': 0.2639059884326071, 'num_leaves': 980, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:44,533] Trial 1284 finished with value: 0.7993928585405125 and parameters: {'iterations': 925, 'learning_rate': 0.2807847169512731, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:45,360] Trial 1285 finished with value: 0.7965640042265095 and parameters: {'iterations': 729, 'learning_rate': 0.24975819719447773, 'num_leaves': 890, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:46,342] Trial 1286 finished with value: 0.7980617316390025 and parameters: {'iterations': 699, 'learning_rate': 0.2709406069573446, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 24, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:47,529] Trial 1287 finished with value: 0.7884106423064666 and parameters: {'iterations': 618, 'learning_rate': 0.0648723793924111, 'num_leaves': 720, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:48,461] Trial 1288 finished with value: 0.7859982892699208 and parameters: {'iterations': 895, 'learning_rate': 0.03929387725604867, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 30, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:49,367] Trial 1289 finished with value: 0.7941304068295029 and parameters: {'iterations': 916, 'learning_rate': 0.2599140788845321, 'num_leaves': 640, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:50,185] Trial 1290 finished with value: 0.800100072119013 and parameters: {'iterations': 737, 'learning_rate': 0.27650682423548, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:51,034] Trial 1291 finished with value: 0.7956281342419956 and parameters: {'iterations': 747, 'learning_rate': 0.27980624678131044, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:51,945] Trial 1292 finished with value: 0.7981237875340328 and parameters: {'iterations': 669, 'learning_rate': 0.2551347986266993, 'num_leaves': 950, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:52,735] Trial 1293 finished with value: 0.8007239854420224 and parameters: {'iterations': 719, 'learning_rate': 0.2844455143227818, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:53,560] Trial 1294 finished with value: 0.7993095582850209 and parameters: {'iterations': 912, 'learning_rate': 0.27330458428791504, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:54,319] Trial 1295 finished with value: 0.7950662768140169 and parameters: {'iterations': 694, 'learning_rate': 0.2680743305864576, 'num_leaves': 770, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.7}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:55,186] Trial 1296 finished with value: 0.7967093603770316 and parameters: {'iterations': 710, 'learning_rate': 0.2833435221556244, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:56,087] Trial 1297 finished with value: 0.7929233917180539 and parameters: {'iterations': 743, 'learning_rate': 0.2764336674187516, 'num_leaves': 580, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:56,870] Trial 1298 finished with value: 0.7953782334755214 and parameters: {'iterations': 749, 'learning_rate': 0.26045018469706044, 'num_leaves': 640, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:57,727] Trial 1299 finished with value: 0.7986856449620117 and parameters: {'iterations': 673, 'learning_rate': 0.28478097794340357, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:58,641] Trial 1300 finished with value: 0.7985402888114899 and parameters: {'iterations': 720, 'learning_rate': 0.2734680448168654, 'num_leaves': 820, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:14:59,710] Trial 1301 finished with value: 0.7959188465430393 and parameters: {'iterations': 641, 'learning_rate': 0.279455376864351, 'num_leaves': 1010, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:01,086] Trial 1302 finished with value: 0.7985190444510288 and parameters: {'iterations': 698, 'learning_rate': 0.26353268182452827, 'num_leaves': 890, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:02,003] Trial 1303 finished with value: 0.7974998742110235 and parameters: {'iterations': 755, 'learning_rate': 0.2830095661580378, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:02,789] Trial 1304 finished with value: 0.794109162469042 and parameters: {'iterations': 886, 'learning_rate': 0.27152198532241634, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:03,636] Trial 1305 finished with value: 0.7979163754884805 and parameters: {'iterations': 718, 'learning_rate': 0.26706058333177984, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:04,553] Trial 1306 finished with value: 0.794899676303034 and parameters: {'iterations': 753, 'learning_rate': 0.27746868688651083, 'num_leaves': 960, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:05,385] Trial 1307 finished with value: 0.7960233911589917 and parameters: {'iterations': 728, 'learning_rate': 0.28652928102558173, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:06,234] Trial 1308 finished with value: 0.79396380631852 and parameters: {'iterations': 697, 'learning_rate': 0.24957258685144706, 'num_leaves': 610, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:07,002] Trial 1309 finished with value: 0.7967093603770316 and parameters: {'iterations': 676, 'learning_rate': 0.2825789337111353, 'num_leaves': 900, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:07,914] Trial 1310 finished with value: 0.7939017504234896 and parameters: {'iterations': 739, 'learning_rate': 0.26106644832741405, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:08,712] Trial 1311 finished with value: 0.7964807039710183 and parameters: {'iterations': 658, 'learning_rate': 0.27437198583169103, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:09,577] Trial 1312 finished with value: 0.7974998742110235 and parameters: {'iterations': 948, 'learning_rate': 0.2832473754054823, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:10,394] Trial 1313 finished with value: 0.799704815202017 and parameters: {'iterations': 708, 'learning_rate': 0.2681786041096864, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:11,169] Trial 1314 finished with value: 0.7941924627245333 and parameters: {'iterations': 722, 'learning_rate': 0.278898521467776, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 39, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:12,441] Trial 1315 finished with value: 0.7963141034600354 and parameters: {'iterations': 751, 'learning_rate': 0.2864445436029578, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:13,727] Trial 1316 finished with value: 0.7992475023899906 and parameters: {'iterations': 696, 'learning_rate': 0.27334641144737704, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:14,720] Trial 1317 finished with value: 0.7989976016235163 and parameters: {'iterations': 878, 'learning_rate': 0.2660650231670142, 'num_leaves': 640, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:15,901] Trial 1318 finished with value: 0.7950042209189864 and parameters: {'iterations': 904, 'learning_rate': 0.27926455048899007, 'num_leaves': 990, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:17,081] Trial 1319 finished with value: 0.7959188465430393 and parameters: {'iterations': 680, 'learning_rate': 0.28539135619699646, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:18,113] Trial 1320 finished with value: 0.7941924627245333 and parameters: {'iterations': 729, 'learning_rate': 0.25691236827639347, 'num_leaves': 570, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:19,034] Trial 1321 finished with value: 0.7960021467985308 and parameters: {'iterations': 757, 'learning_rate': 0.2871900484051692, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:19,869] Trial 1322 finished with value: 0.791446908666022 and parameters: {'iterations': 707, 'learning_rate': 0.2742678357870224, 'num_leaves': 700, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:20,691] Trial 1323 finished with value: 0.7907396950875212 and parameters: {'iterations': 737, 'learning_rate': 0.09628885193920658, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:21,506] Trial 1324 finished with value: 0.7932565927400194 and parameters: {'iterations': 762, 'learning_rate': 0.26438178524349865, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:22,263] Trial 1325 finished with value: 0.8000167718635217 and parameters: {'iterations': 691, 'learning_rate': 0.27857065142545007, 'num_leaves': 640, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:23,108] Trial 1326 finished with value: 0.7966473044820009 and parameters: {'iterations': 654, 'learning_rate': 0.2531429366430386, 'num_leaves': 930, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:24,010] Trial 1327 finished with value: 0.7945256637464989 and parameters: {'iterations': 728, 'learning_rate': 0.2710115358750014, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:25,029] Trial 1328 finished with value: 0.7985190444510288 and parameters: {'iterations': 710, 'learning_rate': 0.2863127475892508, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:25,993] Trial 1329 finished with value: 0.7919875217335397 and parameters: {'iterations': 757, 'learning_rate': 0.28191001559251866, 'num_leaves': 1030, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 0.30000000000000004}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:27,053] Trial 1330 finished with value: 0.7975831744665149 and parameters: {'iterations': 845, 'learning_rate': 0.2765113584295876, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:27,974] Trial 1331 finished with value: 0.7978330752329892 and parameters: {'iterations': 684, 'learning_rate': 0.26920120779658974, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:28,782] Trial 1332 finished with value: 0.7879533294944403 and parameters: {'iterations': 738, 'learning_rate': 0.050571970065271726, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:29,564] Trial 1333 finished with value: 0.7974998742110235 and parameters: {'iterations': 732, 'learning_rate': 0.2461878997268427, 'num_leaves': 740, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:30,394] Trial 1334 finished with value: 0.7966473044820009 and parameters: {'iterations': 707, 'learning_rate': 0.28914282092402815, 'num_leaves': 100, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:31,037] Trial 1335 finished with value: 0.7977497749774978 and parameters: {'iterations': 878, 'learning_rate': 0.26055564297504885, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 42, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:31,813] Trial 1336 finished with value: 0.797978431383511 and parameters: {'iterations': 763, 'learning_rate': 0.2811842537614608, 'num_leaves': 2960, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:32,562] Trial 1337 finished with value: 0.8008072856975138 and parameters: {'iterations': 679, 'learning_rate': 0.2902147572092735, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:33,374] Trial 1338 finished with value: 0.7944423634910074 and parameters: {'iterations': 823, 'learning_rate': 0.10957570365351336, 'num_leaves': 680, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:34,285] Trial 1339 finished with value: 0.7992262580295296 and parameters: {'iterations': 859, 'learning_rate': 0.2730340308857619, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:35,101] Trial 1340 finished with value: 0.794504419386038 and parameters: {'iterations': 719, 'learning_rate': 0.26560947298345494, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:36,011] Trial 1341 finished with value: 0.7961687473095135 and parameters: {'iterations': 749, 'learning_rate': 0.284950585809334, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:36,863] Trial 1342 finished with value: 0.7958567906480088 and parameters: {'iterations': 709, 'learning_rate': 0.2565506264631061, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:37,827] Trial 1343 finished with value: 0.7978118308725282 and parameters: {'iterations': 825, 'learning_rate': 0.2770900242222032, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:38,890] Trial 1344 finished with value: 0.7904277384260165 and parameters: {'iterations': 843, 'learning_rate': 0.28970343035799445, 'num_leaves': 3000, 'max_depth': 9, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:40,067] Trial 1345 finished with value: 0.8028456261775246 and parameters: {'iterations': 630, 'learning_rate': 0.2693429271544626, 'num_leaves': 930, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:41,077] Trial 1346 finished with value: 0.7976664747220064 and parameters: {'iterations': 638, 'learning_rate': 0.26386983282848825, 'num_leaves': 1040, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:41,980] Trial 1347 finished with value: 0.7985190444510288 and parameters: {'iterations': 622, 'learning_rate': 0.2618807706941843, 'num_leaves': 1010, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:42,701] Trial 1348 finished with value: 0.7929446360785146 and parameters: {'iterations': 598, 'learning_rate': 0.26880030121848236, 'num_leaves': 1000, 'max_depth': 10, 'lambda_l1': 21, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:43,423] Trial 1349 finished with value: 0.7950662768140169 and parameters: {'iterations': 586, 'learning_rate': 0.2677177214693979, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:44,272] Trial 1350 finished with value: 0.7950662768140169 and parameters: {'iterations': 614, 'learning_rate': 0.25854427023361726, 'num_leaves': 1140, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:45,088] Trial 1351 finished with value: 0.7989976016235163 and parameters: {'iterations': 592, 'learning_rate': 0.2729764177814042, 'num_leaves': 970, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:45,798] Trial 1352 finished with value: 0.7932565927400194 and parameters: {'iterations': 610, 'learning_rate': 0.2524819977597155, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 0.5}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:46,626] Trial 1353 finished with value: 0.7963974037155269 and parameters: {'iterations': 561, 'learning_rate': 0.2734327865148266, 'num_leaves': 900, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:47,484] Trial 1354 finished with value: 0.7982070877895242 and parameters: {'iterations': 668, 'learning_rate': 0.2630504931967023, 'num_leaves': 980, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 0.8}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:48,413] Trial 1355 finished with value: 0.7980404872785416 and parameters: {'iterations': 645, 'learning_rate': 0.27077668053029086, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:49,252] Trial 1356 finished with value: 0.7935685494015239 and parameters: {'iterations': 599, 'learning_rate': 0.2672295214710889, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:50,154] Trial 1357 finished with value: 0.7984357441955376 and parameters: {'iterations': 650, 'learning_rate': 0.2740090660465342, 'num_leaves': 1050, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:51,142] Trial 1358 finished with value: 0.7968759608880143 and parameters: {'iterations': 622, 'learning_rate': 0.25797012685544235, 'num_leaves': 1060, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:52,269] Trial 1359 finished with value: 0.793194536844989 and parameters: {'iterations': 747, 'learning_rate': 0.2640381535253439, 'num_leaves': 930, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:53,517] Trial 1360 finished with value: 0.7971879175495189 and parameters: {'iterations': 585, 'learning_rate': 0.26835003197635227, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:54,578] Trial 1361 finished with value: 0.794920920663495 and parameters: {'iterations': 725, 'learning_rate': 0.27935831973275776, 'num_leaves': 940, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:55,441] Trial 1362 finished with value: 0.7978951311280196 and parameters: {'iterations': 671, 'learning_rate': 0.27558790253207777, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:56,263] Trial 1363 finished with value: 0.7962520475650049 and parameters: {'iterations': 625, 'learning_rate': 0.2553567948111454, 'num_leaves': 1090, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:57,210] Trial 1364 finished with value: 0.7931112365894974 and parameters: {'iterations': 637, 'learning_rate': 0.2718239297066008, 'num_leaves': 980, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:57,966] Trial 1365 finished with value: 0.7968759608880143 and parameters: {'iterations': 691, 'learning_rate': 0.2774293102592205, 'num_leaves': 880, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:58,726] Trial 1366 finished with value: 0.7952949332200301 and parameters: {'iterations': 732, 'learning_rate': 0.2624207495236424, 'num_leaves': 900, 'max_depth': 7, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:15:59,578] Trial 1367 finished with value: 0.7988522454729946 and parameters: {'iterations': 564, 'learning_rate': 0.24935821755462095, 'num_leaves': 800, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:00,417] Trial 1368 finished with value: 0.8006406851865312 and parameters: {'iterations': 711, 'learning_rate': 0.2686954898543203, 'num_leaves': 810, 'max_depth': 9, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:01,163] Trial 1369 finished with value: 0.7975211185714844 and parameters: {'iterations': 744, 'learning_rate': 0.2443173380772364, 'num_leaves': 990, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:02,073] Trial 1370 finished with value: 0.7909895958539953 and parameters: {'iterations': 919, 'learning_rate': 0.2806575211266662, 'num_leaves': 900, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:02,908] Trial 1371 finished with value: 0.7974998742110235 and parameters: {'iterations': 938, 'learning_rate': 0.2817912944565696, 'num_leaves': 2920, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:03,671] Trial 1372 finished with value: 0.7941304068295029 and parameters: {'iterations': 769, 'learning_rate': 0.27546849971608883, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:04,823] Trial 1373 finished with value: 0.7962520475650049 and parameters: {'iterations': 537, 'learning_rate': 0.270463386326288, 'num_leaves': 770, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:06,193] Trial 1374 finished with value: 0.7968759608880143 and parameters: {'iterations': 656, 'learning_rate': 0.2603903016360946, 'num_leaves': 800, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:07,374] Trial 1375 finished with value: 0.7971879175495189 and parameters: {'iterations': 694, 'learning_rate': 0.28002910293849376, 'num_leaves': 950, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:08,249] Trial 1376 finished with value: 0.7908850512380431 and parameters: {'iterations': 718, 'learning_rate': 0.26670209511088244, 'num_leaves': 830, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 33, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:09,039] Trial 1377 finished with value: 0.7931732924845278 and parameters: {'iterations': 578, 'learning_rate': 0.27578050672590515, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:09,802] Trial 1378 finished with value: 0.7956901901370262 and parameters: {'iterations': 550, 'learning_rate': 0.2835890071720171, 'num_leaves': 880, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:10,749] Trial 1379 finished with value: 0.7961475029490527 and parameters: {'iterations': 757, 'learning_rate': 0.255181935438308, 'num_leaves': 1070, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:11,587] Trial 1380 finished with value: 0.7943590632355161 and parameters: {'iterations': 386, 'learning_rate': 0.2709861936142653, 'num_leaves': 760, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:12,454] Trial 1381 finished with value: 0.79662606012154 and parameters: {'iterations': 734, 'learning_rate': 0.2759243192746743, 'num_leaves': 920, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:13,306] Trial 1382 finished with value: 0.7969592611435057 and parameters: {'iterations': 606, 'learning_rate': 0.26231845399492515, 'num_leaves': 650, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:14,147] Trial 1383 finished with value: 0.7967093603770316 and parameters: {'iterations': 665, 'learning_rate': 0.28457672637351494, 'num_leaves': 840, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:14,960] Trial 1384 finished with value: 0.7981237875340328 and parameters: {'iterations': 697, 'learning_rate': 0.28282234807539947, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:15,914] Trial 1385 finished with value: 0.7961687473095135 and parameters: {'iterations': 907, 'learning_rate': 0.2655419491912402, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:16,601] Trial 1386 finished with value: 0.7972091619099798 and parameters: {'iterations': 729, 'learning_rate': 0.27701556751202994, 'num_leaves': 960, 'max_depth': 10, 'lambda_l1': 33, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:17,690] Trial 1387 finished with value: 0.8000167718635217 and parameters: {'iterations': 750, 'learning_rate': 0.2727505708872905, 'num_leaves': 550, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:18,820] Trial 1388 finished with value: 0.797978431383511 and parameters: {'iterations': 771, 'learning_rate': 0.285172324990959, 'num_leaves': 770, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:20,100] Trial 1389 finished with value: 0.7952116329645387 and parameters: {'iterations': 679, 'learning_rate': 0.27917729954988385, 'num_leaves': 1010, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:20,987] Trial 1390 finished with value: 0.7972712178050104 and parameters: {'iterations': 700, 'learning_rate': 0.26846561853351186, 'num_leaves': 690, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:21,781] Trial 1391 finished with value: 0.7963141034600354 and parameters: {'iterations': 886, 'learning_rate': 0.2424793193656568, 'num_leaves': 1130, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:22,651] Trial 1392 finished with value: 0.7961687473095135 and parameters: {'iterations': 718, 'learning_rate': 0.2886165496440239, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:23,411] Trial 1393 finished with value: 0.7955235896260434 and parameters: {'iterations': 742, 'learning_rate': 0.25816772650195324, 'num_leaves': 2570, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:24,356] Trial 1394 finished with value: 0.7908017509825517 and parameters: {'iterations': 328, 'learning_rate': 0.2717284609183887, 'num_leaves': 610, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 45, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:25,275] Trial 1395 finished with value: 0.7989355457284859 and parameters: {'iterations': 626, 'learning_rate': 0.2494963559478359, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:26,075] Trial 1396 finished with value: 0.7950662768140169 and parameters: {'iterations': 767, 'learning_rate': 0.26460258273770276, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:26,688] Trial 1397 finished with value: 0.7912182522600086 and parameters: {'iterations': 708, 'learning_rate': 0.27908421233537106, 'num_leaves': 640, 'max_depth': 9, 'lambda_l1': 96, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:27,559] Trial 1398 finished with value: 0.7974165739555322 and parameters: {'iterations': 514, 'learning_rate': 0.2858700492614044, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:28,374] Trial 1399 finished with value: 0.7989976016235163 and parameters: {'iterations': 721, 'learning_rate': 0.27835205551193765, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:29,138] Trial 1400 finished with value: 0.7945877196415294 and parameters: {'iterations': 684, 'learning_rate': 0.2880799280336675, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:30,007] Trial 1401 finished with value: 0.7993716141800515 and parameters: {'iterations': 650, 'learning_rate': 0.2714497332182425, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:30,943] Trial 1402 finished with value: 0.794504419386038 and parameters: {'iterations': 780, 'learning_rate': 0.26208232843772916, 'num_leaves': 960, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:31,921] Trial 1403 finished with value: 0.7978330752329892 and parameters: {'iterations': 861, 'learning_rate': 0.28104068077329875, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 27, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:33,234] Trial 1404 finished with value: 0.7974165739555322 and parameters: {'iterations': 742, 'learning_rate': 0.25458089147458307, 'num_leaves': 180, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:34,202] Trial 1405 finished with value: 0.8007239854420224 and parameters: {'iterations': 703, 'learning_rate': 0.2736717789978309, 'num_leaves': 1030, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:35,047] Trial 1406 finished with value: 0.8006406851865312 and parameters: {'iterations': 789, 'learning_rate': 0.2868068931487294, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:35,769] Trial 1407 finished with value: 0.7931112365894974 and parameters: {'iterations': 763, 'learning_rate': 0.2680325062278715, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 21, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:36,397] Trial 1408 finished with value: 0.7911561963649781 and parameters: {'iterations': 729, 'learning_rate': 0.28201550147809074, 'num_leaves': 700, 'max_depth': 10, 'lambda_l1': 81, 'lambda_l2': 18, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:37,215] Trial 1409 finished with value: 0.7954615337310129 and parameters: {'iterations': 753, 'learning_rate': 0.2903697296578309, 'num_leaves': 630, 'max_depth': 9, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:38,067] Trial 1410 finished with value: 0.7974998742110235 and parameters: {'iterations': 665, 'learning_rate': 0.2753040095773424, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:38,818] Trial 1411 finished with value: 0.7961687473095135 and parameters: {'iterations': 690, 'learning_rate': 0.26442188753174023, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:39,670] Trial 1412 finished with value: 0.7972712178050104 and parameters: {'iterations': 794, 'learning_rate': 0.28201057024834847, 'num_leaves': 570, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:40,600] Trial 1413 finished with value: 0.7964807039710183 and parameters: {'iterations': 723, 'learning_rate': 0.2699151076222097, 'num_leaves': 930, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:41,363] Trial 1414 finished with value: 0.7936518496570153 and parameters: {'iterations': 747, 'learning_rate': 0.27595404577862753, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.6000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:42,378] Trial 1415 finished with value: 0.7943590632355161 and parameters: {'iterations': 633, 'learning_rate': 0.2881196886286412, 'num_leaves': 730, 'max_depth': 10, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:43,170] Trial 1416 finished with value: 0.7958567906480088 and parameters: {'iterations': 839, 'learning_rate': 0.259104141389753, 'num_leaves': 980, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:44,303] Trial 1417 finished with value: 0.7979571870230502 and parameters: {'iterations': 777, 'learning_rate': 0.2911821286738623, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:45,262] Trial 1418 finished with value: 0.7965852485869706 and parameters: {'iterations': 878, 'learning_rate': 0.28324691579525435, 'num_leaves': 790, 'max_depth': 10, 'lambda_l1': 45, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:46,618] Trial 1419 finished with value: 0.7936730940174763 and parameters: {'iterations': 709, 'learning_rate': 0.2679408785100857, 'num_leaves': 530, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:47,541] Trial 1420 finished with value: 0.794899676303034 and parameters: {'iterations': 734, 'learning_rate': 0.2760358019909692, 'num_leaves': 850, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:48,360] Trial 1421 finished with value: 0.7982903880450157 and parameters: {'iterations': 683, 'learning_rate': 0.2816455956631904, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:49,236] Trial 1422 finished with value: 0.7986856449620117 and parameters: {'iterations': 798, 'learning_rate': 0.24863750349533387, 'num_leaves': 920, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:50,062] Trial 1423 finished with value: 0.7938805060630287 and parameters: {'iterations': 764, 'learning_rate': 0.26220807710738914, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:50,764] Trial 1424 finished with value: 0.7927567912070711 and parameters: {'iterations': 713, 'learning_rate': 0.27140277780489286, 'num_leaves': 630, 'max_depth': 9, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.2}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:51,584] Trial 1425 finished with value: 0.7996215149465257 and parameters: {'iterations': 855, 'learning_rate': 0.29114409100068234, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 678 with value: 0.8032621274549816.\n",
            "[I 2024-05-15 22:16:52,455] Trial 1426 finished with value: 0.803324183350012 and parameters: {'iterations': 830, 'learning_rate': 0.25472064827104673, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:53,315] Trial 1427 finished with value: 0.7950662768140169 and parameters: {'iterations': 839, 'learning_rate': 0.24751700554360243, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:54,133] Trial 1428 finished with value: 0.7956068898815348 and parameters: {'iterations': 862, 'learning_rate': 0.2593190176116838, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:55,146] Trial 1429 finished with value: 0.795711434497487 and parameters: {'iterations': 810, 'learning_rate': 0.25509341401487423, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:55,945] Trial 1430 finished with value: 0.7967926606325229 and parameters: {'iterations': 846, 'learning_rate': 0.2586528129249312, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:56,822] Trial 1431 finished with value: 0.7967093603770316 and parameters: {'iterations': 829, 'learning_rate': 0.2577872378277368, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:16:58,779] Trial 1432 finished with value: 0.7947543201525121 and parameters: {'iterations': 827, 'learning_rate': 0.2546760031419246, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:00,501] Trial 1433 finished with value: 0.7970425613989971 and parameters: {'iterations': 815, 'learning_rate': 0.2533927776776436, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:01,374] Trial 1434 finished with value: 0.7946710198970207 and parameters: {'iterations': 850, 'learning_rate': 0.2510971969940038, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 0.5}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:02,148] Trial 1435 finished with value: 0.7932565927400194 and parameters: {'iterations': 870, 'learning_rate': 0.2625773224547284, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:03,080] Trial 1436 finished with value: 0.7970213170385362 and parameters: {'iterations': 829, 'learning_rate': 0.26275602595099823, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:03,907] Trial 1437 finished with value: 0.7986856449620117 and parameters: {'iterations': 814, 'learning_rate': 0.24893703503168893, 'num_leaves': 1000, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:04,655] Trial 1438 finished with value: 0.794899676303034 and parameters: {'iterations': 887, 'learning_rate': 0.2664944185399754, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:05,522] Trial 1439 finished with value: 0.7942137070849944 and parameters: {'iterations': 788, 'learning_rate': 0.2563209682966127, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:06,378] Trial 1440 finished with value: 0.7982903880450157 and parameters: {'iterations': 863, 'learning_rate': 0.26334077930928285, 'num_leaves': 960, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:07,224] Trial 1441 finished with value: 0.7997881154575085 and parameters: {'iterations': 835, 'learning_rate': 0.2533939899191293, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:08,035] Trial 1442 finished with value: 0.7978951311280196 and parameters: {'iterations': 802, 'learning_rate': 0.2663443564238184, 'num_leaves': 720, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:08,824] Trial 1443 finished with value: 0.7946922642574819 and parameters: {'iterations': 820, 'learning_rate': 0.24319568813145714, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:09,494] Trial 1444 finished with value: 0.7911561963649781 and parameters: {'iterations': 845, 'learning_rate': 0.257899325051974, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 63, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:10,448] Trial 1445 finished with value: 0.7952949332200301 and parameters: {'iterations': 801, 'learning_rate': 0.2677337213305721, 'num_leaves': 1090, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:11,554] Trial 1446 finished with value: 0.8010359421035271 and parameters: {'iterations': 789, 'learning_rate': 0.2612789029724991, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:13,027] Trial 1447 finished with value: 0.7974998742110235 and parameters: {'iterations': 772, 'learning_rate': 0.272997953217923, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:14,105] Trial 1448 finished with value: 0.7934852491460327 and parameters: {'iterations': 899, 'learning_rate': 0.252324874028953, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:14,974] Trial 1449 finished with value: 0.7930279363340061 and parameters: {'iterations': 833, 'learning_rate': 0.26842499734358694, 'num_leaves': 720, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:15,908] Trial 1450 finished with value: 0.7911349520045172 and parameters: {'iterations': 811, 'learning_rate': 0.27394045827068264, 'num_leaves': 830, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:16,648] Trial 1451 finished with value: 0.7961687473095135 and parameters: {'iterations': 749, 'learning_rate': 0.2650707612111355, 'num_leaves': 770, 'max_depth': 7, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:17,588] Trial 1452 finished with value: 0.799704815202017 and parameters: {'iterations': 690, 'learning_rate': 0.28000248588171195, 'num_leaves': 620, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:18,425] Trial 1453 finished with value: 0.7964807039710183 and parameters: {'iterations': 779, 'learning_rate': 0.24566378436356043, 'num_leaves': 890, 'max_depth': 8, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:19,249] Trial 1454 finished with value: 0.7953782334755214 and parameters: {'iterations': 729, 'learning_rate': 0.2710075969377372, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 78, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:20,325] Trial 1455 finished with value: 0.7963353478204963 and parameters: {'iterations': 657, 'learning_rate': 0.2600497156281487, 'num_leaves': 1030, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:21,118] Trial 1456 finished with value: 0.7937972058075372 and parameters: {'iterations': 853, 'learning_rate': 0.28691495879274603, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:21,947] Trial 1457 finished with value: 0.7971046172940276 and parameters: {'iterations': 822, 'learning_rate': 0.2763363239967034, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:22,721] Trial 1458 finished with value: 0.7913636084105304 and parameters: {'iterations': 759, 'learning_rate': 0.26764413805267057, 'num_leaves': 750, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.7}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:23,665] Trial 1459 finished with value: 0.7963141034600354 and parameters: {'iterations': 705, 'learning_rate': 0.2794095769820472, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:24,736] Trial 1460 finished with value: 0.7970213170385362 and parameters: {'iterations': 871, 'learning_rate': 0.2542608584341873, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:26,038] Trial 1461 finished with value: 0.7957734903925175 and parameters: {'iterations': 954, 'learning_rate': 0.2927580488082217, 'num_leaves': 720, 'max_depth': 12, 'lambda_l1': 9, 'lambda_l2': 81, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:27,214] Trial 1462 finished with value: 0.7990188459839773 and parameters: {'iterations': 787, 'learning_rate': 0.28365792400134704, 'num_leaves': 630, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:28,233] Trial 1463 finished with value: 0.7950662768140169 and parameters: {'iterations': 804, 'learning_rate': 0.2725399600741912, 'num_leaves': 970, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:29,037] Trial 1464 finished with value: 0.8027002700270027 and parameters: {'iterations': 714, 'learning_rate': 0.27701555777825726, 'num_leaves': 510, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:29,777] Trial 1465 finished with value: 0.7941924627245333 and parameters: {'iterations': 695, 'learning_rate': 0.27594139191635225, 'num_leaves': 440, 'max_depth': 5, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:30,714] Trial 1466 finished with value: 0.7971046172940276 and parameters: {'iterations': 709, 'learning_rate': 0.2785341322827141, 'num_leaves': 620, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:31,460] Trial 1467 finished with value: 0.7963974037155269 and parameters: {'iterations': 723, 'learning_rate': 0.27025469443198197, 'num_leaves': 560, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:32,277] Trial 1468 finished with value: 0.7978118308725282 and parameters: {'iterations': 733, 'learning_rate': 0.2865713153394923, 'num_leaves': 350, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:33,016] Trial 1469 finished with value: 0.7955235896260434 and parameters: {'iterations': 679, 'learning_rate': 0.27558745148265823, 'num_leaves': 480, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 0.8}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:33,884] Trial 1470 finished with value: 0.7988310011125336 and parameters: {'iterations': 713, 'learning_rate': 0.2811779535019411, 'num_leaves': 380, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:34,705] Trial 1471 finished with value: 0.7967306047374924 and parameters: {'iterations': 738, 'learning_rate': 0.2647175188321593, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:35,522] Trial 1472 finished with value: 0.7975831744665149 and parameters: {'iterations': 672, 'learning_rate': 0.26914078389181034, 'num_leaves': 500, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:36,444] Trial 1473 finished with value: 0.7952949332200301 and parameters: {'iterations': 704, 'learning_rate': 0.28540444101064105, 'num_leaves': 290, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:37,475] Trial 1474 finished with value: 0.799704815202017 and parameters: {'iterations': 719, 'learning_rate': 0.2776650791731722, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:38,537] Trial 1475 finished with value: 0.7931732924845278 and parameters: {'iterations': 689, 'learning_rate': 0.27345872339021826, 'num_leaves': 440, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:39,760] Trial 1476 finished with value: 0.7986856449620117 and parameters: {'iterations': 749, 'learning_rate': 0.2914072097617327, 'num_leaves': 520, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:40,687] Trial 1477 finished with value: 0.7974998742110235 and parameters: {'iterations': 738, 'learning_rate': 0.26128044538236217, 'num_leaves': 490, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:41,677] Trial 1478 finished with value: 0.7922994783950444 and parameters: {'iterations': 700, 'learning_rate': 0.28401700647367617, 'num_leaves': 550, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:42,402] Trial 1479 finished with value: 0.7957734903925175 and parameters: {'iterations': 719, 'learning_rate': 0.26743362561603934, 'num_leaves': 590, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:43,332] Trial 1480 finished with value: 0.7979996757439719 and parameters: {'iterations': 667, 'learning_rate': 0.2745270128777604, 'num_leaves': 550, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:44,159] Trial 1481 finished with value: 0.797978431383511 and parameters: {'iterations': 759, 'learning_rate': 0.279610871587987, 'num_leaves': 500, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:45,032] Trial 1482 finished with value: 0.7995594590514953 and parameters: {'iterations': 684, 'learning_rate': 0.28970636226594154, 'num_leaves': 420, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:45,769] Trial 1483 finished with value: 0.7927780355675318 and parameters: {'iterations': 735, 'learning_rate': 0.283502206431575, 'num_leaves': 400, 'max_depth': 6, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:46,696] Trial 1484 finished with value: 0.7970425613989971 and parameters: {'iterations': 700, 'learning_rate': 0.26300184594821613, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:47,441] Trial 1485 finished with value: 0.7829407785499046 and parameters: {'iterations': 721, 'learning_rate': 0.030347196259980086, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:48,308] Trial 1486 finished with value: 0.7977497749774978 and parameters: {'iterations': 652, 'learning_rate': 0.2928779413786801, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:49,114] Trial 1487 finished with value: 0.7950662768140169 and parameters: {'iterations': 777, 'learning_rate': 0.2720639329467481, 'num_leaves': 490, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:49,898] Trial 1488 finished with value: 0.7922994783950444 and parameters: {'iterations': 753, 'learning_rate': 0.2786826020262022, 'num_leaves': 810, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.5}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:50,695] Trial 1489 finished with value: 0.7920920663494921 and parameters: {'iterations': 724, 'learning_rate': 0.2671137496154317, 'num_leaves': 680, 'max_depth': 12, 'lambda_l1': 69, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:51,769] Trial 1490 finished with value: 0.7933398929955107 and parameters: {'iterations': 843, 'learning_rate': 0.2873332118541504, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:53,107] Trial 1491 finished with value: 0.7999547159684913 and parameters: {'iterations': 762, 'learning_rate': 0.2600143824927478, 'num_leaves': 560, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:54,013] Trial 1492 finished with value: 0.7986023447065203 and parameters: {'iterations': 694, 'learning_rate': 0.29507216263398744, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:54,717] Trial 1493 finished with value: 0.7909683514935344 and parameters: {'iterations': 744, 'learning_rate': 0.2775794628252088, 'num_leaves': 970, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.30000000000000004}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:55,453] Trial 1494 finished with value: 0.7935897937619849 and parameters: {'iterations': 820, 'learning_rate': 0.2698703961446637, 'num_leaves': 620, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:56,231] Trial 1495 finished with value: 0.7903444381705251 and parameters: {'iterations': 799, 'learning_rate': 0.28280403143592364, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 0.4}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:56,860] Trial 1496 finished with value: 0.78863929871248 and parameters: {'iterations': 710, 'learning_rate': 0.2866554406266996, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 87, 'lambda_l2': 48, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:57,514] Trial 1497 finished with value: 0.7892419676750283 and parameters: {'iterations': 677, 'learning_rate': 0.2744549031741517, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 99, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:58,363] Trial 1498 finished with value: 0.7989143013680249 and parameters: {'iterations': 775, 'learning_rate': 0.2556968675796529, 'num_leaves': 1040, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:17:59,362] Trial 1499 finished with value: 0.7968547165275535 and parameters: {'iterations': 632, 'learning_rate': 0.2658082903539349, 'num_leaves': 680, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:00,133] Trial 1500 finished with value: 0.8001833723745044 and parameters: {'iterations': 737, 'learning_rate': 0.28131334290318605, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:01,084] Trial 1501 finished with value: 0.7957734903925175 and parameters: {'iterations': 886, 'learning_rate': 0.27298899019168626, 'num_leaves': 1160, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:01,922] Trial 1502 finished with value: 0.7993928585405125 and parameters: {'iterations': 830, 'learning_rate': 0.2928489722829982, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:02,567] Trial 1503 finished with value: 0.793194536844989 and parameters: {'iterations': 717, 'learning_rate': 0.2880936561065947, 'num_leaves': 580, 'max_depth': 11, 'lambda_l1': 54, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:03,438] Trial 1504 finished with value: 0.7983736883005071 and parameters: {'iterations': 801, 'learning_rate': 0.2789517492855062, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:04,762] Trial 1505 finished with value: 0.7999334716080304 and parameters: {'iterations': 761, 'learning_rate': 0.26038417629830757, 'num_leaves': 720, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:05,792] Trial 1506 finished with value: 0.7976044188269759 and parameters: {'iterations': 920, 'learning_rate': 0.2684553887625866, 'num_leaves': 790, 'max_depth': 11, 'lambda_l1': 27, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:06,826] Trial 1507 finished with value: 0.7985190444510288 and parameters: {'iterations': 694, 'learning_rate': 0.2945927206888642, 'num_leaves': 1470, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:07,695] Trial 1508 finished with value: 0.7965019483314792 and parameters: {'iterations': 851, 'learning_rate': 0.2839804842750788, 'num_leaves': 650, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:08,491] Trial 1509 finished with value: 0.7957734903925175 and parameters: {'iterations': 670, 'learning_rate': 0.25180252908338785, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:09,302] Trial 1510 finished with value: 0.7981237875340328 and parameters: {'iterations': 732, 'learning_rate': 0.27709435268017535, 'num_leaves': 1000, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:10,161] Trial 1511 finished with value: 0.801576555171045 and parameters: {'iterations': 786, 'learning_rate': 0.27098879517160873, 'num_leaves': 910, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:11,108] Trial 1512 finished with value: 0.7966473044820009 and parameters: {'iterations': 782, 'learning_rate': 0.2744775709814516, 'num_leaves': 970, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:11,934] Trial 1513 finished with value: 0.7968759608880143 and parameters: {'iterations': 788, 'learning_rate': 0.26292295008705935, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:12,808] Trial 1514 finished with value: 0.794504419386038 and parameters: {'iterations': 808, 'learning_rate': 0.26826710379218477, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:13,593] Trial 1515 finished with value: 0.7931732924845278 and parameters: {'iterations': 768, 'learning_rate': 0.2694876815393156, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:14,589] Trial 1516 finished with value: 0.7971046172940276 and parameters: {'iterations': 786, 'learning_rate': 0.2649050135910314, 'num_leaves': 1020, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:15,513] Trial 1517 finished with value: 0.791842165583018 and parameters: {'iterations': 798, 'learning_rate': 0.27238637931248094, 'num_leaves': 870, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:16,357] Trial 1518 finished with value: 0.7972712178050104 and parameters: {'iterations': 773, 'learning_rate': 0.25778336388199463, 'num_leaves': 960, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:17,407] Trial 1519 finished with value: 0.7959400909035003 and parameters: {'iterations': 808, 'learning_rate': 0.27798990603327595, 'num_leaves': 820, 'max_depth': 12, 'lambda_l1': 21, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:18,686] Trial 1520 finished with value: 0.7977497749774978 and parameters: {'iterations': 824, 'learning_rate': 0.27172914534872655, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:19,845] Trial 1521 finished with value: 0.7977497749774978 and parameters: {'iterations': 755, 'learning_rate': 0.26432307656783977, 'num_leaves': 1040, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:20,624] Trial 1522 finished with value: 0.7950662768140169 and parameters: {'iterations': 787, 'learning_rate': 0.28151150605785946, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:21,557] Trial 1523 finished with value: 0.7927159796725015 and parameters: {'iterations': 773, 'learning_rate': 0.27150573900259645, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:22,302] Trial 1524 finished with value: 0.79396380631852 and parameters: {'iterations': 816, 'learning_rate': 0.2763474979463056, 'num_leaves': 770, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 90, 'feature_fraction': 0.8}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:23,335] Trial 1525 finished with value: 0.7945877196415294 and parameters: {'iterations': 801, 'learning_rate': 0.26190234551120867, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:24,180] Trial 1526 finished with value: 0.7968759608880143 and parameters: {'iterations': 752, 'learning_rate': 0.2844607875696995, 'num_leaves': 1090, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:25,009] Trial 1527 finished with value: 0.7973332737000408 and parameters: {'iterations': 829, 'learning_rate': 0.26826459379439616, 'num_leaves': 830, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:25,875] Trial 1528 finished with value: 0.7992262580295296 and parameters: {'iterations': 744, 'learning_rate': 0.2901728067389675, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:26,667] Trial 1529 finished with value: 0.78997042561399 and parameters: {'iterations': 769, 'learning_rate': 0.2795408357659731, 'num_leaves': 710, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 42, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:27,668] Trial 1530 finished with value: 0.7963141034600354 and parameters: {'iterations': 843, 'learning_rate': 0.25683769459092964, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:28,509] Trial 1531 finished with value: 0.7965427598660487 and parameters: {'iterations': 868, 'learning_rate': 0.27464843099909503, 'num_leaves': 900, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 30, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:29,409] Trial 1532 finished with value: 0.7987689452175031 and parameters: {'iterations': 788, 'learning_rate': 0.28780685280308493, 'num_leaves': 850, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:30,637] Trial 1533 finished with value: 0.7960021467985308 and parameters: {'iterations': 722, 'learning_rate': 0.2627590409274354, 'num_leaves': 790, 'max_depth': 12, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 0.7}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:31,868] Trial 1534 finished with value: 0.79662606012154 and parameters: {'iterations': 803, 'learning_rate': 0.2807553101804729, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:32,981] Trial 1535 finished with value: 0.7956068898815348 and parameters: {'iterations': 757, 'learning_rate': 0.27121015724600134, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:33,858] Trial 1536 finished with value: 0.7982903880450157 and parameters: {'iterations': 738, 'learning_rate': 0.2853301482564793, 'num_leaves': 1000, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:34,630] Trial 1537 finished with value: 0.7956901901370262 and parameters: {'iterations': 819, 'learning_rate': 0.26607445218956394, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:35,472] Trial 1538 finished with value: 0.7996215149465257 and parameters: {'iterations': 706, 'learning_rate': 0.29385634317888953, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:36,346] Trial 1539 finished with value: 0.7974378183159931 and parameters: {'iterations': 766, 'learning_rate': 0.27407723653662164, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:37,119] Trial 1540 finished with value: 0.7961687473095135 and parameters: {'iterations': 645, 'learning_rate': 0.27959042965858816, 'num_leaves': 610, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:37,988] Trial 1541 finished with value: 0.7977497749774978 and parameters: {'iterations': 789, 'learning_rate': 0.2582246240949959, 'num_leaves': 890, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:38,744] Trial 1542 finished with value: 0.7970425613989971 and parameters: {'iterations': 841, 'learning_rate': 0.28773740860632163, 'num_leaves': 730, 'max_depth': 6, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:39,503] Trial 1543 finished with value: 0.7971879175495189 and parameters: {'iterations': 719, 'learning_rate': 0.2683442400027394, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:40,215] Trial 1544 finished with value: 0.793985050678981 and parameters: {'iterations': 685, 'learning_rate': 0.2507265344759349, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 24, 'lambda_l2': 90, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:41,475] Trial 1545 finished with value: 0.7975619301060541 and parameters: {'iterations': 741, 'learning_rate': 0.295270724769006, 'num_leaves': 540, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:42,807] Trial 1546 finished with value: 0.800245428269535 and parameters: {'iterations': 901, 'learning_rate': 0.2771607269706922, 'num_leaves': 950, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:44,485] Trial 1547 finished with value: 0.7947330757920512 and parameters: {'iterations': 820, 'learning_rate': 0.2846564042717486, 'num_leaves': 860, 'max_depth': 11, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:45,747] Trial 1548 finished with value: 0.7971046172940276 and parameters: {'iterations': 697, 'learning_rate': 0.27194211251008266, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:46,801] Trial 1549 finished with value: 0.7985402888114899 and parameters: {'iterations': 775, 'learning_rate': 0.26397006807044715, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:47,743] Trial 1550 finished with value: 0.7955235896260434 and parameters: {'iterations': 798, 'learning_rate': 0.28161609181641434, 'num_leaves': 670, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:48,441] Trial 1551 finished with value: 0.7959400909035003 and parameters: {'iterations': 612, 'learning_rate': 0.29034668010089665, 'num_leaves': 1050, 'max_depth': 11, 'lambda_l1': 42, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:49,263] Trial 1552 finished with value: 0.7988522454729946 and parameters: {'iterations': 753, 'learning_rate': 0.2755738086858429, 'num_leaves': 940, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:50,039] Trial 1553 finished with value: 0.7974998742110235 and parameters: {'iterations': 708, 'learning_rate': 0.2570149530841467, 'num_leaves': 860, 'max_depth': 12, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:50,952] Trial 1554 finished with value: 0.7938184501679981 and parameters: {'iterations': 732, 'learning_rate': 0.2701613862301334, 'num_leaves': 600, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:51,810] Trial 1555 finished with value: 0.7981237875340328 and parameters: {'iterations': 668, 'learning_rate': 0.28319023441430313, 'num_leaves': 700, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:52,603] Trial 1556 finished with value: 0.7919467101989702 and parameters: {'iterations': 865, 'learning_rate': 0.07770131540853811, 'num_leaves': 780, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:53,472] Trial 1557 finished with value: 0.7971046172940276 and parameters: {'iterations': 839, 'learning_rate': 0.2952210796199277, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:54,293] Trial 1558 finished with value: 0.7955235896260434 and parameters: {'iterations': 782, 'learning_rate': 0.2779852524270979, 'num_leaves': 740, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:55,081] Trial 1559 finished with value: 0.7940471065740113 and parameters: {'iterations': 810, 'learning_rate': 0.26475318682391025, 'num_leaves': 1000, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:55,789] Trial 1560 finished with value: 0.7983949326609681 and parameters: {'iterations': 718, 'learning_rate': 0.28851847131162567, 'num_leaves': 790, 'max_depth': 12, 'lambda_l1': 30, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:56,684] Trial 1561 finished with value: 0.797063805759458 and parameters: {'iterations': 766, 'learning_rate': 0.2725027520208817, 'num_leaves': 450, 'max_depth': 11, 'lambda_l1': 18, 'lambda_l2': 54, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:58,028] Trial 1562 finished with value: 0.7986023447065203 and parameters: {'iterations': 736, 'learning_rate': 0.28253147522573496, 'num_leaves': 930, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:18:59,164] Trial 1563 finished with value: 0.7960854470540222 and parameters: {'iterations': 690, 'learning_rate': 0.25932758686505836, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 90, 'feature_fraction': 0.6000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:00,087] Trial 1564 finished with value: 0.7985402888114899 and parameters: {'iterations': 797, 'learning_rate': 0.26723141887758284, 'num_leaves': 640, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:00,869] Trial 1565 finished with value: 0.79662606012154 and parameters: {'iterations': 746, 'learning_rate': 0.28991403835848195, 'num_leaves': 730, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:01,520] Trial 1566 finished with value: 0.7934231932510021 and parameters: {'iterations': 439, 'learning_rate': 0.27777371289180675, 'num_leaves': 820, 'max_depth': 11, 'lambda_l1': 90, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:02,346] Trial 1567 finished with value: 0.7987689452175031 and parameters: {'iterations': 855, 'learning_rate': 0.2516102751650099, 'num_leaves': 920, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:03,242] Trial 1568 finished with value: 0.7975831744665149 and parameters: {'iterations': 826, 'learning_rate': 0.28446621243939124, 'num_leaves': 1560, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:04,264] Trial 1569 finished with value: 0.7928400914625625 and parameters: {'iterations': 662, 'learning_rate': 0.2954465777894461, 'num_leaves': 680, 'max_depth': 12, 'lambda_l1': 3, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:05,202] Trial 1570 finished with value: 0.7961687473095135 and parameters: {'iterations': 706, 'learning_rate': 0.2722206641205583, 'num_leaves': 590, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:06,060] Trial 1571 finished with value: 0.7988522454729946 and parameters: {'iterations': 930, 'learning_rate': 0.2663616534090369, 'num_leaves': 510, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:06,880] Trial 1572 finished with value: 0.7937563942729676 and parameters: {'iterations': 771, 'learning_rate': 0.12330644836893406, 'num_leaves': 880, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:07,670] Trial 1573 finished with value: 0.7967926606325229 and parameters: {'iterations': 721, 'learning_rate': 0.27864602869983257, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:08,546] Trial 1574 finished with value: 0.7982070877895242 and parameters: {'iterations': 596, 'learning_rate': 0.2603051479361455, 'num_leaves': 810, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:09,323] Trial 1575 finished with value: 0.7920087660940006 and parameters: {'iterations': 809, 'learning_rate': 0.2881405884512041, 'num_leaves': 960, 'max_depth': 5, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:10,597] Trial 1576 finished with value: 0.7959400909035003 and parameters: {'iterations': 752, 'learning_rate': 0.24622885286248158, 'num_leaves': 370, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:11,817] Trial 1577 finished with value: 0.7963141034600354 and parameters: {'iterations': 574, 'learning_rate': 0.27484230221858486, 'num_leaves': 1080, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:12,838] Trial 1578 finished with value: 0.7941304068295029 and parameters: {'iterations': 681, 'learning_rate': 0.294812301861716, 'num_leaves': 660, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 84, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:13,548] Trial 1579 finished with value: 0.7919875217335397 and parameters: {'iterations': 882, 'learning_rate': 0.280830970129417, 'num_leaves': 720, 'max_depth': 4, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:14,497] Trial 1580 finished with value: 0.7971046172940276 and parameters: {'iterations': 729, 'learning_rate': 0.28589523956930446, 'num_leaves': 870, 'max_depth': 10, 'lambda_l1': 15, 'lambda_l2': 96, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:15,453] Trial 1581 finished with value: 0.7953782334755214 and parameters: {'iterations': 787, 'learning_rate': 0.26887133618672554, 'num_leaves': 760, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:16,110] Trial 1582 finished with value: 0.7894706240810416 and parameters: {'iterations': 839, 'learning_rate': 0.2639982645646932, 'num_leaves': 1010, 'max_depth': 3, 'lambda_l1': 3, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:16,763] Trial 1583 finished with value: 0.7947543201525121 and parameters: {'iterations': 822, 'learning_rate': 0.2545231265856285, 'num_leaves': 820, 'max_depth': 10, 'lambda_l1': 48, 'lambda_l2': 93, 'feature_fraction': 0.4}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:17,588] Trial 1584 finished with value: 0.7963141034600354 and parameters: {'iterations': 707, 'learning_rate': 0.27331629425305937, 'num_leaves': 640, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:18,467] Trial 1585 finished with value: 0.7978118308725282 and parameters: {'iterations': 769, 'learning_rate': 0.29134282806727396, 'num_leaves': 540, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:19,279] Trial 1586 finished with value: 0.7964807039710183 and parameters: {'iterations': 797, 'learning_rate': 0.2782461009756603, 'num_leaves': 920, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:20,221] Trial 1587 finished with value: 0.7920708219890312 and parameters: {'iterations': 695, 'learning_rate': 0.28401946815121787, 'num_leaves': 710, 'max_depth': 10, 'lambda_l1': 6, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:21,056] Trial 1588 finished with value: 0.7967093603770316 and parameters: {'iterations': 638, 'learning_rate': 0.2701956589539733, 'num_leaves': 800, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:21,923] Trial 1589 finished with value: 0.79662606012154 and parameters: {'iterations': 751, 'learning_rate': 0.2598502709022918, 'num_leaves': 860, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 0.7}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:23,089] Trial 1590 finished with value: 0.7959188465430393 and parameters: {'iterations': 731, 'learning_rate': 0.2764619608362752, 'num_leaves': 980, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 84, 'feature_fraction': 0.9000000000000001}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:24,285] Trial 1591 finished with value: 0.7955235896260434 and parameters: {'iterations': 811, 'learning_rate': 0.288998028018439, 'num_leaves': 760, 'max_depth': 10, 'lambda_l1': 12, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:25,448] Trial 1592 finished with value: 0.7982070877895242 and parameters: {'iterations': 851, 'learning_rate': 0.29530609935424473, 'num_leaves': 610, 'max_depth': 11, 'lambda_l1': 15, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:26,338] Trial 1593 finished with value: 0.7999334716080304 and parameters: {'iterations': 787, 'learning_rate': 0.2841005032579195, 'num_leaves': 910, 'max_depth': 10, 'lambda_l1': 9, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:27,171] Trial 1594 finished with value: 0.7992262580295296 and parameters: {'iterations': 683, 'learning_rate': 0.2625731471989226, 'num_leaves': 840, 'max_depth': 11, 'lambda_l1': 12, 'lambda_l2': 87, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:27,919] Trial 1595 finished with value: 0.7863102459314254 and parameters: {'iterations': 717, 'learning_rate': 0.05291446492877851, 'num_leaves': 780, 'max_depth': 10, 'lambda_l1': 18, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:28,800] Trial 1596 finished with value: 0.7948376204080034 and parameters: {'iterations': 834, 'learning_rate': 0.1427179329084137, 'num_leaves': 1520, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 96, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:29,750] Trial 1597 finished with value: 0.7974998742110235 and parameters: {'iterations': 948, 'learning_rate': 0.29994387856126675, 'num_leaves': 690, 'max_depth': 11, 'lambda_l1': 6, 'lambda_l2': 93, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:30,605] Trial 1598 finished with value: 0.797978431383511 and parameters: {'iterations': 761, 'learning_rate': 0.26852551266352775, 'num_leaves': 870, 'max_depth': 12, 'lambda_l1': 12, 'lambda_l2': 99, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n",
            "[I 2024-05-15 22:19:31,532] Trial 1599 finished with value: 0.7982903880450157 and parameters: {'iterations': 656, 'learning_rate': 0.27682463612057323, 'num_leaves': 1610, 'max_depth': 11, 'lambda_l1': 9, 'lambda_l2': 90, 'feature_fraction': 1.0}. Best is trial 1426 with value: 0.803324183350012.\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(study_name=\"lgbm_optuna\", storage=\"sqlite:///db.sqlite3\", direction='maximize')\n",
        "study.optimize(objective, n_trials=1600)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T19:23:17.616252Z",
          "end_time": "2024-05-14T20:26:39.947667Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03xSYEWjlLVf",
        "outputId": "86b09880-3c38-4e6e-b8e7-0c459671467b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: iterations\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] lambda_l2 is set=93, reg_lambda=0.0 will be ignored. Current value: lambda_l2=93\n",
            "[LightGBM] [Warning] lambda_l1 is set=9, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9\n",
            "[LightGBM] [Warning] Unknown parameter: iterations\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] lambda_l2 is set=93, reg_lambda=0.0 will be ignored. Current value: lambda_l2=93\n",
            "[LightGBM] [Warning] lambda_l1 is set=9, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9\n",
            "[LightGBM] [Info] Number of positive: 2217, number of negative: 3698\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3745\n",
            "[LightGBM] [Info] Number of data points in the train set: 5915, number of used features: 20\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.374810 -> initscore=-0.511637\n",
            "[LightGBM] [Info] Start training from score -0.511637\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(feature_fraction=1.0, iterations=830, lambda_l1=9, lambda_l2=93,\n",
              "               learning_rate=0.25472064827104673, max_depth=11, num_leaves=800)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(feature_fraction=1.0, iterations=830, lambda_l1=9, lambda_l2=93,\n",
              "               learning_rate=0.25472064827104673, max_depth=11, num_leaves=800)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(feature_fraction=1.0, iterations=830, lambda_l1=9, lambda_l2=93,\n",
              "               learning_rate=0.25472064827104673, max_depth=11, num_leaves=800)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "params = study.best_params\n",
        "model9 = LGBMClassifier(**params)\n",
        "model9.fit(x_train.to_numpy(), y_train.to_numpy())"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T21:06:02.467047Z",
          "end_time": "2024-05-14T21:06:02.991218Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8m8V6ugzlLVf",
        "outputId": "74363a31-23d9-4298-a482-8fa317e897f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: iterations\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] lambda_l2 is set=93, reg_lambda=0.0 will be ignored. Current value: lambda_l2=93\n",
            "[LightGBM] [Warning] lambda_l1 is set=9, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9\n",
            "[LightGBM] [Warning] Unknown parameter: iterations\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] lambda_l2 is set=93, reg_lambda=0.0 will be ignored. Current value: lambda_l2=93\n",
            "[LightGBM] [Warning] lambda_l1 is set=9, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9\n",
            "[LightGBM] [Warning] Unknown parameter: iterations\n",
            "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
            "[LightGBM] [Warning] lambda_l2 is set=93, reg_lambda=0.0 will be ignored. Current value: lambda_l2=93\n",
            "[LightGBM] [Warning] lambda_l1 is set=9, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9\n",
            "Testing performance\n",
            "RMSE: 0.428\n",
            "R2: 0.204\n",
            "ROC AUC: 0.79399\n",
            "Score: 0.8169\n",
            "Local Score: 0.8221\n",
            "Best params:  {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.25472064827104673, 'max_depth': 11, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 800, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'iterations': 830, 'lambda_l1': 9, 'lambda_l2': 93, 'feature_fraction': 1.0}\n"
          ]
        }
      ],
      "source": [
        "estimate_model(model9)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T21:06:07.700811Z",
          "end_time": "2024-05-14T21:06:07.783577Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjeRN87QlLVf",
        "outputId": "3457d229-6e80-42db-c410-857b43462dbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Предсказание"
      ],
      "metadata": {
        "collapsed": false,
        "id": "zv707cvDlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "!mkdir results"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:53:05.188679Z",
          "end_time": "2024-05-13T21:53:05.329832Z"
        },
        "id": "oKjeV9RBlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "source": [
        "val = model5.predict_proba(test_dataset)[:, 0]\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model5_pred.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T21:53:23.137145Z",
          "end_time": "2024-05-13T21:53:23.191653Z"
        },
        "id": "lfsm_fsDlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "source": [
        "val = model6.predict_proba(test_dataset)[:, 0]\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model6_pred.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T22:12:46.049307Z",
          "end_time": "2024-05-13T22:12:46.113135Z"
        },
        "id": "g3BdSn_UlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "source": [
        "val = model7.predict_proba(test_dataset)[:, 0]\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model7_pred.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-13T23:04:50.960921Z",
          "end_time": "2024-05-13T23:04:50.990106Z"
        },
        "id": "VRcg7zswlLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "source": [
        "val = model8.predict_proba(test_dataset)[:, 0]\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model8_pred.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:40:13.758947Z",
          "end_time": "2024-05-14T18:40:13.824251Z"
        },
        "id": "eQkr1Z1olLVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KFold prediction"
      ],
      "metadata": {
        "collapsed": false,
        "id": "G3q104A1lLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:40:24.812801Z",
          "end_time": "2024-05-14T18:40:24.879491Z"
        },
        "id": "7tsFYu1ulLVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold: 1 ==> roc_auc: 0.7642154357270636\n",
            "fold: 2 ==> roc_auc: 0.7802736290525962\n",
            "fold: 3 ==> roc_auc: 0.7733678300121105\n",
            "fold: 4 ==> roc_auc: 0.7698304498269896\n",
            "fold: 5 ==> roc_auc: 0.7523892548107968\n",
            "fold: 6 ==> roc_auc: 0.7809713200484422\n",
            "fold: 7 ==> roc_auc: 0.7869726327992381\n",
            "fold: 8 ==> roc_auc: 0.7831626848691695\n",
            "fold: 9 ==> roc_auc: 0.7961991158842513\n",
            "fold: 10 ==> roc_auc: 0.7820775805391189\n",
            "average roc_auc 0.7769459933569778\n"
          ]
        }
      ],
      "source": [
        "preds = np.zeros(test_dataset.shape[0], dtype=np.float64)\n",
        "n_splits =10\n",
        "kf = KFold(n_splits=n_splits,random_state=44,shuffle=True)\n",
        "roc_auc = []\n",
        "n=0\n",
        "my_x, my_y = train_dataset.drop([\"churn\"], axis=1), train_dataset['churn']\n",
        "for train_index, test_index in kf.split(my_x):\n",
        "    train_x, test_x = my_x.iloc[train_index], my_x.iloc[test_index]\n",
        "    train_y, test_y = my_y.iloc[train_index], my_y.iloc[test_index]\n",
        "\n",
        "    model = CatBoostClassifier(**model8.get_params())\n",
        "    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],verbose=False)\n",
        "    preds+=model.predict_proba(test_dataset)[:, 0]\n",
        "    roc_auc.append(roc_auc_score(test_y, model.predict(test_x)))\n",
        "\n",
        "    print(f\"fold: {n+1} ==> roc_auc: {roc_auc[n]}\")\n",
        "    n+=1\n",
        "print(\"average roc_auc\", sum(roc_auc) / n_splits)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:40:36.813295Z",
          "end_time": "2024-05-14T18:40:48.506016Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zEuRaQHlLVf",
        "outputId": "5c716386-1251-402e-c2b9-b1c7c2195f2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "source": [
        "val = preds / n_splits\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model8_10folds.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T18:40:56.524378Z",
          "end_time": "2024-05-14T18:40:56.590852Z"
        },
        "id": "2s4i8rOflLVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Тоже самое для LGBM"
      ],
      "metadata": {
        "collapsed": false,
        "id": "iBjq5CZHlLVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold: 1 ==> roc_auc: 0.7963662535310256\n",
            "fold: 2 ==> roc_auc: 0.7911158672206134\n",
            "fold: 3 ==> roc_auc: 0.754597801665568\n",
            "fold: 4 ==> roc_auc: 0.7699043597068688\n",
            "fold: 5 ==> roc_auc: 0.7589015857243407\n",
            "fold: 6 ==> roc_auc: 0.7480010062961503\n",
            "fold: 7 ==> roc_auc: 0.789115670321896\n",
            "fold: 8 ==> roc_auc: 0.7533292828530452\n",
            "fold: 9 ==> roc_auc: 0.7883974839707858\n",
            "fold: 10 ==> roc_auc: 0.7707571948651866\n",
            "average roc_auc 0.7720486506155481\n"
          ]
        }
      ],
      "source": [
        "preds = np.zeros(test_dataset.shape[0], dtype=np.float64)\n",
        "n_splits =10\n",
        "kf = KFold(n_splits=n_splits,random_state=35,shuffle=True)\n",
        "roc_auc = []\n",
        "n=0\n",
        "my_x, my_y = train_dataset.drop([\"churn\"], axis=1), train_dataset['churn']\n",
        "for train_index, test_index in kf.split(my_x):\n",
        "    train_x, test_x = my_x.iloc[train_index], my_x.iloc[test_index]\n",
        "    train_y, test_y = my_y.iloc[train_index], my_y.iloc[test_index]\n",
        "\n",
        "    model = LGBMClassifier(**model9.get_params(), verbosity=-1)\n",
        "    model.fit(train_x.to_numpy(),train_y.to_numpy(),eval_set=[(test_x.to_numpy(),test_y.to_numpy())])\n",
        "    preds+=model.predict_proba(test_dataset)[:, 0]\n",
        "    roc_auc.append(roc_auc_score(test_y, model.predict(test_x)))\n",
        "\n",
        "    print(f\"fold: {n+1} ==> roc_auc: {roc_auc[n]}\")\n",
        "    n+=1\n",
        "print(\"average roc_auc\", sum(roc_auc) / n_splits)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T21:07:14.696987Z",
          "end_time": "2024-05-14T21:07:21.208484Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw7AVYU9lLVg",
        "outputId": "dabb69ae-b78d-4d94-a4c7-e287cc9573b0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "source": [
        "val = preds / n_splits\n",
        "preds = pd.DataFrame({\"clientbankpartner_pin\": test_dataset.index, \"score\": val})\n",
        "preds.to_csv(\"results/model9_10folds.csv\", index=False)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-05-14T21:06:56.701250Z",
          "end_time": "2024-05-14T21:06:56.779904Z"
        },
        "id": "ySWC-Ej9lLVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Объединение предсказаний разных моделей"
      ],
      "metadata": {
        "collapsed": false,
        "id": "m_4OQdUvlLVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "source": [
        "file1 = pd.read_csv(\"results/model8_10folds.csv\")\n",
        "file2 = pd.read_csv(\"results/model9_10folds.csv\")\n",
        "file1[\"score\"] = file1[\"score\"] * 0.7 + file2[\"score\"] * 0.3\n",
        "file1.to_csv(\"results/united_model8_model9.csv\", index=False)"
      ],
      "metadata": {
        "id": "VfIoJlaGlLVg"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}